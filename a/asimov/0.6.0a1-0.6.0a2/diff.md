# Comparing `tmp/asimov-0.6.0a1-py2.py3-none-any.whl.zip` & `tmp/asimov-0.6.0a2-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,59 +1,54 @@
-Zip file size: 89740 bytes, number of entries: 57
--rw-r--r--  2.0 unx     2866 b- defN 23-Mar-21 21:08 asimov/__init__.py
--rw-r--r--  2.0 unx    27234 b- defN 23-Mar-21 21:08 asimov/analysis.py
--rw-rw-rw-  2.0 unx      796 b- defN 23-Mar-21 21:08 asimov/asimov.conf
--rw-r--r--  2.0 unx    11665 b- defN 23-Mar-21 21:08 asimov/condor.py
--rw-r--r--  2.0 unx     1617 b- defN 23-Mar-21 21:08 asimov/database.py
--rw-r--r--  2.0 unx    15204 b- defN 23-Mar-21 21:08 asimov/event.py
--rw-r--r--  2.0 unx    12166 b- defN 23-Mar-21 21:08 asimov/git.py
--rw-r--r--  2.0 unx     7734 b- defN 23-Mar-21 21:08 asimov/gitlab.py
--rw-r--r--  2.0 unx     5398 b- defN 23-Mar-21 21:08 asimov/ini.py
--rw-r--r--  2.0 unx    11341 b- defN 23-Mar-21 21:08 asimov/ledger.py
--rw-r--r--  2.0 unx     2051 b- defN 23-Mar-21 21:08 asimov/locutus.py
--rw-r--r--  2.0 unx     7051 b- defN 23-Mar-21 21:08 asimov/logging.py
--rw-r--r--  2.0 unx     1646 b- defN 23-Mar-21 21:08 asimov/mattermost.py
--rw-r--r--  2.0 unx     1590 b- defN 23-Mar-21 21:08 asimov/olivaw.py
--rw-r--r--  2.0 unx    14537 b- defN 23-Mar-21 21:08 asimov/pipeline.py
--rw-r--r--  2.0 unx     4513 b- defN 23-Mar-21 21:08 asimov/review.py
--rw-r--r--  2.0 unx    10087 b- defN 23-Mar-21 21:08 asimov/storage.py
--rw-r--r--  2.0 unx     1146 b- defN 23-Mar-21 21:08 asimov/testing.py
--rw-r--r--  2.0 unx     2134 b- defN 23-Mar-21 21:08 asimov/utils.py
--rw-rw-rw-  2.0 unx      395 b- defN 23-Mar-21 21:08 asimov/cli/__init__.py
--rw-rw-rw-  2.0 unx     4960 b- defN 23-Mar-21 21:08 asimov/cli/application.py
--rw-rw-rw-  2.0 unx     1141 b- defN 23-Mar-21 21:08 asimov/cli/configuration.py
--rw-rw-rw-  2.0 unx     1285 b- defN 23-Mar-21 21:08 asimov/cli/data.py
--rw-rw-rw-  2.0 unx     9011 b- defN 23-Mar-21 21:08 asimov/cli/event.py
--rw-rw-rw-  2.0 unx        0 b- defN 23-Mar-21 21:08 asimov/cli/ledger.py
--rw-rw-rw-  2.0 unx    10291 b- defN 23-Mar-21 21:08 asimov/cli/manage.py
--rw-rw-rw-  2.0 unx    15526 b- defN 23-Mar-21 21:08 asimov/cli/monitor.py
--rw-rw-rw-  2.0 unx     5606 b- defN 23-Mar-21 21:08 asimov/cli/production.py
--rw-rw-rw-  2.0 unx     6947 b- defN 23-Mar-21 21:08 asimov/cli/project.py
--rw-rw-rw-  2.0 unx     5225 b- defN 23-Mar-21 21:08 asimov/cli/report.py
--rw-rw-rw-  2.0 unx     2817 b- defN 23-Mar-21 21:08 asimov/cli/review.py
--rw-rw-rw-  2.0 unx       96 b- defN 23-Mar-21 21:08 asimov/cli/report-theme/body.html
--rw-rw-rw-  2.0 unx      247 b- defN 23-Mar-21 21:08 asimov/cli/report-theme/footer.html
--rw-rw-rw-  2.0 unx     1467 b- defN 23-Mar-21 21:08 asimov/cli/report-theme/head.html
--rw-rw-rw-  2.0 unx      841 b- defN 23-Mar-21 21:08 asimov/cli/report-theme/header.html
--rw-rw-rw-  2.0 unx      220 b- defN 23-Mar-21 21:08 asimov/configs/README.rst
--rw-rw-rw-  2.0 unx     2903 b- defN 23-Mar-21 21:08 asimov/configs/bayeswave.ini
--rw-rw-rw-  2.0 unx     8197 b- defN 23-Mar-21 21:08 asimov/configs/bilby.ini
--rw-rw-rw-  2.0 unx     6238 b- defN 23-Mar-21 21:08 asimov/configs/lalinference.ini
--rw-rw-rw-  2.0 unx     7718 b- defN 23-Mar-21 21:08 asimov/configs/rift.ini
--rw-rw-rw-  2.0 unx      610 b- defN 23-Mar-21 21:08 asimov/pipelines/__init__.py
--rw-rw-rw-  2.0 unx    20083 b- defN 23-Mar-21 21:08 asimov/pipelines/bayeswave.py
--rw-rw-rw-  2.0 unx    15790 b- defN 23-Mar-21 21:08 asimov/pipelines/bilby.py
--rw-rw-rw-  2.0 unx     9489 b- defN 23-Mar-21 21:08 asimov/pipelines/lalinference.py
--rw-rw-rw-  2.0 unx    17453 b- defN 23-Mar-21 21:08 asimov/pipelines/rift.py
--rw-rw-rw-  2.0 unx     2844 b- defN 23-Mar-21 21:08 asimov/priors/bbh.prior.template
--rw-rw-rw-  2.0 unx       96 b- defN 23-Mar-21 21:08 asimov/report-theme/body.html
--rw-rw-rw-  2.0 unx      247 b- defN 23-Mar-21 21:08 asimov/report-theme/footer.html
--rw-rw-rw-  2.0 unx     1467 b- defN 23-Mar-21 21:08 asimov/report-theme/head.html
--rw-rw-rw-  2.0 unx      841 b- defN 23-Mar-21 21:08 asimov/report-theme/header.html
--rw-rw-rw-  2.0 unx      755 b- defN 23-Mar-21 21:09 asimov-0.6.0a1.dist-info/LICENSE
--rw-r--r--  2.0 unx     1392 b- defN 23-Mar-21 21:09 asimov-0.6.0a1.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-Mar-21 21:09 asimov-0.6.0a1.dist-info/WHEEL
--rw-r--r--  2.0 unx      108 b- defN 23-Mar-21 21:09 asimov-0.6.0a1.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        7 b- defN 23-Mar-21 21:09 asimov-0.6.0a1.dist-info/top_level.txt
--rw-r--r--  2.0 unx        1 b- defN 23-Mar-21 21:09 asimov-0.6.0a1.dist-info/zip-safe
-?rw-rw-r--  2.0 unx     4549 b- defN 23-Mar-21 21:09 asimov-0.6.0a1.dist-info/RECORD
-57 files, 307749 bytes uncompressed, 82654 bytes compressed:  73.1%
+Zip file size: 87066 bytes, number of entries: 52
+-rw-r--r--  2.0 unx     2808 b- defN 23-Apr-11 10:03 asimov/__init__.py
+-rw-r--r--  2.0 unx    27309 b- defN 23-Apr-11 10:03 asimov/analysis.py
+-rw-rw-rw-  2.0 unx      843 b- defN 23-Apr-11 10:03 asimov/asimov.conf
+-rw-r--r--  2.0 unx    11152 b- defN 23-Apr-11 10:03 asimov/condor.py
+-rw-r--r--  2.0 unx     1617 b- defN 23-Apr-11 10:03 asimov/database.py
+-rw-r--r--  2.0 unx    14634 b- defN 23-Apr-11 10:03 asimov/event.py
+-rw-r--r--  2.0 unx    12166 b- defN 23-Apr-11 10:03 asimov/git.py
+-rw-r--r--  2.0 unx     7226 b- defN 23-Apr-11 10:03 asimov/gitlab.py
+-rw-r--r--  2.0 unx     5398 b- defN 23-Apr-11 10:03 asimov/ini.py
+-rw-r--r--  2.0 unx     9374 b- defN 23-Apr-11 10:03 asimov/ledger.py
+-rw-r--r--  2.0 unx     2051 b- defN 23-Apr-11 10:03 asimov/locutus.py
+-rw-r--r--  2.0 unx     7051 b- defN 23-Apr-11 10:03 asimov/logging.py
+-rw-r--r--  2.0 unx     1646 b- defN 23-Apr-11 10:03 asimov/mattermost.py
+-rw-r--r--  2.0 unx     1586 b- defN 23-Apr-11 10:03 asimov/olivaw.py
+-rw-r--r--  2.0 unx    14096 b- defN 23-Apr-11 10:03 asimov/pipeline.py
+-rw-r--r--  2.0 unx     4513 b- defN 23-Apr-11 10:03 asimov/review.py
+-rw-r--r--  2.0 unx    10087 b- defN 23-Apr-11 10:03 asimov/storage.py
+-rw-r--r--  2.0 unx     1156 b- defN 23-Apr-11 10:03 asimov/testing.py
+-rw-r--r--  2.0 unx     2134 b- defN 23-Apr-11 10:03 asimov/utils.py
+-rw-rw-rw-  2.0 unx      395 b- defN 23-Apr-11 10:03 asimov/cli/__init__.py
+-rw-rw-rw-  2.0 unx     4948 b- defN 23-Apr-11 10:03 asimov/cli/application.py
+-rw-rw-rw-  2.0 unx     1141 b- defN 23-Apr-11 10:03 asimov/cli/configuration.py
+-rw-rw-rw-  2.0 unx     1285 b- defN 23-Apr-11 10:03 asimov/cli/data.py
+-rw-rw-rw-  2.0 unx     9010 b- defN 23-Apr-11 10:03 asimov/cli/event.py
+-rw-rw-rw-  2.0 unx        0 b- defN 23-Apr-11 10:03 asimov/cli/ledger.py
+-rw-rw-rw-  2.0 unx    10312 b- defN 23-Apr-11 10:03 asimov/cli/manage.py
+-rw-rw-rw-  2.0 unx    14640 b- defN 23-Apr-11 10:03 asimov/cli/monitor.py
+-rw-rw-rw-  2.0 unx     5606 b- defN 23-Apr-11 10:03 asimov/cli/production.py
+-rw-rw-rw-  2.0 unx     6016 b- defN 23-Apr-11 10:03 asimov/cli/project.py
+-rw-rw-rw-  2.0 unx     5225 b- defN 23-Apr-11 10:03 asimov/cli/report.py
+-rw-rw-rw-  2.0 unx     2817 b- defN 23-Apr-11 10:03 asimov/cli/review.py
+-rw-rw-rw-  2.0 unx       96 b- defN 23-Apr-11 10:03 asimov/cli/report-theme/body.html
+-rw-rw-rw-  2.0 unx      247 b- defN 23-Apr-11 10:03 asimov/cli/report-theme/footer.html
+-rw-rw-rw-  2.0 unx     1467 b- defN 23-Apr-11 10:03 asimov/cli/report-theme/head.html
+-rw-rw-rw-  2.0 unx      841 b- defN 23-Apr-11 10:03 asimov/cli/report-theme/header.html
+-rw-rw-rw-  2.0 unx      220 b- defN 23-Apr-11 10:03 asimov/configs/README.rst
+-rw-rw-rw-  2.0 unx     2903 b- defN 23-Apr-11 10:03 asimov/configs/bayeswave.ini
+-rw-rw-rw-  2.0 unx    12328 b- defN 23-Apr-11 10:03 asimov/configs/bilby.ini
+-rw-rw-rw-  2.0 unx     6238 b- defN 23-Apr-11 10:03 asimov/configs/lalinference.ini
+-rw-rw-rw-  2.0 unx     7569 b- defN 23-Apr-11 10:03 asimov/configs/rift.ini
+-rw-rw-rw-  2.0 unx      610 b- defN 23-Apr-11 10:03 asimov/pipelines/__init__.py
+-rw-rw-rw-  2.0 unx    19216 b- defN 23-Apr-11 10:03 asimov/pipelines/bayeswave.py
+-rw-rw-rw-  2.0 unx    12809 b- defN 23-Apr-11 10:03 asimov/pipelines/bilby.py
+-rw-rw-rw-  2.0 unx     9489 b- defN 23-Apr-11 10:03 asimov/pipelines/lalinference.py
+-rw-rw-rw-  2.0 unx    17635 b- defN 23-Apr-11 10:03 asimov/pipelines/rift.py
+-rw-rw-rw-  2.0 unx      755 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1392 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/WHEEL
+-rw-r--r--  2.0 unx      108 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        7 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/zip-safe
+-rw-rw-r--  2.0 unx     4116 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/RECORD
+52 files, 296399 bytes uncompressed, 80664 bytes compressed:  72.8%
```

## zipnote {}

```diff
@@ -129,44 +129,29 @@
 
 Filename: asimov/pipelines/lalinference.py
 Comment: 
 
 Filename: asimov/pipelines/rift.py
 Comment: 
 
-Filename: asimov/priors/bbh.prior.template
+Filename: asimov-0.6.0a2.dist-info/LICENSE
 Comment: 
 
-Filename: asimov/report-theme/body.html
+Filename: asimov-0.6.0a2.dist-info/METADATA
 Comment: 
 
-Filename: asimov/report-theme/footer.html
+Filename: asimov-0.6.0a2.dist-info/WHEEL
 Comment: 
 
-Filename: asimov/report-theme/head.html
+Filename: asimov-0.6.0a2.dist-info/entry_points.txt
 Comment: 
 
-Filename: asimov/report-theme/header.html
+Filename: asimov-0.6.0a2.dist-info/top_level.txt
 Comment: 
 
-Filename: asimov-0.6.0a1.dist-info/LICENSE
+Filename: asimov-0.6.0a2.dist-info/zip-safe
 Comment: 
 
-Filename: asimov-0.6.0a1.dist-info/METADATA
-Comment: 
-
-Filename: asimov-0.6.0a1.dist-info/WHEEL
-Comment: 
-
-Filename: asimov-0.6.0a1.dist-info/entry_points.txt
-Comment: 
-
-Filename: asimov-0.6.0a1.dist-info/top_level.txt
-Comment: 
-
-Filename: asimov-0.6.0a1.dist-info/zip-safe
-Comment: 
-
-Filename: asimov-0.6.0a1.dist-info/RECORD
+Filename: asimov-0.6.0a2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## asimov/__init__.py

```diff
@@ -9,16 +9,14 @@
 __packagename__ = __name__
 
 import os
 import logging
 
 from pkg_resources import DistributionNotFound, get_distribution, resource_string
 
-from pkg_resources import DistributionNotFound, get_distribution, resource_string
-
 try:
     __version__ = get_distribution(__name__).version
 except DistributionNotFound:
     # package is not installed
     __version__ = "dev"
     pass
 
@@ -29,15 +27,15 @@
 default_config = resource_string(__name__, "{}.conf".format(__packagename__))
 
 config = configparser.ConfigParser()
 # if not config_file:
 
 config.read_string(default_config.decode("utf8"))
 config_locations = [
-    os.path.join(os.curdir, "{}.conf".format(__packagename__)),
+    os.path.join(os.curdir, ".asimov", "{}.conf".format(__packagename__)),
     os.path.join(
         os.path.expanduser("~"),
         ".config",
         __packagename__,
         "{}.conf".format(__packagename__),
     ),
     os.path.join(os.path.expanduser("~"), ".{}".format(__packagename__)),
@@ -76,15 +74,15 @@
     PRINT_LEVEL = logging.ERROR
 
 ch = logging.StreamHandler()
 print_formatter = logging.Formatter("[%(levelname)s] %(message)s", "%Y-%m-%d %H:%M:%S")
 ch.setFormatter(print_formatter)
 ch.setLevel(PRINT_LEVEL)
 
-logfile = "asimov.log"
+logfile = os.path.join("asimov.log")
 fh = logging.FileHandler(logfile)
 formatter = logging.Formatter(
     "%(asctime)s [%(name)s][%(levelname)s] %(message)s", "%Y-%m-%d %H:%M:%S"
 )
 fh.setFormatter(formatter)
 fh.setLevel(LOGGER_LEVEL)
```

## asimov/analysis.py

```diff
@@ -36,31 +36,33 @@
 from asimov.pipelines import known_pipelines
 from asimov.utils import update
 from asimov.storage import Store
 
 from .review import Review
 from .ini import RunConfiguration
 
+
 class Analysis:
     """
     The base class for all other types of analysis.
 
     TODO: Add a check to make sure names cannot conflict
     """
+
     meta = {}
     meta_defaults = {"scheduler": {}, "sampler": {}, "review": {}}
     _reviews = Review()
-    
+
     @property
     def review(self):
         """
         Return the review information attached to the analysis.
         """
         if "review" in self.meta:
-            if len(self.meta['review']) > 0:
+            if len(self.meta["review"]) > 0:
                 self._reviews = Review.from_dict(self.meta["review"], production=self)
                 self.meta.pop("review")
         return self._reviews
 
     def _process_dependencies(self, needs):
         """
         Process the dependencies list for this production.
@@ -90,37 +92,42 @@
 
     @property
     def job_id(self):
         """
         Get the ID number of this job as it resides in the scheduler.
         """
         if "scheduler" in self.meta:
-            if "job id" in self.meta['scheduler']:
-                return self.meta['scheduler']["job id"]
+            if "job id" in self.meta["scheduler"]:
+                return self.meta["scheduler"]["job id"]
             else:
                 return None
 
     @job_id.setter
     def job_id(self, value):
         if "scheduler" not in self.meta:
-            self.meta['scheduler'] = {}
+            self.meta["scheduler"] = {}
         self.meta["scheduler"]["job id"] = value
 
     @property
     def dependencies(self):
         """Return a list of analyses which this analysis depends upon."""
         all_matches = []
         if len(self._needs) == 0:
             return []
         else:
-            matches = set({})# set(self.event.analyses)
-            #matches.remove(self)
+            matches = set({})  # set(self.event.analyses)
+            # matches.remove(self)
             requirements = self._process_dependencies(self._needs)
             for attribute, match in requirements:
-                filtered_analyses = list(filter(lambda x: x.matches_filter(attribute, match), self.event.analyses))
+                filtered_analyses = list(
+                    filter(
+                        lambda x: x.matches_filter(attribute, match),
+                        self.event.analyses,
+                    )
+                )
                 matches = set.union(matches, set(filtered_analyses))
             for analysis in matches:
                 all_matches.append(analysis)
             return all_matches
 
     @property
     def priors(self):
@@ -134,15 +141,15 @@
     def finished(self):
         finished_states = ["uploaded"]
         return self.status in finished_states
 
     @property
     def status(self):
         return self.status_str.lower()
-    
+
     @status.setter
     def status(self, value):
         self.status_str = value.lower()
 
     def matches_filter(self, attribute, match):
         """
         Checks to see if this analysis matches a given filtering
@@ -187,15 +194,15 @@
             is_name = match == self.name
         try:
             in_meta = reduce(operator.getitem, attribute, self.meta) == match
         except KeyError:
             in_meta = False
 
         return is_name | in_meta | is_status | is_review
-        
+
     def results(self, filename=None, handle=False, hash=None):
         store = Store(root=config.get("storage", "results_store"))
         if not filename:
             try:
                 items = store.manifest.list_resources(self.subject.name, self.name)
                 return items
             except KeyError:
@@ -237,15 +244,14 @@
         exist.
         """
         if key in self.meta:
             return self.meta[key]
         else:
             return None
 
-        
     def set_meta(self, key, value):
         """
         Set a metadata attribute which doesn't currently exist.
         """
         if key not in self.meta:
             self.meta[key] = value
             self.event.ledger.update_event(self.event)
@@ -298,37 +304,36 @@
 
         self.event = self.subject = subject
         self.name = name
 
         self.logger = logger.getChild("event").getChild(f"{self.name}")
         self.logger.setLevel(LOGGER_LEVEL)
 
-
         if "status" in kwargs:
-            self.status_str = kwargs['status'].lower()
+            self.status_str = kwargs["status"].lower()
         else:
             self.status_str = "none"
 
         self.meta = deepcopy(self.meta_defaults)
         self.meta = update(self.meta, deepcopy(self.subject.meta))
         if "productions" in self.meta:
-           self.meta.pop("productions")
+            self.meta.pop("productions")
         # if "needs" in self.meta:
         #    self.meta.pop("needs")
 
         self.meta = update(self.meta, deepcopy(kwargs))
-        self.meta['pipeline'] = pipeline.lower()
+        self.meta["pipeline"] = pipeline.lower()
         self.pipeline = pipeline.lower()
         self.pipeline = known_pipelines[pipeline.lower()](self)
         if "needs" in self.meta:
             self._needs = self.meta.pop("needs")
         else:
             self._needs = []
         if "comment" in kwargs:
-            self.comment = kwargs['comment']
+            self.comment = kwargs["comment"]
         else:
             self.comment = None
 
     def to_dict(self, event=True):
         """
         Return this production as a dictionary.
 
@@ -341,24 +346,24 @@
         dictionary = {}
         if not event:
             dictionary["event"] = self.event.name
             dictionary["name"] = self.name
 
         dictionary["status"] = self.status
         if isinstance(self.pipeline, str):
-            dictionary['pipeline'] = self.pipeline
+            dictionary["pipeline"] = self.pipeline
         else:
             dictionary["pipeline"] = self.pipeline.name.lower()
         dictionary["comment"] = self.comment
 
         if self.review:
             dictionary["review"] = self.review.to_dicts()
 
-        dictionary['needs'] = self._needs #self.dependencies
-            
+        dictionary["needs"] = self._needs  # self.dependencies
+
         if "quality" in self.meta:
             dictionary["quality"] = self.meta["quality"]
         if "priors" in self.meta:
             dictionary["priors"] = self.meta["priors"]
         for key, value in self.meta.items():
             dictionary[key] = value
         if "repository" in self.meta:
@@ -380,23 +385,23 @@
         # Check that pars is a dictionary
         if not {"pipeline", "name"} <= parameters.keys():
             raise ValueError(
                 f"Some of the required parameters are missing."
                 f"Found {parameters.keys()}"
             )
         if "status" not in parameters:
-            parameters['status'] = "ready"
+            parameters["status"] = "ready"
         if "event" in parameters:
             parameters.pop("event")
         pipeline = parameters.pop("pipeline")
         name = parameters.pop("name")
         if "comment" not in parameters:
-            parameters['comment'] = None
+            parameters["comment"] = None
 
-        return cls(subject, name, pipeline,  **parameters)
+        return cls(subject, name, pipeline, **parameters)
 
 
 class SubjectAnalysis(Analysis):
     """
     A single subject analysis which requires results from multiple pipelines.
     """
 
@@ -404,38 +409,38 @@
         self.event = self.subject = subject
         self.name = name
 
         self.logger = logger.getChild("event").getChild(f"{self.name}")
         self.logger.setLevel(LOGGER_LEVEL)
 
         if "status" in kwargs:
-            self.status_str = kwargs['status'].lower()
+            self.status_str = kwargs["status"].lower()
         else:
             self.status_str = "none"
 
         self.meta = deepcopy(self.meta_defaults)
         self.meta = update(self.meta, deepcopy(self.subject.meta))
         if "productions" in self.meta:
-           self.meta.pop("productions")
+            self.meta.pop("productions")
         if "needs" in self.meta:
-           self.meta.pop("needs")
+            self.meta.pop("needs")
 
         self.meta = update(self.meta, deepcopy(kwargs))
         self.pipeline = pipeline.lower()
         self.pipeline = known_pipelines[pipeline.lower()](self)
         if "needs" in self.meta:
             self._needs = self.meta.pop("needs")
         else:
             self._needs = []
-        
+
         if "comment" in kwargs:
-            self.comment = kwargs['comment']
+            self.comment = kwargs["comment"]
         else:
             self.comment = None
-        
+
     def to_dict(self, event=True):
         """
         Return this production as a dictionary.
 
         Parameters
         ----------
         event : bool
@@ -445,24 +450,24 @@
         dictionary = {}
         if not event:
             dictionary["event"] = self.event.name
             dictionary["name"] = self.name
 
         dictionary["status"] = self.status
         if isinstance(self.pipeline, str):
-            dictionary['pipeline'] = self.pipeline
+            dictionary["pipeline"] = self.pipeline
         else:
             dictionary["pipeline"] = self.pipeline.name.lower()
         dictionary["comment"] = self.comment
 
         if self.review:
             dictionary["review"] = self.review.to_dicts()
 
-        dictionary['needs'] = self.dependencies
-            
+        dictionary["needs"] = self.dependencies
+
         if "quality" in self.meta:
             dictionary["quality"] = self.meta["quality"]
         if "priors" in self.meta:
             dictionary["priors"] = self.meta["priors"]
         for key, value in self.meta.items():
             dictionary[key] = value
         if "repository" in self.meta:
@@ -484,40 +489,36 @@
         # Check that pars is a dictionary
         if not {"pipeline", "name"} <= parameters.keys():
             raise ValueError(
                 f"Some of the required parameters are missing."
                 f"Found {parameters.keys()}"
             )
         if "status" not in parameters:
-            parameters['status'] = "ready"
+            parameters["status"] = "ready"
         if "event" in parameters:
             parameters.pop("event")
         pipeline = parameters.pop("pipeline")
         name = parameters.pop("name")
         if "comment" not in parameters:
-            parameters['comment'] = None
-
-        return cls(subject, name, pipeline,  **parameters)
-
+            parameters["comment"] = None
 
+        return cls(subject, name, pipeline, **parameters)
 
 
 class ProjectAnalysis(Analysis):
     """
     A multi-subject analysis.
     """
 
-    def __init__(
-            self, subjects, analyses, name, pipeline, ledger=None, **kwargs
-    ):
+    def __init__(self, subjects, analyses, name, pipeline, ledger=None, **kwargs):
         """ """
         super().__init__()
 
-        self.name = name # if name else "unnamed project analysis"
-        
+        self.name = name  # if name else "unnamed project analysis"
+
         self.logger = logger.getChild("project analyses").getChild(f"{self.name}")
         self.logger.setLevel(LOGGER_LEVEL)
 
         self.subjects = subjects
         self.events = self.subjects
 
         self._analysis_spec = analyses
@@ -529,35 +530,39 @@
         self._subject_obs = []
         for subject in self.subjects:
             sub = self.ledger.get_event(subject)[0]
             self._subject_obs.append(sub)
             if self._analysis_spec:
                 matches = set(sub.analyses)
                 for attribute, match in requirements:
-                    filtered_analyses = list(filter(lambda x: x.matches_filter(attribute, match), sub.analyses))
+                    filtered_analyses = list(
+                        filter(
+                            lambda x: x.matches_filter(attribute, match), sub.analyses
+                        )
+                    )
                     matches = set.intersection(matches, set(filtered_analyses))
                 for analysis in matches:
                     self.analyses.append(analysis)
         if "status" in kwargs:
-            self.status_str = kwargs['status'].lower()
+            self.status_str = kwargs["status"].lower()
         else:
             self.status_str = "none"
 
         self.pipeline = pipeline.lower()
         try:
             self.pipeline = known_pipelines[str(pipeline).lower()](self)
-        except:
+        except KeyError:
             self.logger.warning(f"The pipeline {pipeline} could not be found.")
         if "needs" in self.meta:
             self._needs = self.meta.pop("needs")
         else:
             self._needs = []
 
         if "comment" in kwargs:
-            self.comment = kwargs['comment']
+            self.comment = kwargs["comment"]
         else:
             self.comment = None
 
     def __repr__(self):
         """
         A human-friendly representation of this project.
 
@@ -573,96 +578,97 @@
         # Check that pars is a dictionary
         if not {"pipeline", "name"} <= parameters.keys():
             raise ValueError(
                 f"Some of the required parameters are missing. "
                 f"Found {parameters.keys()}"
             )
         if "status" not in parameters:
-            parameters['status'] = "ready"
+            parameters["status"] = "ready"
         if "event" in parameters:
             parameters.pop("event")
         pipeline = parameters.pop("pipeline")
         name = parameters.pop("name")
         if "comment" not in parameters:
-            parameters['comment'] = None
-        return cls(name=name, pipeline=pipeline, ledger=ledger,  **parameters)
+            parameters["comment"] = None
+        return cls(name=name, pipeline=pipeline, ledger=ledger, **parameters)
 
     def to_dict(self):
         """
         Return this project production as a dictionary.
 
         Parameters
         ----------
         event : bool
            If set to True the output is designed to be included nested within an event.
            The event name is not included in the representation, and the production name is provided as a key.
         """
         dictionary = {}
-        dictionary['name'] = self.name
+        dictionary["name"] = self.name
         dictionary["status"] = self.status
         if isinstance(self.pipeline, str):
-            dictionary['pipeline'] = self.pipeline
+            dictionary["pipeline"] = self.pipeline
         else:
             dictionary["pipeline"] = self.pipeline.name.lower()
         dictionary["comment"] = self.comment
 
         if self.review:
             dictionary["review"] = self.review.to_dicts()
 
-        dictionary['needs'] = self.dependencies
-            
+        dictionary["needs"] = self.dependencies
+
         if "quality" in self.meta:
             dictionary["quality"] = self.meta["quality"]
         if "priors" in self.meta:
             dictionary["priors"] = self.meta["priors"]
         for key, value in self.meta.items():
             dictionary[key] = value
         if "repository" in self.meta:
             dictionary["repository"] = self.repository.url
         if "ledger" in dictionary:
             dictionary.pop("ledger")
         if "pipelines" in dictionary:
             dictionary.pop("pipelines")
 
-        dictionary['subjects'] = self.subjects
-        dictionary['analyses'] = self._analysis_spec
-            
+        dictionary["subjects"] = self.subjects
+        dictionary["analyses"] = self._analysis_spec
+
         output = dictionary
-        
+
         return output
 
-    
+
 class GravitationalWaveTransient(SimpleAnalysis):
     """
     A single subject, single pipeline analysis for a gravitational wave transient.
     """
+
     def __init__(self, subject, name, pipeline, **kwargs):
 
         self.category = config.get("general", "calibration_directory")
 
         super().__init__(subject, name, pipeline, **kwargs)
         self._add_missing_parameters()
         self._checks()
-        
+
         self.psds = self._set_psds()
         self.xml_psds = self._collect_psds(format="xml")
 
     def _collect_psds(self, format="ascii"):
         """
         Collect the required psds for this production.
         """
         psds = {}
         # If the PSDs are specifically provided in the ledger,
         # use those.
 
-        if format=="ascii":
+        if format == "ascii":
             keyword = "psds"
-        elif format=="xml":
+        elif format == "xml":
             keyword = "xml psds"
-        
+
         if keyword in self.meta:
             if self.meta["likelihood"]["sample rate"] in self.meta[keyword]:
                 psds = self.meta[keyword][self.meta["likelihood"]["sample rate"]]
 
         # First look through the list of the job's dependencies
         # to see if they're provided by a job there.
         elif self.dependencies:
@@ -686,34 +692,34 @@
         else:
             psds = {}
 
         for ifo, psd in psds.items():
             self.logger.debug(f"PSD-{ifo}: {psd}")
 
         return psds
-        
+
     def _add_missing_parameters(self):
         for parameter in {"quality", "waveform", "likelihood"}:
-            if not parameter in self.meta:
+            if parameter not in self.meta:
                 self.meta[parameter] = {}
-                
+
         for parameter in {"marginalization"}:
-            if not parameter in self.meta['likelihood']:
-                self.meta['likelihood'][parameter] = {}
+            if parameter not in self.meta["likelihood"]:
+                self.meta["likelihood"][parameter] = {}
 
         for parameter in {"maximum frequency"}:
-            if not parameter in self.meta['quality']:
-                self.meta['quality'][parameter] = {}
-        
+            if parameter not in self.meta["quality"]:
+                self.meta["quality"][parameter] = {}
+
     def _checks(self):
         """
         Carry-out a number of data consistency checks on the information from the ledger.
         """
         # Check that the upper frequency is included, otherwise calculate it
-        
+
         if self.quality:
             if ("high-frequency" not in self.quality) and (
                 "sample-rate" in self.quality
             ):
                 # Account for the PSD roll-off with the 0.875 factor
                 self.meta["quality"]["high-frequency"] = int(
                     0.875 * self.meta["quality"]["sample-rate"] / 2
@@ -800,24 +806,26 @@
         Get the configuration file contents for this event.
         """
         if "ini" in self.meta:
             ini_loc = self.meta["ini"]
         else:
             # We'll need to search the repository for it.
             try:
-                ini_loc = self.subject.repository.find_prods(self.name, self.category)[0]
+                ini_loc = self.subject.repository.find_prods(self.name, self.category)[
+                    0
+                ]
                 if not os.path.exists(ini_loc):
                     raise ValueError("Could not open the ini file.")
             except IndexError:
                 raise ValueError("Could not open the ini file.")
         try:
             ini = RunConfiguration(ini_loc)
         except ValueError:
             raise ValueError("Could not open the ini file")
         except configparser.MissingSectionHeaderError:
             raise ValueError("This isn't a valid ini file")
 
         return ini
-        
+
 
 class Production(SimpleAnalysis):
     pass
```

## asimov/asimov.conf

```diff
@@ -1,8 +1,8 @@
-	[general]
+[general]
 git_default = .
 rundir_default = working
 calibration=C01
 calibration_directory=C01_offline
 webroot = pages/
 logger = file
 
@@ -37,7 +37,11 @@
 url = https://gracedb.ligo.org/api/
 
 [pesummary]
 executable = /cvmfs/oasis.opensciencegrid.org/ligo/sw/conda/envs/igwn-py39/bin/summarypages
 
 [condor]
 cache_time = 900
+cron_minute = */15
+
+[theme]
+name = report-theme
```

## asimov/condor.py

```diff
@@ -34,39 +34,21 @@
     Returns
     -------
     datetime.datetime
         the datetime that represents the given Unix epoch
     """
     return datetime.datetime.utcfromtimestamp(dt).replace(tzinfo=tzinfo)
 
+
 UTC = tz.tzutc()
 
 logger = logger.getChild("condor")
 logger.setLevel(LOGGER_LEVEL)
 
 
-def datetime_from_epoch(dt, tzinfo=UTC):
-    """Returns the `datetime.datetime` for a given Unix epoch
-
-    Parameters
-    ----------
-    dt : `float`
-        a Unix timestamp
-
-    tzinfo : `datetime.tzinfo`, optional
-        the desired timezone for the output `datetime.datetime`
-
-    Returns
-    -------
-    datetime.datetime
-        the datetime that represents the given Unix epoch
-    """
-    return datetime.datetime.utcfromtimestamp(dt).replace(tzinfo=tzinfo)
-
-
 def submit_job(submit_description):
     """
     Submit a new job to the condor scheduller
     """
 
     hostname_job = htcondor.Submit(submit_description)
 
@@ -118,23 +100,23 @@
         # If you can't find a specified scheduler, use the first one you find
         collectors = htcondor.Collector().locateAll(htcondor.DaemonTypes.Schedd)
         logger.info("Searching for a scheduler of any kind")
         for collector in collectors:
             logger.info(f"Found {collector}")
             schedd = htcondor.Schedd(collector)
             HISTORY_CLASSADS = [
-                    "CompletionDate",
-                    "CpusProvisioned",
-                    "GpusProvisioned",
-                    "CumulativeSuspensionTime",
-                    "EnteredCurrentStatus",
-                    "MaxHosts",
-                    "RemoteWallClockTime",
-                    "RequestCpus",
-                ]
+                "CompletionDate",
+                "CpusProvisioned",
+                "GpusProvisioned",
+                "CumulativeSuspensionTime",
+                "EnteredCurrentStatus",
+                "MaxHosts",
+                "RemoteWallClockTime",
+                "RequestCpus",
+            ]
             try:
                 jobs = schedd.history(
                     f"ClusterId == {cluster_id}", projection=HISTORY_CLASSADS
                 )
                 logger.info(f"Jobs found: {jobs}")
                 break
             except htcondor.HTCondorIOError:
@@ -196,15 +178,15 @@
         self._status = status
 
         for key, value in kwargs.items():
             setattr(self, key, value)
 
         for key, value in kwargs.items():
             setattr(self, key, value)
-        
+
     def __repr__(self):
         out = f"<htcondor job | {self.idno} | {self.status} "
         out += f"| {self.hosts} | {self.name} | {len(self.subjobs)} subjobs >"
         return out
 
     def __str__(self):
         return repr(self)
```

## asimov/event.py

```diff
@@ -1,33 +1,24 @@
 """
 Trigger handling code.
 """
 
-import glob
 import os
 import pathlib
-from copy import deepcopy
 import logging
-import configparser
 import subprocess
 
 import networkx as nx
 import yaml
 from ligo.gracedb.rest import GraceDb, HTTPError
-from liquid import Liquid
 
 from asimov import config, logger, LOGGER_LEVEL
-from asimov.pipelines import known_pipelines
-from asimov.storage import Store
-from asimov.utils import update
 from asimov.analysis import GravitationalWaveTransient
 
 from .git import EventRepo
-from .ini import RunConfiguration
-from .review import Review
 
 status_map = {
     "cancelled": "light",
     "finished": "success",
     "uploaded": "success",
     "processing": "primary",
     "running": "primary",
@@ -37,26 +28,29 @@
     "wait": "light",
     "stop": "danger",
     "manual": "light",
     "stopped": "light",
 }
 
 
-status_map = {"cancelled": "light",
-              "finished": "success",
-              "uploaded": "success",
-              "processing": "primary",
-              "running": "primary",
-              "stuck": "warning",
-              "restart": "secondary",
-              "ready": "secondary",
-              "wait": "light",
-              "stop": "danger",
-              "manual": "light",
-              "stopped": "light"}
+status_map = {
+    "cancelled": "light",
+    "finished": "success",
+    "uploaded": "success",
+    "processing": "primary",
+    "running": "primary",
+    "stuck": "warning",
+    "restart": "secondary",
+    "ready": "secondary",
+    "wait": "light",
+    "stop": "danger",
+    "manual": "light",
+    "stopped": "light",
+}
+
 
 class DescriptionException(Exception):
     """Exception for event description problems."""
 
     def __init__(self, message, production=None):
         super(DescriptionException, self).__init__(message)
         self.message = message
@@ -74,14 +68,15 @@
   </details>
 </p>
 
 - [ ] Resolved
 """
         return text
 
+
 class Event:
     """
     A specific gravitational wave event or trigger.
     """
 
     def __init__(self, name, repository=None, update=False, **kwargs):
         """
@@ -118,15 +113,15 @@
         if "ledger" in kwargs:
             if kwargs["ledger"]:
                 self.ledger = kwargs["ledger"]
         else:
             self.ledger = None
 
         if "ledger" in kwargs:
-            self.ledger = kwargs['ledger']
+            self.ledger = kwargs["ledger"]
         else:
             self.ledger = None
 
         if repository:
             if "git@" in repository or "https://" in repository:
                 self.repository = EventRepo.from_url(
                     repository, self.name, directory=None, update=update
@@ -152,52 +147,44 @@
         self.productions = []
         self.graph = nx.DiGraph()
 
         if "productions" in kwargs:
             for production in kwargs["productions"]:
                 self.add_production(
                     Production.from_dict(
-                        production, subject=self,
+                        production,
+                        subject=self,
                     )
                 )
         self._check_required()
 
         if ("interferometers" in self.meta) and ("calibration" in self.meta):
             try:
                 self._check_calibration()
             except DescriptionException:
                 pass
 
     @property
     def analyses(self):
         return self.productions
-            
+
     def __eq__(self, other):
         if isinstance(other, Event):
             if other.name == self.name:
                 return True
             else:
                 return False
         else:
             return False
 
     def update_data(self):
         if self.ledger:
             self.ledger.update_event(self)
         pass
 
-    def __eq__(self, other):
-        if isinstance(other, Event):
-            if other.name == self.name:
-                return True
-            else:
-                return False
-        else:
-            return False
-            
     def _check_required(self):
         """
         Find all of the required metadata is provided.
         """
         return True
 
     def _check_calibration(self):
@@ -247,18 +234,18 @@
         if production.name in [production_o.name for production_o in self.productions]:
             raise ValueError(
                 f"A production with this name already exists for {self.name}. New productions must have unique names."
             )
 
         self.productions.append(production)
         self.graph.add_node(production)
-        
+
         if production.dependencies:
             for dependency in production.dependencies:
-                if (dependency == production):
+                if dependency == production:
                     continue
                 self.graph.add_edge(dependency, production)
 
     def __repr__(self):
         return f"<Event {self.name}>"
 
     @classmethod
@@ -335,16 +322,16 @@
             )
 
         if not repo and "repository" in data:
             data.pop("repository")
         event = cls.from_dict(data, update=update, ledger=ledger)
 
         if "productions" in data:
-            if isinstance(data['productions'], type(None)):
-                data['productions'] = []
+            if isinstance(data["productions"], type(None)):
+                data["productions"] = []
         else:
             data["productions"] = []
 
         if "interferometers" in data and "event time" in data:
 
             if calibration.keys() != data["interferometers"]:
                 # We need to fetch the calibration data
@@ -374,15 +361,15 @@
         Parameters
         ----------
         gfile : str
            The name of the gracedb file, e.g. `coinc.xml`.
         destination : str
            The location in the repository for this file.
         """
-        
+
         if "gid" in self.meta:
             gid = self.meta["gid"]
         else:
             raise ValueError("No GID is included in this event's metadata.")
 
         try:
             client = GraceDb(service_url=config.get("gracedb", "url"))
@@ -468,15 +455,17 @@
             ]
         )
         ends = [
             x
             for x in unfinished.reverse().nodes()
             if unfinished.reverse().out_degree(x) == 0
         ]
-        return {end for end in ends if end.status.lower()=="ready"}  # only want to return one version of each production!
+        return {
+            end for end in ends if end.status.lower() == "ready"
+        }  # only want to return one version of each production!
 
     def build_report(self):
         for production in self.productions:
             production.build_report()
 
     def html(self):
         card = f"""
@@ -496,8 +485,9 @@
         card += """
         </div>
         </div>
         """
 
         return card
 
+
 Production = GravitationalWaveTransient
```

## asimov/gitlab.py

```diff
@@ -72,31 +72,14 @@
         """
         server = gitlab.gitlab.Gitlab(
             config.get("gitlab", "server"), private_token=config.get("gitlab", "token")
         )
         repository = server.projects.get(config.get("gitlab", "tracking_repository"))
         return server, repository
 
-    def _connect_gitlab(self):
-        """
-        Connect to the gitlab server.
-
-        Returns
-        -------
-        server : `Gitlab`
-           The gitlab server.
-        repository: `Gitlab.project`
-           The gitlab project.
-        """
-        server = gitlab.gitlab.Gitlab(
-            config.get("gitlab", "server"), private_token=config.get("gitlab", "token")
-        )
-        repository = server.projects.get(config.get("gitlab", "tracking_repository"))
-        return server, repository
-
     def get_event(self, event=None):
         if event:
             return self.events[event]
         else:
             return self.events.values()
 
     @classmethod
```

## asimov/ledger.py

```diff
@@ -1,20 +1,23 @@
 """
 Code for the project ledger.
 """
 from functools import reduce
 
 import yaml
 
+import os
+import shutil
+
 import asimov
 import asimov.database
 from asimov import config
 from asimov.analysis import ProjectAnalysis
 from asimov.event import Event, Production
-from asimov.utils import update
+from asimov.utils import update, set_directory
 
 
 class Ledger:
     @classmethod
     def create(cls, name=None, engine=None, location=None):
         """
         Create a ledger.
@@ -36,29 +39,29 @@
         elif engine == "gitlab":
             raise NotImplementedError(
                 "This hasn't been ported to the new interface yet. Stay tuned!"
             )
 
 
 class YAMLLedger(Ledger):
-    def __init__(self, location="ledger.yml"):
+    def __init__(self, location=".asimov/ledger.yml"):
         self.location = location
         with open(location, "r") as ledger_file:
             self.data = yaml.safe_load(ledger_file)
 
         self.data["events"] = [
             update(self.get_defaults(), event, inplace=False)
             for event in self.data["events"]
         ]
-        
+
         self.events = {ev["name"]: ev for ev in self.data["events"]}
         self.data.pop("events")
 
     @classmethod
-    def create(cls, name, location="ledger.yml"):
+    def create(cls, name, location=".asimov/ledger.yml"):
 
         data = {}
         data["asimov"] = {}
         data["asimov"]["version"] = asimov.__version__
         data["events"] = []
         data["project analyses"] = []
         data["project"] = {}
@@ -86,62 +89,43 @@
         if "trash" not in self.data:
             self.data["trash"] = {}
         if "events" not in self.data["trash"]:
             self.data["trash"]["events"] = {}
         self.data["trash"]["events"][event_name] = event
         self.save()
 
-    def update_event(self, event):
-        """
-        Update an event in the ledger with a changed event object.
-        """
-        self.events[event.name] = event.to_dict()
-        self.save()
-
-    def delete_event(self, event_name):
-        """
-        Remove an event from the ledger.
-
-        Parameters
-        ----------
-        event_name : str
-           The name of the event to remove from the ledger.
-        """
-        event = self.events.pop(event_name)
-        if "trash" not in self.data:
-            self.data["trash"] = {}
-        if "events" not in self.data["trash"]:
-            self.data["trash"]["events"] = {}
-        self.data["trash"]["events"][event_name] = event
-        self.save()
-
     def save(self):
         """
         Update the ledger YAML file with the data from the various events.
 
         Notes
         -----
         The save function checks the difference between the default values for each production and event
         before saving them, in order to attempt to reduce the duplication within the ledger.
 
 
         """
         self.data["events"] = list(self.events.values())
-
-        with open(self.location, "w") as ledger_file:
-            ledger_file.write(yaml.dump(self.data, default_flow_style=False))
+        with set_directory(config.get("project", "root")):
+            # First produce a backup of the ledger
+            shutil.copy(self.location, self.location + ".bak")
+            with open(self.location + "_tmp", "w") as ledger_file:
+                ledger_file.write(yaml.dump(self.data, default_flow_style=False))
+                ledger_file.flush()
+                # os.fsync(ledger_file.fileno())
+            os.replace(self.location + "_tmp", self.location)
 
     def add_subject(self, subject):
         """Add a new subject to the ledger."""
         if "events" not in self.data:
             self.data["events"] = []
 
         self.events[subject.name] = subject.to_dict()
         self.save()
-        
+
     def add_event(self, event):
         self.add_subject(subject=event)
 
     def add_analysis(self, analysis, event=None):
         """
         Add an analysis to the ledger.
 
@@ -151,32 +135,24 @@
         Parameters
         ----------
         analysis : `asimov.Analysis`
            The analysis to be added to the ledger.
         event : str, optional
            The name of the event which the analysis should be added to.
            This is not required for project analyses.
-        
+
         Examples
         --------
         """
         if isinstance(analysis, ProjectAnalysis):
-            self.data['project analyses'].append(analysis.to_dict())
+            self.data["project analyses"].append(analysis.to_dict())
         else:
             event.add_production(analysis)
             self.events[event.name] = event.to_dict()
         self.save()
-        
-    def add_production(self, event, production):
-        self.add_analysis(production=production, event=event)
-
-    def add_production(self, event, production):
-        event.add_production(production)
-        self.events[event.name] = event.to_dict()
-        self.save()
 
     def add_production(self, event, production):
         event.add_production(production)
         self.events[event.name] = event.to_dict()
         self.save()
 
     def get_defaults(self):
@@ -196,16 +172,19 @@
             defaults["likelihood"] = self.data["likelihood"]
         if "scheduler" in self.data:
             defaults["scheduler"] = self.data["scheduler"]
         return defaults
 
     @property
     def project_analyses(self):
-        return [ProjectAnalysis.from_dict(analysis, ledger=self) for analysis in self.data['project analyses']]
-    
+        return [
+            ProjectAnalysis.from_dict(analysis, ledger=self)
+            for analysis in self.data["project analyses"]
+        ]
+
     def get_event(self, event=None):
         if event:
             return [Event(**self.events[event], ledger=self)]
         else:
             return [
                 Event(**self.events[event], ledger=self) for event in self.events.keys()
             ]
@@ -317,47 +296,7 @@
                 queries_1 & reduce(lambda x, y: x & y, queries)
             )
 
         event = self.get_event(event)
         return [
             Production.from_dict(dict(production), event) for production in productions
         ]
-
-    def get_productions(self, event=None, filters=None):
-        """Get a list of productions either for a single event or for all events.
-
-        Parameters
-        ----------
-        event : str
-           The name of the event to pull productions from.
-           Optional; if no event is specified then all of the productions are
-           returned.
-
-        filters : dict
-           A dictionary of parameters to filter on.
-
-        Examples
-        --------
-        FIXME: Add docs.
-
-        """
-
-        if event:
-            productions = self.get_event(event).productions
-        else:
-            productions = []
-            for event_i in self.get_event():
-                for production in event_i.productions:
-                    productions.append(production)
-
-        def apply_filter(productions, parameter, value):
-            productions = filter(lambda x: x.meta[parameter] == value
-                                 if (parameter in x.meta)
-                                 else (getattr(x, parameter) == value
-                                       if hasattr(x, parameter) else False),
-                                 productions)
-            return productions
-
-        if filters:
-            for parameter, value in filters.items():
-                productions = apply_filter(productions, parameter, value)
-        return list(productions)
```

## asimov/olivaw.py

```diff
@@ -32,15 +32,15 @@
 @click.pass_context
 def olivaw(ctx):
     """
     This is the main program which runs the DAGs for each event issue.
     """
 
     # Check that we're running in an actual asimov project
-    if not os.path.exists("asimov.conf") and ctx.invoked_subcommand != "init":
+    if not os.path.exists(".asimov") and ctx.invoked_subcommand != "init":
         # This isn't the root of an asimov project, let's fail.
         click.secho("This isn't an asimov project", fg="white", bg="red")
         sys.exit(1)
     pass
 
 
 # Project initialisation
```

## asimov/pipeline.py

```diff
@@ -13,15 +13,14 @@
 from asimov import utils  # NoQA
 from asimov import config, logger, logging, LOGGER_LEVEL  # NoQA
 
 import otter  # NoQA
 from .storage import Store  # NoQA
 
 
-
 class PipelineException(Exception):
     """Exception for pipeline problems."""
 
     def __init__(self, message, issue=None, production=None):
         super(PipelineException, self).__init__(message)
         self.message = message
         self.issue = issue
@@ -100,15 +99,14 @@
             f"analysis.{production.event.name}/{production.name}"
         )
         self.logger.setLevel(LOGGER_LEVEL)
 
     def __repr__(self):
         return self.name.lower()
 
-        
     def detect_completion(self):
         """
         Check to see if the job has in fact completed.
         """
         pass
 
     def before_config(self, dryrun=False):
@@ -118,15 +116,15 @@
         pass
 
     def before_build(self, dryrun=False):
         """
         Define a hook to be run before the DAG is built.
         """
         pass
-    
+
     def before_submit(self, dryrun=False):
         """
         Define a hook to run before the DAG file is generated and submitted.
 
         Note, this method should be over-written in the specific pipeline implementation
         if required.
         It allows the `dryrun` option to be specified in order to only print the commands
@@ -191,29 +189,14 @@
             "samples",
             files,
         )
         if os.path.exists(results):
             return True
         else:
             return False
-
-    def detect_completion_processing(self):
-        files = f"{self.production.name}_pesummary.dat"
-        results = os.path.join(
-            config.get("general", "webroot"),
-            self.production.event.name,
-            self.production.name,
-            "results",
-            "samples",
-            files,
-        )
-        if os.path.exists(results):
-            return True
-        else:
-            return False
 
     def after_processing(self):
         """
         Run the after processing jobs.
         """
         try:
             self.store_results()
```

## asimov/testing.py

```diff
@@ -1,37 +1,39 @@
 """
 This file contains code to allow unittests to be written with
 Asimov so that productions can be tested with minimal boilerplate.
 This module contains the factory classes for other asimov tests.
 """
 
 import os
-import shutil
 import unittest
+import shutil
 import git
 from asimov import current_ledger as ledger
 from asimov.cli.project import make_project
 from asimov.ledger import YAMLLedger
 
+
 class AsimovTestCase(unittest.TestCase):
     @classmethod
     def setUpClass(cls):
         cls.cwd = os.getcwd()
         git.Repo.init(cls.cwd + "/tests/test_data/s000000xx/")
 
     def setUp(self):
         os.makedirs(f"{self.cwd}/tests/tmp/project")
         os.chdir(f"{self.cwd}/tests/tmp/project")
         make_project(name="Test project", root=f"{self.cwd}/tests/tmp/project")
-        self.ledger = YAMLLedger("ledger.yml")
+        self.ledger = YAMLLedger(".asimov/ledger.yml")
 
     def tearDown(self):
         os.chdir(self.cwd)
         shutil.rmtree(f"{self.cwd}/tests/tmp/")
 
+
 class AsimovTest(unittest.TestCase):
     """
     Overloads the unittest.TestCase code.
     Simply makes `self.events` available to the test case.
     """
 
     @classmethod
```

## asimov/cli/application.py

```diff
@@ -103,15 +103,15 @@
             except ValueError as e:
                 click.echo(
                     click.style("", fg="red")
                     + f" Could not apply {analysis.name} to project as "
                     + "an analysis already exists with this name"
                 )
                 logger.exception(e)
-            
+
         elif document["kind"] == "configuration":
             logger.info("Found configurations")
             document.pop("kind")
             update(ledger.data, document)
             ledger.save()
             click.echo(
                 click.style("", fg="green")
```

## asimov/cli/event.py

```diff
@@ -12,15 +12,14 @@
 
 from asimov import config
 from asimov import current_ledger as ledger
 from asimov.utils import find_calibrations, update
 from asimov.event import DescriptionException, Event
 
 
-
 @click.group()
 def event():
     """
     Commands to handle events & collections.
     """
     pass
```

## asimov/cli/manage.py

```diff
@@ -4,14 +4,15 @@
 import os
 import pathlib
 
 import click
 
 from asimov import current_ledger as ledger
 import asimov
+from asimov import condor
 from asimov import LOGGER_LEVEL
 from asimov.event import DescriptionException
 from asimov.pipeline import PipelineException
 
 
 @click.group(chain=True)
 def manage():
@@ -60,15 +61,17 @@
                 if dryrun:
                     click.echo(
                         click.style("", fg="yellow")
                         + f" {production.name} is marked as {production.status.lower()} so no action will be performed"
                     )
                 continue  # I think this test might be unused
             try:
-                ini_loc = production.event.repository.find_prods(production.name, production.category)[0]
+                ini_loc = production.event.repository.find_prods(
+                    production.name, production.category
+                )[0]
                 if not os.path.exists(ini_loc):
                     raise KeyError
             except KeyError:
                 try:
 
                     # if production.rundir:
                     #     path = pathlib.Path(production.rundir)
@@ -178,16 +181,15 @@
                         "The pipeline failed to build a DAG file.",
                     )
                     logger.exception(e)
                     click.echo(
                         click.style("", fg="red")
                         + f" Unable to submit {production.name}"
                     )
-                except ValueError as e:
-                    print("ERROR", e)
+                except ValueError:
                     logger.info("Unable to submit an unbuilt production")
                     click.echo(
                         click.style("", fg="red")
                         + f" Unable to submit {production.name} as it hasn't been built yet."
                     )
                     click.echo("Try running `asimov manage build` first.")
                 try:
```

## asimov/cli/monitor.py

```diff
@@ -1,8 +1,10 @@
 import shutil
+import configparser
+import os
 import sys
 import click
 
 from asimov import condor, config, logger, LOGGER_LEVEL
 from asimov import current_ledger as ledger
 from asimov.cli import ACTIVE_STATES, manage, report
 
@@ -25,61 +27,19 @@
     except (configparser.NoOptionError, configparser.NoSectionError):
         minute_expression = "*/15"
 
     submit_description = {
         "executable": shutil.which("asimov"),
         "arguments": "monitor --chain",
         "accounting_group": config.get("pipelines", "accounting"),
-        "output": "asimov_cron.out",
+        "output": os.path.join(".asimov", "asimov_cron.out"),
         "on_exit_remove": "false",
-        "error": "asimov_cron.err",
-        "log": "asimov_cron.log",
-        "request_cpus": "1",
-        "cron_minute": minute_expression,
-        "getenv": "true",
-        "batch_name": f"asimov/monitor/{ledger.data['project']['name']}",
-        "request_memory": "8192MB",
-        "request_disk": "8192MB",
-    }
-    cluster = condor.submit_job(submit_description)
-    ledger.data["cronjob"] = cluster
-    ledger.save()
-    click.secho(f"  \t   Asimov is running ({cluster})", fg="green")
-    logger.info(f"Running asimov cronjob as  {cluster}")
-
-
-@click.option("--dry-run", "-n", "dry_run", is_flag=True)
-@click.command()
-def stop(dry_run):
-    """Set up a cron job on condor to monitor the project."""
-    cluster = ledger.data["cronjob"]
-    condor.delete_job(cluster)
-    click.secho("  \t   Asimov has been stopped", fg="red")
-    logger.info(f"Stopped asimov cronjob {cluster}")
-
-
-
-@click.option("--dry-run", "-n", "dry_run", is_flag=True)
-@click.command()
-def start(dry_run):
-    """Set up a cron job on condor to monitor the project."""
-
-    try:
-        minute_expression = config.get("condor", "cron_minute")
-    except (configparser.NoOptionError, configparser.NoSectionError):
-        minute_expression = "*/15"
-
-    submit_description = {
-        "executable": shutil.which("asimov"),
-        "arguments": "monitor --chain",
-        "accounting_group": config.get("asimov start", "accounting"),
-        "output": "asimov_cron.out",
-        "on_exit_remove": "false",
-        "error": "asimov_cron.err",
-        "log": "asimov_cron.log",
+        "universe": "local",
+        "error": os.path.join(".asimov", "asimov_cron.err"),
+        "log": os.path.join(".asimov", "asimov_cron.log"),
         "request_cpus": "1",
         "cron_minute": minute_expression,
         "getenv": "true",
         "batch_name": f"asimov/monitor/{ledger.data['project']['name']}",
         "request_memory": "8192MB",
         "request_disk": "8192MB",
     }
@@ -316,18 +276,27 @@
                         pipe.detect_completion()
                         and production.status.lower() == "running"
                     ):
                         # The job has been completed, collect its assets
                         if "profiling" not in production.meta:
                             production.meta["profiling"] = {}
                         try:
+                            config.get("condor", "scheduler")
                             production.meta["profiling"] = condor.collect_history(
                                 production.meta["job id"]
                             )
                             production.meta["job id"] = None
+                        except (
+                            configparser.NoOptionError,
+                            configparser.NoSectionError,
+                        ):
+                            logger.warning(
+                                "Could not collect condor profiling data as no " +
+                                "scheduler was specified in the config file."
+                            )
                         except ValueError as e:
                             logger.error("Could not collect condor profiling data.")
                             logger.exception(e)
                             pass
 
                         finish += 1
                         production.status = "finished"
```

## asimov/cli/project.py

```diff
@@ -69,31 +69,34 @@
     pathlib.Path(logs).mkdir(parents=True, exist_ok=True)
     config.set("logging", "directory", logs)
 
     # Make the results store
     storage.Store.create(root=results, name=f"{project_name} storage")
     config.set("storage", "directory", results)
 
-    # Make the ledger
+    # Make the ledger and operative files
+    pathlib.Path('.asimov').mkdir(parents=True, exist_ok=True)
     config.set("ledger", "engine", "yamlfile")
-    config.set("ledger", "location", "ledger.yml")
+    config.set("ledger", "location", os.path.join(".asimov", "ledger.yml"))
 
     # Set the default environment
     python_loc = shutil.which("python").split("/")[:-2]
     config.set("pipelines", "environment", os.path.join("/", *python_loc))
 
     # Set the default condor user
     if not user:
         config.set("condor", "user", getpass.getuser())
     else:
         config.set("condor", "user", user)
 
-    Ledger.create(engine="yamlfile", name=project_name, location="ledger.yml")
+    Ledger.create(engine="yamlfile",
+                  name=project_name,
+                  location=os.path.join(".asimov", "ledger.yml"))
 
-    with open("asimov.conf", "w") as config_file:
+    with open(os.path.join(".asimov", "asimov.conf"), "w") as config_file:
         config.write(config_file)
 
 
 @click.command()
 @click.argument("name")
 @click.option(
     "--root",
@@ -129,53 +132,14 @@
     """
     make_project(name, root, working=working, checkouts=checkouts, results=results)
     click.echo(click.style("", fg="green") + " New project created successfully!")
     logger.info(f"A new project was created in {os.getcwd()}")
 
 
 @click.command()
-@click.argument("name")
-@click.option(
-    "--root",
-    default=os.getcwd(),
-    help="Location to create the project, default is the current directory.",
-)
-@click.option(
-    "--working",
-    default="working",
-    help="""The location to store working directories,
- default is a directory called 'working' inside the current directory.""",
-)
-@click.option(
-    "--checkouts",
-    default="checkouts",
-    help="The location to store cloned git repositories.",
-)
-@click.option(
-    "--results",
-    default="results",
-    help="The location where the results store should be created.",
-)
-@click.option(
-    "--user",
-    default=None,
-    help="The user account to be used for accounting purposes. Defaults to the current user if not set.",
-)
-def init(
-    name, root, working="working", checkouts="checkouts", results="results", user=None
-):
-    """
-    Roll-out a new project.
-    """
-    make_project(name, root, working=working, checkouts=checkouts, results=results)
-    click.echo(click.style("", fg="green") + " New project created successfully!")
-    logger.info(f"A new project was created in {os.getcwd()}")
-
-
-@click.command()
 @click.argument("location")
 def clone(location):
     import pathlib
     import shutil
 
     results = "results"
 
@@ -215,19 +179,19 @@
         os.path.join(location, config.get("storage", "results_store")), results
     )
     config.set("storage", "results_store", results)
 
     # Make the ledger
     if config.get("ledger", "engine") == "yamlfile":
         shutil.copyfile(
-            os.path.join(location, config.get("ledger", "location")), "ledger.yml"
+            os.path.join(location, config.get("ledger", "location")), ".asimov/ledger.yml"
         )
     elif config.get("ledger", "engine") == "gitlab":
         raise NotImplementedError(
             "The gitlab interface has been removed from this version of asimov."
         )
 
     config.set("ledger", "engine", "yamlfile")
-    config.set("ledger", "location", "ledger.yml")
+    config.set("ledger", "location", ".asimov/ledger.yml")
 
     with open("asimov.conf", "w") as config_file:
         config.write(config_file)
```

## asimov/configs/bilby.ini

```diff
@@ -1,29 +1,40 @@
 {%- if production.event.repository -%}
 {%- assign repo_dir = production.event.repository.directory -%}
 {%- else -%}
 {%- assign repo_dir = "." -%}
 {%- endif -%}
+{%- if production.meta['likelihood'] contains "calibration" -%}
+{%- assign calibration_on = production.meta['likelihood']['calibration']['sample'] -%}
+{%- else -%}
+{%- assign calibration_on = True %}
+{%- endif -%}
 {%- assign meta = production.meta -%}
 {%- assign sampler = production.meta['sampler'] -%}
 {%- assign scheduler = production.meta['scheduler'] -%}
 {%- assign likelihood = production.meta['likelihood'] -%}
 {%- assign priors = production.meta['priors'] -%}
 {%- assign data = production.meta['data'] -%}
 {%- assign quality = production.meta['quality'] -%}
 {%- assign ifos = production.meta['interferometers'] -%}
+
+{%- if data contains "calibration" %}
+{%- if calibration_on %}
+{%- if data['calibration'] contains ifos[0] %}
 ################################################################################
 ## Calibration arguments
 ################################################################################
-
 calibration-model=CubicSpline
 spline-calibration-envelope-dict={ {% for ifo in ifos %}{{ifo}}:{{data['calibration'][ifo]}},{% endfor %} }
 spline-calibration-nodes=10
 spline-calibration-amplitude-uncertainty-dict=None
 spline-calibration-phase-uncertainty-dict=None
+{%- endif %}
+{%- endif %}
+{%- endif %}
 
 ################################################################################
 ## Data generation arguments
 ################################################################################
 
 ignore-gwpy-data-quality-check=True
 gps-tuple=None
@@ -106,14 +117,16 @@
 jitter-time=True
 reference-frame={% if production.meta['interferometers'] contains "H1" %}H1{% endif %}{% if production.meta['interferometers'] contains "L1" %}L1{% endif %}{% if production.meta['interferometers'] contains "V1" %}V1{% endif %}
 time-reference=geocent
 likelihood-type={{ likelihood['type'] | default: "GravitationalWaveTransient" }}
 
 {% if likelihood contains "roq" %}
 roq-folder={{ likelihood['roq']['folder'] | default: "None" }}
+roq-weights={{ likelihood['roq']['weights'] | default: "None" }}
+roq-weight-format={{ likelihood['roq']['weight format'] | default: "None" }}
 roq-scale-factor={{ likelihood['roq']['scale'] | default: 1 }}
 {% endif %}
 
 extra-likelihood-kwargs={{ likelihood['kwargs'] | default: "None" }}
 
 ################################################################################
 ## Output arguments
@@ -136,15 +149,61 @@
 
 ################################################################################
 ## Prior arguments
 ################################################################################
 
 default-prior = {{ priors['default'] | default: "BBHPriorDict" }}
 deltaT=0.2
-prior-file={{repo_dir}}/C01_offline/{{production.name}}.prior
+{% if production.meta contains "priors" %}
+prior-dict = {
+{%- if priors.keys() contains "geocentric time" %}
+{%- assign p = priors['geocentric time'] %}
+   geocent_time = {{p['type']}}(name="geocentric time", minimum={{p['minimum']}}, maximum={{p['maximum']}}, boundary={{p['boundary'] | default: None}}),
+{% endif %}
+{%- if priors.keys() contains "chirp mass" %}{% assign p = priors['chirp mass'] %}{% else %}{% assign p = None %}{% endif %}
+   chirp_mass = {{p['type'] | default: "bilby.gw.prior.UniformInComponentsChirpMass" }}(name='chirp_mass', minimum={{p['minimum'] | default: 1}}, maximum={{p['maximum'] | default: 100}}, unit='$M_\{\{\odot\}\}$'),
+{%- if priors.keys() contains "mass ratio" %}{% assign p = priors['mass ratio'] %}{% else %}{% assign p = None %}{% endif %}
+   mass_ratio = {{p['type'] | default: "bilby.gw.prior.UniformInComponentsMassRatio" }}(name='mass_ratio', minimum={{p['minimum']}}, maximum={{p['maximum']}}),
+{%- if priors.keys() contains "mass 1" %}{% assign p = priors['mass 1'] %}{% else %}{% assign p = None %}{% endif %}
+   mass_1 = {{p['type'] | default: Constraint}}(name='mass_1', minimum={{p['minimum']}}, maximum={{p['maximum']}}),
+{%- if priors.keys() contains "mass 2" %}{% assign p = priors['mass 2'] %}{% else %}{% assign p = None %}{% endif %}
+   mass_2 = {{p['type'] | default: Constraint}}(name='mass_2', minimum={{p['minimum'] | default: 1 }}, maximum={{p['maximum'] | default: 100}}),
+{%- if priors.keys() contains "spin 1" %}
+{%- assign p = priors['spin 1'] %}
+{%- else %}
+{%- assign p = None %}
+{% endif %}
+   a_1 = {{ p['type'] | default: Uniform}}(name='a_1', minimum={{ p['minimum'] | default: 0}}, maximum={{ p['maximum'] | default: 0.99}}),
+{%- if priors.keys() contains "spin 2" %}
+{%- assign p = priors['spin 2'] %}
+{%- else %}
+{%- assign p = None %}
+{%- endif %}
+   a_2 = {{ p['type'] | default: Uniform}}(name='a_2', minimum={{ p['minimum'] | default: 0}}, maximum={{ p['maximum'] | default: 0.99}}),
+{%- if priors.keys() contains "tilt 1" %}{% assign p = priors['tilt 1'] %}{% else %}{% assign p = None %}{% endif %}
+   tilt_1 = {{ p['type'] | default: Sine}}(name='tilt_1'),
+{%- if priors.keys() contains "tilt 2" %}{% assign p = priors['tilt 2'] %}{% else %}{% assign p = None %}{% endif %}
+   tilt_2 = {{ p['type'] | default: Sine}}(name='tilt_2'),
+{%- if priors.keys() contains "phi 12" %}{% assign p = priors['phi 12'] %}{% else %}{% assign p = None %}{% endif %}
+   phi_12 = {{ p['type'] | default: Uniform}}(name='phi_12', minimum={{ p['minimum'] | default: 0}}, maximum={{ p['maximum'] | default: "2 * np.pi"}}, boundary={{p['boundary'] | default: "'periodic'"}}),
+{%- if priors.keys() contains "phi jl" %}{% assign p = priors['phi jl'] %}{% else %}{% assign p = None %}{% endif %}
+   phi_jl = {{ p['type'] | default: Uniform}}(name='phi_jl', minimum={{ p['minimum'] | default: 0}}, maximum={{ p['maximum'] | default: "2 * np.pi"}}, boundary={{p['boundary'] | default: "'periodic'"}}),
+{%- if priors.keys() contains "luminosity distance" %}
+{%- assign p = priors['luminosity distance'] %}
+   luminosity_distance =  {{ p['type'] | default: PowerLaw}}({% for key in p.keys() %}{% if key != "type" %}{{key | replace: " ", "_"}}={{p[key]}},{% endif %} {% endfor %} unit='Mpc'),
+{%- else %}
+{%- assign p = None %}
+   luminosity_distance =  {{ p['type'] | default: PowerLaw}}(unit='Mpc'),
+{%- endif %}
+   dec = Cosine(name='dec'),
+   ra = Uniform(name='ra', minimum=0, maximum=2 * np.pi, boundary='periodic'),
+   theta_jn = Sine(name='theta_jn'),
+   psi = Uniform(name='psi', minimum=0, maximum=np.pi, boundary='periodic'),
+   phase = Uniform(name='phase', minimum=0, maximum=2 * np.pi, boundary='periodic')}
+{% endif %}
 enforce-signal-duration=True
 
 ################################################################################
 ## Post processing arguments
 ################################################################################
 
 postprocessing-executable=None
@@ -174,8 +233,8 @@
 pn-phase-order={{ production.meta['waveform']['pn phase order'] | default: -1 }}
 pn-amplitude-order={{ production.meta['waveform']['pn amplitude order'] | default: 1 }}
 numerical-relativity-file={{ production.meta['waveform']['file'] | default: "None" }}
 waveform-arguments-dict={{ production.meta['waveform']['arguments'] | default: "None" }}
 mode-array={{ production.meta['waveform']['mode array'] | default: "None" }}
 frequency-domain-source-model={{ production.meta['likelihood']['frequency domain source model'] | default: "lal_binary_black_hole" }}
 conversion-function={{ production.meta['waveform']['conversion function'] | default: "None" }}
-generation-function={{ production.meta['waveform']['generation function'] | default: "None" }}
+generation-function={{ production.meta['waveform']['generation function'] | default: "None" }}
```

## asimov/configs/rift.ini

```diff
@@ -120,19 +120,14 @@
 [skyarea]
 maxpts=2000
 
 [resultspage]
 skyres=0.5
 deltaLogP = 7.5
 
-[statevector]
-state-vector-channel={ {% for ifo in ifos %}{{ifo}}:{{quality['state vector'][ifo]}},{% endfor %} }
-bits=['Bit 0', 'Bit 1', 'Bit 2']
-
-
 [ligo-skymap-from-samples]
 enable-multiresolution=
 
 [ligo-skymap-plot]
 annotate=
 contour= 50 90
```

## asimov/pipelines/bayeswave.py

```diff
@@ -105,15 +105,15 @@
                 queue = "Priority_PE"
 
             ini.set_queue(queue)
 
             ini.save()
 
             ini = ini.ini_loc
-            
+
         else:
             ini = f"{self.production.name}.ini"
 
         if self.production.rundir:
             rundir = self.production.rundir
         else:
             rundir = os.path.join(
@@ -227,41 +227,36 @@
                 self._convert_psd(ascii_format=psd, ifo=ifo)
         except Exception as e:
             self.logger.error("Failed to convert the PSDs to XML")
             self.logger.exception(e)
 
         try:
             self.collect_pages()
-        except FileNotFoundError:
-            PipelineLogger(
-                message=b"Failed to copy megaplot pages.",
-                production=self.production.name,
-            )
+        except FileNotFoundError as e:
+            self.logger.error("Failed to copy the megaplot output")
+            self.logger.exception(e)
 
         try:
             self.collect_assets()
             self.store_assets()
-        except Exception:
-            PipelineLogger(
-                message=b"Failed to store PSDs.",
-                issue=self.production.event.issue_object,
-                production=self.production.name,
-            )
+        except Exception as e:
+            self.logger.error("Failed to store the PSDs")
+            self.logger.exception(e)
 
         if "supress" in self.production.meta["quality"]:
             for ifo in self.production.meta["quality"]["supress"]:
                 if ifo in self.production.meta["interferometers"]:
                     self.supress_psd(
                         ifo,
                         self.production.meta["quality"]["supress"][ifo]["lower"],
                         self.production.meta["quality"]["supress"][ifo]["upper"],
                     )
 
         self.production.meta.update(self.collect_assets())
-                    
+
         self.production.status = "uploaded"
 
     def before_submit(self):
         """
         Horribly hack the sub files to add `request_disk`
         """
         sub_files = glob.glob(f"{self.production.rundir}/*.sub")
@@ -272,15 +267,15 @@
                 self.logger.info(f"Adding request_disk = {64000} to {sub_file}")
                 f_handle.write(f"request_disk = {64000}\n" + original)
         python_files = glob.glob(f"{self.production.rundir}/*.py")
         for py_file in python_files:
             with open(py_file, "r") as f_handle:
                 original = f_handle.read()
             with open(py_file, "w") as f_handle:
-                self.logger.info(f"Fixing shebang")
+                self.logger.info("Fixing shebang")
                 path = os.path.join(
                     config.get("pipelines", "environment"), "bin", "python"
                 )
                 f_handle.write(f"#! {path}\n" + original)
 
     def submit_dag(self, dryrun=False):
         """
@@ -308,15 +303,15 @@
 
         command = [
             "condor_submit_dag",
             "-batch-name",
             f"bwave/{self.production.event.name}/{self.production.name}",
             f"{self.production.name}.dag",
         ]
-        
+
         self.logger.info((" ".join(command)))
 
         if dryrun:
             print(" ".join(command))
 
         else:
             with set_directory(self.production.rundir):
@@ -329,31 +324,31 @@
                     raise PipelineException(
                         "It looks like condor isn't installed on this system.\n"
                         f"""I wanted to run {" ".join(command)}."""
                     ) from e
 
                 stdout, stderr = dagman.communicate()
 
-                if "submitted to cluster" in str(stdout):
-                    cluster = re.search(
-                        r"submitted to cluster ([\d]+)", str(stdout)
-                    ).groups()[0]
-                    self.production.status = "running"
-                    self.production.job_id = int(cluster)
-                    self.logger.info(
-                        f"Successfully submitted to cluster {self.production.job_id}"
-                    )
-                    self.logger.debug(stdout)
-                    return (int(cluster),)
-                else:
-                    self.logger.info(stdout)
-                    self.logger.error(stderr)
-                    raise PipelineException(
-                        f"The DAG file could not be submitted.\n\n{stdout}\n\n{stderr}",
-                    )
+            if "submitted to cluster" in str(stdout):
+                cluster = re.search(
+                    r"submitted to cluster ([\d]+)", str(stdout)
+                ).groups()[0]
+                self.production.status = "running"
+                self.production.job_id = int(cluster)
+                self.logger.info(
+                    f"Successfully submitted to cluster {self.production.job_id}"
+                )
+                self.logger.debug(stdout)
+                return (int(cluster),)
+            else:
+                self.logger.info(stdout)
+                self.logger.error(stderr)
+                raise PipelineException(
+                    f"The DAG file could not be submitted.\n\n{stdout}\n\n{stderr}",
+                )
 
     def upload_assets(self):
         """
         Upload the PSDs from this job.
         """
         sample = self.production.meta["likelihood"]["sample rate"]
         git_location = os.path.join(self.category, "psds")
@@ -383,35 +378,14 @@
                 )
             except Exception as e:
                 self.logger.error(
                     f"There was a problem committing the PSD for {detector} to the store."
                 )
                 self.logger.exception(e)
 
-    def store_assets(self):
-        """
-        Add the assets to the store.
-        """
-
-        sample_rate = self.production.meta["quality"]["sample-rate"]
-        for detector, asset in self.collect_assets()["psds"]:
-            store = Store(root=config.get("storage", "directory"))
-            try:
-                store.add_file(
-                    self.production.event.name,
-                    self.production.name,
-                    file=asset,
-                    new_name=f"{detector}-{sample_rate}-psd.dat"
-                )
-            except Exception as e:
-                self.logger.error(
-                    f"There was a problem committing the PSD for {detector} to the store."
-                )
-                self.logger.exception(e)
-
     def collect_logs(self):
         """
         Collect all of the log files which have been produced by this production and
         return their contents as a dictionary.
         """
         messages = {}
 
@@ -436,28 +410,33 @@
 
     def collect_assets(self):
         """
         Collect the assets for this job and commit them to the event repository.
         Since this job also generates the PSDs these should be added to the production ledger.
         """
         psds = {}
+        results_dir = glob.glob(f"{self.production.rundir}/trigtime_*")[0]
         for det in self.production.meta["interferometers"]:
             asset = os.path.join(
-                self.production.rundir, "post", "clean", f"glitch_median_PSD_forLI_{det}.dat"
+                results_dir, "post", "clean", f"glitch_median_PSD_forLI_{det}.dat"
             )
             if os.path.exists(asset):
                 psds[det] = asset
 
         outputs = {}
         outputs["psds"] = psds
 
         xml_psds = {}
         for det in self.production.meta["interferometers"]:
             asset = os.path.join(
-                f"{self.production.event.repository.directory}/{self.production.category}/psds/{self.production.meta['likelihood']['sample rate']}/{det.upper()}-psd.xml.gz"
+                f"{self.production.event.repository.directory}",
+                f"{self.production.category}",
+                "psds",
+                f"{self.production.meta['likelihood']['sample rate']}",
+                f"{det.upper()}-psd.xml.gz",
             )
             if os.path.exists(asset):
                 xml_psds[det] = os.path.abspath(asset)
 
         outputs["xml psds"] = xml_psds
 
         return outputs
```

## asimov/pipelines/bilby.py

```diff
@@ -2,20 +2,17 @@
 
 import glob
 import os
 import re
 import subprocess
 import configparser
 
-import git.exc
 import time
 
-from liquid import Liquid
-
-from asimov.utils import update, set_directory
+from asimov.utils import set_directory
 
 from .. import config
 from ..pipeline import Pipeline, PipelineException, PipelineLogger, PESummaryPipeline
 
 
 class Bilby(Pipeline):
     """
@@ -75,87 +72,14 @@
                 continue
             with open(sub_file, "r") as f_handle:
                 original = f_handle.read()
             with open(sub_file, "w") as f_handle:
                 self.logger.info(f"Adding preserve_relative_paths to {sub_file}")
                 f_handle.write("preserve_relative_paths = True\n" + original)
 
-    def _determine_prior(self):
-        """
-        Determine the correct choice of prior file for this production.
-        """
-
-        self.logger.info("Determining the prior file for this production")
-
-        if "prior file" in self.production.meta:
-            self.logger.info("A prior file has already been specified.")
-            self.logger.info(f"{self.production.meta['prior file']}")
-            return self.production.meta["prior file"]
-        else:
-            template = None
-
-            if "event type" in self.production.meta:
-                event_type = self.production.meta["event type"].lower()
-            else:
-                event_type = "bbh"
-                self.production.meta["event type"] = event_type
-
-            if template is None:
-                template_filename = f"{event_type}.prior.template"
-                self.logger.info(
-                    f"[bilby] Constructing a prior using {event_type}.prior.template."
-                )
-                try:
-                    template = os.path.join(
-                        config.get("bilby", "priors"), template_filename
-                    )
-                except (configparser.NoOptionError, configparser.NoSectionError):
-                    from pkg_resources import resource_filename
-
-                    template = resource_filename(
-                        "asimov", f"priors/{template_filename}"
-                    )
-
-            priors = {}
-            priors = update(priors, self.production.event.ledger.data["priors"])
-            priors = update(priors, self.production.event.meta["priors"])
-            priors = update(priors, self.production.meta["priors"])
-
-            priors = {}
-            priors = update(priors, self.production.event.ledger.data["priors"])
-            priors = update(priors, self.production.event.meta["priors"])
-            priors = update(priors, self.production.meta["priors"])
-
-            liq = Liquid(template)
-            rendered = liq.render(priors=priors, config=config)
-
-            prior_name = f"{self.production.name}.prior"
-            prior_file = os.path.join(os.getcwd(), prior_name)
-            self.logger.info(f"Saving the new prior file as {prior_file}")
-            with open(prior_file, "w") as new_prior:
-                new_prior.write(rendered)
-
-            repo = self.production.event.repository
-            try:
-
-                repo.add_file(
-                    prior_file,
-                    os.path.join(
-                        config.get("general", "calibration_directory"), prior_name
-                    ),
-                )
-                os.remove(prior_file)
-            except git.exc.GitCommandError:
-                pass
-            return os.path.join(
-                self.production.event.repository.directory,
-                config.get("general", "calibration_directory"),
-                prior_name,
-            )
-
     def build_dag(self, psds=None, user=None, clobber_psd=False, dryrun=False):
         """
         Construct a DAG file in order to submit a production to the
         condor scheduler using bilby_pipe.
 
         Parameters
         ----------
@@ -176,18 +100,14 @@
            Raised if the construction of the DAG fails.
         """
 
         cwd = os.getcwd()
 
         self.logger.info(f"Working in {cwd}")
 
-        self._determine_prior()  # Build the prior file
-
-        self._determine_prior()  # Build the prior file
-
         if self.production.event.repository:
             ini = self.production.event.repository.find_prods(
                 self.production.name, self.category
             )[0]
             ini = os.path.join(cwd, ini)
         else:
             ini = f"{self.production.name}.ini"
@@ -431,14 +351,15 @@
         config_parser = configparser.RawConfigParser()
         config_parser.read_string(file_content)
 
         return config_parser
 
     def html(self):
         """Return the HTML representation of this pipeline."""
+        # pages_dir = os.path.join(self.production.event.name, self.production.name)
         out = ""
 
         return out
 
     def resurrect(self):
         """
         Attempt to ressurrect a failed job.
```

## asimov/pipelines/rift.py

```diff
@@ -40,20 +40,23 @@
 
         if "bootstrap" in self.production.meta:
             self.bootstrap = self.production.meta["bootstrap"]
         else:
             self.bootstrap = False
 
         self._create_ledger_entries()
-            
+
     def _create_ledger_entries(self):
         """Create entries in the ledger which might be required in the templating."""
         if "sampler" not in self.production.meta:
-            self.production.meta['sampler'] = {}
-        required_args = {"sampler": {"ile", "cip"}, "likelihood": {"marginalization", "assume"}}
+            self.production.meta["sampler"] = {}
+        required_args = {
+            "sampler": {"ile", "cip"},
+            "likelihood": {"marginalization", "assume"},
+        }
         for section in required_args.keys():
             section_data = self.production.meta[section]
             for section_arg in required_args[section]:
                 if section_arg not in section_data:
                     section_data[section_arg] = {}
 
     def after_completion(self):
@@ -63,15 +66,15 @@
         cluster = post_pipeline.submit_dag()
 
         self.production.meta["job id"] = int(cluster)
         self.production.status = "processing"
 
     def before_submit(self):
         pass
-        
+
     def before_config(self, dryrun=False):
         """
         Convert the text-based PSD to an XML psd if the xml doesn't exist already.
         """
         event = self.production.event
         category = config.get("general", "calibration_directory")
         self.logger.info("Checking for XML format PSDs")
@@ -83,15 +86,17 @@
                     self.logger.info(f"Converting {ifo} {sample}-Hz PSD to XML")
                     self._convert_psd(
                         self.production.meta["psds"][sample][ifo], ifo, dryrun=dryrun
                     )
                     asset = f"{ifo.upper()}-psd.xml.gz"
                     self.logger.info(f"Conversion complete as {asset}")
                     git_location = os.path.join(category, "psds")
-                    saveloc = os.path.join(git_location, str(sample), f"psd_{ifo}.xml.gz")
+                    saveloc = os.path.join(
+                        git_location, str(sample), f"psd_{ifo}.xml.gz"
+                    )
                     self.production.event.repository.add_file(
                         asset,
                         saveloc,
                         commit_message=f"Added the xml format PSD for {ifo}.",
                     )
                     self.logger.info(f"Saved at {saveloc}")
 
@@ -146,18 +151,22 @@
                 calibration = config.get("general", "calibration_directory")
                 coinc_file = os.path.abspath(coinc_file)
                 self.logger.info(f"Coinc found at {coinc_file}")
             except HTTPError:
                 print(
                     "Unable to download the coinc file because it was not possible to connect to GraceDB"
                 )
-                self.logger.warning("Could not download a coinc file for this event; could not connect to GraceDB.")
+                self.logger.warning(
+                    "Could not download a coinc file for this event; could not connect to GraceDB."
+                )
                 coinc_file = None
             except ValueError:
-                self.logger.warning("Could not download a coinc file for this event as no GraceDB ID was supplied.")
+                self.logger.warning(
+                    "Could not download a coinc file for this event as no GraceDB ID was supplied."
+                )
                 coinc_file = None
             try:
                 ini = self.production.get_configuration().ini_loc
                 calibration = config.get("general", "calibration_directory")
                 self.logger.info(f"Using the {calibration} calibration")
                 ini = os.path.join(
                     self.production.event.repository.directory, calibration, ini
@@ -185,15 +194,17 @@
             calibration = config.get("general", "calibration")
         except configparser.NoOptionError:
             calibration = "C01"
 
         try:
             approximant = self.production.meta["waveform"]["approximant"]
         except KeyError:
-            self.logger.error("Could not find a waveform approximant specified for this job.")
+            self.logger.error(
+                "Could not find a waveform approximant specified for this job."
+            )
 
         if self.production.rundir:
             rundir = os.path.abspath(self.production.rundir)
         else:
             rundir = os.path.join(
                 os.path.expanduser("~"),
                 self.production.event.name,
@@ -205,20 +216,20 @@
 
         command = [
             os.path.join(
                 config.get("pipelines", "environment"),
                 "bin",
                 "util_RIFT_pseudo_pipe.py",
             ),
-            ]
+        ]
         if coinc_file:
             command += ["--use-coinc", coinc_file]
 
-        if "non-spin" in self.production.meta['waveform']:
-            if self.production.meta['waveform']['non-spin']:
+        if "non-spin" in self.production.meta["waveform"]:
+            if self.production.meta["waveform"]["non-spin"]:
                 command += ["--assume-nospin"]
 
         command += [
             "--calibration",
             f"{calibration}",
             "--approx",
             f"{approximant}",
@@ -280,15 +291,15 @@
                         self.logger.error(err, production=self.production)
                         raise PipelineException(
                             f"DAG file could not be created.\n{command}\n{out}\n\n{err}",
                             production=self.production.name,
                         )
                 else:
                     if self.production.event.repository:
-                        #with set_directory(os.path.abspath(self.production.rundir)):
+                        # with set_directory(os.path.abspath(self.production.rundir)):
                         for psdfile in self.production.get_psds("xml"):
                             ifo = psdfile.split("/")[-1].split("-")[1].split(".")[0]
                             os.system(f"cp {psdfile} {ifo}-psd.xml.gz")
 
                         # os.system("cat *_local.cache > local.cache")
 
                         if hasattr(self.production.event, "issue_object"):
@@ -342,19 +353,21 @@
         if dryrun:
             for psdfile in self.production.get_psds("xml"):
                 print(f"cp {psdfile} {self.production.rundir}/{psdfile.split('/')[-1]}")
             print("")
             print(" ".join(command))
         else:
             for psdfile in self.production.get_psds("xml"):
-                os.system(f"cp {psdfile} {self.production.rundir}/{psdfile.split('/')[-1]}")
+                os.system(
+                    f"cp {psdfile} {self.production.rundir}/{psdfile.split('/')[-1]}"
+                )
 
             try:
                 with set_directory(self.production.rundir):
-                    
+
                     dagman = subprocess.Popen(
                         command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
                     )
                     self.logger.info(command, production=self.production)
             except FileNotFoundError as exception:
                 raise PipelineException(
                     "It looks like condor isn't installed on this system.\n"
```

## Comparing `asimov-0.6.0a1.dist-info/LICENSE` & `asimov-0.6.0a2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `asimov-0.6.0a1.dist-info/METADATA` & `asimov-0.6.0a2.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: asimov
-Version: 0.6.0a1
+Version: 0.6.0a2
 Summary: A Python package for managing and interacting with data analysis jobs.
 Home-page: https://git.ligo.org/asimov/asimov
 Author: Daniel Williams
 Author-email: daniel.williams@ligo.org
 License: MIT license
 Keywords: pe,ligo,asimov
 Platform: UNKNOWN
```

## Comparing `asimov-0.6.0a1.dist-info/RECORD` & `asimov-0.6.0a2.dist-info/RECORD`

 * *Files 17% similar despite different names*

```diff
@@ -1,57 +1,52 @@
-asimov/__init__.py,sha256=W-YcCuFqXxYf2Uh4dc2NEdmldgjrx7CQ4fvxcCyge-Q,2866
-asimov/analysis.py,sha256=DC5oLDCnoBf8C1ADC8x0BF3HIFUq0jjSbFufixQOdUk,27234
-asimov/asimov.conf,sha256=bddG-GghinDt4NfD2jryMc0S1LF_eENPyW_7bu_SM0E,796
-asimov/condor.py,sha256=ephFL34GqUQPntw0mTzamy5HOtVFV6VN6dVNsrZ4yl4,11665
+asimov/__init__.py,sha256=5xYX9iZ42fY31ifmRcuODWo1OPD4aGLKRP5vywFYKjg,2808
+asimov/analysis.py,sha256=cId5XNLXVUQ46RJFEBmsmk6eucfa7XjUwtTQE8NsyGo,27309
+asimov/asimov.conf,sha256=NTs8PEmemE_cZSAEofWXBXNzT0eKSE1c76fHyOkytjI,843
+asimov/condor.py,sha256=7DOCG8eMJ5CdoTcRPH1OSI2_YkIBwHQ1BdGEF5o61eA,11152
 asimov/database.py,sha256=H9jgEoS2GLDMXjkK0d8nBY2ds1bwYUIMDbQmFsHst5U,1617
-asimov/event.py,sha256=V7X0LCadIkj6YbIOJY_6by70KYmselnQ1pClFWBA-yc,15204
+asimov/event.py,sha256=5M98gbZ5qnWXtmRRy9tOgUKD2XnMJ6ZEdtYQgsieQbI,14634
 asimov/git.py,sha256=CxxUpJzfQApt4d3RiGdA7fSln2kwc1sAUy1Z81UYDW0,12166
-asimov/gitlab.py,sha256=LYiXStrR9rSpLprnSRt3K_6H6ZLDv0A-K2cXV2kfxgw,7734
+asimov/gitlab.py,sha256=g5Zg4lSgqAadnf0759wkJchDeSPaJuXTru7CB9a6K9w,7226
 asimov/ini.py,sha256=K3mo8257HpvO0ipX3w1ofmgCN9vEMhvVCi_kupkJzAs,5398
-asimov/ledger.py,sha256=ifSzGGCnlNBbaGxwbV3dSlF5j2PfPJGfU0ZU7JZVhzU,11341
+asimov/ledger.py,sha256=vN-LsfLiV1Z2WW5DaVx8rn0TMQGVFW-Aw96AJar7cEQ,9374
 asimov/locutus.py,sha256=XfiDYrIWQunbTKBAE8BadeCikVI63O67EeQ1jWH28Pg,2051
 asimov/logging.py,sha256=HCE_giZgeXAFeDv3AaRSJBxPj20vAVmyN3uqcnpUJOM,7051
 asimov/mattermost.py,sha256=Lp-WwwTJor4Do9Gab4L79mBvQzZ2WAonmfZT9JO6RT4,1646
-asimov/olivaw.py,sha256=ryrIB0dq7AtnUvxOGn5Dlxk7l7QTSEDVH98UhU57iZM,1590
-asimov/pipeline.py,sha256=kqw8ZIW5AF3vebIE__vRbKD-F65yaDM1xWF2NXh9jKg,14537
+asimov/olivaw.py,sha256=ihsBqkpjxuvP6E96W2OxBkrmnCuVQyOiS6TCrMdo428,1586
+asimov/pipeline.py,sha256=_IOtfPYvrxycDEYW5YDuJkWlRK8CTlL6SdxkfaNivIY,14096
 asimov/review.py,sha256=72Kv9ccK3wMBD_aBDCnX4ozm0BHwTvekz43WNXmeQuQ,4513
 asimov/storage.py,sha256=5IUNyysKFhnhHCj8-J3xX5qDTqnMTsOUKQGmO9_dQsM,10087
-asimov/testing.py,sha256=dBZ88_VHYg-UUnl8x8XZPHJYuKIi6YnivBL8RqBSh5k,1146
+asimov/testing.py,sha256=GHN83ozigC--s0tMH8P_5yaZr-X_elT4-q5sjz0Gu_g,1156
 asimov/utils.py,sha256=gBrWOMNy1hXrSIx4z-3e3t8aBRjzhqbZCItt28D6kK8,2134
 asimov/cli/__init__.py,sha256=4fD4aGBcYH0pS-RmtxhijztzZsaXOwfvv8odUJVTxcA,395
-asimov/cli/application.py,sha256=CAao5z8cvtNORWJgMW7lBzxbY2d8odMV0HwXMujFi30,4960
+asimov/cli/application.py,sha256=wndJ22V2ay6PBSmwz3MsGyMolShfde7nvg-8wcIXHJo,4948
 asimov/cli/configuration.py,sha256=rXRFzoZaxfTfN9S6y4tT1Bo30cimEey2Lkg0srGeIVk,1141
 asimov/cli/data.py,sha256=iYGRdyhFs4h7nUMTb1G3wWtqJpxixSNfUQF813pHujE,1285
-asimov/cli/event.py,sha256=wZWNUWx-MADsAgFJTypkTaDqfZK3JbEs4VMotjQVIaM,9011
+asimov/cli/event.py,sha256=6XPVPuUH1BoJmF5zGXYUEH6Ij0W67gWn4Wglp-P39_8,9010
 asimov/cli/ledger.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-asimov/cli/manage.py,sha256=W6-g-dlDVHsp3fFfr_5ZcOKLwl5TG1Lm0wZUpAYNqQo,10291
-asimov/cli/monitor.py,sha256=fjHwKg1Y4W-oHILdvO_ECW2C8kJR1RQS3hAbeZ8TCzM,15526
+asimov/cli/manage.py,sha256=6f9N95yxaGzU9IJr2GdqHz5dX3SLdRA3uL8D9LBRCG8,10312
+asimov/cli/monitor.py,sha256=nNBrnPMwkZK8qlwOLBwxbAe5T3C6WIK1AgQi2uP0Vdk,14640
 asimov/cli/production.py,sha256=8wysaQimcKOhEJFiHdWetk6U6VB37NItUbQ7a-SNccA,5606
-asimov/cli/project.py,sha256=-pOI7EkBvC-i03F5FaHRKeUhK7XmSm-7IS9RUgfkpbg,6947
+asimov/cli/project.py,sha256=U73sMCOSQ3A0N3mcwEBYMMHfl-lpdt6xSe889Q5-GS8,6016
 asimov/cli/report.py,sha256=Upm1JZFNSnMde-iiMMXYMl-i0GIoUF_dtRN5WtBrL-w,5225
 asimov/cli/review.py,sha256=gMT5iGNf4au32mFaoDI70F2yQ3f-we0QaUN3GIlT9zk,2817
 asimov/cli/report-theme/body.html,sha256=_EO2Ce1k6LkoL4t-dhhk7RZ6FPpIlSNO_p0J8EEiycs,96
 asimov/cli/report-theme/footer.html,sha256=DhOISyuw80kVSTy5xy84xIbV-CzLEqYsoZVfwZEiGMc,247
 asimov/cli/report-theme/head.html,sha256=3NZAIpIlraJAx8TsXTfa6VRycLq6JyRiun2TGjdfu64,1467
 asimov/cli/report-theme/header.html,sha256=KRo_10q0HIdXXlM8DYCZqw-JpMf3uFithQRun8qYMJE,841
 asimov/configs/README.rst,sha256=Dyk7Rkf8KZ5hL7dcFOye2EoZjn_dG7RARB7t67u7pk8,220
 asimov/configs/bayeswave.ini,sha256=Lhp7Ud5lLBqiymmNTBRGGAT5bUHaAse7SnMJ-fpgo0M,2903
-asimov/configs/bilby.ini,sha256=YB-GmAG_dqsr_fA-8k1BbNt-rG3teZp5n_3OZY0ulVk,8197
+asimov/configs/bilby.ini,sha256=j0M2DFysY4tHmG9_uQXRS3LVTOZVEi2vGg2MX9ZhvBo,12328
 asimov/configs/lalinference.ini,sha256=HdvRkaRqYC4Lj9kk0uTvEtV92iWovAD75BIbSE0F0cY,6238
-asimov/configs/rift.ini,sha256=laboJEXoob09TRSVppvxfUfgRoH06zcCbiiAMwLXS7Q,7718
+asimov/configs/rift.ini,sha256=Lkpwgslv_fTK6t-snjYVo1eDkl55a8Ht_gFPHVlgtL8,7569
 asimov/pipelines/__init__.py,sha256=n3EJ7BRwjio1hqdLiDA4g9eAV5k9O_YZXiPjKshTtZs,610
-asimov/pipelines/bayeswave.py,sha256=Yhh25zwqT2AR6QRRZQmdjLWmAuc79Wpuw4Xs_bPZD8E,20083
-asimov/pipelines/bilby.py,sha256=zkr8OO-PUkSWuvcKMlNVIvE6hZsIPkhXF9JvaasfmyQ,15790
+asimov/pipelines/bayeswave.py,sha256=nUeI8TIaLryd_a2bx4KIOUXATgd1HE4cgRiOMwtpE6E,19216
+asimov/pipelines/bilby.py,sha256=zhvS2LEOR7Fhveg3jkPx8Pl6HiT5eNdf-293haPK-PU,12809
 asimov/pipelines/lalinference.py,sha256=bi3k9fcj4K5Vr2HiNV6xS7RSHuTtJ6FJW9cd1G2eyzA,9489
-asimov/pipelines/rift.py,sha256=OZIVdIPmh4P2jNrfJpEmzG1yOE2i7yRdXPzpGApzf9U,17453
-asimov/priors/bbh.prior.template,sha256=B4-CkVVWs02ktI-sij3nkX0w_XU_Wb1S43KQWA29vQI,2844
-asimov/report-theme/body.html,sha256=_EO2Ce1k6LkoL4t-dhhk7RZ6FPpIlSNO_p0J8EEiycs,96
-asimov/report-theme/footer.html,sha256=DhOISyuw80kVSTy5xy84xIbV-CzLEqYsoZVfwZEiGMc,247
-asimov/report-theme/head.html,sha256=3NZAIpIlraJAx8TsXTfa6VRycLq6JyRiun2TGjdfu64,1467
-asimov/report-theme/header.html,sha256=KRo_10q0HIdXXlM8DYCZqw-JpMf3uFithQRun8qYMJE,841
-asimov-0.6.0a1.dist-info/LICENSE,sha256=3OWyNSDNgd5tDbORK72QyJEUBMBVwVUjtcZppOdfrFM,755
-asimov-0.6.0a1.dist-info/METADATA,sha256=guhTxHTzctoy_6NAQU78WdaJ1Bbmd5PbqqhLqDw_Rm8,1392
-asimov-0.6.0a1.dist-info/WHEEL,sha256=bb2Ot9scclHKMOLDEHY6B2sicWOgugjFKaJsT7vwMQo,110
-asimov-0.6.0a1.dist-info/entry_points.txt,sha256=jbTPZwBFpPW3VKNLnjeKb327DPTChpjY4B1IPC3ZDEc,108
-asimov-0.6.0a1.dist-info/top_level.txt,sha256=_R0fwGHM71D6a2YHrKUPX0Z21P8tfW6qn96Xl0MIVHY,7
-asimov-0.6.0a1.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-asimov-0.6.0a1.dist-info/RECORD,,
+asimov/pipelines/rift.py,sha256=HwEijPYMldY9lEL4PHgevZkUgK0HhGc8kNahRRFgnX8,17635
+asimov-0.6.0a2.dist-info/LICENSE,sha256=3OWyNSDNgd5tDbORK72QyJEUBMBVwVUjtcZppOdfrFM,755
+asimov-0.6.0a2.dist-info/METADATA,sha256=ARtGmwLBJ35G6rAICAKfX7PWAUkzQ6PEsoMSEFT5gtw,1392
+asimov-0.6.0a2.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
+asimov-0.6.0a2.dist-info/entry_points.txt,sha256=jbTPZwBFpPW3VKNLnjeKb327DPTChpjY4B1IPC3ZDEc,108
+asimov-0.6.0a2.dist-info/top_level.txt,sha256=_R0fwGHM71D6a2YHrKUPX0Z21P8tfW6qn96Xl0MIVHY,7
+asimov-0.6.0a2.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+asimov-0.6.0a2.dist-info/RECORD,,
```

