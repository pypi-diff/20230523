# Comparing `tmp/dvc-2.9.5.tar.gz` & `tmp/dvc-3.0.0a0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "/home/runner/work/dvc/dvc/dist/tmpylbi2mzu/dvc-2.9.5.tar", last modified: Mon Feb 21 02:27:12 2022, max compression
+gzip compressed data, was "dvc-3.0.0a0.tar", last modified: Tue May 23 04:11:21 2023, max compression
```

## Comparing `dvc-2.9.5.tar` & `dvc-3.0.0a0.tar`

### file list

```diff
@@ -1,661 +1,690 @@
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/
--rw-r--r--   0 runner    (1001) docker     (116)      154 2022-02-21 02:25:05.000000 dvc-2.9.5/.codecov.yml
--rw-r--r--   0 runner    (1001) docker     (116)      263 2022-02-21 02:25:05.000000 dvc-2.9.5/.coveragerc
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/.dvc/
--rw-r--r--   0 runner    (1001) docker     (116)      113 2022-02-21 02:25:05.000000 dvc-2.9.5/.dvc/.gitignore
--rw-r--r--   0 runner    (1001) docker     (116)       70 2022-02-21 02:25:05.000000 dvc-2.9.5/.dvc/config
--rw-r--r--   0 runner    (1001) docker     (116)       23 2022-02-21 02:25:05.000000 dvc-2.9.5/.git_archival.txt
--rw-r--r--   0 runner    (1001) docker     (116)       32 2022-02-21 02:25:05.000000 dvc-2.9.5/.gitattributes
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/.github/
--rw-r--r--   0 runner    (1001) docker     (116)       23 2022-02-21 02:25:05.000000 dvc-2.9.5/.github/CODEOWNERS
--rw-r--r--   0 runner    (1001) docker     (116)       63 2022-02-21 02:25:05.000000 dvc-2.9.5/.github/FUNDING.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/.github/ISSUE_TEMPLATE/
--rw-r--r--   0 runner    (1001) docker     (116)     1296 2022-02-21 02:25:05.000000 dvc-2.9.5/.github/ISSUE_TEMPLATE/bug_report.md
--rw-r--r--   0 runner    (1001) docker     (116)      189 2022-02-21 02:25:05.000000 dvc-2.9.5/.github/ISSUE_TEMPLATE/config.yml
--rw-r--r--   0 runner    (1001) docker     (116)       84 2022-02-21 02:25:05.000000 dvc-2.9.5/.github/ISSUE_TEMPLATE/feature_request.md
--rw-r--r--   0 runner    (1001) docker     (116)      395 2022-02-21 02:25:05.000000 dvc-2.9.5/.github/PULL_REQUEST_TEMPLATE.md
--rw-r--r--   0 runner    (1001) docker     (116)      273 2022-02-21 02:25:05.000000 dvc-2.9.5/.github/dependabot.yml
--rw-r--r--   0 runner    (1001) docker     (116)      183 2022-02-21 02:25:05.000000 dvc-2.9.5/.github/mergify.yml
--rw-r--r--   0 runner    (1001) docker     (116)     1020 2022-02-21 02:25:05.000000 dvc-2.9.5/.github/release-drafter.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/.github/workflows/
--rw-r--r--   0 runner    (1001) docker     (116)      554 2022-02-21 02:25:05.000000 dvc-2.9.5/.github/workflows/benchmarks.yaml
--rw-r--r--   0 runner    (1001) docker     (116)     4786 2022-02-21 02:25:05.000000 dvc-2.9.5/.github/workflows/packages.yaml
--rw-r--r--   0 runner    (1001) docker     (116)     2533 2022-02-21 02:25:05.000000 dvc-2.9.5/.github/workflows/tests.yaml
--rw-r--r--   0 runner    (1001) docker     (116)      359 2022-02-21 02:25:05.000000 dvc-2.9.5/.gitignore
--rw-r--r--   0 runner    (1001) docker     (116)      238 2022-02-21 02:25:05.000000 dvc-2.9.5/.mailmap
--rw-r--r--   0 runner    (1001) docker     (116)     2110 2022-02-21 02:25:05.000000 dvc-2.9.5/.pre-commit-config.yaml
--rw-r--r--   0 runner    (1001) docker     (116)      631 2022-02-21 02:25:05.000000 dvc-2.9.5/.pre-commit-hooks.yaml
--rw-r--r--   0 runner    (1001) docker     (116)      322 2022-02-21 02:25:05.000000 dvc-2.9.5/.zenodo.json
--rw-r--r--   0 runner    (1001) docker     (116)     3348 2022-02-21 02:25:05.000000 dvc-2.9.5/CODE_OF_CONDUCT.md
--rw-r--r--   0 runner    (1001) docker     (116)       95 2022-02-21 02:25:05.000000 dvc-2.9.5/CONTRIBUTING.md
--rw-r--r--   0 runner    (1001) docker     (116)    11350 2022-02-21 02:25:05.000000 dvc-2.9.5/LICENSE
--rw-r--r--   0 runner    (1001) docker     (116)      103 2022-02-21 02:25:05.000000 dvc-2.9.5/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (116)    13942 2022-02-21 02:27:12.000000 dvc-2.9.5/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (116)    12661 2022-02-21 02:25:05.000000 dvc-2.9.5/README.rst
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/bin/
--rwxr-xr-x   0 runner    (1001) docker     (116)       51 2022-02-21 02:25:05.000000 dvc-2.9.5/bin/dvc
--rw-r--r--   0 runner    (1001) docker     (116)       53 2022-02-21 02:25:05.000000 dvc-2.9.5/bin/dvc.bat
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/
--rw-r--r--   0 runner    (1001) docker     (116)      165 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      148 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/__main__.py
--rw-r--r--   0 runner    (1001) docker     (116)     4365 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/_debug.py
--rw-r--r--   0 runner    (1001) docker     (116)      142 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/_dvc_version.py
--rw-r--r--   0 runner    (1001) docker     (116)     5091 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/analytics.py
--rw-r--r--   0 runner    (1001) docker     (116)     4122 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/api.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/cli/
--rw-r--r--   0 runner    (1001) docker     (116)     3649 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/cli/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      697 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/cli/command.py
--rw-r--r--   0 runner    (1001) docker     (116)     4943 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/cli/parser.py
--rw-r--r--   0 runner    (1001) docker     (116)      954 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/cli/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/commands/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     3453 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/add.py
--rw-r--r--   0 runner    (1001) docker     (116)     2554 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/cache.py
--rw-r--r--   0 runner    (1001) docker     (116)     3708 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/check_ignore.py
--rw-r--r--   0 runner    (1001) docker     (116)     3029 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/checkout.py
--rw-r--r--   0 runner    (1001) docker     (116)     2211 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/commit.py
--rw-r--r--   0 runner    (1001) docker     (116)     3031 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/completion.py
--rw-r--r--   0 runner    (1001) docker     (116)     7055 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/config.py
--rw-r--r--   0 runner    (1001) docker     (116)     2078 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/daemon.py
--rw-r--r--   0 runner    (1001) docker     (116)     3663 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/dag.py
--rw-r--r--   0 runner    (1001) docker     (116)    11637 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/data_sync.py
--rw-r--r--   0 runner    (1001) docker     (116)     1514 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/destroy.py
--rw-r--r--   0 runner    (1001) docker     (116)     6419 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/diff.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/commands/experiments/
--rw-r--r--   0 runner    (1001) docker     (116)     1200 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/experiments/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1221 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/experiments/apply.py
--rw-r--r--   0 runner    (1001) docker     (116)     1006 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/experiments/branch.py
--rw-r--r--   0 runner    (1001) docker     (116)     3754 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/experiments/diff.py
--rw-r--r--   0 runner    (1001) docker     (116)     1112 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/experiments/exec_run.py
--rw-r--r--   0 runner    (1001) docker     (116)     3996 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/experiments/gc.py
--rw-r--r--   0 runner    (1001) docker     (116)     5656 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/experiments/init.py
--rw-r--r--   0 runner    (1001) docker     (116)     2352 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/experiments/ls.py
--rw-r--r--   0 runner    (1001) docker     (116)     2787 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/experiments/pull.py
--rw-r--r--   0 runner    (1001) docker     (116)     2869 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/experiments/push.py
--rw-r--r--   0 runner    (1001) docker     (116)     1593 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/experiments/remove.py
--rw-r--r--   0 runner    (1001) docker     (116)     4331 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/experiments/run.py
--rw-r--r--   0 runner    (1001) docker     (116)    19727 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/experiments/show.py
--rw-r--r--   0 runner    (1001) docker     (116)     1780 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/freeze.py
--rw-r--r--   0 runner    (1001) docker     (116)     4501 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/gc.py
--rw-r--r--   0 runner    (1001) docker     (116)     2753 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/get.py
--rw-r--r--   0 runner    (1001) docker     (116)     1451 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/get_url.py
--rw-r--r--   0 runner    (1001) docker     (116)     4469 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/git_hook.py
--rw-r--r--   0 runner    (1001) docker     (116)     2792 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/imp.py
--rw-r--r--   0 runner    (1001) docker     (116)     3269 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/imp_url.py
--rw-r--r--   0 runner    (1001) docker     (116)     2928 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/init.py
--rw-r--r--   0 runner    (1001) docker     (116)     1083 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/install.py
--rw-r--r--   0 runner    (1001) docker     (116)     2776 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/live.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/commands/ls/
--rw-r--r--   0 runner    (1001) docker     (116)     2491 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/ls/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1348 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/ls/ls_colors.py
--rw-r--r--   0 runner    (1001) docker     (116)    17045 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/machine.py
--rw-r--r--   0 runner    (1001) docker     (116)     7508 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/metrics.py
--rw-r--r--   0 runner    (1001) docker     (116)     1392 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/move.py
--rw-r--r--   0 runner    (1001) docker     (116)     3667 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/params.py
--rw-r--r--   0 runner    (1001) docker     (116)    11089 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/plots.py
--rw-r--r--   0 runner    (1001) docker     (116)     9583 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/remote.py
--rw-r--r--   0 runner    (1001) docker     (116)     1278 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/remove.py
--rw-r--r--   0 runner    (1001) docker     (116)     4962 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/repro.py
--rw-r--r--   0 runner    (1001) docker     (116)      762 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/root.py
--rw-r--r--   0 runner    (1001) docker     (116)     3005 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/run.py
--rw-r--r--   0 runner    (1001) docker     (116)    10436 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/stage.py
--rw-r--r--   0 runner    (1001) docker     (116)     2787 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/status.py
--rw-r--r--   0 runner    (1001) docker     (116)     1214 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/unprotect.py
--rw-r--r--   0 runner    (1001) docker     (116)     2218 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/update.py
--rw-r--r--   0 runner    (1001) docker     (116)      908 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/commands/version.py
--rw-r--r--   0 runner    (1001) docker     (116)    13694 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/compare.py
--rw-r--r--   0 runner    (1001) docker     (116)     9721 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/config.py
--rw-r--r--   0 runner    (1001) docker     (116)     9040 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/config_schema.py
--rw-r--r--   0 runner    (1001) docker     (116)     3003 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/daemon.py
--rw-r--r--   0 runner    (1001) docker     (116)     7955 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/dagascii.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/data/
--rw-r--r--   0 runner    (1001) docker     (116)      814 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     6366 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/checkout.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/data/db/
--rw-r--r--   0 runner    (1001) docker     (116)     2365 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/db/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     3750 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/db/index.py
--rw-r--r--   0 runner    (1001) docker     (116)     5481 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/db/local.py
--rw-r--r--   0 runner    (1001) docker     (116)     2750 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/db/reference.py
--rw-r--r--   0 runner    (1001) docker     (116)     3254 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/diff.py
--rw-r--r--   0 runner    (1001) docker     (116)     1255 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/gc.py
--rw-r--r--   0 runner    (1001) docker     (116)     1224 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/meta.py
--rw-r--r--   0 runner    (1001) docker     (116)     3796 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/reference.py
--rw-r--r--   0 runner    (1001) docker     (116)     1171 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/slow_link_detection.py
--rw-r--r--   0 runner    (1001) docker     (116)    10372 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/stage.py
--rw-r--r--   0 runner    (1001) docker     (116)     6138 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/status.py
--rw-r--r--   0 runner    (1001) docker     (116)     5455 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/transfer.py
--rw-r--r--   0 runner    (1001) docker     (116)     7551 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data/tree.py
--rw-r--r--   0 runner    (1001) docker     (116)     4932 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/data_cloud.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/dependency/
--rw-r--r--   0 runner    (1001) docker     (116)     2264 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/dependency/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      944 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/dependency/base.py
--rw-r--r--   0 runner    (1001) docker     (116)     5015 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/dependency/param.py
--rw-r--r--   0 runner    (1001) docker     (116)     6830 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/dependency/repo.py
--rw-r--r--   0 runner    (1001) docker     (116)    12662 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/dvcfile.py
--rw-r--r--   0 runner    (1001) docker     (116)      396 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/env.py
--rw-r--r--   0 runner    (1001) docker     (116)     9844 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (116)     8290 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/external_repo.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/fs/
--rw-r--r--   0 runner    (1001) docker     (116)     3927 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1277 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/_callback.py
--rw-r--r--   0 runner    (1001) docker     (116)     5712 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/azure.py
--rw-r--r--   0 runner    (1001) docker     (116)    10916 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/base.py
--rw-r--r--   0 runner    (1001) docker     (116)     6083 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/dvc.py
--rw-r--r--   0 runner    (1001) docker     (116)     7857 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/fsspec_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (116)     9731 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/gdrive.py
--rw-r--r--   0 runner    (1001) docker     (116)     1275 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/git.py
--rw-r--r--   0 runner    (1001) docker     (116)      720 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/gs.py
--rw-r--r--   0 runner    (1001) docker     (116)     2882 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/hdfs.py
--rw-r--r--   0 runner    (1001) docker     (116)     4581 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/http.py
--rw-r--r--   0 runner    (1001) docker     (116)      167 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/https.py
--rw-r--r--   0 runner    (1001) docker     (116)     5037 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/local.py
--rw-r--r--   0 runner    (1001) docker     (116)      901 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/memory.py
--rw-r--r--   0 runner    (1001) docker     (116)      985 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/oss.py
--rw-r--r--   0 runner    (1001) docker     (116)     2408 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/path.py
--rw-r--r--   0 runner    (1001) docker     (116)     1354 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/pool.py
--rw-r--r--   0 runner    (1001) docker     (116)    15947 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/repo.py
--rw-r--r--   0 runner    (1001) docker     (116)     7785 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/s3.py
--rw-r--r--   0 runner    (1001) docker     (116)     4364 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/ssh.py
--rw-r--r--   0 runner    (1001) docker     (116)     4547 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/utils.py
--rw-r--r--   0 runner    (1001) docker     (116)     3116 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/webdav.py
--rw-r--r--   0 runner    (1001) docker     (116)     1575 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/fs/webhdfs.py
--rw-r--r--   0 runner    (1001) docker     (116)      923 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/hash_info.py
--rw-r--r--   0 runner    (1001) docker     (116)    13236 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/ignore.py
--rw-r--r--   0 runner    (1001) docker     (116)     4145 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/info.py
--rw-r--r--   0 runner    (1001) docker     (116)     1134 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/istextfile.py
--rw-r--r--   0 runner    (1001) docker     (116)     5016 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/lock.py
--rw-r--r--   0 runner    (1001) docker     (116)     8845 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/logger.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/machine/
--rw-r--r--   0 runner    (1001) docker     (116)     6738 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/machine/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/machine/backend/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/machine/backend/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1611 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/machine/backend/base.py
--rw-r--r--   0 runner    (1001) docker     (116)     3288 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/machine/backend/terraform.py
--rw-r--r--   0 runner    (1001) docker     (116)      122 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/main.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/objects/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/objects/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    16475 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/objects/db.py
--rw-r--r--   0 runner    (1001) docker     (116)      236 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/objects/errors.py
--rw-r--r--   0 runner    (1001) docker     (116)     2186 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/objects/file.py
--rw-r--r--   0 runner    (1001) docker     (116)    30231 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/output.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/parsing/
--rw-r--r--   0 runner    (1001) docker     (116)    15258 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/parsing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    16710 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/parsing/context.py
--rw-r--r--   0 runner    (1001) docker     (116)     4451 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/parsing/interpolate.py
--rw-r--r--   0 runner    (1001) docker     (116)     1158 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/parsing/versions.py
--rw-r--r--   0 runner    (1001) docker     (116)     2598 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/pathspec_math.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/proc/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/proc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      705 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/proc/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (116)     4964 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/proc/manager.py
--rw-r--r--   0 runner    (1001) docker     (116)     5057 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/proc/process.py
--rw-r--r--   0 runner    (1001) docker     (116)     5007 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/progress.py
--rw-r--r--   0 runner    (1001) docker     (116)     1257 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/prompt.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/render/
--rw-r--r--   0 runner    (1001) docker     (116)      107 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/render/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1720 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/render/base.py
--rw-r--r--   0 runner    (1001) docker     (116)     5803 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/render/data.py
--rw-r--r--   0 runner    (1001) docker     (116)     3207 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/render/html.py
--rw-r--r--   0 runner    (1001) docker     (116)     3073 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/render/image.py
--rw-r--r--   0 runner    (1001) docker     (116)     3010 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/render/plotly.py
--rw-r--r--   0 runner    (1001) docker     (116)     2004 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/render/utils.py
--rw-r--r--   0 runner    (1001) docker     (116)     3187 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/render/vega.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/repo/
--rw-r--r--   0 runner    (1001) docker     (116)    15551 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     8038 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/add.py
--rw-r--r--   0 runner    (1001) docker     (116)     1840 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/brancher.py
--rw-r--r--   0 runner    (1001) docker     (116)     2853 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/checkout.py
--rw-r--r--   0 runner    (1001) docker     (116)     2313 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/collect.py
--rw-r--r--   0 runner    (1001) docker     (116)     1860 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/commit.py
--rw-r--r--   0 runner    (1001) docker     (116)      519 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/destroy.py
--rw-r--r--   0 runner    (1001) docker     (116)     7102 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/diff.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/repo/experiments/
--rw-r--r--   0 runner    (1001) docker     (116)    29474 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     2541 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/apply.py
--rw-r--r--   0 runner    (1001) docker     (116)     5082 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/base.py
--rw-r--r--   0 runner    (1001) docker     (116)     1870 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/branch.py
--rw-r--r--   0 runner    (1001) docker     (116)     1131 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/diff.py
--rw-r--r--   0 runner    (1001) docker     (116)     1295 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/exceptions.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/repo/experiments/executor/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/executor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    21717 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/executor/base.py
--rw-r--r--   0 runner    (1001) docker     (116)     5301 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/executor/local.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/repo/experiments/executor/manager/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/executor/manager/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     9094 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/executor/manager/base.py
--rw-r--r--   0 runner    (1001) docker     (116)     3433 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/executor/manager/local.py
--rw-r--r--   0 runner    (1001) docker     (116)     3335 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/executor/manager/ssh.py
--rw-r--r--   0 runner    (1001) docker     (116)     9213 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/executor/ssh.py
--rw-r--r--   0 runner    (1001) docker     (116)     1232 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/gc.py
--rw-r--r--   0 runner    (1001) docker     (116)    10101 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/init.py
--rw-r--r--   0 runner    (1001) docker     (116)     1276 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/ls.py
--rw-r--r--   0 runner    (1001) docker     (116)     2256 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/pull.py
--rw-r--r--   0 runner    (1001) docker     (116)     2234 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/push.py
--rw-r--r--   0 runner    (1001) docker     (116)     3074 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/remove.py
--rw-r--r--   0 runner    (1001) docker     (116)      867 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/run.py
--rw-r--r--   0 runner    (1001) docker     (116)     6259 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/show.py
--rw-r--r--   0 runner    (1001) docker     (116)     6332 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/experiments/utils.py
--rw-r--r--   0 runner    (1001) docker     (116)     2489 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/fetch.py
--rw-r--r--   0 runner    (1001) docker     (116)      401 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/freeze.py
--rw-r--r--   0 runner    (1001) docker     (116)     2334 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/gc.py
--rw-r--r--   0 runner    (1001) docker     (116)     1918 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/get.py
--rw-r--r--   0 runner    (1001) docker     (116)      470 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/get_url.py
--rw-r--r--   0 runner    (1001) docker     (116)     4286 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/graph.py
--rw-r--r--   0 runner    (1001) docker     (116)      247 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/imp.py
--rw-r--r--   0 runner    (1001) docker     (116)     2068 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/imp_url.py
--rw-r--r--   0 runner    (1001) docker     (116)     9796 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/index.py
--rw-r--r--   0 runner    (1001) docker     (116)     2554 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/init.py
--rw-r--r--   0 runner    (1001) docker     (116)     2349 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/install.py
--rw-r--r--   0 runner    (1001) docker     (116)     1557 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/live.py
--rw-r--r--   0 runner    (1001) docker     (116)     2316 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/ls.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/repo/metrics/
--rw-r--r--   0 runner    (1001) docker     (116)      322 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      562 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/metrics/diff.py
--rw-r--r--   0 runner    (1001) docker     (116)     3481 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/metrics/show.py
--rw-r--r--   0 runner    (1001) docker     (116)     2206 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/move.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/repo/params/
--rw-r--r--   0 runner    (1001) docker     (116)      305 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/params/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      559 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/params/diff.py
--rw-r--r--   0 runner    (1001) docker     (116)     4301 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/params/show.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/repo/plots/
--rw-r--r--   0 runner    (1001) docker     (116)     8754 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/plots/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      580 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/plots/diff.py
--rw-r--r--   0 runner    (1001) docker     (116)    20092 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/plots/template.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/repo/plots/templates/
--rw-r--r--   0 runner    (1001) docker     (116)     3000 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/plots/templates/confusion.json
--rw-r--r--   0 runner    (1001) docker     (116)     3145 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/plots/templates/confusion_normalized.json
--rw-r--r--   0 runner    (1001) docker     (116)     3655 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/plots/templates/linear.json
--rw-r--r--   0 runner    (1001) docker     (116)     3266 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/plots/templates/scatter.json
--rw-r--r--   0 runner    (1001) docker     (116)      714 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/plots/templates/simple.json
--rw-r--r--   0 runner    (1001) docker     (116)      889 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/plots/templates/smooth.json
--rw-r--r--   0 runner    (1001) docker     (116)     1144 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/pull.py
--rw-r--r--   0 runner    (1001) docker     (116)     1889 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/push.py
--rw-r--r--   0 runner    (1001) docker     (116)      354 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/remove.py
--rw-r--r--   0 runner    (1001) docker     (116)     8369 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/reproduce.py
--rw-r--r--   0 runner    (1001) docker     (116)      758 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/run.py
--rw-r--r--   0 runner    (1001) docker     (116)     4619 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/scm_context.py
--rw-r--r--   0 runner    (1001) docker     (116)    16732 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/stage.py
--rw-r--r--   0 runner    (1001) docker     (116)     3574 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/status.py
--rw-r--r--   0 runner    (1001) docker     (116)     1456 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/trie.py
--rw-r--r--   0 runner    (1001) docker     (116)      830 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/repo/update.py
--rw-r--r--   0 runner    (1001) docker     (116)     4796 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/rwlock.py
--rw-r--r--   0 runner    (1001) docker     (116)     4049 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/schema.py
--rw-r--r--   0 runner    (1001) docker     (116)      285 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/scheme.py
--rw-r--r--   0 runner    (1001) docker     (116)     5175 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/scm.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/stage/
--rw-r--r--   0 runner    (1001) docker     (116)    21857 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/stage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     8387 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/stage/cache.py
--rw-r--r--   0 runner    (1001) docker     (116)     1516 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/stage/decorators.py
--rw-r--r--   0 runner    (1001) docker     (116)     2650 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/stage/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (116)     1293 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/stage/imports.py
--rw-r--r--   0 runner    (1001) docker     (116)     6851 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/stage/loader.py
--rw-r--r--   0 runner    (1001) docker     (116)     4362 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/stage/monitor.py
--rw-r--r--   0 runner    (1001) docker     (116)      417 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/stage/params.py
--rw-r--r--   0 runner    (1001) docker     (116)     4798 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/stage/run.py
--rw-r--r--   0 runner    (1001) docker     (116)     6522 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/stage/serialize.py
--rw-r--r--   0 runner    (1001) docker     (116)     9456 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/stage/utils.py
--rw-r--r--   0 runner    (1001) docker     (116)     5033 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/state.py
--rw-r--r--   0 runner    (1001) docker     (116)     3816 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/system.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/testing/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/testing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     2017 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/testing/cloud.py
--rw-r--r--   0 runner    (1001) docker     (116)       65 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/testing/conftest.py
--rw-r--r--   0 runner    (1001) docker     (116)     6049 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/testing/fixtures.py
--rw-r--r--   0 runner    (1001) docker     (116)    10002 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/testing/path_info.py
--rw-r--r--   0 runner    (1001) docker     (116)      554 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/testing/test_api.py
--rw-r--r--   0 runner    (1001) docker     (116)     2945 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/testing/test_remote.py
--rw-r--r--   0 runner    (1001) docker     (116)     4166 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/testing/test_workspace.py
--rw-r--r--   0 runner    (1001) docker     (116)     8595 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/testing/tmp_dir.py
--rw-r--r--   0 runner    (1001) docker     (116)      295 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/types.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/ui/
--rw-r--r--   0 runner    (1001) docker     (116)     8269 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/ui/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     2414 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/ui/pager.py
--rw-r--r--   0 runner    (1001) docker     (116)     5577 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/ui/prompt.py
--rw-r--r--   0 runner    (1001) docker     (116)     3200 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/ui/table.py
--rw-r--r--   0 runner    (1001) docker     (116)     5458 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/updater.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/utils/
--rw-r--r--   0 runner    (1001) docker     (116)       10 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/.gitignore
--rw-r--r--   0 runner    (1001) docker     (116)    14335 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      844 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/_benedict.py
--rw-r--r--   0 runner    (1001) docker     (116)       12 2022-02-21 02:27:00.000000 dvc-2.9.5/dvc/utils/build.py
--rw-r--r--   0 runner    (1001) docker     (116)     1804 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/cli_parse.py
--rw-r--r--   0 runner    (1001) docker     (116)     5267 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/collections.py
--rw-r--r--   0 runner    (1001) docker     (116)      613 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/conversions.py
--rw-r--r--   0 runner    (1001) docker     (116)      813 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/decorators.py
--rw-r--r--   0 runner    (1001) docker     (116)     2384 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/diff.py
--rw-r--r--   0 runner    (1001) docker     (116)      189 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/flatten.py
--rw-r--r--   0 runner    (1001) docker     (116)     6839 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/fs.py
--rw-r--r--   0 runner    (1001) docker     (116)      787 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/humanize.py
--rw-r--r--   0 runner    (1001) docker     (116)      172 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/pkg.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc/utils/serialize/
--rw-r--r--   0 runner    (1001) docker     (116)     1042 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/serialize/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     2310 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/serialize/_common.py
--rw-r--r--   0 runner    (1001) docker     (116)      895 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/serialize/_json.py
--rw-r--r--   0 runner    (1001) docker     (116)     5235 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/serialize/_py.py
--rw-r--r--   0 runner    (1001) docker     (116)     1344 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/serialize/_toml.py
--rw-r--r--   0 runner    (1001) docker     (116)     1922 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/serialize/_yaml.py
--rw-r--r--   0 runner    (1001) docker     (116)     1148 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/stream.py
--rw-r--r--   0 runner    (1001) docker     (116)     9295 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/strictyaml.py
--rw-r--r--   0 runner    (1001) docker     (116)     2824 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/table.py
--rw-r--r--   0 runner    (1001) docker     (116)     1046 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/utils/threadpool.py
--rw-r--r--   0 runner    (1001) docker     (116)      196 2022-02-21 02:25:05.000000 dvc-2.9.5/dvc/version.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc.egg-info/
--rw-r--r--   0 runner    (1001) docker     (116)    13942 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (116)    15393 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (116)        1 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (116)       37 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (116)        1 2022-02-21 02:26:22.000000 dvc-2.9.5/dvc.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (116)     3019 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (116)        4 2022-02-21 02:27:12.000000 dvc-2.9.5/dvc.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (116)     2168 2022-02-21 02:25:05.000000 dvc-2.9.5/pyproject.toml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/scripts/
--rw-r--r--   0 runner    (1001) docker     (116)       17 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/build-requirements.txt
--rw-r--r--   0 runner    (1001) docker     (116)     2912 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/build.py
--rwxr-xr-x   0 runner    (1001) docker     (116)      257 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/build_package.sh
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/scripts/fpm/
--rw-r--r--   0 runner    (1001) docker     (116)       28 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/fpm/.gitignore
--rw-r--r--   0 runner    (1001) docker     (116)       59 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/fpm/after-install.sh
--rw-r--r--   0 runner    (1001) docker     (116)       36 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/fpm/after-remove.sh
--rw-r--r--   0 runner    (1001) docker     (116)     2260 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/fpm/build.py
--rw-r--r--   0 runner    (1001) docker     (116)     1403 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/fpm/notarize.py
--rw-r--r--   0 runner    (1001) docker     (116)     1087 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/fpm/sign.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/scripts/innosetup/
--rw-r--r--   0 runner    (1001) docker     (116)       64 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/innosetup/.gitignore
--rw-r--r--   0 runner    (1001) docker     (116)     2686 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/innosetup/addSymLinkPermissions.ps1
--rw-r--r--   0 runner    (1001) docker     (116)     1153 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/innosetup/addsymlink.iss
--rw-r--r--   0 runner    (1001) docker     (116)      934 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/innosetup/build.py
--rw-r--r--   0 runner    (1001) docker     (116)       62 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/innosetup/dvc.ico.dvc
--rw-r--r--   0 runner    (1001) docker     (116)       67 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/innosetup/dvc_left.bmp.dvc
--rw-r--r--   0 runner    (1001) docker     (116)       65 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/innosetup/dvc_up.bmp.dvc
--rw-r--r--   0 runner    (1001) docker     (116)     6681 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/innosetup/modpath.iss
--rw-r--r--   0 runner    (1001) docker     (116)     2324 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/innosetup/setup.iss
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/scripts/pyinstaller/
--rw-r--r--   0 runner    (1001) docker     (116)       28 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/pyinstaller/.gitignore
--rw-r--r--   0 runner    (1001) docker     (116)      521 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/pyinstaller/build.py
--rw-r--r--   0 runner    (1001) docker     (116)      380 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/pyinstaller/entitlements.plist
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/scripts/pyinstaller/hooks/
--rw-r--r--   0 runner    (1001) docker     (116)      662 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/pyinstaller/hooks/hook-dvc.py
--rw-r--r--   0 runner    (1001) docker     (116)       65 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/pyinstaller/hooks/hook-dvc.system.py
--rw-r--r--   0 runner    (1001) docker     (116)      353 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/pyinstaller/hooks/hook-dvc.tree.gs.py
--rw-r--r--   0 runner    (1001) docker     (116)      129 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/pyinstaller/hooks/hook-dvc.utils.flatten.py
--rw-r--r--   0 runner    (1001) docker     (116)      178 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/pyinstaller/hooks/hook-google_compute_engine.logger.py
--rw-r--r--   0 runner    (1001) docker     (116)      292 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/pyinstaller/hooks/hook-googleapiclient.model.py
--rw-r--r--   0 runner    (1001) docker     (116)      176 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/pyinstaller/hooks/hook-pydrive2.py
--rw-r--r--   0 runner    (1001) docker     (116)      958 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/pyinstaller/sign.py
--rwxr-xr-x   0 runner    (1001) docker     (116)      257 2022-02-21 02:25:05.000000 dvc-2.9.5/scripts/pypi_upload.sh
--rw-r--r--   0 runner    (1001) docker     (116)     3653 2022-02-21 02:27:12.000000 dvc-2.9.5/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (116)       38 2022-02-21 02:25:05.000000 dvc-2.9.5/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/
--rw-r--r--   0 runner    (1001) docker     (116)     1450 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      584 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/__main__.py
--rw-r--r--   0 runner    (1001) docker     (116)     5364 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/basic_env.py
--rw-r--r--   0 runner    (1001) docker     (116)     6448 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/conftest.py
--rw-r--r--   0 runner    (1001) docker     (116)     4678 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/dir_helpers.py
--rw-r--r--   0 runner    (1001) docker     (116)      293 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/docker-compose.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/func/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      840 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/func/experiments/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/experiments/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     2653 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/experiments/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/func/experiments/executor/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/experiments/executor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     5067 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/experiments/executor/test_ssh.py
--rw-r--r--   0 runner    (1001) docker     (116)     9169 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/experiments/test_checkpoints.py
--rw-r--r--   0 runner    (1001) docker     (116)      999 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/experiments/test_diff.py
--rw-r--r--   0 runner    (1001) docker     (116)    23097 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/experiments/test_experiments.py
--rw-r--r--   0 runner    (1001) docker     (116)     4095 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/experiments/test_gc.py
--rw-r--r--   0 runner    (1001) docker     (116)    13936 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/experiments/test_init.py
--rw-r--r--   0 runner    (1001) docker     (116)    11362 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/experiments/test_remote.py
--rw-r--r--   0 runner    (1001) docker     (116)     4502 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/experiments/test_remove.py
--rw-r--r--   0 runner    (1001) docker     (116)    22212 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/experiments/test_show.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/func/machine/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/machine/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1324 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/machine/conftest.py
--rw-r--r--   0 runner    (1001) docker     (116)     5373 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/machine/test_machine_config.py
--rw-r--r--   0 runner    (1001) docker     (116)      800 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/machine/test_machine_status.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/func/metrics/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     5660 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/metrics/test_diff.py
--rw-r--r--   0 runner    (1001) docker     (116)     8383 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/metrics/test_show.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/func/objects/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/objects/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/func/objects/db/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/objects/db/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     2770 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/objects/db/test_index.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/func/params/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/params/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     6491 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/params/test_diff.py
--rw-r--r--   0 runner    (1001) docker     (116)     4907 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/params/test_show.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/func/parsing/
--rw-r--r--   0 runner    (1001) docker     (116)     1469 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/parsing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     9617 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/parsing/test_errors.py
--rw-r--r--   0 runner    (1001) docker     (116)    15175 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/parsing/test_foreach.py
--rw-r--r--   0 runner    (1001) docker     (116)     7713 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/parsing/test_interpolated_entry.py
--rw-r--r--   0 runner    (1001) docker     (116)     6132 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/parsing/test_resolver.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/func/plots/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/plots/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1441 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/plots/test_diff.py
--rw-r--r--   0 runner    (1001) docker     (116)     2855 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/plots/test_modify.py
--rw-r--r--   0 runner    (1001) docker     (116)     8855 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/plots/test_show.py
--rw-r--r--   0 runner    (1001) docker     (116)    34281 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_add.py
--rw-r--r--   0 runner    (1001) docker     (116)     1724 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_analytics.py
--rw-r--r--   0 runner    (1001) docker     (116)     6432 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_api.py
--rw-r--r--   0 runner    (1001) docker     (116)     4317 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_check_ignore.py
--rw-r--r--   0 runner    (1001) docker     (116)    30043 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_checkout.py
--rw-r--r--   0 runner    (1001) docker     (116)     6429 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_cli.py
--rw-r--r--   0 runner    (1001) docker     (116)     6230 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_commit.py
--rw-r--r--   0 runner    (1001) docker     (116)    10046 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_config.py
--rw-r--r--   0 runner    (1001) docker     (116)    16383 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_data_cloud.py
--rw-r--r--   0 runner    (1001) docker     (116)    17519 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_diff.py
--rw-r--r--   0 runner    (1001) docker     (116)    10871 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_dvcfile.py
--rw-r--r--   0 runner    (1001) docker     (116)     7878 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_external_repo.py
--rw-r--r--   0 runner    (1001) docker     (116)    10858 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_fs.py
--rw-r--r--   0 runner    (1001) docker     (116)    12435 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_gc.py
--rw-r--r--   0 runner    (1001) docker     (116)    10591 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_get.py
--rw-r--r--   0 runner    (1001) docker     (116)     1126 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_get_url.py
--rw-r--r--   0 runner    (1001) docker     (116)    13381 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_ignore.py
--rw-r--r--   0 runner    (1001) docker     (116)    19313 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_import.py
--rw-r--r--   0 runner    (1001) docker     (116)     7698 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_import_url.py
--rw-r--r--   0 runner    (1001) docker     (116)     3209 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_init.py
--rw-r--r--   0 runner    (1001) docker     (116)     5582 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_install.py
--rw-r--r--   0 runner    (1001) docker     (116)     8753 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_live.py
--rw-r--r--   0 runner    (1001) docker     (116)      913 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_lock.py
--rw-r--r--   0 runner    (1001) docker     (116)     7079 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_lockfile.py
--rw-r--r--   0 runner    (1001) docker     (116)    17664 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_ls.py
--rw-r--r--   0 runner    (1001) docker     (116)     5133 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_merge_driver.py
--rw-r--r--   0 runner    (1001) docker     (116)     8297 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_move.py
--rw-r--r--   0 runner    (1001) docker     (116)     6428 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_odb.py
--rw-r--r--   0 runner    (1001) docker     (116)    15556 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_remote.py
--rw-r--r--   0 runner    (1001) docker     (116)     3178 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_remove.py
--rw-r--r--   0 runner    (1001) docker     (116)     1289 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_repo.py
--rw-r--r--   0 runner    (1001) docker     (116)     8788 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_repo_index.py
--rw-r--r--   0 runner    (1001) docker     (116)    37964 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_repro.py
--rw-r--r--   0 runner    (1001) docker     (116)    14978 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_repro_multistage.py
--rw-r--r--   0 runner    (1001) docker     (116)      340 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_root.py
--rw-r--r--   0 runner    (1001) docker     (116)     5766 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_run_cache.py
--rw-r--r--   0 runner    (1001) docker     (116)    11561 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_run_multistage.py
--rw-r--r--   0 runner    (1001) docker     (116)    30285 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_run_single_stage.py
--rw-r--r--   0 runner    (1001) docker     (116)      602 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_scm.py
--rw-r--r--   0 runner    (1001) docker     (116)     1839 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_scm_context.py
--rw-r--r--   0 runner    (1001) docker     (116)     9641 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_stage.py
--rw-r--r--   0 runner    (1001) docker     (116)    15585 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_stage_load.py
--rw-r--r--   0 runner    (1001) docker     (116)     2352 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_state.py
--rw-r--r--   0 runner    (1001) docker     (116)     3941 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_status.py
--rw-r--r--   0 runner    (1001) docker     (116)     1196 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_unprotect.py
--rw-r--r--   0 runner    (1001) docker     (116)    11945 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_update.py
--rw-r--r--   0 runner    (1001) docker     (116)     1139 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (116)      599 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/test_version.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/func/utils/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      896 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/utils/test_fs.py
--rw-r--r--   0 runner    (1001) docker     (116)     9083 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/func/utils/test_strict_yaml.py
--rw-r--r--   0 runner    (1001) docker     (116)     1412 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/pylint_plugin_disable.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/remotes/
--rw-r--r--   0 runner    (1001) docker     (116)      316 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/remotes/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/remotes/git-init/
--rw-r--r--   0 runner    (1001) docker     (116)       35 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/remotes/git-init/git.sh
--rw-r--r--   0 runner    (1001) docker     (116)     1430 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/remotes/git_server.py
--rw-r--r--   0 runner    (1001) docker     (116)     1679 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/remotes/user.key
--rw-r--r--   0 runner    (1001) docker     (116)      394 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/remotes/user.key.pub
--rw-r--r--   0 runner    (1001) docker     (116)      924 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/remotes_env.sample
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/command/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/command/ls/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/ls/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     3171 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/ls/test_ls.py
--rw-r--r--   0 runner    (1001) docker     (116)     2134 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/ls/test_ls_colors.py
--rw-r--r--   0 runner    (1001) docker     (116)     2781 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_add.py
--rw-r--r--   0 runner    (1001) docker     (116)      674 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_cache.py
--rw-r--r--   0 runner    (1001) docker     (116)     1255 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_checkout.py
--rw-r--r--   0 runner    (1001) docker     (116)     1400 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_compat_flag.py
--rw-r--r--   0 runner    (1001) docker     (116)      889 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_config.py
--rw-r--r--   0 runner    (1001) docker     (116)     4542 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_dag.py
--rw-r--r--   0 runner    (1001) docker     (116)     2741 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_data_sync.py
--rw-r--r--   0 runner    (1001) docker     (116)      604 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_debug.py
--rw-r--r--   0 runner    (1001) docker     (116)    11023 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_diff.py
--rw-r--r--   0 runner    (1001) docker     (116)    24047 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_experiments.py
--rw-r--r--   0 runner    (1001) docker     (116)     1011 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_get.py
--rw-r--r--   0 runner    (1001) docker     (116)      369 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_get_url.py
--rw-r--r--   0 runner    (1001) docker     (116)      563 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_git_hook.py
--rw-r--r--   0 runner    (1001) docker     (116)     1484 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_imp.py
--rw-r--r--   0 runner    (1001) docker     (116)     3590 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_imp_url.py
--rw-r--r--   0 runner    (1001) docker     (116)      961 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_live.py
--rw-r--r--   0 runner    (1001) docker     (116)     4928 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_machine.py
--rw-r--r--   0 runner    (1001) docker     (116)     4240 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_metrics.py
--rw-r--r--   0 runner    (1001) docker     (116)     2045 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_params.py
--rw-r--r--   0 runner    (1001) docker     (116)    10158 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_plots.py
--rw-r--r--   0 runner    (1001) docker     (116)      988 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_repro.py
--rw-r--r--   0 runner    (1001) docker     (116)     4230 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_run.py
--rw-r--r--   0 runner    (1001) docker     (116)     2523 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_stage.py
--rw-r--r--   0 runner    (1001) docker     (116)     3180 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_status.py
--rw-r--r--   0 runner    (1001) docker     (116)     1276 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/command/test_update.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/dependency/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/dependency/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      320 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/dependency/test_dependency.py
--rw-r--r--   0 runner    (1001) docker     (116)     6223 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/dependency/test_params.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/fs/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/fs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     2600 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/fs/test_azure.py
--rw-r--r--   0 runner    (1001) docker     (116)      506 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/fs/test_base.py
--rw-r--r--   0 runner    (1001) docker     (116)     7577 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/fs/test_dvc.py
--rw-r--r--   0 runner    (1001) docker     (116)     1203 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/fs/test_fs.py
--rw-r--r--   0 runner    (1001) docker     (116)      481 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/fs/test_hdfs.py
--rw-r--r--   0 runner    (1001) docker     (116)      733 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/fs/test_path.py
--rw-r--r--   0 runner    (1001) docker     (116)    21502 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/fs/test_repo.py
--rw-r--r--   0 runner    (1001) docker     (116)     4281 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/fs/test_repo_info.py
--rw-r--r--   0 runner    (1001) docker     (116)     3922 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/fs/test_s3.py
--rw-r--r--   0 runner    (1001) docker     (116)     4643 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/fs/test_ssh.py
--rw-r--r--   0 runner    (1001) docker     (116)     1401 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/fs/test_tree.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/machine/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/machine/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1096 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/machine/test_machine.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/objects/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/objects/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/objects/db/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/objects/db/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     3952 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/objects/db/test_local.py
--rw-r--r--   0 runner    (1001) docker     (116)     6260 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/objects/test_tree.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/output/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/output/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     3898 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/output/test_load.py
--rw-r--r--   0 runner    (1001) docker     (116)     2122 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/output/test_local.py
--rw-r--r--   0 runner    (1001) docker     (116)     3345 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/output/test_output.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/proc/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/proc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     3691 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/proc/test_manager.py
--rw-r--r--   0 runner    (1001) docker     (116)     1466 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/proc/test_process.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/remote/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/remote/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     3629 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/remote/test_base.py
--rw-r--r--   0 runner    (1001) docker     (116)     1940 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/remote/test_gdrive.py
--rw-r--r--   0 runner    (1001) docker     (116)     2109 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/remote/test_http.py
--rw-r--r--   0 runner    (1001) docker     (116)     1210 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/remote/test_index.py
--rw-r--r--   0 runner    (1001) docker     (116)      576 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/remote/test_oss.py
--rw-r--r--   0 runner    (1001) docker     (116)     2103 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/remote/test_remote.py
--rw-r--r--   0 runner    (1001) docker     (116)     1324 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/remote/test_slow_link_detection.py
--rw-r--r--   0 runner    (1001) docker     (116)     3323 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/remote/test_webdav.py
--rw-r--r--   0 runner    (1001) docker     (116)     1621 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/remote/test_webhdfs.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/render/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/render/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     4552 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/render/test_data.py
--rw-r--r--   0 runner    (1001) docker     (116)     1095 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/render/test_html.py
--rw-r--r--   0 runner    (1001) docker     (116)     2061 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/render/test_image.py
--rw-r--r--   0 runner    (1001) docker     (116)     4234 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/render/test_parallel_coordinates.py
--rw-r--r--   0 runner    (1001) docker     (116)     2570 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/render/test_render.py
--rw-r--r--   0 runner    (1001) docker     (116)      275 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/render/test_renderer.py
--rw-r--r--   0 runner    (1001) docker     (116)    10726 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/render/test_vega.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/repo/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/repo/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/repo/experiments/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/repo/experiments/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1547 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/repo/experiments/test_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/repo/plots/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/repo/plots/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1456 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/repo/plots/test_diff.py
--rw-r--r--   0 runner    (1001) docker     (116)     2229 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/repo/plots/test_templates.py
--rw-r--r--   0 runner    (1001) docker     (116)     3700 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/repo/test_repo.py
--rw-r--r--   0 runner    (1001) docker     (116)      714 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/repo/test_reproduce.py
--rw-r--r--   0 runner    (1001) docker     (116)      335 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/repo/test_run.py
--rw-r--r--   0 runner    (1001) docker     (116)     4410 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/repo/test_scm_context.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/scm/
--rw-r--r--   0 runner    (1001) docker     (116)     1814 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/scm/test_scm.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/stage/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/stage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     6829 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/stage/test_cache.py
--rw-r--r--   0 runner    (1001) docker     (116)     8582 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/stage/test_loader_pipeline_file.py
--rw-r--r--   0 runner    (1001) docker     (116)      574 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/stage/test_run.py
--rw-r--r--   0 runner    (1001) docker     (116)     5588 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/stage/test_serialize_pipeline_file.py
--rw-r--r--   0 runner    (1001) docker     (116)     7211 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/stage/test_serialize_pipeline_lock.py
--rw-r--r--   0 runner    (1001) docker     (116)     4373 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/stage/test_stage.py
--rw-r--r--   0 runner    (1001) docker     (116)      624 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/stage/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (116)     4309 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_analytics.py
--rw-r--r--   0 runner    (1001) docker     (116)      307 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_collect.py
--rw-r--r--   0 runner    (1001) docker     (116)    14096 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_compare.py
--rw-r--r--   0 runner    (1001) docker     (116)     1914 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_config.py
--rw-r--r--   0 runner    (1001) docker     (116)    12487 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_context.py
--rw-r--r--   0 runner    (1001) docker     (116)     1025 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_daemon.py
--rw-r--r--   0 runner    (1001) docker     (116)     4012 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_dvcfile.py
--rw-r--r--   0 runner    (1001) docker     (116)     2871 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_external_repo.py
--rw-r--r--   0 runner    (1001) docker     (116)     8856 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_ignore.py
--rw-r--r--   0 runner    (1001) docker     (116)      520 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_imports.py
--rw-r--r--   0 runner    (1001) docker     (116)     4299 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_info.py
--rw-r--r--   0 runner    (1001) docker     (116)     3061 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_interpolate.py
--rw-r--r--   0 runner    (1001) docker     (116)     3427 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_lockfile.py
--rw-r--r--   0 runner    (1001) docker     (116)     9107 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_logger.py
--rw-r--r--   0 runner    (1001) docker     (116)      819 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_metrics.py
--rw-r--r--   0 runner    (1001) docker     (116)     1240 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_params.py
--rw-r--r--   0 runner    (1001) docker     (116)     3747 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_pathspec_math.py
--rw-r--r--   0 runner    (1001) docker     (116)      896 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_plots.py
--rw-r--r--   0 runner    (1001) docker     (116)     1418 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_progress.py
--rw-r--r--   0 runner    (1001) docker     (116)      339 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_prompt.py
--rw-r--r--   0 runner    (1001) docker     (116)      354 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_run.py
--rw-r--r--   0 runner    (1001) docker     (116)     2553 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_rwlock.py
--rw-r--r--   0 runner    (1001) docker     (116)    10239 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_tabular_data.py
--rw-r--r--   0 runner    (1001) docker     (116)     5319 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/test_updater.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/ui/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/ui/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1714 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/ui/test_console.py
--rw-r--r--   0 runner    (1001) docker     (116)     2747 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/ui/test_pager.py
--rw-r--r--   0 runner    (1001) docker     (116)     4514 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/ui/test_prompt.py
--rw-r--r--   0 runner    (1001) docker     (116)     4624 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/ui/test_table.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/utils/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/utils/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/unit/utils/serialize/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/utils/serialize/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1221 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/utils/serialize/test_python.py
--rw-r--r--   0 runner    (1001) docker     (116)      629 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/utils/serialize/test_yaml.py
--rw-r--r--   0 runner    (1001) docker     (116)      513 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/utils/test_cli_parse.py
--rw-r--r--   0 runner    (1001) docker     (116)     5888 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/utils/test_collections.py
--rw-r--r--   0 runner    (1001) docker     (116)      728 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/utils/test_conversions.py
--rw-r--r--   0 runner    (1001) docker     (116)     1358 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/utils/test_decorators.py
--rw-r--r--   0 runner    (1001) docker     (116)     7261 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/utils/test_fs.py
--rw-r--r--   0 runner    (1001) docker     (116)     2090 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/utils/test_humanize.py
--rw-r--r--   0 runner    (1001) docker     (116)     2203 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/utils/test_stream.py
--rw-r--r--   0 runner    (1001) docker     (116)     6723 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/unit/utils/test_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-02-21 02:27:12.000000 dvc-2.9.5/tests/utils/
--rw-r--r--   0 runner    (1001) docker     (116)     1913 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      524 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/utils/asserts.py
--rw-r--r--   0 runner    (1001) docker     (116)      749 2022-02-21 02:25:05.000000 dvc-2.9.5/tests/utils/scriptify.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.601779 dvc-3.0.0a0/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.525778 dvc-3.0.0a0/.dvc/
+-rw-r--r--   0 runner    (1001) docker     (122)      113 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.dvc/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (122)       70 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.dvc/config
+-rw-r--r--   0 runner    (1001) docker     (122)       21 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.dvcignore
+-rw-r--r--   0 runner    (1001) docker     (122)      471 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.flake8
+-rw-r--r--   0 runner    (1001) docker     (122)      634 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.git-blame-ignore-revs
+-rw-r--r--   0 runner    (1001) docker     (122)      111 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.git_archival.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       32 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.gitattributes
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.525778 dvc-3.0.0a0/.github/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.525778 dvc-3.0.0a0/.github/ISSUE_TEMPLATE/
+-rw-r--r--   0 runner    (1001) docker     (122)     1295 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/ISSUE_TEMPLATE/bug_report.md
+-rw-r--r--   0 runner    (1001) docker     (122)      189 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/ISSUE_TEMPLATE/config.yml
+-rw-r--r--   0 runner    (1001) docker     (122)     1358 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/ISSUE_TEMPLATE/epic_story.md
+-rw-r--r--   0 runner    (1001) docker     (122)       83 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/ISSUE_TEMPLATE/feature_request.md
+-rw-r--r--   0 runner    (1001) docker     (122)      395 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/PULL_REQUEST_TEMPLATE.md
+-rw-r--r--   0 runner    (1001) docker     (122)      183 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/codecov.yml
+-rw-r--r--   0 runner    (1001) docker     (122)      273 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/dependabot.yml
+-rw-r--r--   0 runner    (1001) docker     (122)      255 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/mergify.yml
+-rw-r--r--   0 runner    (1001) docker     (122)      510 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/release.yml
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.525778 dvc-3.0.0a0/.github/workflows/
+-rw-r--r--   0 runner    (1001) docker     (122)      445 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/workflows/benchmarks.yaml
+-rw-r--r--   0 runner    (1001) docker     (122)      701 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/workflows/codeql.yml
+-rw-r--r--   0 runner    (1001) docker     (122)     3272 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/workflows/packages.yaml
+-rw-r--r--   0 runner    (1001) docker     (122)     1184 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/workflows/plugin_tests.yaml
+-rw-r--r--   0 runner    (1001) docker     (122)     1850 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.github/workflows/tests.yaml
+-rw-r--r--   0 runner    (1001) docker     (122)      424 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (122)      238 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.mailmap
+-rw-r--r--   0 runner    (1001) docker     (122)     2952 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.pre-commit-config.yaml
+-rw-r--r--   0 runner    (1001) docker     (122)      631 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.pre-commit-hooks.yaml
+-rw-r--r--   0 runner    (1001) docker     (122)      322 2023-05-23 04:11:03.000000 dvc-3.0.0a0/.zenodo.json
+-rw-r--r--   0 runner    (1001) docker     (122)     3347 2023-05-23 04:11:03.000000 dvc-3.0.0a0/CODE_OF_CONDUCT.md
+-rw-r--r--   0 runner    (1001) docker     (122)       95 2023-05-23 04:11:03.000000 dvc-3.0.0a0/CONTRIBUTING.md
+-rw-r--r--   0 runner    (1001) docker     (122)    11350 2023-05-23 04:11:03.000000 dvc-3.0.0a0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (122)    14468 2023-05-23 04:11:21.601779 dvc-3.0.0a0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)    13193 2023-05-23 04:11:03.000000 dvc-3.0.0a0/README.rst
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.533778 dvc-3.0.0a0/dvc/
+-rw-r--r--   0 runner    (1001) docker     (122)      180 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      148 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6574 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/_debug.py
+-rw-r--r--   0 runner    (1001) docker     (122)      162 2023-05-23 04:11:21.000000 dvc-3.0.0a0/dvc/_dvc_version.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4115 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/analytics.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1469 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/annotations.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.533778 dvc-3.0.0a0/dvc/api/
+-rw-r--r--   0 runner    (1001) docker     (122)      530 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9452 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/api/data.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4974 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/api/experiments.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1895 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/api/scm.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11606 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/api/show.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2089 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/cachemgr.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.533778 dvc-3.0.0a0/dvc/cli/
+-rw-r--r--   0 runner    (1001) docker     (122)     7647 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      811 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/cli/command.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1940 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/cli/completion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4639 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/cli/parser.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1113 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/cli/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.537778 dvc-3.0.0a0/dvc/commands/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3394 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/add.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2876 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/cache.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3707 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/check_ignore.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3062 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/checkout.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2134 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/commit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1181 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/completion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6939 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2070 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/daemon.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5427 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/dag.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6292 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/data.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11868 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/data_sync.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1514 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/destroy.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6332 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/diff.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.541778 dvc-3.0.0a0/dvc/commands/experiments/
+-rw-r--r--   0 runner    (1001) docker     (122)     2391 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1506 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/apply.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1147 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/branch.py
+-rw-r--r--   0 runner    (1001) docker     (122)      757 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/clean.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3670 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1646 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/exec_run.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1859 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/ls.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3481 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/pull.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5138 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/push.py
+-rw-r--r--   0 runner    (1001) docker     (122)      851 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/queue_worker.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2365 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/remove.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4560 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/run.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2467 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/save.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9597 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/experiments/show.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1778 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/freeze.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6423 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/gc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2911 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/get.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1688 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/get_url.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4514 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/git_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3025 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/imp.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3896 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/imp_url.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2926 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/init.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1111 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/install.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.541778 dvc-3.0.0a0/dvc/commands/ls/
+-rw-r--r--   0 runner    (1001) docker     (122)     2464 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/ls/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1362 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/ls/ls_colors.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1176 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/ls_url.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16883 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/machine.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7415 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1333 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/move.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3616 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/params.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14462 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/plots.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.541778 dvc-3.0.0a0/dvc/commands/queue/
+-rw-r--r--   0 runner    (1001) docker     (122)      856 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/queue/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1202 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/queue/kill.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1478 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/queue/logs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2793 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/queue/remove.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1157 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/queue/start.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1860 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/queue/status.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1214 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/queue/stop.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9506 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/remote.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1262 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/remove.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5245 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/repro.py
+-rw-r--r--   0 runner    (1001) docker     (122)      762 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/root.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2973 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/run.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10561 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/stage.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2749 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/status.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/unprotect.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2533 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/update.py
+-rw-r--r--   0 runner    (1001) docker     (122)      892 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/commands/version.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13223 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/compare.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10154 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10948 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/config_schema.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3950 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/daemon.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7944 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/dagascii.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7287 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/data_cloud.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.541778 dvc-3.0.0a0/dvc/dependency/
+-rw-r--r--   0 runner    (1001) docker     (122)     2962 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/dependency/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2129 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/dependency/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5184 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/dependency/param.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2786 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/dependency/repo.py
+-rw-r--r--   0 runner    (1001) docker     (122)      521 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/dirs.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12574 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/dvcfile.py
+-rw-r--r--   0 runner    (1001) docker     (122)      741 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/env.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10279 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/exceptions.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.541778 dvc-3.0.0a0/dvc/fs/
+-rw-r--r--   0 runner    (1001) docker     (122)     4472 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/fs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2717 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/fs/callbacks.py
+-rw-r--r--   0 runner    (1001) docker     (122)      899 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/fs/data.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13720 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/fs/dvc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1531 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/fs/git.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14832 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/ignore.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4990 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/info.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5489 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/lock.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7867 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/logger.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.541778 dvc-3.0.0a0/dvc/machine/
+-rw-r--r--   0 runner    (1001) docker     (122)     6843 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/machine/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.545778 dvc-3.0.0a0/dvc/machine/backend/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/machine/backend/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1609 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/machine/backend/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3230 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/machine/backend/terraform.py
+-rw-r--r--   0 runner    (1001) docker     (122)    44839 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/output.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.545778 dvc-3.0.0a0/dvc/parsing/
+-rw-r--r--   0 runner    (1001) docker     (122)    15488 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/parsing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16536 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/parsing/context.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5953 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/parsing/interpolate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2581 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/pathspec_math.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4938 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/progress.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1298 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/prompt.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.545778 dvc-3.0.0a0/dvc/render/
+-rw-r--r--   0 runner    (1001) docker     (122)      176 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/render/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1948 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/render/convert.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.545778 dvc-3.0.0a0/dvc/render/converter/
+-rw-r--r--   0 runner    (1001) docker     (122)      536 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/render/converter/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2017 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/render/converter/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11094 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/render/converter/vega.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4230 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/render/match.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.549779 dvc-3.0.0a0/dvc/repo/
+-rw-r--r--   0 runner    (1001) docker     (122)    18486 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9293 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/add.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2857 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/artifacts.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4620 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/brancher.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2921 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/checkout.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2069 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/collect.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1929 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/commit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6613 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/data.py
+-rw-r--r--   0 runner    (1001) docker     (122)      519 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/destroy.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4468 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/diff.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.553779 dvc-3.0.0a0/dvc/repo/experiments/
+-rw-r--r--   0 runner    (1001) docker     (122)    16678 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2086 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/apply.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1974 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/branch.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2197 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/cache.py
+-rw-r--r--   0 runner    (1001) docker     (122)      252 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/clean.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11268 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/collect.py
+-rw-r--r--   0 runner    (1001) docker     (122)      955 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3568 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/exceptions.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.553779 dvc-3.0.0a0/dvc/repo/experiments/executor/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/executor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28015 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/executor/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8313 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/executor/local.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9800 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/executor/ssh.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1253 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/ls.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3586 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/pull.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5404 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/push.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.553779 dvc-3.0.0a0/dvc/repo/experiments/queue/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/queue/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27803 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/queue/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23538 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/queue/celery.py
+-rw-r--r--   0 runner    (1001) docker     (122)      386 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/queue/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4733 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/queue/remove.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3988 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/queue/tasks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7393 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/queue/tempdir.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2804 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/queue/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9540 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/queue/workspace.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2237 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/refs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3899 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/remove.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3412 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/run.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1055 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/save.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6579 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/serialize.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11978 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/show.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6776 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/stash.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25084 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/experiments/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5357 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/fetch.py
+-rw-r--r--   0 runner    (1001) docker     (122)      386 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/freeze.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4672 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/gc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1470 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/get.py
+-rw-r--r--   0 runner    (1001) docker     (122)      546 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/get_url.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4352 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/graph.py
+-rw-r--r--   0 runner    (1001) docker     (122)      233 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/imp.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2369 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/imp_url.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4121 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/imports.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20412 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/index.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2826 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/init.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2559 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/install.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2285 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/ls.py
+-rw-r--r--   0 runner    (1001) docker     (122)      818 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/ls_url.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.553779 dvc-3.0.0a0/dvc/repo/metrics/
+-rw-r--r--   0 runner    (1001) docker     (122)      322 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      585 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/metrics/diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4339 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/metrics/show.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2147 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/move.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8225 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/open_repo.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.553779 dvc-3.0.0a0/dvc/repo/params/
+-rw-r--r--   0 runner    (1001) docker     (122)      305 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/params/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      581 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/params/diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5827 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/params/show.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.553779 dvc-3.0.0a0/dvc/repo/plots/
+-rw-r--r--   0 runner    (1001) docker     (122)    16703 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/plots/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      633 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/plots/diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1218 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/pull.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3514 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/push.py
+-rw-r--r--   0 runner    (1001) docker     (122)      913 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/remove.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8706 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/reproduce.py
+-rw-r--r--   0 runner    (1001) docker     (122)      744 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/run.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4711 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/scm_context.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14437 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/stage.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3575 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/status.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1456 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/trie.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1847 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/update.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14157 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/repo/worktree.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6577 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/rwlock.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4342 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/schema.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7178 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/scm.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.557779 dvc-3.0.0a0/dvc/stage/
+-rw-r--r--   0 runner    (1001) docker     (122)    25959 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/stage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9042 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/stage/cache.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/stage/decorators.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2951 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/stage/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1642 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/stage/imports.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7439 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/stage/loader.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3489 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/stage/monitor.py
+-rw-r--r--   0 runner    (1001) docker     (122)      393 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/stage/params.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5034 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/stage/run.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7010 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/stage/serialize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8752 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/stage/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.557779 dvc-3.0.0a0/dvc/testing/
+-rw-r--r--   0 runner    (1001) docker     (122)     1174 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/README.rst
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4088 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/api_tests.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.557779 dvc-3.0.0a0/dvc/testing/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.557779 dvc-3.0.0a0/dvc/testing/benchmarks/api/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/api/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.557779 dvc-3.0.0a0/dvc/testing/benchmarks/cli/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.561779 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      112 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_add.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1217 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_checkout.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1235 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_data_status.py
+-rw-r--r--   0 runner    (1001) docker     (122)      350 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)      407 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_exp_show.py
+-rw-r--r--   0 runner    (1001) docker     (122)      175 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_gc.py
+-rw-r--r--   0 runner    (1001) docker     (122)      264 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_get.py
+-rw-r--r--   0 runner    (1001) docker     (122)      251 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_get_url.py
+-rw-r--r--   0 runner    (1001) docker     (122)       62 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_help.py
+-rw-r--r--   0 runner    (1001) docker     (122)      270 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_import.py
+-rw-r--r--   0 runner    (1001) docker     (122)      257 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_import_url.py
+-rw-r--r--   0 runner    (1001) docker     (122)      383 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_init.py
+-rw-r--r--   0 runner    (1001) docker     (122)      399 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_ls.py
+-rw-r--r--   0 runner    (1001) docker     (122)      675 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_plots.py
+-rw-r--r--   0 runner    (1001) docker     (122)      188 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_pull.py
+-rw-r--r--   0 runner    (1001) docker     (122)      174 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_push.py
+-rw-r--r--   0 runner    (1001) docker     (122)      342 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_status.py
+-rw-r--r--   0 runner    (1001) docker     (122)      359 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/commands/test_update.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.561779 dvc-3.0.0a0/dvc/testing/benchmarks/cli/stories/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/stories/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1860 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/stories/test_modify_data.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.561779 dvc-3.0.0a0/dvc/testing/benchmarks/cli/stories/use_cases/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/stories/use_cases/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      414 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/cli/stories/use_cases/test_sharing.py
+-rw-r--r--   0 runner    (1001) docker     (122)       65 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6571 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/fixtures.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2957 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/benchmarks/plugin.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2031 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/cloud.py
+-rw-r--r--   0 runner    (1001) docker     (122)       65 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7316 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/fixtures.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10376 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/path_info.py
+-rw-r--r--   0 runner    (1001) docker     (122)      561 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/plugin.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12035 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/remote_tests.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8948 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/tmp_dir.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13091 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/testing/workspace_tests.py
+-rw-r--r--   0 runner    (1001) docker     (122)      378 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/types.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.561779 dvc-3.0.0a0/dvc/ui/
+-rw-r--r--   0 runner    (1001) docker     (122)    10748 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/ui/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1414 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/ui/_rich_progress.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2381 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/ui/pager.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3228 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/ui/table.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5385 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/updater.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.565779 dvc-3.0.0a0/dvc/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)       10 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (122)    12261 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)       12 2023-05-23 04:11:06.000000 dvc-3.0.0a0/dvc/utils/build.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1295 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/cli_parse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5226 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/collections.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2411 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)      189 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/flatten.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2079 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/fs.py
+-rw-r--r--   0 runner    (1001) docker     (122)      743 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/humanize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3982 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/hydra.py
+-rw-r--r--   0 runner    (1001) docker     (122)      185 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/objects.py
+-rw-r--r--   0 runner    (1001) docker     (122)      193 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/pkg.py
+-rw-r--r--   0 runner    (1001) docker     (122)      173 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/plots.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.565779 dvc-3.0.0a0/dvc/utils/serialize/
+-rw-r--r--   0 runner    (1001) docker     (122)     1370 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/serialize/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2510 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/serialize/_common.py
+-rw-r--r--   0 runner    (1001) docker     (122)      984 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/serialize/_json.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5163 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/serialize/_py.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1221 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/serialize/_toml.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1969 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/serialize/_yaml.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9779 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/strictyaml.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3169 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/studio.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2832 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/table.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2647 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/utils/threadpool.py
+-rw-r--r--   0 runner    (1001) docker     (122)      298 2023-05-23 04:11:03.000000 dvc-3.0.0a0/dvc/version.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.533778 dvc-3.0.0a0/dvc.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (122)    14468 2023-05-23 04:11:21.000000 dvc-3.0.0a0/dvc.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)    17231 2023-05-23 04:11:21.000000 dvc-3.0.0a0/dvc.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-05-23 04:11:21.000000 dvc-3.0.0a0/dvc.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      126 2023-05-23 04:11:21.000000 dvc-3.0.0a0/dvc.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (122)     1495 2023-05-23 04:11:21.000000 dvc-3.0.0a0/dvc.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        4 2023-05-23 04:11:21.000000 dvc-3.0.0a0/dvc.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (122)     9367 2023-05-23 04:11:03.000000 dvc-3.0.0a0/pyproject.toml
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.565779 dvc-3.0.0a0/scripts/
+-rw-r--r--   0 runner    (1001) docker     (122)      102 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/build-requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (122)     2503 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/build.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)      231 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/build_package.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.565779 dvc-3.0.0a0/scripts/fpm/
+-rw-r--r--   0 runner    (1001) docker     (122)       28 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/fpm/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (122)       59 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/fpm/after-install.sh
+-rw-r--r--   0 runner    (1001) docker     (122)       36 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/fpm/after-remove.sh
+-rw-r--r--   0 runner    (1001) docker     (122)     2411 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/fpm/build.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1405 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/fpm/notarize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1095 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/fpm/sign.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.565779 dvc-3.0.0a0/scripts/innosetup/
+-rw-r--r--   0 runner    (1001) docker     (122)       64 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/innosetup/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (122)     2674 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/innosetup/addSymLinkPermissions.ps1
+-rw-r--r--   0 runner    (1001) docker     (122)     1153 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/innosetup/addsymlink.iss
+-rw-r--r--   0 runner    (1001) docker     (122)      934 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/innosetup/build.py
+-rw-r--r--   0 runner    (1001) docker     (122)       62 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/innosetup/dvc.ico.dvc
+-rw-r--r--   0 runner    (1001) docker     (122)       67 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/innosetup/dvc_left.bmp.dvc
+-rw-r--r--   0 runner    (1001) docker     (122)       65 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/innosetup/dvc_up.bmp.dvc
+-rw-r--r--   0 runner    (1001) docker     (122)     6681 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/innosetup/modpath.iss
+-rw-r--r--   0 runner    (1001) docker     (122)     2324 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/innosetup/setup.iss
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.569779 dvc-3.0.0a0/scripts/pyinstaller/
+-rw-r--r--   0 runner    (1001) docker     (122)       28 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (122)      789 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/build.py
+-rw-r--r--   0 runner    (1001) docker     (122)      380 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/entitlements.plist
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.569779 dvc-3.0.0a0/scripts/pyinstaller/hooks/
+-rw-r--r--   0 runner    (1001) docker     (122)       34 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/hooks/hook-asyncssh.py
+-rw-r--r--   0 runner    (1001) docker     (122)      482 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/hooks/hook-celery.py
+-rw-r--r--   0 runner    (1001) docker     (122)      795 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/hooks/hook-dvc.py
+-rw-r--r--   0 runner    (1001) docker     (122)       65 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/hooks/hook-dvc.system.py
+-rw-r--r--   0 runner    (1001) docker     (122)      344 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/hooks/hook-dvc.tree.gs.py
+-rw-r--r--   0 runner    (1001) docker     (122)      120 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/hooks/hook-dvc.utils.flatten.py
+-rw-r--r--   0 runner    (1001) docker     (122)      134 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/hooks/hook-dvc_task.py
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/hooks/hook-fsspec.py
+-rw-r--r--   0 runner    (1001) docker     (122)      169 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/hooks/hook-google_compute_engine.logger.py
+-rw-r--r--   0 runner    (1001) docker     (122)      116 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/hooks/hook-pydrive2.py
+-rw-r--r--   0 runner    (1001) docker     (122)      958 2023-05-23 04:11:03.000000 dvc-3.0.0a0/scripts/pyinstaller/sign.py
+-rw-r--r--   0 runner    (1001) docker     (122)       38 2023-05-23 04:11:21.601779 dvc-3.0.0a0/setup.cfg
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.569779 dvc-3.0.0a0/tests/
+-rw-r--r--   0 runner    (1001) docker     (122)      414 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      125 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7176 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3748 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/dir_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (122)      286 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/docker-compose.yml
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.577779 dvc-3.0.0a0/tests/func/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.577779 dvc-3.0.0a0/tests/func/api/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6885 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/api/test_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)      988 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/api/test_experiments.py
+-rw-r--r--   0 runner    (1001) docker     (122)      719 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/api/test_scm.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10796 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/api/test_show.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.577779 dvc-3.0.0a0/tests/func/artifacts/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/artifacts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3890 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/artifacts/test_artifacts.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.577779 dvc-3.0.0a0/tests/func/data/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/data/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.577779 dvc-3.0.0a0/tests/func/data/db/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/data/db/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3820 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/data/db/test_index.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.581779 dvc-3.0.0a0/tests/func/experiments/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1150 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.581779 dvc-3.0.0a0/tests/func/experiments/executor/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/executor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5129 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/executor/test_ssh.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3819 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/test_apply.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8946 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/test_checkpoints.py
+-rw-r--r--   0 runner    (1001) docker     (122)      999 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/test_diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26141 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/test_experiments.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2340 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/test_queue.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15984 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/test_remote.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6753 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/test_remove.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4924 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/test_save.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3994 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/test_set_params.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23537 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/test_show.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1915 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/test_stash_exp.py
+-rw-r--r--   0 runner    (1001) docker     (122)      937 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/experiments/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.581779 dvc-3.0.0a0/tests/func/machine/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/machine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1326 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/machine/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5344 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/machine/test_machine_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)      799 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/machine/test_machine_status.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.581779 dvc-3.0.0a0/tests/func/metrics/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7112 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/metrics/test_diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9489 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/metrics/test_show.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.581779 dvc-3.0.0a0/tests/func/params/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/params/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8152 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/params/test_diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6928 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/params/test_show.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.581779 dvc-3.0.0a0/tests/func/parsing/
+-rw-r--r--   0 runner    (1001) docker     (122)     1469 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/parsing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10114 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/parsing/test_errors.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15835 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/parsing/test_foreach.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9670 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/parsing/test_interpolated_entry.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6091 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/parsing/test_resolver.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.581779 dvc-3.0.0a0/tests/func/plots/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/plots/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1896 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/plots/test_diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2816 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/plots/test_modify.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12871 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/plots/test_show.py
+-rw-r--r--   0 runner    (1001) docker     (122)    35506 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_add.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1715 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_analytics.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4281 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_check_ignore.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26547 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_checkout.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4928 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_cli.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8797 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_commit.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10174 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16832 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_data_cloud.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16348 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_data_status.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18824 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12248 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_dvcfile.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7554 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_external_repo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1412 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_fs.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14728 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_gc.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11136 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_get.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1893 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_get_url.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14876 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_ignore.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20849 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_import.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7625 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_import_url.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3215 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_init.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5558 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_install.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1740 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_lock.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5057 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_lockfile.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17854 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_ls.py
+-rw-r--r--   0 runner    (1001) docker     (122)      104 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_ls_url.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5573 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_merge_driver.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5734 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_move.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5936 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_odb.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15547 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_remote.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3465 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_remove.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1948 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_repo.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10062 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_repo_index.py
+-rw-r--r--   0 runner    (1001) docker     (122)    34572 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_repro.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13663 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_repro_multistage.py
+-rw-r--r--   0 runner    (1001) docker     (122)      340 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_root.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6037 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_run_cache.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11484 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_run_multistage.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24708 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_run_single_stage.py
+-rw-r--r--   0 runner    (1001) docker     (122)      639 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_scm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1839 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_scm_context.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10257 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_stage.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14749 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_stage_load.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1970 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_state.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5156 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_status.py
+-rw-r--r--   0 runner    (1001) docker     (122)      830 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_unprotect.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14335 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_update.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1951 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_used_objs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1148 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1339 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_version.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5562 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/test_virtual_directory.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.581779 dvc-3.0.0a0/tests/func/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8318 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/utils/test_hydra.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9702 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/func/utils/test_strict_yaml.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.581779 dvc-3.0.0a0/tests/integration/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/integration/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      118 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/integration/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.585779 dvc-3.0.0a0/tests/integration/plots/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/integration/plots/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5922 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/integration/plots/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13779 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/integration/plots/test_plots.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3978 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/integration/plots/test_repo_plots_api.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3253 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/integration/test_studio_live_experiments.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.585779 dvc-3.0.0a0/tests/remotes/
+-rw-r--r--   0 runner    (1001) docker     (122)      316 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/remotes/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.585779 dvc-3.0.0a0/tests/remotes/git-init/
+-rw-r--r--   0 runner    (1001) docker     (122)       35 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/remotes/git-init/git.sh
+-rw-r--r--   0 runner    (1001) docker     (122)     1436 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/remotes/git_server.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1679 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/remotes/user.key
+-rw-r--r--   0 runner    (1001) docker     (122)      394 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/remotes/user.key.pub
+-rw-r--r--   0 runner    (1001) docker     (122)      924 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/remotes_env.sample
+-rw-r--r--   0 runner    (1001) docker     (122)      118 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/ruff.toml
+-rw-r--r--   0 runner    (1001) docker     (122)     1192 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/scripts.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.585779 dvc-3.0.0a0/tests/unit/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.589779 dvc-3.0.0a0/tests/unit/cli/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2467 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/cli/test_main.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.589779 dvc-3.0.0a0/tests/unit/command/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.593779 dvc-3.0.0a0/tests/unit/command/ls/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/ls/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3087 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/ls/test_ls.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2086 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/ls/test_ls_colors.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2687 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_add.py
+-rw-r--r--   0 runner    (1001) docker     (122)      674 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_cache.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1241 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_checkout.py
+-rw-r--r--   0 runner    (1001) docker     (122)      610 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_compat_flag.py
+-rw-r--r--   0 runner    (1001) docker     (122)      883 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7404 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_dag.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3951 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_data_status.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2800 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_data_sync.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11210 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10398 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_experiments.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1770 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_gc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1010 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_get.py
+-rw-r--r--   0 runner    (1001) docker     (122)      382 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_get_url.py
+-rw-r--r--   0 runner    (1001) docker     (122)      563 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_git_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2028 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_imp.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4010 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_imp_url.py
+-rw-r--r--   0 runner    (1001) docker     (122)      656 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_ls_url.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4858 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_machine.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4212 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2031 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_params.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11193 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_plots.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6178 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_queue.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1028 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_repro.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3738 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_run.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2800 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_stage.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3245 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_status.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1330 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/command/test_update.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.593779 dvc-3.0.0a0/tests/unit/data/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/data/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.593779 dvc-3.0.0a0/tests/unit/data/db/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/data/db/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3994 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/data/db/test_local.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.593779 dvc-3.0.0a0/tests/unit/dependency/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/dependency/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      306 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/dependency/test_dependency.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8862 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/dependency/test_params.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.593779 dvc-3.0.0a0/tests/unit/fs/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/fs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2482 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/fs/test_azure.py
+-rw-r--r--   0 runner    (1001) docker     (122)      297 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/fs/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7529 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/fs/test_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19085 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/fs/test_dvc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3638 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/fs/test_dvc_info.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2108 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/fs/test_fs.py
+-rw-r--r--   0 runner    (1001) docker     (122)      728 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/fs/test_path.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3907 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/fs/test_s3.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1373 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/fs/test_tree.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.593779 dvc-3.0.0a0/tests/unit/machine/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/machine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1096 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/machine/test_machine.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.593779 dvc-3.0.0a0/tests/unit/output/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/output/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      360 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/output/test_annotations.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4098 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/output/test_load.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1903 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/output/test_local.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6664 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/output/test_output.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.593779 dvc-3.0.0a0/tests/unit/remote/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/remote/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      573 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/remote/test_oss.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1571 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/remote/test_remote.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4407 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/remote/test_webdav.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1587 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/remote/test_webhdfs.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.597779 dvc-3.0.0a0/tests/unit/render/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/render/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3196 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/render/test_convert.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1395 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/render/test_image_converter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4240 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/render/test_match.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14720 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/render/test_vega_converter.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.597779 dvc-3.0.0a0/tests/unit/repo/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.597779 dvc-3.0.0a0/tests/unit/repo/experiments/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/experiments/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6140 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/experiments/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.597779 dvc-3.0.0a0/tests/unit/repo/experiments/queue/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/experiments/queue/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7652 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/experiments/queue/test_celery.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3951 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/experiments/queue/test_remove.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4239 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/experiments/test_executor_status.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1563 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/experiments/test_remove.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2343 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/experiments/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.597779 dvc-3.0.0a0/tests/unit/repo/plots/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/plots/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1450 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/plots/test_diff.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2957 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/test_open_repo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3661 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/test_repo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1749 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/test_reproduce.py
+-rw-r--r--   0 runner    (1001) docker     (122)      335 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/test_run.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4396 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/repo/test_scm_context.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.597779 dvc-3.0.0a0/tests/unit/scm/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/scm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2768 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/scm/test_scm.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.597779 dvc-3.0.0a0/tests/unit/stage/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/stage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5924 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/stage/test_cache.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8626 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/stage/test_loader_pipeline_file.py
+-rw-r--r--   0 runner    (1001) docker     (122)      578 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/stage/test_run.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6087 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/stage/test_serialize_pipeline_file.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8394 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/stage/test_serialize_pipeline_lock.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4318 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/stage/test_stage.py
+-rw-r--r--   0 runner    (1001) docker     (122)      687 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/stage/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4263 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_analytics.py
+-rw-r--r--   0 runner    (1001) docker     (122)      499 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_api.py
+-rw-r--r--   0 runner    (1001) docker     (122)      588 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_collect.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13495 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_compare.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3118 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12356 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_context.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1018 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_daemon.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3971 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_dvcfile.py
+-rw-r--r--   0 runner    (1001) docker     (122)      457 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_hashinfo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8869 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_ignore.py
+-rw-r--r--   0 runner    (1001) docker     (122)      520 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_imports.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4352 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_info.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3045 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_interpolate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3447 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_lockfile.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8687 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_logger.py
+-rw-r--r--   0 runner    (1001) docker     (122)      819 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1240 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_params.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3747 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_pathspec_math.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1404 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_progress.py
+-rw-r--r--   0 runner    (1001) docker     (122)      339 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_prompt.py
+-rw-r--r--   0 runner    (1001) docker     (122)      354 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_run.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4282 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_rwlock.py
+-rw-r--r--   0 runner    (1001) docker     (122)      239 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_scm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9682 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_tabular_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5357 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/test_updater.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.597779 dvc-3.0.0a0/tests/unit/ui/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/ui/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1558 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/ui/test_console.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2697 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/ui/test_pager.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4393 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/ui/test_table.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.601779 dvc-3.0.0a0/tests/unit/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/utils/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.601779 dvc-3.0.0a0/tests/unit/utils/serialize/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/utils/serialize/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1221 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/utils/serialize/test_python.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1080 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/utils/serialize/test_toml.py
+-rw-r--r--   0 runner    (1001) docker     (122)      629 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/utils/serialize/test_yaml.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1162 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/utils/test_cli_parse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5635 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/utils/test_collections.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2546 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/utils/test_executors.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4640 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/utils/test_fs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2050 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/utils/test_humanize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1758 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/utils/test_studio.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5103 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/unit/utils/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-23 04:11:21.601779 dvc-3.0.0a0/tests/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)     1633 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      593 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/utils/asserts.py
+-rw-r--r--   0 runner    (1001) docker     (122)      297 2023-05-23 04:11:03.000000 dvc-3.0.0a0/tests/utils/plots.py
```

### filetype from file(1)

```diff
@@ -1 +1 @@
-POSIX tar archive (GNU)
+POSIX tar archive
```

### Comparing `dvc-2.9.5/.github/ISSUE_TEMPLATE/bug_report.md` & `dvc-3.0.0a0/.github/ISSUE_TEMPLATE/bug_report.md`

 * *Files 1% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 ---
 
 # Bug Report
 
 <!--
 ## Issue name
 
-Issue names must follow the pattern `command: description` where the command is the dvc command that you are trying to run. The description should describe the consequence of the bug. 
+Issue names must follow the pattern `command: description` where the command is the dvc command that you are trying to run. The description should describe the consequence of the bug.
 
 Example: `repro: doesn't detect input changes`
 -->
 
 ## Description
 
 <!--
```

### Comparing `dvc-2.9.5/.github/workflows/tests.yaml` & `dvc-3.0.0a0/.github/workflows/tests.yaml`

 * *Files 20% similar despite different names*

```diff
@@ -1,92 +1,72 @@
 name: Tests
 
 on:
-  pull_request: {}
-  push: {}
+  push:
+    branches: [main, 2.x]
+  pull_request:
   schedule:
     - cron: '5 1 * * *'  # every day at 01:05
+  workflow_dispatch:
 
-env:
-  DVC_TEST: "true"
-  HOMEBREW_NO_AUTO_UPDATE: 1
-  SHELL: /bin/bash
+concurrency:
+  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
+  cancel-in-progress: true
+
+permissions:
+  contents: read
 
 jobs:
-  lint:
-    timeout-minutes: 10
-    runs-on: ubuntu-latest
-    steps:
-    - name: Cancel Previous Runs
-      uses: styfle/cancel-workflow-action@0.9.1
-      with:
-        access_token: ${{ github.token }}
-    - uses: actions/checkout@v2.4.0
-      with:
-        fetch-depth: 0
-    - name: Set up Python 3.8
-      uses: actions/setup-python@v2.2.2
-      with:
-        python-version: 3.8
-    - name: Install requirements
-      run: |
-        pip install --upgrade pip setuptools wheel
-        pip install ".[dev]" pre-commit
-    - name: Check README
-      run: python setup.py checkdocs
-    - uses: pre-commit/action@v2.0.3
   tests:
-    timeout-minutes: 45
+    timeout-minutes: 50
     runs-on: ${{ matrix.os }}
     strategy:
       fail-fast: false
       matrix:
         os: [ubuntu-20.04, windows-latest, macos-latest]
-        pyv: ["3.7", "3.8", "3.9", "3.10"]
+        pyv: ["3.8", "3.9", "3.10", "3.11"]
+
     steps:
-    - uses: actions/checkout@v2.4.0
+    - uses: actions/checkout@v3
       with:
         fetch-depth: 0
     - name: Set up Python
-      uses: actions/setup-python@v2.2.2
+      uses: actions/setup-python@v4
       with:
         python-version: ${{ matrix.pyv }}
-    - name: get pip cache dir
-      id: pip-cache-dir
-      run: |
-        echo "::set-output name=dir::$(pip cache dir)"
-    - name: set pip cache
-      id: pip-cache
-      uses: actions/cache@v2.1.7
-      with:
-        path: ${{ steps.pip-cache-dir.outputs.dir }}
-        key: ${{ runner.os }}-pip-${{ hashFiles('setup.cfg') }}
-        restore-keys: |
-          ${{ runner.os }}-pip-
+        cache: pip
+        cache-dependency-path: pyproject.toml
     - name: install
-      if: steps.cache.pip-cache-dir.cache-hit != 'true'
       run: |
-        pip install --upgrade pip setuptools wheel
+        pip install --upgrade pip wheel
         pip install -e ".[dev]"
+    - uses: pre-commit/action@v3.0.0
     - name: run tests
       timeout-minutes: 40
-      run: >-
-        python -m tests -n=auto
-        --cov-report=xml --cov-report=term
-        ${{ env.extra_test_args }}
+      env:
+        PYTHONUTF8: 1
+      run: >
+        pytest -n=logical --timeout=300 --durations=100
+        --cov --cov-report=xml --cov-report=term
     - name: upload coverage report
-      uses: codecov/codecov-action@v2.1.0
+      uses: codecov/codecov-action@v3
       with:
         file: ./coverage.xml
         fail_ci_if_error: false
+  check:
+    if: always()
+    needs: [tests]
+    runs-on: ubuntu-latest
+    steps:
+      - uses: re-actors/alls-green@release/v1
+        with:
+          jobs: ${{ toJSON(needs) }}
   notify:
     if: github.ref == 'refs/heads/main' && failure()
-    needs:
-      - lint
-      - tests
+    needs: [tests]
     runs-on: ubuntu-latest
     steps:
     - name: Slack Notification
       uses: rtCamp/action-slack-notify@v2.2.0
       env:
         SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
         SLACK_COLOR: ${{ job.status }}
```

### Comparing `dvc-2.9.5/.pre-commit-hooks.yaml` & `dvc-3.0.0a0/.pre-commit-hooks.yaml`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/CODE_OF_CONDUCT.md` & `dvc-3.0.0a0/CODE_OF_CONDUCT.md`

 * *Files 0% similar despite different names*

```diff
@@ -70,8 +70,7 @@
 This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
 available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html
 
 [homepage]: https://www.contributor-covenant.org
 
 For answers to common questions about this code of conduct, see
 https://www.contributor-covenant.org/faq
-
```

### Comparing `dvc-2.9.5/LICENSE` & `dvc-3.0.0a0/LICENSE`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/PKG-INFO` & `dvc-3.0.0a0/PKG-INFO`

 * *Files 10% similar despite different names*

```diff
@@ -1,287 +1,268 @@
 Metadata-Version: 2.1
 Name: dvc
-Version: 2.9.5
+Version: 3.0.0a0
 Summary: Git for data scientists - manage your code and data together
-Home-page: http://dvc.org
-Download-URL: https://github.com/iterative/dvc
-Author: Dmitry Petrov
-Author-email: dmitry@dvc.org
-Maintainer: Iterative
-Maintainer-email: support@dvc.org
+Author-email: Dmitry Petrov <dmitry@dvc.org>
+Maintainer-email: Iterative <support@dvc.org>
 License: Apache License 2.0
 Project-URL: Documentation, https://dvc.org/doc
+Project-URL: Issues, https://github.com/iterative/dvc/issues
 Project-URL: Source, https://github.com/iterative/dvc
-Keywords: data-science, data-version-control, machine-learning, git,developer-tools, reproducibility, collaboration, ai
-Platform: UNKNOWN
+Keywords: ai,collaboration,data-science,data-version-control,developer-tools,git,machine-learning,reproducibility
 Classifier: Development Status :: 4 - Beta
 Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
-Requires-Python: >=3.7
+Classifier: Programming Language :: Python :: 3.11
+Requires-Python: >=3.8
 Description-Content-Type: text/x-rst
 Provides-Extra: all
-Provides-Extra: dev
 Provides-Extra: azure
+Provides-Extra: dev
 Provides-Extra: gdrive
 Provides-Extra: gs
 Provides-Extra: hdfs
+Provides-Extra: lint
 Provides-Extra: oss
 Provides-Extra: s3
 Provides-Extra: ssh
 Provides-Extra: ssh_gssapi
-Provides-Extra: webdav
-Provides-Extra: webhdfs
-Provides-Extra: webdhfs_kerberos
 Provides-Extra: terraform
+Provides-Extra: testing
 Provides-Extra: tests
+Provides-Extra: webdav
+Provides-Extra: webhdfs
+Provides-Extra: webhdfs_kerberos
 License-File: LICENSE
 
 |Banner|
 
 `Website <https://dvc.org>`_
 • `Docs <https://dvc.org/doc>`_
 • `Blog <http://blog.dataversioncontrol.com>`_
-• `Twitter <https://twitter.com/DVCorg>`_
-• `Chat (Community & Support) <https://dvc.org/chat>`_
 • `Tutorial <https://dvc.org/doc/get-started>`_
-• `Mailing List <https://sweedom.us10.list-manage.com/subscribe/post?u=a08bf93caae4063c4e6a351f6&id=24c0ecc49a>`_
+• `Related Technologies <https://dvc.org/doc/user-guide/related-technologies>`_
+• `How DVC works`_
+• `VS Code Extension`_
+• `Installation`_
+• `Contributing`_
+• `Community and Support`_
 
-|CI| |Maintainability| |Coverage| |Donate| |DOI|
+|CI| |Python Version| |Coverage| |VS Code| |DOI|
 
-|PyPI| |Packages| |Brew| |Conda| |Choco| |Snap|
+|PyPI| |PyPI Downloads| |Packages| |Brew| |Conda| |Choco| |Snap|
 
 |
 
-**Data Version Control** or **DVC** is an **open-source** tool for data science and machine
-learning projects. Key features:
+**Data Version Control** or **DVC** is a command line tool and `VS Code Extension`_ to help you develop reproducible machine learning projects:
+
+#. **Version** your data and models.
+   Store them in your cloud storage but keep their version info in your Git repo.
 
-#. Simple **command line** Git-like experience. Does not require installing and maintaining
-   any databases. Does not depend on any proprietary online services.
+#. **Iterate** fast with lightweight pipelines.
+   When you make changes, only run the steps impacted by those changes.
 
-#. Management and versioning of **datasets** and **machine learning
-   models**. Data can be saved in S3, Google cloud, Azure, Alibaba 
-   cloud, SSH server, HDFS, or even local HDD RAID.
+#. **Track** experiments in your local Git repo (no servers needed).
 
-#. Makes projects **reproducible** and **shareable**; helping to answer questions about how
-   a model was built.
+#. **Compare** any data, code, parameters, model, or performance plots.
 
-#. Helps manage experiments with Git tags/branches and **metrics** tracking.
+#. **Share** experiments and automatically reproduce anyone's experiment.
 
-**DVC** aims to replace spreadsheet and document sharing tools (such as Excel or Google Docs)
-frequently used as both knowledge repositories and team ledgers.
-DVC also replaces both ad-hoc scripts to track, move, and deploy different model versions
-and ad-hoc data file suffixes and prefixes.
+Quick start
+===========
+
+    Please read our `Command Reference <https://dvc.org/doc/command-reference>`_ for a complete list.
+
+A common CLI workflow includes:
 
-.. contents:: **Contents**
-  :backlinks: none
+
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Task                              | Terminal                                                                                           |
++===================================+====================================================================================================+
+| Track data                        | | ``$ git add train.py params.yaml``                                                               |
+|                                   | | ``$ dvc add images/``                                                                            |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Connect code and data             | | ``$ dvc stage add -n featurize -d images/ -o features/ python featurize.py``                     |
+|                                   | | ``$ dvc stage add -n train -d features/ -d train.py -o model.p -M metrics.json python train.py`` |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Make changes and experiment       | | ``$ dvc exp run -n exp-baseline``                                                                |
+|                                   | | ``$ vi train.py``                                                                                |
+|                                   | | ``$ dvc exp run -n exp-code-change``                                                             |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Compare and select experiments    | | ``$ dvc exp show``                                                                               |
+|                                   | | ``$ dvc exp apply exp-baseline``                                                                 |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Share code                        | | ``$ git add .``                                                                                  |
+|                                   | | ``$ git commit -m 'The baseline model'``                                                         |
+|                                   | | ``$ git push``                                                                                   |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Share data and ML models          | | ``$ dvc remote add myremote -d s3://mybucket/image_cnn``                                         |
+|                                   | | ``$ dvc push``                                                                                   |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
 
 How DVC works
 =============
 
-We encourage you to read our `Get Started <https://dvc.org/doc/get-started>`_ guide to better understand what DVC
-is and how it can fit your scenarios.
+    We encourage you to read our `Get Started
+    <https://dvc.org/doc/get-started>`_ docs to better understand what DVC
+    does and how it can fit your scenarios.
+
+The closest *analogies* to describe the main DVC features are these:
+
+#. **Git for data**: Store and share data artifacts (like Git-LFS but without a server) and models, connecting them with a Git repository. Data management meets GitOps!
+#. **Makefiles** for ML: Describes how data or model artifacts are built from other data and code in a standard format. Now you can version your data pipelines with Git.
+#. Local **experiment tracking**: Turn your machine into an ML experiment management platform, and collaborate with others using existing Git hosting (Github, Gitlab, etc.).
+
+Git is employed as usual to store and version code (including DVC meta-files as placeholders for data).
+DVC `stores data and model files <https://dvc.org/doc/start/data-management>`_ seamlessly in a cache outside of Git, while preserving almost the same user experience as if they were in the repo.
+To share and back up the *data cache*, DVC supports multiple remote storage platforms - any cloud (S3, Azure, Google Cloud, etc.) or on-premise network storage (via SSH, for example).
 
-The easiest (but not perfect!) *analogy* to describe it: DVC is Git (or Git-LFS to be precise) & Makefiles
-made right and tailored specifically for ML and Data Science scenarios.
+|Flowchart|
 
-#. ``Git/Git-LFS`` part - DVC helps store and share data artifacts and models, connecting them with a Git repository.
-#. ``Makefile``\ s part - DVC describes how one data or model artifact was built from other data and code.
+`DVC pipelines <https://dvc.org/doc/start/data-management/data-pipelines>`_ (computational graphs) connect code and data together.
+They specify all steps required to produce a model: input dependencies including code, data, commands to run; and output information to be saved.
 
-DVC usually runs along with Git. Git is used as usual to store and version code (including DVC meta-files). DVC helps
-to store data and model files seamlessly out of Git, while preserving almost the same user experience as if they
-were stored in Git itself. To store and share the data cache, DVC supports multiple remotes - any cloud (S3, Azure,
-Google Cloud, etc) or any on-premise network storage (via SSH, for example).
+Last but not least, `DVC Experiment Versioning <https://dvc.org/doc/start/experiments>`_ lets you prepare and run a large number of experiments.
+Their results can be filtered and compared based on hyperparameters and metrics, and visualized with multiple plots.
 
-|Flowchart|
+.. _`VS Code Extension`:
 
-The DVC pipelines (computational graph) feature connects code and data together. It is possible to explicitly
-specify all steps required to produce a model: input dependencies including data, commands to run,
-and output information to be saved. See the quick start section below or
-the `Get Started <https://dvc.org/doc/get-started>`_ tutorial to learn more.
+Visual Studio Code Extension
+============================
 
-Quick start
-===========
+|VS Code|
 
-Please read `Get Started <https://dvc.org/doc/get-started>`_ guide for a full version. Common workflow commands include:
+To use DVC as a GUI right from your VS Code IDE, install the `DVC Extension <https://marketplace.visualstudio.com/items?itemName=Iterative.dvc>`_ from the Marketplace.
+It currently features experiment tracking and data management, and more features (data pipeline support, etc.) are coming soon!
 
-+-----------------------------------+----------------------------------------------------------------------------+
-| Step                              | Command                                                                    |
-+===================================+============================================================================+
-| Track data                        | | ``$ git add train.py``                                                   |
-|                                   | | ``$ dvc add images.zip``                                                 |
-+-----------------------------------+----------------------------------------------------------------------------+
-| Connect code and data by commands | | ``$ dvc run -n prepare -d images.zip -o images/ unzip -q images.zip``    |
-|                                   | | ``$ dvc run -n train -d images/ -d train.py -o model.p python train.py`` |
-+-----------------------------------+----------------------------------------------------------------------------+
-| Make changes and reproduce        | | ``$ vi train.py``                                                        |
-|                                   | | ``$ dvc repro model.p.dvc``                                              |
-+-----------------------------------+----------------------------------------------------------------------------+
-| Share code                        | | ``$ git add .``                                                          |
-|                                   | | ``$ git commit -m 'The baseline model'``                                 |
-|                                   | | ``$ git push``                                                           |
-+-----------------------------------+----------------------------------------------------------------------------+
-| Share data and ML models          | | ``$ dvc remote add myremote -d s3://mybucket/image_cnn``                 |
-|                                   | | ``$ dvc push``                                                           |
-+-----------------------------------+----------------------------------------------------------------------------+
+|VS Code Extension Overview|
+
+    Note: You'll have to install core DVC on your system separately (as detailed
+    below). The Extension will guide you if needed.
 
 Installation
 ============
 
-There are four options to install DVC: ``pip``, Homebrew, Conda (Anaconda) or an OS-specific package.
+There are several ways to install DVC: in VS Code; using ``snap``, ``choco``, ``brew``, ``conda``, ``pip``; or with an OS-specific package.
 Full instructions are `available here <https://dvc.org/doc/get-started/install>`_.
 
-Snap (Snapcraft/Linux)
-----------------------
+Snapcraft (Linux)
+-----------------
 
 |Snap|
 
 .. code-block:: bash
 
    snap install dvc --classic
 
 This corresponds to the latest tagged release.
-Add ``--beta`` for the latest tagged release candidate,
-or ``--edge`` for the latest ``main`` version.
+Add ``--beta`` for the latest tagged release candidate, or ``--edge`` for the latest ``main`` version.
 
-Choco (Chocolatey/Windows)
---------------------------
+Chocolatey (Windows)
+--------------------
 
 |Choco|
 
 .. code-block:: bash
 
    choco install dvc
 
-Brew (Homebrew/Mac OS)
-----------------------
+Brew (mac OS)
+-------------
 
 |Brew|
 
 .. code-block:: bash
 
    brew install dvc
 
-Conda (Anaconda)
-----------------
+Anaconda (Any platform)
+-----------------------
 
 |Conda|
 
 .. code-block:: bash
 
    conda install -c conda-forge mamba # installs much faster than conda
    mamba install -c conda-forge dvc
 
-Depending on the remote storage type you plan to use to keep and share your data, you might need to
-install optional dependencies: `dvc-s3`, `dvc-azure`, `dvc-gdrive`, `dvc-gs`, `dvc-oss`, `dvc-ssh`.
+Depending on the remote storage type you plan to use to keep and share your data, you might need to install optional dependencies: `dvc-s3`, `dvc-azure`, `dvc-gdrive`, `dvc-gs`, `dvc-oss`, `dvc-ssh`.
 
-pip (PyPI)
-----------
+PyPI (Python)
+-------------
 
 |PyPI|
 
 .. code-block:: bash
 
    pip install dvc
 
-Depending on the remote storage type you plan to use to keep and share your data, you might need to specify
-one of the optional dependencies: ``s3``, ``gs``, ``azure``, ``oss``, ``ssh``. Or ``all`` to include them all.
-The command should look like this: ``pip install dvc[s3]`` (in this case AWS S3 dependencies such as ``boto3``
-will be installed automatically).
+Depending on the remote storage type you plan to use to keep and share your data, you might need to specify one of the optional dependencies: ``s3``, ``gs``, ``azure``, ``oss``, ``ssh``. Or ``all`` to include them all.
+The command should look like this: ``pip install 'dvc[s3]'`` (in this case AWS S3 dependencies such as ``boto3`` will be installed automatically).
 
 To install the development version, run:
 
 .. code-block:: bash
 
    pip install git+git://github.com/iterative/dvc
 
-Package
--------
+Package (Platform-specific)
+---------------------------
 
 |Packages|
 
-Self-contained packages for Linux, Windows, and Mac are available. The latest version of the packages
-can be found on the GitHub `releases page <https://github.com/iterative/dvc/releases>`_.
+Self-contained packages for Linux, Windows, and Mac are available.
+The latest version of the packages can be found on the GitHub `releases page <https://github.com/iterative/dvc/releases>`_.
 
 Ubuntu / Debian (deb)
 ^^^^^^^^^^^^^^^^^^^^^
 .. code-block:: bash
 
    sudo wget https://dvc.org/deb/dvc.list -O /etc/apt/sources.list.d/dvc.list
-   sudo apt-get update
-   sudo apt-get install dvc
+   wget -qO - https://dvc.org/deb/iterative.asc | sudo apt-key add -
+   sudo apt update
+   sudo apt install dvc
 
 Fedora / CentOS (rpm)
 ^^^^^^^^^^^^^^^^^^^^^
 .. code-block:: bash
 
    sudo wget https://dvc.org/rpm/dvc.repo -O /etc/yum.repos.d/dvc.repo
+   sudo rpm --import https://dvc.org/rpm/iterative.asc
    sudo yum update
    sudo yum install dvc
 
-Comparison to related technologies
-==================================
-
-#. Data Engineering tools such as `AirFlow <https://airflow.apache.org/>`_,
-   `Luigi <https://github.com/spotify/luigi>`_, and others - in DVC data,
-   model and ML pipelines represent a single ML project focused on data
-   scientists' experience.  Data engineering tools orchestrate multiple data
-   projects and focus on efficient execution. A DVC project can be used from
-   existing data pipelines as a single execution step.
-
-#. `Git-annex <https://git-annex.branchable.com/>`_ - DVC uses the idea of storing the content of large files (which should
-   not be in a Git repository) in a local key-value store, and uses file hardlinks/symlinks instead of
-   copying/duplicating files.
-
-#. `Git-LFS <https://git-lfs.github.com/>`_ - DVC is compatible with many
-   remote storage services (S3, Google Cloud, Azure, SSH, etc). DVC also
-   uses reflinks or hardlinks to avoid copy operations on checkouts; thus
-   handling large data files much more efficiently.
-
-#. Makefile (and analogues including ad-hoc scripts) - DVC tracks
-   dependencies (in a directed acyclic graph).
-
-#. `Workflow Management Systems <https://en.wikipedia.org/wiki/Workflow_management_system>`_ - DVC is a workflow
-   management system designed specifically to manage machine learning experiments. DVC is built on top of Git.
-
-#. `DAGsHub <https://dagshub.com/>`_ - online service to host DVC
-   projects.  It provides a useful UI around DVC repositories and integrates
-   other tools.
-
-#. `DVC Studio <https://studio.iterative.ai/>`_ - official online
-   platform for DVC projects.  It can be used to manage data and models, run
-   and track experiments, and visualize and share results.  Also, it
-   integrates with `CML (CI/CD for ML) <https://cml.dev/>`__ for training
-   models in the cloud or Kubernetes.
-
-
 Contributing
 ============
 
-|Maintainability| |Donate|
+|Maintainability|
 
-Contributions are welcome! Please see our `Contributing Guide <https://dvc.org/doc/user-guide/contributing/core>`_ for more
-details. Thanks to all our contributors!
+Contributions are welcome!
+Please see our `Contributing Guide <https://dvc.org/doc/user-guide/contributing/core>`_ for more details.
+Thanks to all our contributors!
 
 |Contribs|
 
-Mailing List
-============
+Community and Support
+=====================
 
-Want to stay up to date? Want to help improve DVC by participating in our occasional polls? Subscribe to our `mailing list <https://sweedom.us10.list-manage.com/subscribe/post?u=a08bf93caae4063c4e6a351f6&id=24c0ecc49a>`_. No spam, really low traffic.
+* `Twitter <https://twitter.com/DVCorg>`_
+* `Forum <https://discuss.dvc.org/>`_
+* `Discord Chat <https://dvc.org/chat>`_
+* `Email <mailto:support@dvc.org>`_
+* `Mailing List <https://sweedom.us10.list-manage.com/subscribe/post?u=a08bf93caae4063c4e6a351f6&id=24c0ecc49a>`_
 
 Copyright
 =========
 
 This project is distributed under the Apache license version 2.0 (see the LICENSE file in the project root).
 
-By submitting a pull request to this project, you agree to license your contribution under the Apache license version
-2.0 to this project.
+By submitting a pull request to this project, you agree to license your contribution under the Apache license version 2.0 to this project.
 
 Citation
 ========
 
 |DOI|
 
 Iterative, *DVC: Data Version Control - Git for Data & Models* (2020)
@@ -290,30 +271,33 @@
 Barrak, A., Eghan, E.E. and Adams, B. `On the Co-evolution of ML Pipelines and Source Code - Empirical Study of DVC Projects <https://mcis.cs.queensu.ca/publications/2021/saner.pdf>`_ , in Proceedings of the 28th IEEE International Conference on Software Analysis, Evolution, and Reengineering, SANER 2021. Hawaii, USA.
 
 
 .. |Banner| image:: https://dvc.org/img/logo-github-readme.png
    :target: https://dvc.org
    :alt: DVC logo
 
+.. |VS Code Extension Overview| image:: https://raw.githubusercontent.com/iterative/vscode-dvc/main/extension/docs/overview.gif
+   :alt: DVC Extension for VS Code
+
 .. |CI| image:: https://github.com/iterative/dvc/workflows/Tests/badge.svg?branch=main
    :target: https://github.com/iterative/dvc/actions
    :alt: GHA Tests
 
 .. |Maintainability| image:: https://codeclimate.com/github/iterative/dvc/badges/gpa.svg
    :target: https://codeclimate.com/github/iterative/dvc
    :alt: Code Climate
 
+.. |Python Version| image:: https://img.shields.io/pypi/pyversions/dvc
+   :target: https://pypi.org/project/dvc
+   :alt: Python Version
+
 .. |Coverage| image:: https://codecov.io/gh/iterative/dvc/branch/main/graph/badge.svg
    :target: https://codecov.io/gh/iterative/dvc
    :alt: Codecov
 
-.. |Donate| image:: https://img.shields.io/badge/patreon-donate-green.svg?logo=patreon
-   :target: https://www.patreon.com/DVCorg/overview
-   :alt: Donate
-
 .. |Snap| image:: https://img.shields.io/badge/snap-install-82BEA0.svg?logo=snapcraft
    :target: https://snapcraft.io/dvc
    :alt: Snapcraft
 
 .. |Choco| image:: https://img.shields.io/chocolatey/v/dvc?label=choco
    :target: https://chocolatey.org/packages/dvc
    :alt: Chocolatey
@@ -326,14 +310,18 @@
    :target: https://anaconda.org/conda-forge/dvc
    :alt: Conda-forge
 
 .. |PyPI| image:: https://img.shields.io/pypi/v/dvc.svg?label=pip&logo=PyPI&logoColor=white
    :target: https://pypi.org/project/dvc
    :alt: PyPI
 
+.. |PyPI Downloads| image:: https://img.shields.io/pypi/dm/dvc.svg?color=blue&label=Downloads&logo=pypi&logoColor=gold
+   :target: https://pypi.org/project/dvc
+   :alt: PyPI Downloads
+
 .. |Packages| image:: https://img.shields.io/github/v/release/iterative/dvc?label=deb|pkg|rpm|exe&logo=GitHub
    :target: https://github.com/iterative/dvc/releases/latest
    :alt: deb|pkg|rpm|exe
 
 .. |DOI| image:: https://img.shields.io/badge/DOI-10.5281/zenodo.3677553-blue.svg
    :target: https://doi.org/10.5281/zenodo.3677553
    :alt: DOI
@@ -342,8 +330,10 @@
    :target: https://dvc.org/img/flow.gif
    :alt: how_dvc_works
 
 .. |Contribs| image:: https://contrib.rocks/image?repo=iterative/dvc
    :target: https://github.com/iterative/dvc/graphs/contributors
    :alt: Contributors
 
-
+.. |VS Code| image:: https://img.shields.io/visual-studio-marketplace/v/Iterative.dvc?color=blue&label=VSCode&logo=visualstudiocode&logoColor=blue
+   :target: https://marketplace.visualstudio.com/items?itemName=Iterative.dvc
+   :alt: VS Code Extension
```

### Comparing `dvc-2.9.5/README.rst` & `dvc-3.0.0a0/dvc.egg-info/PKG-INFO`

 * *Files 14% similar despite different names*

```diff
@@ -1,247 +1,268 @@
+Metadata-Version: 2.1
+Name: dvc
+Version: 3.0.0a0
+Summary: Git for data scientists - manage your code and data together
+Author-email: Dmitry Petrov <dmitry@dvc.org>
+Maintainer-email: Iterative <support@dvc.org>
+License: Apache License 2.0
+Project-URL: Documentation, https://dvc.org/doc
+Project-URL: Issues, https://github.com/iterative/dvc/issues
+Project-URL: Source, https://github.com/iterative/dvc
+Keywords: ai,collaboration,data-science,data-version-control,developer-tools,git,machine-learning,reproducibility
+Classifier: Development Status :: 4 - Beta
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Requires-Python: >=3.8
+Description-Content-Type: text/x-rst
+Provides-Extra: all
+Provides-Extra: azure
+Provides-Extra: dev
+Provides-Extra: gdrive
+Provides-Extra: gs
+Provides-Extra: hdfs
+Provides-Extra: lint
+Provides-Extra: oss
+Provides-Extra: s3
+Provides-Extra: ssh
+Provides-Extra: ssh_gssapi
+Provides-Extra: terraform
+Provides-Extra: testing
+Provides-Extra: tests
+Provides-Extra: webdav
+Provides-Extra: webhdfs
+Provides-Extra: webhdfs_kerberos
+License-File: LICENSE
+
 |Banner|
 
 `Website <https://dvc.org>`_
 • `Docs <https://dvc.org/doc>`_
 • `Blog <http://blog.dataversioncontrol.com>`_
-• `Twitter <https://twitter.com/DVCorg>`_
-• `Chat (Community & Support) <https://dvc.org/chat>`_
 • `Tutorial <https://dvc.org/doc/get-started>`_
-• `Mailing List <https://sweedom.us10.list-manage.com/subscribe/post?u=a08bf93caae4063c4e6a351f6&id=24c0ecc49a>`_
+• `Related Technologies <https://dvc.org/doc/user-guide/related-technologies>`_
+• `How DVC works`_
+• `VS Code Extension`_
+• `Installation`_
+• `Contributing`_
+• `Community and Support`_
 
-|CI| |Maintainability| |Coverage| |Donate| |DOI|
+|CI| |Python Version| |Coverage| |VS Code| |DOI|
 
-|PyPI| |Packages| |Brew| |Conda| |Choco| |Snap|
+|PyPI| |PyPI Downloads| |Packages| |Brew| |Conda| |Choco| |Snap|
 
 |
 
-**Data Version Control** or **DVC** is an **open-source** tool for data science and machine
-learning projects. Key features:
+**Data Version Control** or **DVC** is a command line tool and `VS Code Extension`_ to help you develop reproducible machine learning projects:
+
+#. **Version** your data and models.
+   Store them in your cloud storage but keep their version info in your Git repo.
+
+#. **Iterate** fast with lightweight pipelines.
+   When you make changes, only run the steps impacted by those changes.
 
-#. Simple **command line** Git-like experience. Does not require installing and maintaining
-   any databases. Does not depend on any proprietary online services.
+#. **Track** experiments in your local Git repo (no servers needed).
 
-#. Management and versioning of **datasets** and **machine learning
-   models**. Data can be saved in S3, Google cloud, Azure, Alibaba 
-   cloud, SSH server, HDFS, or even local HDD RAID.
+#. **Compare** any data, code, parameters, model, or performance plots.
+
+#. **Share** experiments and automatically reproduce anyone's experiment.
+
+Quick start
+===========
 
-#. Makes projects **reproducible** and **shareable**; helping to answer questions about how
-   a model was built.
+    Please read our `Command Reference <https://dvc.org/doc/command-reference>`_ for a complete list.
 
-#. Helps manage experiments with Git tags/branches and **metrics** tracking.
+A common CLI workflow includes:
 
-**DVC** aims to replace spreadsheet and document sharing tools (such as Excel or Google Docs)
-frequently used as both knowledge repositories and team ledgers.
-DVC also replaces both ad-hoc scripts to track, move, and deploy different model versions
-and ad-hoc data file suffixes and prefixes.
 
-.. contents:: **Contents**
-  :backlinks: none
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Task                              | Terminal                                                                                           |
++===================================+====================================================================================================+
+| Track data                        | | ``$ git add train.py params.yaml``                                                               |
+|                                   | | ``$ dvc add images/``                                                                            |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Connect code and data             | | ``$ dvc stage add -n featurize -d images/ -o features/ python featurize.py``                     |
+|                                   | | ``$ dvc stage add -n train -d features/ -d train.py -o model.p -M metrics.json python train.py`` |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Make changes and experiment       | | ``$ dvc exp run -n exp-baseline``                                                                |
+|                                   | | ``$ vi train.py``                                                                                |
+|                                   | | ``$ dvc exp run -n exp-code-change``                                                             |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Compare and select experiments    | | ``$ dvc exp show``                                                                               |
+|                                   | | ``$ dvc exp apply exp-baseline``                                                                 |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Share code                        | | ``$ git add .``                                                                                  |
+|                                   | | ``$ git commit -m 'The baseline model'``                                                         |
+|                                   | | ``$ git push``                                                                                   |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Share data and ML models          | | ``$ dvc remote add myremote -d s3://mybucket/image_cnn``                                         |
+|                                   | | ``$ dvc push``                                                                                   |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
 
 How DVC works
 =============
 
-We encourage you to read our `Get Started <https://dvc.org/doc/get-started>`_ guide to better understand what DVC
-is and how it can fit your scenarios.
+    We encourage you to read our `Get Started
+    <https://dvc.org/doc/get-started>`_ docs to better understand what DVC
+    does and how it can fit your scenarios.
+
+The closest *analogies* to describe the main DVC features are these:
+
+#. **Git for data**: Store and share data artifacts (like Git-LFS but without a server) and models, connecting them with a Git repository. Data management meets GitOps!
+#. **Makefiles** for ML: Describes how data or model artifacts are built from other data and code in a standard format. Now you can version your data pipelines with Git.
+#. Local **experiment tracking**: Turn your machine into an ML experiment management platform, and collaborate with others using existing Git hosting (Github, Gitlab, etc.).
+
+Git is employed as usual to store and version code (including DVC meta-files as placeholders for data).
+DVC `stores data and model files <https://dvc.org/doc/start/data-management>`_ seamlessly in a cache outside of Git, while preserving almost the same user experience as if they were in the repo.
+To share and back up the *data cache*, DVC supports multiple remote storage platforms - any cloud (S3, Azure, Google Cloud, etc.) or on-premise network storage (via SSH, for example).
 
-The easiest (but not perfect!) *analogy* to describe it: DVC is Git (or Git-LFS to be precise) & Makefiles
-made right and tailored specifically for ML and Data Science scenarios.
+|Flowchart|
 
-#. ``Git/Git-LFS`` part - DVC helps store and share data artifacts and models, connecting them with a Git repository.
-#. ``Makefile``\ s part - DVC describes how one data or model artifact was built from other data and code.
+`DVC pipelines <https://dvc.org/doc/start/data-management/data-pipelines>`_ (computational graphs) connect code and data together.
+They specify all steps required to produce a model: input dependencies including code, data, commands to run; and output information to be saved.
 
-DVC usually runs along with Git. Git is used as usual to store and version code (including DVC meta-files). DVC helps
-to store data and model files seamlessly out of Git, while preserving almost the same user experience as if they
-were stored in Git itself. To store and share the data cache, DVC supports multiple remotes - any cloud (S3, Azure,
-Google Cloud, etc) or any on-premise network storage (via SSH, for example).
+Last but not least, `DVC Experiment Versioning <https://dvc.org/doc/start/experiments>`_ lets you prepare and run a large number of experiments.
+Their results can be filtered and compared based on hyperparameters and metrics, and visualized with multiple plots.
 
-|Flowchart|
+.. _`VS Code Extension`:
 
-The DVC pipelines (computational graph) feature connects code and data together. It is possible to explicitly
-specify all steps required to produce a model: input dependencies including data, commands to run,
-and output information to be saved. See the quick start section below or
-the `Get Started <https://dvc.org/doc/get-started>`_ tutorial to learn more.
+Visual Studio Code Extension
+============================
 
-Quick start
-===========
+|VS Code|
+
+To use DVC as a GUI right from your VS Code IDE, install the `DVC Extension <https://marketplace.visualstudio.com/items?itemName=Iterative.dvc>`_ from the Marketplace.
+It currently features experiment tracking and data management, and more features (data pipeline support, etc.) are coming soon!
 
-Please read `Get Started <https://dvc.org/doc/get-started>`_ guide for a full version. Common workflow commands include:
+|VS Code Extension Overview|
 
-+-----------------------------------+----------------------------------------------------------------------------+
-| Step                              | Command                                                                    |
-+===================================+============================================================================+
-| Track data                        | | ``$ git add train.py``                                                   |
-|                                   | | ``$ dvc add images.zip``                                                 |
-+-----------------------------------+----------------------------------------------------------------------------+
-| Connect code and data by commands | | ``$ dvc run -n prepare -d images.zip -o images/ unzip -q images.zip``    |
-|                                   | | ``$ dvc run -n train -d images/ -d train.py -o model.p python train.py`` |
-+-----------------------------------+----------------------------------------------------------------------------+
-| Make changes and reproduce        | | ``$ vi train.py``                                                        |
-|                                   | | ``$ dvc repro model.p.dvc``                                              |
-+-----------------------------------+----------------------------------------------------------------------------+
-| Share code                        | | ``$ git add .``                                                          |
-|                                   | | ``$ git commit -m 'The baseline model'``                                 |
-|                                   | | ``$ git push``                                                           |
-+-----------------------------------+----------------------------------------------------------------------------+
-| Share data and ML models          | | ``$ dvc remote add myremote -d s3://mybucket/image_cnn``                 |
-|                                   | | ``$ dvc push``                                                           |
-+-----------------------------------+----------------------------------------------------------------------------+
+    Note: You'll have to install core DVC on your system separately (as detailed
+    below). The Extension will guide you if needed.
 
 Installation
 ============
 
-There are four options to install DVC: ``pip``, Homebrew, Conda (Anaconda) or an OS-specific package.
+There are several ways to install DVC: in VS Code; using ``snap``, ``choco``, ``brew``, ``conda``, ``pip``; or with an OS-specific package.
 Full instructions are `available here <https://dvc.org/doc/get-started/install>`_.
 
-Snap (Snapcraft/Linux)
-----------------------
+Snapcraft (Linux)
+-----------------
 
 |Snap|
 
 .. code-block:: bash
 
    snap install dvc --classic
 
 This corresponds to the latest tagged release.
-Add ``--beta`` for the latest tagged release candidate,
-or ``--edge`` for the latest ``main`` version.
+Add ``--beta`` for the latest tagged release candidate, or ``--edge`` for the latest ``main`` version.
 
-Choco (Chocolatey/Windows)
---------------------------
+Chocolatey (Windows)
+--------------------
 
 |Choco|
 
 .. code-block:: bash
 
    choco install dvc
 
-Brew (Homebrew/Mac OS)
-----------------------
+Brew (mac OS)
+-------------
 
 |Brew|
 
 .. code-block:: bash
 
    brew install dvc
 
-Conda (Anaconda)
-----------------
+Anaconda (Any platform)
+-----------------------
 
 |Conda|
 
 .. code-block:: bash
 
    conda install -c conda-forge mamba # installs much faster than conda
    mamba install -c conda-forge dvc
 
-Depending on the remote storage type you plan to use to keep and share your data, you might need to
-install optional dependencies: `dvc-s3`, `dvc-azure`, `dvc-gdrive`, `dvc-gs`, `dvc-oss`, `dvc-ssh`.
+Depending on the remote storage type you plan to use to keep and share your data, you might need to install optional dependencies: `dvc-s3`, `dvc-azure`, `dvc-gdrive`, `dvc-gs`, `dvc-oss`, `dvc-ssh`.
 
-pip (PyPI)
-----------
+PyPI (Python)
+-------------
 
 |PyPI|
 
 .. code-block:: bash
 
    pip install dvc
 
-Depending on the remote storage type you plan to use to keep and share your data, you might need to specify
-one of the optional dependencies: ``s3``, ``gs``, ``azure``, ``oss``, ``ssh``. Or ``all`` to include them all.
-The command should look like this: ``pip install dvc[s3]`` (in this case AWS S3 dependencies such as ``boto3``
-will be installed automatically).
+Depending on the remote storage type you plan to use to keep and share your data, you might need to specify one of the optional dependencies: ``s3``, ``gs``, ``azure``, ``oss``, ``ssh``. Or ``all`` to include them all.
+The command should look like this: ``pip install 'dvc[s3]'`` (in this case AWS S3 dependencies such as ``boto3`` will be installed automatically).
 
 To install the development version, run:
 
 .. code-block:: bash
 
    pip install git+git://github.com/iterative/dvc
 
-Package
--------
+Package (Platform-specific)
+---------------------------
 
 |Packages|
 
-Self-contained packages for Linux, Windows, and Mac are available. The latest version of the packages
-can be found on the GitHub `releases page <https://github.com/iterative/dvc/releases>`_.
+Self-contained packages for Linux, Windows, and Mac are available.
+The latest version of the packages can be found on the GitHub `releases page <https://github.com/iterative/dvc/releases>`_.
 
 Ubuntu / Debian (deb)
 ^^^^^^^^^^^^^^^^^^^^^
 .. code-block:: bash
 
    sudo wget https://dvc.org/deb/dvc.list -O /etc/apt/sources.list.d/dvc.list
-   sudo apt-get update
-   sudo apt-get install dvc
+   wget -qO - https://dvc.org/deb/iterative.asc | sudo apt-key add -
+   sudo apt update
+   sudo apt install dvc
 
 Fedora / CentOS (rpm)
 ^^^^^^^^^^^^^^^^^^^^^
 .. code-block:: bash
 
    sudo wget https://dvc.org/rpm/dvc.repo -O /etc/yum.repos.d/dvc.repo
+   sudo rpm --import https://dvc.org/rpm/iterative.asc
    sudo yum update
    sudo yum install dvc
 
-Comparison to related technologies
-==================================
-
-#. Data Engineering tools such as `AirFlow <https://airflow.apache.org/>`_,
-   `Luigi <https://github.com/spotify/luigi>`_, and others - in DVC data,
-   model and ML pipelines represent a single ML project focused on data
-   scientists' experience.  Data engineering tools orchestrate multiple data
-   projects and focus on efficient execution. A DVC project can be used from
-   existing data pipelines as a single execution step.
-
-#. `Git-annex <https://git-annex.branchable.com/>`_ - DVC uses the idea of storing the content of large files (which should
-   not be in a Git repository) in a local key-value store, and uses file hardlinks/symlinks instead of
-   copying/duplicating files.
-
-#. `Git-LFS <https://git-lfs.github.com/>`_ - DVC is compatible with many
-   remote storage services (S3, Google Cloud, Azure, SSH, etc). DVC also
-   uses reflinks or hardlinks to avoid copy operations on checkouts; thus
-   handling large data files much more efficiently.
-
-#. Makefile (and analogues including ad-hoc scripts) - DVC tracks
-   dependencies (in a directed acyclic graph).
-
-#. `Workflow Management Systems <https://en.wikipedia.org/wiki/Workflow_management_system>`_ - DVC is a workflow
-   management system designed specifically to manage machine learning experiments. DVC is built on top of Git.
-
-#. `DAGsHub <https://dagshub.com/>`_ - online service to host DVC
-   projects.  It provides a useful UI around DVC repositories and integrates
-   other tools.
-
-#. `DVC Studio <https://studio.iterative.ai/>`_ - official online
-   platform for DVC projects.  It can be used to manage data and models, run
-   and track experiments, and visualize and share results.  Also, it
-   integrates with `CML (CI/CD for ML) <https://cml.dev/>`__ for training
-   models in the cloud or Kubernetes.
-
-
 Contributing
 ============
 
-|Maintainability| |Donate|
+|Maintainability|
 
-Contributions are welcome! Please see our `Contributing Guide <https://dvc.org/doc/user-guide/contributing/core>`_ for more
-details. Thanks to all our contributors!
+Contributions are welcome!
+Please see our `Contributing Guide <https://dvc.org/doc/user-guide/contributing/core>`_ for more details.
+Thanks to all our contributors!
 
 |Contribs|
 
-Mailing List
-============
+Community and Support
+=====================
 
-Want to stay up to date? Want to help improve DVC by participating in our occasional polls? Subscribe to our `mailing list <https://sweedom.us10.list-manage.com/subscribe/post?u=a08bf93caae4063c4e6a351f6&id=24c0ecc49a>`_. No spam, really low traffic.
+* `Twitter <https://twitter.com/DVCorg>`_
+* `Forum <https://discuss.dvc.org/>`_
+* `Discord Chat <https://dvc.org/chat>`_
+* `Email <mailto:support@dvc.org>`_
+* `Mailing List <https://sweedom.us10.list-manage.com/subscribe/post?u=a08bf93caae4063c4e6a351f6&id=24c0ecc49a>`_
 
 Copyright
 =========
 
 This project is distributed under the Apache license version 2.0 (see the LICENSE file in the project root).
 
-By submitting a pull request to this project, you agree to license your contribution under the Apache license version
-2.0 to this project.
+By submitting a pull request to this project, you agree to license your contribution under the Apache license version 2.0 to this project.
 
 Citation
 ========
 
 |DOI|
 
 Iterative, *DVC: Data Version Control - Git for Data & Models* (2020)
@@ -250,30 +271,33 @@
 Barrak, A., Eghan, E.E. and Adams, B. `On the Co-evolution of ML Pipelines and Source Code - Empirical Study of DVC Projects <https://mcis.cs.queensu.ca/publications/2021/saner.pdf>`_ , in Proceedings of the 28th IEEE International Conference on Software Analysis, Evolution, and Reengineering, SANER 2021. Hawaii, USA.
 
 
 .. |Banner| image:: https://dvc.org/img/logo-github-readme.png
    :target: https://dvc.org
    :alt: DVC logo
 
+.. |VS Code Extension Overview| image:: https://raw.githubusercontent.com/iterative/vscode-dvc/main/extension/docs/overview.gif
+   :alt: DVC Extension for VS Code
+
 .. |CI| image:: https://github.com/iterative/dvc/workflows/Tests/badge.svg?branch=main
    :target: https://github.com/iterative/dvc/actions
    :alt: GHA Tests
 
 .. |Maintainability| image:: https://codeclimate.com/github/iterative/dvc/badges/gpa.svg
    :target: https://codeclimate.com/github/iterative/dvc
    :alt: Code Climate
 
+.. |Python Version| image:: https://img.shields.io/pypi/pyversions/dvc
+   :target: https://pypi.org/project/dvc
+   :alt: Python Version
+
 .. |Coverage| image:: https://codecov.io/gh/iterative/dvc/branch/main/graph/badge.svg
    :target: https://codecov.io/gh/iterative/dvc
    :alt: Codecov
 
-.. |Donate| image:: https://img.shields.io/badge/patreon-donate-green.svg?logo=patreon
-   :target: https://www.patreon.com/DVCorg/overview
-   :alt: Donate
-
 .. |Snap| image:: https://img.shields.io/badge/snap-install-82BEA0.svg?logo=snapcraft
    :target: https://snapcraft.io/dvc
    :alt: Snapcraft
 
 .. |Choco| image:: https://img.shields.io/chocolatey/v/dvc?label=choco
    :target: https://chocolatey.org/packages/dvc
    :alt: Chocolatey
@@ -286,14 +310,18 @@
    :target: https://anaconda.org/conda-forge/dvc
    :alt: Conda-forge
 
 .. |PyPI| image:: https://img.shields.io/pypi/v/dvc.svg?label=pip&logo=PyPI&logoColor=white
    :target: https://pypi.org/project/dvc
    :alt: PyPI
 
+.. |PyPI Downloads| image:: https://img.shields.io/pypi/dm/dvc.svg?color=blue&label=Downloads&logo=pypi&logoColor=gold
+   :target: https://pypi.org/project/dvc
+   :alt: PyPI Downloads
+
 .. |Packages| image:: https://img.shields.io/github/v/release/iterative/dvc?label=deb|pkg|rpm|exe&logo=GitHub
    :target: https://github.com/iterative/dvc/releases/latest
    :alt: deb|pkg|rpm|exe
 
 .. |DOI| image:: https://img.shields.io/badge/DOI-10.5281/zenodo.3677553-blue.svg
    :target: https://doi.org/10.5281/zenodo.3677553
    :alt: DOI
@@ -301,7 +329,11 @@
 .. |Flowchart| image:: https://dvc.org/img/flow.gif
    :target: https://dvc.org/img/flow.gif
    :alt: how_dvc_works
 
 .. |Contribs| image:: https://contrib.rocks/image?repo=iterative/dvc
    :target: https://github.com/iterative/dvc/graphs/contributors
    :alt: Contributors
+
+.. |VS Code| image:: https://img.shields.io/visual-studio-marketplace/v/Iterative.dvc?color=blue&label=VSCode&logo=visualstudiocode&logoColor=blue
+   :target: https://marketplace.visualstudio.com/items?itemName=Iterative.dvc
+   :alt: VS Code Extension
```

### Comparing `dvc-2.9.5/dvc/analytics.py` & `dvc-3.0.0a0/dvc/updater.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,190 +1,183 @@
-import json
 import logging
 import os
+import sys
+import time
+from typing import TYPE_CHECKING, Optional
 
-from .env import DVC_NO_ANALYTICS
+from packaging import version
 
-logger = logging.getLogger(__name__)
-
-
-def collect_and_send_report(args=None, return_code=None):
-    """
-    Collect information from the runtime/environment and the command
-    being executed into a report and send it over the network.
-
-    To prevent analytics from blocking the execution of the main thread,
-    sending the report is done in a separate process.
-
-    The inter-process communication happens through a file containing the
-    report as a JSON, where the _collector_ generates it and the _sender_
-    removes it after sending it.
-    """
-    import tempfile
-
-    from dvc.daemon import daemon
+from dvc import __version__
+from dvc.utils.pkg import PKG
 
-    report = {}
+if TYPE_CHECKING:
+    from dvc.ui import RichText
 
-    # Include command execution information on the report only when available.
-    if args and hasattr(args, "func"):
-        report.update({"cmd_class": args.func.__name__})
-
-    if return_code is not None:
-        report.update({"cmd_return_code": return_code})
-
-    with tempfile.NamedTemporaryFile(delete=False, mode="w") as fobj:
-        json.dump(report, fobj)
-    daemon(["analytics", fobj.name])
-
-
-def is_enabled():
-    from dvc.config import Config, to_bool
-    from dvc.utils import env2bool
+logger = logging.getLogger(__name__)
 
-    if env2bool("DVC_TEST"):
-        return False
 
-    enabled = not os.getenv(DVC_NO_ANALYTICS)
-    if enabled:
-        enabled = to_bool(
-            Config(validate=False).get("core", {}).get("analytics", "true")
+class Updater:
+    URL = "https://updater.dvc.org"
+    UPDATER_FILE = "updater"
+    TIMEOUT = 24 * 60 * 60  # every day
+    TIMEOUT_GET = 10
+
+    def __init__(self, tmp_dir, friendly=False, hardlink_lock=False):
+        from dvc.lock import make_lock
+
+        self.updater_file = os.path.join(tmp_dir, self.UPDATER_FILE)
+        self.lock = make_lock(
+            self.updater_file + ".lock",
+            tmp_dir=tmp_dir,
+            friendly=friendly,
+            hardlink_lock=hardlink_lock,
         )
+        self.current = version.parse(__version__).base_version
 
-    logger.debug("Analytics is {}abled.".format("en" if enabled else "dis"))
-
-    return enabled
-
-
-def send(path):
-    """
-    Side effect: Removes the report after sending it.
-
-    The report is generated and stored in a temporary file, see:
-    `collect_and_send_report`. Sending happens on another process,
-    thus, the need of removing such file afterwards.
-    """
-    import requests
-
-    url = "https://analytics.dvc.org"
-    headers = {"content-type": "application/json"}
-
-    with open(path, encoding="utf-8") as fobj:
-        report = json.load(fobj)
-
-    report.update(_runtime_info())
-
-    try:
-        requests.post(url, json=report, headers=headers, timeout=5)
-    except requests.exceptions.RequestException:
-        logger.debug("failed to send analytics report", exc_info=True)
-
-    os.remove(path)
-
-
-def _scm_in_use():
-    from scmrepo.noscm import NoSCM
-
-    from dvc.exceptions import NotDvcRepoError
-    from dvc.repo import Repo
+    def _is_outdated_file(self):
+        ctime = os.path.getmtime(self.updater_file)
+        outdated = time.time() - ctime >= self.TIMEOUT
+        if outdated:
+            logger.debug("'%s' is outdated", self.updater_file)
+        return outdated
+
+    def _with_lock(self, func, action):
+        from dvc.lock import LockError
+
+        try:
+            with self.lock:
+                func()
+        except LockError:
+            logger.debug(
+                "Failed to acquire '%s' before %s updates",
+                self.lock.lockfile,
+                action,
+            )
+
+    def check(self):
+        from dvc.utils import env2bool
+
+        if (
+            os.getenv("CI")
+            or env2bool("DVC_TEST")
+            or PKG == "snap"
+            or not self.is_enabled()
+        ):
+            return
+
+        self._with_lock(self._check, "checking")
+
+    def _check(self):
+        if not os.path.exists(self.updater_file) or self._is_outdated_file():
+            self.fetch()
+            return
 
-    from .scm import SCM, SCMError
+        with open(self.updater_file, encoding="utf-8") as fobj:
+            import json
 
-    try:
-        scm = SCM(root_dir=Repo.find_root())
-        return type(scm).__name__
-    except SCMError:
-        return NoSCM.__name__
-    except NotDvcRepoError:
-        pass
-
-
-def _runtime_info():
-    """
-    Gather information from the environment where DVC runs to fill a report.
-    """
-    from dvc import __version__
-    from dvc.utils import is_binary
-
-    return {
-        "dvc_version": __version__,
-        "is_binary": is_binary(),
-        "scm_class": _scm_in_use(),
-        "system_info": _system_info(),
-        "user_id": _find_or_create_user_id(),
-    }
-
-
-def _system_info():
-    import platform
-    import sys
-
-    import distro
-
-    system = platform.system()
-
-    if system == "Windows":
-        version = sys.getwindowsversion()
-
-        return {
-            "os": "windows",
-            "windows_version_build": version.build,
-            "windows_version_major": version.major,
-            "windows_version_minor": version.minor,
-            "windows_version_service_pack": version.service_pack,
-        }
-
-    if system == "Darwin":
-        return {"os": "mac", "mac_version": platform.mac_ver()[0]}
+            try:
+                info = json.load(fobj)
+                latest = info["version"]
+            except Exception as e:  # noqa: BLE001  # pylint: disable=W0703
+                logger.debug("'%s' is not a valid json: %s", self.updater_file, e)
+                self.fetch()
+                return
+
+        if version.parse(self.current) < version.parse(latest):
+            self._notify(latest)
+
+    def fetch(self, detach=True):
+        from dvc.daemon import daemon
+
+        if detach:
+            daemon(["updater"])
+            return
+
+        self._with_lock(self._get_latest_version, "fetching")
+
+    def _get_latest_version(self):
+        import json
+
+        import requests
+
+        try:
+            resp = requests.get(self.URL, timeout=self.TIMEOUT_GET)
+            info = resp.json()
+        except requests.exceptions.RequestException as exc:
+            logger.debug("Failed to retrieve latest version: %s", exc)
+            return
+
+        with open(self.updater_file, "w+", encoding="utf-8") as fobj:
+            json.dump(info, fobj)
+
+    def _notify(self, latest: str, pkg: Optional[str] = PKG) -> None:
+        from dvc.ui import ui
+
+        if not sys.stdout.isatty():
+            return
+
+        message = self._get_message(latest, pkg=pkg)
+        return ui.error_write(message, styled=True)
+
+    def _get_message(
+        self,
+        latest: str,
+        current: Optional[str] = None,
+        color: str = "yellow",
+        pkg: Optional[str] = None,
+    ) -> "RichText":
+        from dvc.ui import ui
+
+        current = current or self.current
+        update_message = ui.rich_text.from_markup(
+            f"You are using dvc version [bold]{current}[/]; "
+            f"however, version [bold]{latest}[/] is available."
+        )
+        instruction = ui.rich_text.from_markup(self._get_update_instructions(pkg=pkg))
+        return ui.rich_text.assemble(
+            "\n", update_message, "\n", instruction, style=color
+        )
 
-    if system == "Linux":
-        return {
-            "os": "linux",
-            "linux_distro": distro.id(),
-            "linux_distro_like": distro.like(),
-            "linux_distro_version": distro.version(),
+    @staticmethod
+    def _get_update_instructions(pkg: Optional[str] = None) -> str:
+        if pkg in ("osxpkg", "exe", "binary"):
+            return (
+                "To upgrade, uninstall dvc and reinstall from [blue]https://dvc.org[/]."
+            )
+
+        instructions = {
+            "pip": "pip install --upgrade dvc",
+            "rpm": "yum update dvc",
+            "brew": "brew upgrade dvc",
+            "deb": "apt-get install --only-upgrade dvc",
+            "conda": "conda update dvc",
+            "choco": "choco upgrade dvc",
         }
 
-    # We don't collect data for any other system.
-    raise NotImplementedError
-
-
-def _find_or_create_user_id():
-    """
-    The user's ID is stored on a file under the global config directory.
-
-    The file should contain a JSON with a "user_id" key:
-
-        {"user_id": "16fd2706-8baf-433b-82eb-8c7fada847da"}
+        if pkg not in instructions:
+            return (
+                "Find the latest release at "
+                "[blue]https://github.com/iterative/dvc/releases/latest[/]."
+            )
 
-    IDs are generated randomly with UUID.
-    """
-    import uuid
+        instruction = instructions[pkg]
+        return f"To upgrade, run '{instruction}'."
 
-    from dvc.config import Config
-    from dvc.lock import Lock, LockError
-    from dvc.utils.fs import makedirs
+    def is_enabled(self):
+        from dvc.config import Config, to_bool
 
-    config_dir = Config.get_dir("global")
-    fname = os.path.join(config_dir, "user_id")
-    lockfile = os.path.join(config_dir, "user_id.lock")
-
-    # Since the `fname` and `lockfile` are under the global config,
-    # we need to make sure such directory exist already.
-    makedirs(config_dir, exist_ok=True)
-
-    try:
-        with Lock(lockfile):
-            try:
-                with open(fname, encoding="utf-8") as fobj:
-                    user_id = json.load(fobj)["user_id"]
+        enabled = to_bool(
+            Config.from_cwd(validate=False).get("core", {}).get("check_update", "true")
+        )
+        logger.debug("Check for update is %sabled.", "en" if enabled else "dis")
+        return enabled
 
-            except (FileNotFoundError, ValueError, KeyError):
-                user_id = str(uuid.uuid4())
 
-                with open(fname, "w", encoding="utf-8") as fobj:
-                    json.dump({"user_id": user_id}, fobj)
+def notify_updates():
+    from contextlib import suppress
 
-            return user_id
+    from dvc.repo import NotDvcRepoError, Repo
 
-    except LockError:
-        logger.debug(f"Failed to acquire '{lockfile}'")
+    with suppress(NotDvcRepoError), Repo() as repo:
+        hardlink_lock = repo.config["core"].get("hardlink_lock", False)
+        updater = Updater(repo.tmp_dir, hardlink_lock=hardlink_lock)
+        updater.check()
```

### Comparing `dvc-2.9.5/dvc/cli/parser.py` & `dvc-3.0.0a0/dvc/cli/parser.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,43 +1,46 @@
 """Main parser for the dvc cli."""
 import argparse
 import logging
 import os
-import sys
+from functools import lru_cache
 
+from dvc import __version__
 from dvc.commands import (
     add,
     cache,
     check_ignore,
     checkout,
     commit,
     completion,
     config,
     daemon,
     dag,
+    data,
     data_sync,
     destroy,
     diff,
     experiments,
     freeze,
     gc,
     get,
     get_url,
     git_hook,
     imp,
     imp_url,
     init,
     install,
-    live,
     ls,
+    ls_url,
     machine,
     metrics,
     move,
     params,
     plots,
+    queue,
     remote,
     remove,
     repro,
     root,
     run,
     stage,
     unprotect,
@@ -47,14 +50,15 @@
 
 from . import DvcParserError
 
 logger = logging.getLogger(__name__)
 
 COMMANDS = [
     init,
+    queue,
     get,
     get_url,
     destroy,
     add,
     remove,
     move,
     unprotect,
@@ -69,29 +73,30 @@
     remote,
     cache,
     metrics,
     params,
     install,
     root,
     ls,
+    ls_url,
     freeze,
     dag,
     daemon,
     commit,
     completion,
     diff,
     version,
     update,
     git_hook,
     plots,
     stage,
     experiments,
     check_ignore,
-    live,
     machine,
+    data,
 ]
 
 
 def _find_parser(parser, cmd_cls):
     defaults = parser._defaults  # pylint: disable=protected-access
     if not cmd_cls or cmd_cls == defaults.get("func"):
         parser.print_help()
@@ -120,25 +125,14 @@
         args, argv = self.parse_known_args(args, namespace)
         if argv:
             msg = "unrecognized arguments: %s"
             self.error(msg % " ".join(argv), getattr(args, "func", None))
         return args
 
 
-class VersionAction(argparse.Action):  # pragma: no cover
-    # pylint: disable=too-few-public-methods
-    """Shows DVC version and exits."""
-
-    def __call__(self, parser, namespace, values, option_string=None):
-        from dvc import __version__
-
-        print(__version__)
-        sys.exit(0)
-
-
 def get_parent_parser():
     """Create instances of a parser containing common arguments shared among
     all the commands.
 
     When overwriting `-q` or `-v`, you need to instantiate a new object
     in order to prevent some weird behavior.
     """
@@ -153,14 +147,15 @@
     log_level_group.add_argument(
         "-v", "--verbose", action="count", default=0, help="Be verbose."
     )
 
     return parent_parser
 
 
+@lru_cache(maxsize=1)
 def get_main_parser():
     parent_parser = get_parent_parser()
 
     # Main parser
     desc = "Data Version Control"
     parser = DvcParser(
         prog="dvc",
@@ -178,21 +173,19 @@
         "-h",
         "--help",
         action="help",
         default=argparse.SUPPRESS,
         help="Show this help message and exit.",
     )
 
-    # NOTE: On some python versions action='version' prints to stderr
-    # instead of stdout https://bugs.python.org/issue18920
     parser.add_argument(
         "-V",
         "--version",
-        action=VersionAction,
-        nargs=0,
+        action="version",
+        version=__version__,
         help="Show program's version.",
     )
 
     parser.add_argument(
         "--cd",
         default=os.path.curdir,
         metavar="<path>",
```

### Comparing `dvc-2.9.5/dvc/commands/add.py` & `dvc-3.0.0a0/dvc/commands/add.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,42 +1,41 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 
 logger = logging.getLogger(__name__)
 
 
 class CmdAdd(CmdBase):
     def run(self):
-        from dvc.exceptions import (
-            DvcException,
-            RecursiveAddingWhileUsingFilename,
-        )
+        from dvc.exceptions import DvcException, RecursiveAddingWhileUsingFilename
 
         try:
             if len(self.args.targets) > 1 and self.args.file:
                 raise RecursiveAddingWhileUsingFilename()
 
             self.repo.add(
                 self.args.targets,
                 recursive=self.args.recursive,
                 no_commit=self.args.no_commit,
                 fname=self.args.file,
                 external=self.args.external,
                 glob=self.args.glob,
-                desc=self.args.desc,
                 out=self.args.out,
                 remote=self.args.remote,
                 to_remote=self.args.to_remote,
                 jobs=self.args.jobs,
+                force=self.args.force,
             )
-
+        except FileNotFoundError:
+            logger.exception("")
+            return 1
         except DvcException:
             logger.exception("")
             return 1
         return 0
 
 
 def add_parser(subparsers, parent_parser):
@@ -102,24 +101,21 @@
         "--jobs",
         type=int,
         help=(
             "Only used along with '--to-remote'. "
             "Number of jobs to run simultaneously "
             "when pushing data to remote."
             "The default value is 4 * cpu_count(). "
-            "For SSH remotes, the default is 4. "
         ),
         metavar="<number>",
     )
     parser.add_argument(
-        "--desc",
-        type=str,
-        metavar="<text>",
-        help=(
-            "User description of the data (optional). "
-            "This doesn't affect any DVC operations."
-        ),
+        "-f",
+        "--force",
+        action="store_true",
+        default=False,
+        help="Override local file or folder if exists.",
     )
     parser.add_argument(
         "targets", nargs="+", help="Input files/directories to add."
     ).complete = completion.FILE
     parser.set_defaults(func=CmdAdd)
```

### Comparing `dvc-2.9.5/dvc/commands/cache.py` & `dvc-3.0.0a0/dvc/commands/cache.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,25 +1,34 @@
 import argparse
+import os
 
+from dvc.cli import completion
 from dvc.cli.utils import append_doc_link, fix_subparsers
-from dvc.commands import completion
 from dvc.commands.config import CmdConfig
 from dvc.ui import ui
 
 
 class CmdCacheDir(CmdConfig):
     def run(self):
         if self.args.value is None and not self.args.unset:
+            from dvc.config import ConfigError
+
             if self.args.level:
                 conf = self.config.read(level=self.args.level)
             else:
                 # Use merged config with default values
                 conf = self.config
-            self._check(conf, False, "cache", "dir")
-            ui.write(conf["cache"]["dir"])
+            try:
+                self._check(conf, False, "cache", "dir")
+                path = conf["cache"]["dir"]
+            except ConfigError:
+                if not self.config.dvc_dir or self.args.level:
+                    raise
+                path = os.path.join(self.config.dvc_dir, "cache")
+            ui.write(path)
             return 0
         with self.config.edit(level=self.args.level) as conf:
             if self.args.unset:
                 self._check(conf, False, "cache", "dir")
                 del conf["cache"]["dir"]
             else:
                 self._check(conf, False, "cache")
@@ -38,15 +47,15 @@
         description=append_doc_link(CACHE_HELP, "cache"),
         help=CACHE_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
 
     cache_subparsers = cache_parser.add_subparsers(
         dest="cmd",
-        help="Use `dvc cache CMD --help` for command-specific " "help.",
+        help="Use `dvc cache CMD --help` for command-specific help.",
     )
 
     fix_subparsers(cache_subparsers)
 
     parent_cache_config_parser = argparse.ArgumentParser(
         add_help=False, parents=[parent_config_parser]
     )
@@ -64,14 +73,16 @@
         "--unset",
         default=False,
         action="store_true",
         help="Unset option.",
     )
     cache_dir_parser.add_argument(
         "value",
-        help="Path to cache directory. Relative paths are resolved relative "
-        "to the current directory and saved to config relative to the "
-        "config file location. If no path is provided, it returns the "
-        "current cache directory.",
+        help=(
+            "Path to cache directory. Relative paths are resolved relative "
+            "to the current directory and saved to config relative to the "
+            "config file location. If no path is provided, it returns the "
+            "current cache directory."
+        ),
         nargs="?",
     ).complete = completion.DIR
     cache_dir_parser.set_defaults(func=CmdCacheDir)
```

### Comparing `dvc-2.9.5/dvc/commands/check_ignore.py` & `dvc-3.0.0a0/dvc/commands/check_ignore.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 import argparse
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 from dvc.ui import ui
 
 
 class CmdCheckIgnore(CmdBase):
     def __init__(self, args):
         super().__init__(args)
         self.ignore_filter = self.repo.dvcignore
@@ -35,15 +35,15 @@
     def _interactive_mode(self):
         ret = 1
         while True:
             try:
                 target = input()
             except (KeyboardInterrupt, EOFError):
                 break
-            if target == "":
+            if not target:
                 break
             if not self._check_one_file(target):
                 ret = 0
         return ret
 
     def _normal_mode(self):
         ret = 1
@@ -58,17 +58,15 @@
         if not self.args.stdin and not self.args.targets:
             raise DvcException("`targets` or `--stdin` needed")
 
         if self.args.stdin and self.args.targets:
             raise DvcException("cannot have both `targets` and `--stdin`")
 
         if self.args.non_matching and not self.args.details:
-            raise DvcException(
-                "`--non-matching` is only valid with `--details`"
-            )
+            raise DvcException("`--non-matching` is only valid with `--details`")
 
         if self.args.all and not self.args.details:
             raise DvcException("`--all` is only valid with `--details`")
 
         if self.args.quiet and self.args.details:
             raise DvcException("cannot use both `--details` and `--quiet`")
 
@@ -76,17 +74,15 @@
         self._check_args()
         if self.args.stdin:
             return self._interactive_mode()
         return self._normal_mode()
 
 
 def add_parser(subparsers, parent_parser):
-    ADD_HELP = (
-        "Check whether files or directories are excluded due to `.dvcignore`."
-    )
+    ADD_HELP = "Check whether files or directories are excluded due to `.dvcignore`."
 
     parser = subparsers.add_parser(
         "check-ignore",
         parents=[parent_parser],
         description=append_doc_link(ADD_HELP, "check-ignore"),
         help=ADD_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
@@ -99,24 +95,28 @@
         help="Show the exclude patterns along with each target path.",
     )
     parser.add_argument(
         "-a",
         "--all",
         action="store_true",
         default=False,
-        help="Include the target paths which don’t match any pattern "
-        "in the `--details` list.",
+        help=(
+            "Include the target paths which don't match any pattern "
+            "in the `--details` list."
+        ),
     )
     parser.add_argument(
         "-n",
         "--non-matching",
         action="store_true",
         default=False,
-        help="Include the target paths which don’t match any pattern "
-        "in the `--details` list.",
+        help=(
+            "Include the target paths which don't match any pattern "
+            "in the `--details` list."
+        ),
     )
     parser.add_argument(
         "--stdin",
         action="store_true",
         default=False,
         help="Read paths from standard input instead of providing `targets`.",
     )
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `dvc-2.9.5/dvc/commands/checkout.py` & `dvc-3.0.0a0/dvc/commands/checkout.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import argparse
 import operator
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 from dvc.exceptions import CheckoutError
 from dvc.ui import ui
 
 
 def log_changes(stats):
     colors = {
         "modified": "yellow",
@@ -18,17 +18,15 @@
     for state, color in colors.items():
         entries = stats.get(state)
 
         if not entries:
             continue
 
         for entry in entries:
-            ui.write(
-                f"[{color}]{state[0].upper()}", entry, styled=True, sep="\t"
-            )
+            ui.write(f"[{color}]{state[0].upper()}", entry, styled=True, sep="\t")
 
 
 class CmdCheckout(CmdBase):
     def run(self):
         from dvc.utils.humanize import get_summary
 
         stats, exc = None, None
@@ -42,23 +40,25 @@
             )
         except CheckoutError as _exc:
             exc = _exc
             stats = exc.stats
 
         if self.args.summary:
             default_message = "No changes."
-            msg = get_summary(
-                sorted(stats.items(), key=operator.itemgetter(0))
-            )
+            msg = get_summary(sorted(stats.items(), key=operator.itemgetter(0)))
             ui.write(msg or default_message)
         else:
             log_changes(stats)
 
         if exc:
             raise exc
+
+        if self.args.relink:
+            msg = "Relinked successfully"
+            ui.write(msg)
         return 0
 
 
 def add_parser(subparsers, parent_parser):
     CHECKOUT_HELP = "Checkout data files from cache."
 
     checkout_parser = subparsers.add_parser(
@@ -102,11 +102,11 @@
         help="Recreate links or copies from cache to workspace.",
     )
     checkout_parser.add_argument(
         "targets",
         nargs="*",
         help=(
             "Limit command scope to these tracked files/directories, "
-            ".dvc files, or stage names."
+            ".dvc files and stage names."
         ),
     ).complete = completion.DVC_FILE
     checkout_parser.set_defaults(func=CmdCheckout)
```

### Comparing `dvc-2.9.5/dvc/commands/commit.py` & `dvc-3.0.0a0/dvc/commands/commit.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 
 logger = logging.getLogger(__name__)
 
 
 class CmdCommit(CmdBase):
     def run(self):
         from dvc.exceptions import DvcException
@@ -20,19 +20,15 @@
                 self.repo.commit(
                     target,
                     with_deps=self.args.with_deps,
                     recursive=self.args.recursive,
                     force=self.args.force,
                 )
             except DvcException:
-                logger.exception(
-                    "failed to commit{}".format(
-                        (" " + target) if target else ""
-                    )
-                )
+                logger.exception("failed to commit%s", (" " + target) if target else "")
                 return 1
         return 0
 
 
 def add_parser(subparsers, parent_parser):
     COMMIT_HELP = (
         "Record changes to files or directories tracked by DVC"
@@ -66,11 +62,13 @@
         action="store_true",
         default=False,
         help="Commit cache for subdirectories of the specified directory.",
     )
     commit_parser.add_argument(
         "targets",
         nargs="*",
-        help="stages or .dvc files to commit. Optional. "
-        "(Finds all DVC files in the workspace by default.)",
+        help=(
+            "Limit command scope to these tracked files/directories, "
+            ".dvc files and stage names."
+        ),
     ).complete = completion.DVCFILES_AND_STAGE
     commit_parser.set_defaults(func=CmdCommit)
```

### Comparing `dvc-2.9.5/dvc/commands/config.py` & `dvc-3.0.0a0/dvc/commands/config.py`

 * *Files 10% similar despite different names*

```diff
@@ -13,41 +13,38 @@
 
 def _name_type(value):
     import re
 
     match = re.match(NAME_REGEX, value)
     if not match:
         raise argparse.ArgumentTypeError(
-            "name argument should look like "
-            "remote.name.option or "
-            "section.option"
+            "name argument should look like remote.name.option or section.option"
         )
     return (
         bool(match.group("remote")),
         match.group("section").lower(),
         match.group("option").lower(),
     )
 
 
 class CmdConfig(CmdBaseNoRepo):
     def __init__(self, args):
         from dvc.config import Config
 
         super().__init__(args)
 
-        self.config = Config(validate=False)
+        self.config = Config.from_cwd(validate=False)
 
     def run(self):
-        if self.args.show_origin:
-            if any((self.args.value, self.args.unset)):
-                logger.error(
-                    "--show-origin can't be used together with any of these "
-                    "options: -u/--unset, value"
-                )
-                return 1
+        if self.args.show_origin and (self.args.value or self.args.unset):
+            logger.error(
+                "--show-origin can't be used together with any of these "
+                "options: -u/--unset, value"
+            )
+            return 1
 
         if self.args.list:
             return self._list()
 
         if self.args.name is None:
             logger.error("name argument is required")
             return 1
@@ -67,17 +64,15 @@
             )
             return 1
 
         levels = self._get_appropriate_levels(self.args.level)
 
         for level in levels:
             conf = self.config.read(level)
-            prefix = self._config_file_prefix(
-                self.args.show_origin, self.config, level
-            )
+            prefix = self._config_file_prefix(self.args.show_origin, self.config, level)
             configs = list(self._format_config(conf, prefix))
             if configs:
                 ui.write("\n".join(configs))
 
         return 0
 
     def _get(self, remote, section, opt):
@@ -128,17 +123,15 @@
         from dvc.config import ConfigError
 
         name = "remote" if remote else "section"
         if section not in conf:
             raise ConfigError(f"{name} '{section}' doesn't exist")
 
         if opt and opt not in conf[section]:
-            raise ConfigError(
-                f"option '{opt}' doesn't exist in {name} '{section}'"
-            )
+            raise ConfigError(f"option '{opt}' doesn't exist in {name} '{section}'")
 
     def _get_appropriate_levels(self, levels):
         if levels:
             self._validate_level_for_non_repo_operation(levels)
             return [levels]
         if self.config.dvc_dir is None:
             return self.config.SYSTEM_LEVELS
```

### Comparing `dvc-2.9.5/dvc/commands/daemon.py` & `dvc-3.0.0a0/dvc/commands/daemon.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
+from dvc.cli import completion
 from dvc.cli.command import CmdBaseNoRepo
 from dvc.cli.utils import fix_subparsers
-from dvc.commands import completion
 
 
 class CmdDaemonBase(CmdBaseNoRepo):
     pass
 
 
 class CmdDaemonUpdater(CmdDaemonBase):
@@ -42,15 +42,15 @@
         parents=[parent_parser],
         description=DAEMON_HELP,
         add_help=False,
     )
 
     daemon_subparsers = daemon_parser.add_subparsers(
         dest="cmd",
-        help="Use `dvc daemon CMD --help` for command-specific " "help.",
+        help="Use `dvc daemon CMD --help` for command-specific help.",
     )
 
     fix_subparsers(daemon_subparsers)
 
     DAEMON_UPDATER_HELP = "Fetch latest available version."
     daemon_updater_parser = daemon_subparsers.add_parser(
         "updater",
```

### Comparing `dvc-2.9.5/dvc/commands/data_sync.py` & `dvc-3.0.0a0/dvc/commands/data_sync.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 
 logger = logging.getLogger(__name__)
 
 
 class CmdDataBase(CmdBase):
     def log_summary(self, stats):
         from dvc.ui import ui
@@ -36,18 +36,20 @@
                 all_tags=self.args.all_tags,
                 all_commits=self.args.all_commits,
                 with_deps=self.args.with_deps,
                 force=self.args.force,
                 recursive=self.args.recursive,
                 run_cache=self.args.run_cache,
                 glob=self.args.glob,
+                allow_missing=self.args.allow_missing,
             )
             self.log_summary(stats)
         except (CheckoutError, DvcException) as exc:
-            self.log_summary(getattr(exc, "stats", {}))
+            if stats := getattr(exc, "stats", {}):
+                self.log_summary(stats)
             logger.exception("failed to pull data from the cloud")
             return 1
 
         return 0
 
 
 class CmdDataPush(CmdDataBase):
@@ -107,26 +109,25 @@
     parent_parser.add_argument(
         "-j",
         "--jobs",
         type=int,
         help=(
             "Number of jobs to run simultaneously. "
             "The default value is 4 * cpu_count(). "
-            "For SSH remotes, the default is 4. "
         ),
         metavar="<number>",
     )
     parent_parser.add_argument(
         "targets",
         nargs="*",
         help=(
             "Limit command scope to these tracked files/directories, "
-            ".dvc files, or stage names."
+            ".dvc files and stage names."
         ),
-    ).complete = completion.DVC_FILE
+    ).complete = completion.DVC_FILE  # type: ignore[attr-defined]
 
     return parent_parser
 
 
 def add_parser(subparsers, _parent_parser):
     from dvc.commands.status import CmdDataStatus
 
@@ -191,15 +192,21 @@
         default=False,
         help="Fetch run history for all stages.",
     )
     pull_parser.add_argument(
         "--glob",
         action="store_true",
         default=False,
-        help="Pull cache for targets matching shell-style wildcards.",
+        help=argparse.SUPPRESS,
+    )
+    pull_parser.add_argument(
+        "--allow-missing",
+        action="store_true",
+        default=False,
+        help="Ignore errors if some of the files or directories are missing.",
     )
     pull_parser.set_defaults(func=CmdDataPull)
 
     # Push
     PUSH_HELP = "Upload tracked files or directories to remote storage."
 
     push_parser = subparsers.add_parser(
@@ -258,17 +265,15 @@
         action="store_true",
         default=False,
         help="Allows targets containing shell-style wildcards.",
     )
     push_parser.set_defaults(func=CmdDataPush)
 
     # Fetch
-    FETCH_HELP = (
-        "Download files or directories from remote storage to the cache."
-    )
+    FETCH_HELP = "Download files or directories from remote storage to the cache."
 
     fetch_parser = subparsers.add_parser(
         "fetch",
         parents=[shared_parent_parser()],
         description=append_doc_link(FETCH_HELP, "fetch"),
         help=FETCH_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
@@ -298,15 +303,15 @@
         help="Fetch cache for all commits.",
     )
     fetch_parser.add_argument(
         "-d",
         "--with-deps",
         action="store_true",
         default=False,
-        help="Fetch cache for all dependencies of the " "specified target.",
+        help="Fetch cache for all dependencies of the specified target.",
     )
     fetch_parser.add_argument(
         "-R",
         "--recursive",
         action="store_true",
         default=False,
         help="Fetch cache for subdirectories of specified directory.",
@@ -316,17 +321,15 @@
         action="store_true",
         default=False,
         help="Fetch run history for all stages.",
     )
     fetch_parser.set_defaults(func=CmdDataFetch)
 
     # Status
-    STATUS_HELP = (
-        "Show changed stages, compare local cache and a remote storage."
-    )
+    STATUS_HELP = "Show changed stages, compare local cache and a remote storage."
 
     status_parser = subparsers.add_parser(
         "status",
         parents=[shared_parent_parser()],
         description=append_doc_link(STATUS_HELP, "status"),
         help=STATUS_HELP,
         conflict_handler="resolve",
@@ -356,32 +359,37 @@
         metavar="<name>",
     )
     status_parser.add_argument(
         "-a",
         "--all-branches",
         action="store_true",
         default=False,
-        help="Show status of a local cache compared to a remote repository "
-        "for all branches.",
+        help=(
+            "Show status of a local cache compared to a remote repository "
+            "for all branches."
+        ),
     )
     status_parser.add_argument(
         "-T",
         "--all-tags",
         action="store_true",
         default=False,
-        help="Show status of a local cache compared to a remote repository "
-        "for all tags.",
+        help=(
+            "Show status of a local cache compared to a remote repository for all tags."
+        ),
     )
     status_parser.add_argument(
         "-A",
         "--all-commits",
         action="store_true",
         default=False,
-        help="Show status of a local cache compared to a remote repository "
-        "for all commits.",
+        help=(
+            "Show status of a local cache compared to a remote repository "
+            "for all commits."
+        ),
     )
     status_parser.add_argument(
         "-d",
         "--with-deps",
         action="store_true",
         default=False,
         help="Show status for all dependencies of the specified target.",
@@ -391,14 +399,13 @@
         "--recursive",
         action="store_true",
         default=False,
         help="Show status of all stages in the specified directory.",
     )
     status_parser.add_argument(
         "--json",
-        "--show-json",
         action="store_true",
         default=False,
         help="Show status in JSON format.",
     )
 
     status_parser.set_defaults(func=CmdDataStatus)
```

### Comparing `dvc-2.9.5/dvc/commands/destroy.py` & `dvc-3.0.0a0/dvc/commands/destroy.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/dvc/commands/diff.py` & `dvc-3.0.0a0/dvc/commands/diff.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import argparse
 import logging
 import os
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 from dvc.ui import ui
 
 logger = logging.getLogger(__name__)
 
 
 def _digest(checksum):
     if isinstance(checksum, str):
@@ -105,41 +105,39 @@
                         path=path,
                     )
                 )
 
             ui.write()
 
         if not sum(summary.values()):
-            return None
+            return
 
         states_summary = ", ".join(
-            f"{summary[state]} {state}"
-            for state in states
-            if summary[state] > 0
+            f"{summary[state]} {state}" for state in states if summary[state] > 0
         )
         ui.write("files summary:", states_summary)
 
     def run(self):
         from dvc.exceptions import DvcException
 
         try:
-            diff = self.repo.diff(
-                self.args.a_rev, self.args.b_rev, self.args.targets
-            )
+            diff = self.repo.diff(self.args.a_rev, self.args.b_rev, self.args.targets)
             show_hash = self.args.show_hash
             hide_missing = self.args.b_rev or self.args.hide_missing
             if hide_missing:
-                del diff["not in cache"]
+                diff.pop("not in cache", None)
 
             for key, entries in diff.items():
                 entries = sorted(
                     entries,
-                    key=lambda entry: entry["path"]["old"]
-                    if isinstance(entry["path"], dict)
-                    else entry["path"],
+                    key=lambda entry: (
+                        entry["path"]["old"]
+                        if isinstance(entry["path"], dict)
+                        else entry["path"]
+                    ),
                 )
                 if not show_hash:
                     for entry in entries:
                         del entry["hash"]
                 diff[key] = entries
 
             if self.args.json:
@@ -166,47 +164,42 @@
         description=append_doc_link(DIFF_DESCRIPTION, "diff"),
         help=DIFF_DESCRIPTION,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
     diff_parser.add_argument(
         "--targets",
         nargs="*",
-        help=(
-            "Specific DVC-tracked files to compare. "
-            "Accepts one or more file paths."
-        ),
+        help="Specific DVC-tracked files to compare. Accepts one or more file paths.",
         metavar="<paths>",
     ).complete = completion.FILE
     diff_parser.add_argument(
         "a_rev",
         help="Old Git commit to compare (defaults to HEAD)",
         nargs="?",
         default="HEAD",
     )
     diff_parser.add_argument(
         "b_rev",
-        help=("New Git commit to compare (defaults to the current workspace)"),
+        help="New Git commit to compare (defaults to the current workspace)",
         nargs="?",
     )
     diff_parser.add_argument(
         "--json",
-        "--show-json",
         help="Format the output into a JSON",
         action="store_true",
         default=False,
     )
     diff_parser.add_argument(
         "--show-hash",
         help="Display hash value for each entry",
         action="store_true",
         default=False,
     )
     diff_parser.add_argument(
         "--md",
-        "--show-md",
         help="Show tabulated output in the Markdown format (GFM).",
         action="store_true",
         dest="markdown",
         default=False,
     )
     diff_parser.add_argument(
         "--hide-missing",
```

### Comparing `dvc-2.9.5/dvc/commands/experiments/__init__.py` & `dvc-3.0.0a0/dvc/commands/queue/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,60 +1,34 @@
 import argparse
 
-from dvc.cli.utils import (
-    append_doc_link,
-    fix_plumbing_subparsers,
-    fix_subparsers,
-)
-from dvc.commands.experiments import (
-    apply,
-    branch,
-    diff,
-    exec_run,
-    gc,
-    init,
-    ls,
-    pull,
-    push,
-    remove,
-    run,
-    show,
-)
+from dvc.cli.utils import append_doc_link, fix_subparsers
+from dvc.commands.queue import kill, logs, remove, start, status, stop
 
 SUB_COMMANDS = [
-    apply,
-    branch,
-    diff,
-    exec_run,
-    gc,
-    init,
-    ls,
-    pull,
-    push,
+    start,
+    stop,
+    status,
+    logs,
     remove,
-    run,
-    show,
+    kill,
 ]
 
 
 def add_parser(subparsers, parent_parser):
-    EXPERIMENTS_HELP = "Commands to run and compare experiments."
+    QUEUE_HELP = "Commands to manage experiments queue."
 
-    experiments_parser = subparsers.add_parser(
-        "experiments",
+    queue_parser = subparsers.add_parser(
+        "queue",
         parents=[parent_parser],
-        aliases=["exp"],
-        description=append_doc_link(EXPERIMENTS_HELP, "exp"),
+        description=append_doc_link(QUEUE_HELP, "queue"),
         formatter_class=argparse.RawDescriptionHelpFormatter,
-        help=EXPERIMENTS_HELP,
+        help=QUEUE_HELP,
     )
 
-    experiments_subparsers = experiments_parser.add_subparsers(
+    queue_subparsers = queue_parser.add_subparsers(
         dest="cmd",
-        help="Use `dvc experiments CMD --help` to display "
-        "command-specific help.",
+        help="Use `dvc queue CMD --help` to display command-specific help.",
     )
 
-    fix_subparsers(experiments_subparsers)
+    fix_subparsers(queue_subparsers)
     for cmd in SUB_COMMANDS:
-        cmd.add_parser(experiments_subparsers, parent_parser)
-    fix_plumbing_subparsers(experiments_subparsers)
+        cmd.add_parser(queue_subparsers, parent_parser)
```

### Comparing `dvc-2.9.5/dvc/commands/experiments/apply.py` & `dvc-3.0.0a0/dvc/commands/experiments/apply.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,32 +1,34 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
+from dvc.ui import ui
 
 logger = logging.getLogger(__name__)
 
 
 class CmdExperimentsApply(CmdBase):
     def run(self):
-
-        self.repo.experiments.apply(
-            self.args.experiment, force=self.args.force
-        )
+        if not self.args.force:
+            ui.write(
+                "The --no-force option is deprecated and will be removed in a future"
+                " DVC release. To revert the result of 'exp apply', run:\n"
+                "\n\tgit reset --hard\n"
+                "\tgit stash apply refs/exps/apply/stash\n"
+            )
+        self.repo.experiments.apply(self.args.experiment)
 
         return 0
 
 
 def add_parser(experiments_subparsers, parent_parser):
-
-    EXPERIMENTS_APPLY_HELP = (
-        "Apply the changes from an experiment to your workspace."
-    )
+    EXPERIMENTS_APPLY_HELP = "Apply the changes from an experiment to your workspace."
     experiments_apply_parser = experiments_subparsers.add_parser(
         "apply",
         parents=[parent_parser],
         description=append_doc_link(EXPERIMENTS_APPLY_HELP, "exp/apply"),
         help=EXPERIMENTS_APPLY_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
```

### Comparing `dvc-2.9.5/dvc/commands/experiments/branch.py` & `dvc-3.0.0a0/dvc/commands/experiments/branch.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,30 +5,34 @@
 from dvc.cli.utils import append_doc_link
 
 logger = logging.getLogger(__name__)
 
 
 class CmdExperimentsBranch(CmdBase):
     def run(self):
-
         self.repo.experiments.branch(self.args.experiment, self.args.branch)
 
         return 0
 
 
 def add_parser(experiments_subparsers, parent_parser):
-
     EXPERIMENTS_BRANCH_HELP = "Promote an experiment to a Git branch."
     experiments_branch_parser = experiments_subparsers.add_parser(
         "branch",
         parents=[parent_parser],
         description=append_doc_link(EXPERIMENTS_BRANCH_HELP, "exp/branch"),
         help=EXPERIMENTS_BRANCH_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
     experiments_branch_parser.add_argument(
         "experiment", help="Experiment to be promoted."
     )
     experiments_branch_parser.add_argument(
-        "branch", help="Git branch name to use."
+        "branch",
+        nargs="?",
+        default=None,
+        help=(
+            "Optional name for the new Git branch. "
+            "Defaults to '{experiment-name}-branch'."
+        ),
     )
     experiments_branch_parser.set_defaults(func=CmdExperimentsBranch)
```

### Comparing `dvc-2.9.5/dvc/commands/experiments/diff.py` & `dvc-3.0.0a0/dvc/commands/experiments/diff.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,37 +1,36 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 from dvc.commands.metrics import DEFAULT_PRECISION
 from dvc.exceptions import DvcException
 from dvc.ui import ui
 
 logger = logging.getLogger(__name__)
 
 
 class CmdExperimentsDiff(CmdBase):
     def run(self):
-
         try:
             diff = self.repo.experiments.diff(
                 a_rev=self.args.a_rev,
                 b_rev=self.args.b_rev,
                 all=self.args.all,
                 param_deps=self.args.param_deps,
             )
         except DvcException:
             logger.exception("failed to show experiments diff")
             return 1
 
         if self.args.json:
             ui.write_json(diff)
-        else:
+        elif diff:
             from dvc.compare import show_diff
 
             precision = self.args.precision or DEFAULT_PRECISION
             diffs = [("metrics", "Metric"), ("params", "Param")]
             for idx, (key, title) in enumerate(diffs):
                 if idx:
                     # we are printing tables even in `--quiet` mode
@@ -49,18 +48,15 @@
                     b_rev=self.args.b_rev,
                 )
 
         return 0
 
 
 def add_parser(experiments_subparsers, parent_parser):
-
-    EXPERIMENTS_DIFF_HELP = (
-        "Show changes between experiments in the DVC repository."
-    )
+    EXPERIMENTS_DIFF_HELP = "Show changes between experiments."
 
     experiments_diff_parser = experiments_subparsers.add_parser(
         "diff",
         parents=[parent_parser],
         description=append_doc_link(EXPERIMENTS_DIFF_HELP, "exp/diff"),
         help=EXPERIMENTS_DIFF_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
@@ -83,22 +79,20 @@
         "--param-deps",
         action="store_true",
         default=False,
         help="Show only params that are stage dependencies.",
     )
     experiments_diff_parser.add_argument(
         "--json",
-        "--show-json",
         action="store_true",
         default=False,
         help="Show output in JSON format.",
     )
     experiments_diff_parser.add_argument(
         "--md",
-        "--show-md",
         action="store_true",
         default=False,
         dest="markdown",
         help="Show tabulated output in the Markdown format (GFM).",
     )
     experiments_diff_parser.add_argument(
         "--no-path",
```

### Comparing `dvc-2.9.5/dvc/commands/experiments/exec_run.py` & `dvc-3.0.0a0/dvc/commands/experiments/exec_run.py`

 * *Files 13% similar despite different names*

```diff
@@ -5,27 +5,26 @@
 logger = logging.getLogger(__name__)
 
 
 class CmdExecutorRun(CmdBaseNoRepo):
     """Run an experiment executor."""
 
     def run(self):
-        from dvc.repo.experiments.executor.base import (
-            BaseExecutor,
-            ExecutorInfo,
-        )
+        from dvc.repo.experiments.executor.base import BaseExecutor, ExecutorInfo
         from dvc.utils.serialize import load_json
 
         info = ExecutorInfo.from_dict(load_json(self.args.infofile))
         BaseExecutor.reproduce(
             info=info,
             rev="",
             queue=None,
             log_level=logger.getEffectiveLevel(),
             infofile=self.args.infofile,
+            copy_paths=self.args.copy_paths,
+            message=self.args.message,
         )
         return 0
 
 
 def add_parser(experiments_subparsers, parent_parser):
     EXEC_RUN_HELP = "Run an experiment executor."
     exec_run_parser = experiments_subparsers.add_parser(
@@ -35,8 +34,25 @@
         add_help=False,
     )
     exec_run_parser.add_argument(
         "--infofile",
         help="Path to executor info file",
         default=None,
     )
+    exec_run_parser.add_argument(
+        "-C",
+        "--copy-paths",
+        action="append",
+        default=[],
+        help=(
+            "List of ignored or untracked paths to copy into the temp directory."
+            " Only used if `--temp` or `--queue` is specified."
+        ),
+    )
+    exec_run_parser.add_argument(
+        "-M",
+        "--message",
+        type=str,
+        default=None,
+        help="Custom commit message to use when committing the experiment.",
+    )
     exec_run_parser.set_defaults(func=CmdExecutorRun)
```

### Comparing `dvc-2.9.5/dvc/commands/experiments/gc.py` & `dvc-3.0.0a0/dvc/commands/experiments/run.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,129 +1,150 @@
 import argparse
 import logging
 
-from dvc.cli.command import CmdBase
+from dvc.cli import completion
 from dvc.cli.utils import append_doc_link
+from dvc.commands.repro import CmdRepro
+from dvc.commands.repro import add_arguments as add_repro_arguments
 from dvc.exceptions import InvalidArgumentError
 from dvc.ui import ui
 
 logger = logging.getLogger(__name__)
 
 
-def _raise_error_if_all_disabled(**kwargs):
-    if not any(kwargs.values()):
-        raise InvalidArgumentError(
-            "Either of `-w|--workspace`, `-a|--all-branches`, `-T|--all-tags` "
-            "or `--all-commits` needs to be set."
-        )
-
-
-class CmdExperimentsGC(CmdBase):
+class CmdExperimentsRun(CmdRepro):
     def run(self):
-        _raise_error_if_all_disabled(
-            all_branches=self.args.all_branches,
-            all_tags=self.args.all_tags,
-            all_commits=self.args.all_commits,
-            workspace=self.args.workspace,
-        )
-
-        msg = "This will remove all experiments except those derived from "
-
-        msg += "the workspace"
-        if self.args.all_commits:
-            msg += " and all git commits"
-        elif self.args.all_branches and self.args.all_tags:
-            msg += " and all git branches and tags"
-        elif self.args.all_branches:
-            msg += " and all git branches"
-        elif self.args.all_tags:
-            msg += " and all git tags"
-        msg += " of the current repo."
-        if self.args.queued:
-            msg += " Run queued experiments will be preserved."
-        else:
-            msg += " Run queued experiments will be removed."
-
-        logger.warning(msg)
-
-        msg = "Are you sure you want to proceed?"
-        if not self.args.force and not ui.confirm(msg):
-            return 1
-
-        removed = self.repo.experiments.gc(
-            all_branches=self.args.all_branches,
-            all_tags=self.args.all_tags,
-            all_commits=self.args.all_commits,
-            workspace=self.args.workspace,
-            queued=self.args.queued,
+        if self.args.checkpoint_resume:
+            if self.args.reset:
+                raise InvalidArgumentError("--reset and --rev are mutually exclusive.")
+            if not (self.args.queue or self.args.tmp_dir):
+                raise InvalidArgumentError(
+                    "--rev can only be used in conjunction with --queue or --temp."
+                )
+
+        if self.args.reset:
+            ui.write("Any existing checkpoints will be reset and re-run.")
+
+        self.repo.experiments.run(
+            name=self.args.name,
+            queue=self.args.queue,
+            run_all=self.args.run_all,
+            jobs=self.args.jobs,
+            params=self.args.set_param,
+            checkpoint_resume=self.args.checkpoint_resume,
+            reset=self.args.reset,
+            tmp_dir=self.args.tmp_dir,
+            machine=self.args.machine,
+            copy_paths=self.args.copy_paths,
+            message=self.args.message,
+            **self._common_kwargs,
         )
 
-        if removed:
-            ui.write(
-                f"Removed {removed} experiments.",
-                "To remove unused cache files",
-                "use 'dvc gc'.",
-            )
-        else:
-            ui.write("No experiments to remove.")
         return 0
 
 
 def add_parser(experiments_subparsers, parent_parser):
-
-    EXPERIMENTS_GC_HELP = "Garbage collect unneeded experiments."
-    EXPERIMENTS_GC_DESCRIPTION = (
-        "Removes all experiments which are not derived from the specified"
-        "Git revisions."
-    )
-    experiments_gc_parser = experiments_subparsers.add_parser(
-        "gc",
+    EXPERIMENTS_RUN_HELP = "Run or resume an experiment."
+    experiments_run_parser = experiments_subparsers.add_parser(
+        "run",
         parents=[parent_parser],
-        description=append_doc_link(EXPERIMENTS_GC_DESCRIPTION, "exp/gc"),
-        help=EXPERIMENTS_GC_HELP,
+        description=append_doc_link(EXPERIMENTS_RUN_HELP, "exp/run"),
+        help=EXPERIMENTS_RUN_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
-    experiments_gc_parser.add_argument(
-        "-w",
-        "--workspace",
-        action="store_true",
-        default=False,
-        help="Keep experiments derived from the current workspace.",
+    _add_run_common(experiments_run_parser)
+    experiments_run_parser.add_argument(
+        "-r",
+        "--rev",
+        type=str,
+        dest="checkpoint_resume",
+        help=(
+            "Continue the specified checkpoint experiment. Can only be used "
+            "in conjunction with --queue or --temp."
+        ),
+        metavar="<experiment_rev>",
+    ).complete = completion.EXPERIMENT
+    experiments_run_parser.add_argument(
+        "--reset",
+        action="store_true",
+        help="Reset existing checkpoints and restart the experiment.",
+    )
+    experiments_run_parser.set_defaults(func=CmdExperimentsRun)
+
+
+def _add_run_common(parser):
+    """Add common args for 'exp run'."""
+    # inherit arguments from `dvc repro`
+    add_repro_arguments(parser)
+    parser.add_argument(
+        "-n",
+        "--name",
+        default=None,
+        help=(
+            "Human-readable experiment name. If not specified, a name will "
+            "be auto-generated."
+        ),
+        metavar="<name>",
     )
-    experiments_gc_parser.add_argument(
-        "-a",
-        "--all-branches",
-        action="store_true",
-        default=False,
-        help="Keep experiments derived from the tips of all Git branches.",
+    parser.add_argument(
+        "-S",
+        "--set-param",
+        action="append",
+        default=[],
+        help="Use the specified param value when reproducing pipelines.",
+        metavar="[<filename>:]<param_name>=<param_value>",
     )
-    experiments_gc_parser.add_argument(
-        "-T",
-        "--all-tags",
+    parser.add_argument(
+        "--queue",
         action="store_true",
         default=False,
-        help="Keep experiments derived from all Git tags.",
+        help="Stage this experiment in the run queue for future execution.",
     )
-    experiments_gc_parser.add_argument(
-        "-A",
-        "--all-commits",
+    parser.add_argument(
+        "--run-all",
         action="store_true",
         default=False,
-        help="Keep experiments derived from all Git commits.",
+        help="Execute all experiments in the run queue. Implies --temp.",
+    )
+    parser.add_argument(
+        "-j",
+        "--jobs",
+        type=int,
+        default=1,
+        help="Run the specified number of experiments at a time in parallel.",
+        metavar="<number>",
     )
-    experiments_gc_parser.add_argument(
-        "--queued",
+    parser.add_argument(
+        "--temp",
         action="store_true",
-        default=False,
+        dest="tmp_dir",
         help=(
-            "Keep queued experiments (experiments run queue will be cleared "
-            "by default)."
+            "Run this experiment in a separate temporary directory instead of "
+            "your workspace."
         ),
     )
-    experiments_gc_parser.add_argument(
-        "-f",
-        "--force",
-        action="store_true",
-        default=False,
-        help="Force garbage collection - automatically agree to all prompts.",
+    parser.add_argument(
+        "--machine",
+        default=None,
+        help=argparse.SUPPRESS,
+        # help=(
+        #     "Run this experiment on the specified 'dvc machine' instance."
+        # )
+        # metavar="<name>",
+    )
+    parser.add_argument(
+        "-C",
+        "--copy-paths",
+        action="append",
+        default=[],
+        help=(
+            "List of ignored or untracked paths to copy into the temp directory."
+            " Only used if `--temp` or `--queue` is specified."
+        ),
+    )
+    parser.add_argument(
+        "-m",
+        "--message",
+        type=str,
+        default=None,
+        help="Custom commit message to use when committing the experiment.",
     )
-    experiments_gc_parser.set_defaults(func=CmdExperimentsGC)
```

### Comparing `dvc-2.9.5/dvc/commands/experiments/ls.py` & `dvc-3.0.0a0/dvc/commands/experiments/ls.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,73 +1,60 @@
 import argparse
 import logging
 
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
+from dvc.ui import ui
 
 logger = logging.getLogger(__name__)
 
 
 class CmdExperimentsList(CmdBase):
     def run(self):
-        names_only = self.args.names_only
+        name_only = self.args.name_only
         exps = self.repo.experiments.ls(
+            all_commits=self.args.all_commits,
             rev=self.args.rev,
+            num=self.args.num,
             git_remote=self.args.git_remote,
-            all_=self.args.all,
         )
+
         for baseline in exps:
-            tag = self.repo.scm.describe(baseline)
-            if not tag:
-                branch = self.repo.scm.describe(baseline, base="refs/heads")
-                if branch:
-                    tag = branch.split("/")[-1]
-            name = tag if tag else baseline[:7]
-            if not names_only:
-                print(f"{name}:")
+            if not name_only:
+                ui.write(f"{baseline}:")
             for exp_name in exps[baseline]:
-                indent = "" if names_only else "\t"
-                print(f"{indent}{exp_name}")
+                indent = "" if name_only else "\t"
+                ui.write(f"{indent}{exp_name}")
 
         return 0
 
 
 def add_parser(experiments_subparsers, parent_parser):
+    from . import add_rev_selection_flags
 
     EXPERIMENTS_LIST_HELP = "List local and remote experiments."
     experiments_list_parser = experiments_subparsers.add_parser(
         "list",
         parents=[parent_parser],
         description=append_doc_link(EXPERIMENTS_LIST_HELP, "exp/list"),
         help=EXPERIMENTS_LIST_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
+    add_rev_selection_flags(experiments_list_parser, "List")
     experiments_list_parser.add_argument(
-        "--rev",
-        type=str,
-        default=None,
-        help=(
-            "List experiments derived from the specified revision. "
-            "Defaults to HEAD if neither `--rev` nor `--all` are specified."
-        ),
-        metavar="<rev>",
-    )
-    experiments_list_parser.add_argument(
-        "--all", action="store_true", help="List all experiments."
-    )
-    experiments_list_parser.add_argument(
+        "--name-only",
         "--names-only",
         action="store_true",
         help="Only output experiment names (without parent commits).",
     )
     experiments_list_parser.add_argument(
         "git_remote",
         nargs="?",
         default=None,
         help=(
             "Optional Git remote name or Git URL. "
             "If provided, experiments from the specified Git repository "
             " will be listed instead of local ones."
         ),
-        metavar="[<git_remote>]",
+        metavar="<git_remote>",
     )
     experiments_list_parser.set_defaults(func=CmdExperimentsList)
```

### Comparing `dvc-2.9.5/dvc/commands/experiments/pull.py` & `dvc-3.0.0a0/dvc/commands/experiments/pull.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,83 +1,97 @@
 import argparse
 import logging
 
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
+from dvc.exceptions import InvalidArgumentError
 from dvc.ui import ui
 
 logger = logging.getLogger(__name__)
 
 
 class CmdExperimentsPull(CmdBase):
+    def raise_error_if_all_disabled(self):
+        if not any([self.args.experiment, self.args.all_commits, self.args.rev]):
+            raise InvalidArgumentError(
+                "Either provide an `experiment` argument, or use the "
+                "`--rev` or `--all-commits` flag."
+            )
+
     def run(self):
+        self.raise_error_if_all_disabled()
 
-        self.repo.experiments.pull(
+        pulled_exps = self.repo.experiments.pull(
             self.args.git_remote,
             self.args.experiment,
+            all_commits=self.args.all_commits,
+            rev=self.args.rev,
+            num=self.args.num,
             force=self.args.force,
             pull_cache=self.args.pull_cache,
             dvc_remote=self.args.dvc_remote,
             jobs=self.args.jobs,
             run_cache=self.args.run_cache,
         )
 
-        ui.write(
-            f"Pulled experiment '{self.args.experiment}'",
-            f"from Git remote '{self.args.git_remote}'.",
-        )
+        if pulled_exps:
+            ui.write(
+                f"Pulled experiment '{pulled_exps}'",
+                f"from Git remote '{self.args.git_remote}'.",
+            )
+        else:
+            ui.write("No experiments to pull.")
         if not self.args.pull_cache:
             ui.write(
                 "To pull cached outputs for this experiment"
                 "from DVC remote storage,"
                 "re-run this command without '--no-cache'."
             )
 
         return 0
 
 
 def add_parser(experiments_subparsers, parent_parser):
+    from . import add_rev_selection_flags
+
     EXPERIMENTS_PULL_HELP = "Pull an experiment from a Git remote."
     experiments_pull_parser = experiments_subparsers.add_parser(
         "pull",
         parents=[parent_parser],
         description=append_doc_link(EXPERIMENTS_PULL_HELP, "exp/pull"),
         help=EXPERIMENTS_PULL_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
+    add_rev_selection_flags(experiments_pull_parser, "Pull", False)
     experiments_pull_parser.add_argument(
         "-f",
         "--force",
         action="store_true",
-        help="Replace local experiment already exists.",
+        help="Replace local experiment if it already exists.",
     )
     experiments_pull_parser.add_argument(
         "--no-cache",
         action="store_false",
         dest="pull_cache",
-        help=(
-            "Do not pull cached outputs for this experiment from DVC remote "
-            "storage."
-        ),
+        help="Do not pull cached outputs for this experiment from DVC remote storage.",
     )
     experiments_pull_parser.add_argument(
         "-r",
         "--remote",
         dest="dvc_remote",
         metavar="<name>",
         help="Name of the DVC remote to use when pulling cached outputs.",
     )
     experiments_pull_parser.add_argument(
         "-j",
         "--jobs",
         type=int,
         metavar="<number>",
         help=(
-            "Number of jobs to run simultaneously when pulling from DVC "
-            "remote storage."
+            "Number of jobs to run simultaneously when pulling from DVC remote storage."
         ),
     )
     experiments_pull_parser.add_argument(
         "--run-cache",
         action="store_true",
         default=False,
         help="Pull run history for all stages.",
@@ -85,12 +99,13 @@
     experiments_pull_parser.add_argument(
         "git_remote",
         help="Git remote name or Git URL.",
         metavar="<git_remote>",
     )
     experiments_pull_parser.add_argument(
         "experiment",
-        nargs="+",
+        nargs="*",
+        default=None,
         help="Experiments to pull.",
         metavar="<experiment>",
     )
     experiments_pull_parser.set_defaults(func=CmdExperimentsPull)
```

### Comparing `dvc-2.9.5/dvc/commands/experiments/push.py` & `dvc-3.0.0a0/dvc/commands/experiments/remove.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,97 +1,75 @@
 import argparse
 import logging
 
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
+from dvc.exceptions import InvalidArgumentError
 from dvc.ui import ui
 
 logger = logging.getLogger(__name__)
 
 
-class CmdExperimentsPush(CmdBase):
+class CmdExperimentsRemove(CmdBase):
+    def check_arguments(self):
+        if not any(
+            [
+                self.args.all_commits,
+                self.args.rev,
+                self.args.queue,
+            ]
+        ) ^ bool(self.args.experiment):
+            raise InvalidArgumentError(
+                "Either provide an `experiment` argument, or use the "
+                "`--rev` or `--all-commits` or `--queue` flag."
+            )
+
     def run(self):
+        from dvc.utils import humanize
 
-        self.repo.experiments.push(
-            self.args.git_remote,
-            self.args.experiment,
-            force=self.args.force,
-            push_cache=self.args.push_cache,
-            dvc_remote=self.args.dvc_remote,
-            jobs=self.args.jobs,
-            run_cache=self.args.run_cache,
-        )
+        self.check_arguments()
 
-        ui.write(
-            f"Pushed experiment '{self.args.experiment}'"
-            f"to Git remote '{self.args.git_remote}'."
+        removed = self.repo.experiments.remove(
+            exp_names=self.args.experiment,
+            all_commits=self.args.all_commits,
+            rev=self.args.rev,
+            num=self.args.num,
+            queue=self.args.queue,
+            git_remote=self.args.git_remote,
         )
-        if not self.args.push_cache:
-            ui.write(
-                "To push cached outputs",
-                "for this experiment to DVC remote storage,"
-                "re-run this command without '--no-cache'.",
-            )
+        if removed:
+            ui.write(f"Removed experiments: {humanize.join(map(repr, removed))}")
+        else:
+            ui.write("No experiments to remove.")
 
         return 0
 
 
 def add_parser(experiments_subparsers, parent_parser):
-    EXPERIMENTS_PUSH_HELP = "Push a local experiment to a Git remote."
-    experiments_push_parser = experiments_subparsers.add_parser(
-        "push",
+    from . import add_rev_selection_flags
+
+    EXPERIMENTS_REMOVE_HELP = "Remove experiments."
+    experiments_remove_parser = experiments_subparsers.add_parser(
+        "remove",
         parents=[parent_parser],
-        description=append_doc_link(EXPERIMENTS_PUSH_HELP, "exp/push"),
-        help=EXPERIMENTS_PUSH_HELP,
+        description=append_doc_link(EXPERIMENTS_REMOVE_HELP, "exp/remove"),
+        help=EXPERIMENTS_REMOVE_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
-    experiments_push_parser.add_argument(
-        "-f",
-        "--force",
-        action="store_true",
-        help="Replace experiment in the Git remote if it already exists.",
-    )
-    experiments_push_parser.add_argument(
-        "--no-cache",
-        action="store_false",
-        dest="push_cache",
-        help=(
-            "Do not push cached outputs for this experiment to DVC remote "
-            "storage."
-        ),
-    )
-    experiments_push_parser.add_argument(
-        "-r",
-        "--remote",
-        dest="dvc_remote",
-        metavar="<name>",
-        help="Name of the DVC remote to use when pushing cached outputs.",
-    )
-    experiments_push_parser.add_argument(
-        "-j",
-        "--jobs",
-        type=int,
-        metavar="<number>",
-        help=(
-            "Number of jobs to run simultaneously when pushing to DVC remote "
-            "storage."
-        ),
-    )
-    experiments_push_parser.add_argument(
-        "--run-cache",
-        action="store_true",
-        default=False,
-        help="Push run history for all stages.",
-    )
-    experiments_push_parser.add_argument(
-        "git_remote",
-        help="Git remote name or Git URL.",
+    remove_group = experiments_remove_parser.add_mutually_exclusive_group()
+    add_rev_selection_flags(experiments_remove_parser, "Remove", False)
+    remove_group.add_argument(
+        "--queue", action="store_true", help="Remove all queued experiments."
+    )
+    remove_group.add_argument(
+        "-g",
+        "--git-remote",
         metavar="<git_remote>",
+        help="Name or URL of the Git remote to remove the experiment from",
     )
-    experiments_push_parser.add_argument(
+    experiments_remove_parser.add_argument(
         "experiment",
-        nargs="+",
-        help="Experiments to push.",
+        nargs="*",
+        help="Experiments to remove.",
         metavar="<experiment>",
-    ).complete = completion.EXPERIMENT
-    experiments_push_parser.set_defaults(func=CmdExperimentsPush)
+    )
+    experiments_remove_parser.set_defaults(func=CmdExperimentsRemove)
```

### Comparing `dvc-2.9.5/dvc/commands/experiments/run.py` & `dvc-3.0.0a0/dvc/commands/experiments/push.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,142 +1,150 @@
 import argparse
 import logging
+from typing import Any, Dict
 
+from dvc.cli import completion
+from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
-from dvc.commands.repro import CmdRepro
-from dvc.commands.repro import add_arguments as add_repro_arguments
 from dvc.exceptions import InvalidArgumentError
 from dvc.ui import ui
 
 logger = logging.getLogger(__name__)
 
 
-class CmdExperimentsRun(CmdRepro):
+class CmdExperimentsPush(CmdBase):
+    def raise_error_if_all_disabled(self):
+        if not any([self.args.experiment, self.args.all_commits, self.args.rev]):
+            raise InvalidArgumentError(
+                "Either provide an `experiment` argument, or use the "
+                "`--rev` or `--all-commits` flag."
+            )
+
+    @staticmethod
+    def log_result(result: Dict[str, Any], remote: str):
+        from dvc.utils import humanize
+
+        def join_exps(exps):
+            return humanize.join([f"[bold]{e}[/]" for e in exps])
+
+        if diverged_exps := result.get("diverged"):
+            exps = join_exps(diverged_exps)
+            ui.error_write(
+                f"[yellow]Local experiment {exps} has diverged "
+                "from remote experiment with the same name.\n"
+                "To override the remote experiment re-run with '--force'.",
+                styled=True,
+            )
+        if uptodate_exps := result.get("up_to_date"):
+            exps = join_exps(uptodate_exps)
+            verb = "are" if len(uptodate_exps) > 1 else "is"
+            ui.write(
+                f"Experiment {exps} {verb} up to date on Git remote {remote!r}.",
+                styled=True,
+            )
+        if pushed_exps := result.get("success"):
+            exps = join_exps(pushed_exps)
+            ui.write(f"Pushed experiment {exps} to Git remote {remote!r}.", styled=True)
+        if not uptodate_exps and not pushed_exps:
+            ui.write("No experiments to push.")
+
+        if uploaded := result.get("uploaded"):
+            stats = {"uploaded": uploaded}
+            ui.write(humanize.get_summary(stats.items()))
+
+        if project_url := result.get("url"):
+            ui.rich_print(
+                "View your experiments at", project_url, style="yellow", soft_wrap=True
+            )
+
     def run(self):
-        from dvc.compare import show_metrics
+        from dvc.repo.experiments.push import UploadError
+
+        self.raise_error_if_all_disabled()
 
-        if self.args.checkpoint_resume:
-            if self.args.reset:
-                raise InvalidArgumentError(
-                    "--reset and --rev are mutually exclusive."
-                )
-            if not (self.args.queue or self.args.tmp_dir):
-                raise InvalidArgumentError(
-                    "--rev can only be used in conjunction with "
-                    "--queue or --temp."
-                )
-
-        if self.args.reset:
-            ui.write("Any existing checkpoints will be reset and re-run.")
-
-        results = self.repo.experiments.run(
-            name=self.args.name,
-            queue=self.args.queue,
-            run_all=self.args.run_all,
-            jobs=self.args.jobs,
-            params=self.args.set_param,
-            checkpoint_resume=self.args.checkpoint_resume,
-            reset=self.args.reset,
-            tmp_dir=self.args.tmp_dir,
-            machine=self.args.machine,
-            **self._repro_kwargs,
-        )
-
-        if self.args.metrics and results:
-            metrics = self.repo.metrics.show(revs=list(results))
-            metrics.pop("workspace", None)
-            show_metrics(metrics)
+        try:
+            result = self.repo.experiments.push(
+                self.args.git_remote,
+                self.args.experiment,
+                all_commits=self.args.all_commits,
+                rev=self.args.rev,
+                num=self.args.num,
+                force=self.args.force,
+                push_cache=self.args.push_cache,
+                dvc_remote=self.args.dvc_remote,
+                jobs=self.args.jobs,
+                run_cache=self.args.run_cache,
+            )
+        except UploadError as e:
+            self.log_result(e.result, self.args.git_remote)
+            raise
+
+        self.log_result(result, self.args.git_remote)
+        if not self.args.push_cache:
+            ui.write(
+                "To push cached outputs",
+                (
+                    "for this experiment to DVC remote storage,"
+                    "re-run this command without '--no-cache'."
+                ),
+            )
 
         return 0
 
 
 def add_parser(experiments_subparsers, parent_parser):
+    from . import add_rev_selection_flags
 
-    EXPERIMENTS_RUN_HELP = "Run or resume an experiment."
-    experiments_run_parser = experiments_subparsers.add_parser(
-        "run",
+    EXPERIMENTS_PUSH_HELP = "Push a local experiment to a Git remote."
+    experiments_push_parser = experiments_subparsers.add_parser(
+        "push",
         parents=[parent_parser],
-        description=append_doc_link(EXPERIMENTS_RUN_HELP, "exp/run"),
-        help=EXPERIMENTS_RUN_HELP,
+        description=append_doc_link(EXPERIMENTS_PUSH_HELP, "exp/push"),
+        help=EXPERIMENTS_PUSH_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
-    _add_run_common(experiments_run_parser)
-    experiments_run_parser.add_argument(
-        "-r",
-        "--rev",
-        type=str,
-        dest="checkpoint_resume",
-        help=(
-            "Continue the specified checkpoint experiment. Can only be used "
-            "in conjunction with --queue or --temp."
-        ),
-        metavar="<experiment_rev>",
-    ).complete = completion.EXPERIMENT
-    experiments_run_parser.add_argument(
-        "--reset",
+    add_rev_selection_flags(experiments_push_parser, "Push", False)
+    experiments_push_parser.add_argument(
+        "-f",
+        "--force",
         action="store_true",
-        help="Reset existing checkpoints and restart the experiment.",
-    )
-    experiments_run_parser.set_defaults(func=CmdExperimentsRun)
-
-
-def _add_run_common(parser):
-    """Add common args for 'exp run' and 'exp resume'."""
-    # inherit arguments from `dvc repro`
-    add_repro_arguments(parser)
-    parser.add_argument(
-        "-n",
-        "--name",
-        default=None,
-        help=(
-            "Human-readable experiment name. If not specified, a name will "
-            "be auto-generated."
-        ),
-        metavar="<name>",
+        help="Replace experiment in the Git remote if it already exists.",
     )
-    parser.add_argument(
-        "-S",
-        "--set-param",
-        action="append",
-        default=[],
-        help="Use the specified param value when reproducing pipelines.",
-        metavar="[<filename>:]<param_name>=<param_value>",
+    experiments_push_parser.add_argument(
+        "--no-cache",
+        action="store_false",
+        dest="push_cache",
+        help="Do not push cached outputs for this experiment to DVC remote storage.",
     )
-    parser.add_argument(
-        "--queue",
-        action="store_true",
-        default=False,
-        help="Stage this experiment in the run queue for future execution.",
-    )
-    parser.add_argument(
-        "--run-all",
-        action="store_true",
-        default=False,
-        help="Execute all experiments in the run queue. Implies --temp.",
+    experiments_push_parser.add_argument(
+        "-r",
+        "--remote",
+        dest="dvc_remote",
+        metavar="<name>",
+        help="Name of the DVC remote to use when pushing cached outputs.",
     )
-    parser.add_argument(
+    experiments_push_parser.add_argument(
         "-j",
         "--jobs",
         type=int,
-        default=1,
-        help="Run the specified number of experiments at a time in parallel.",
         metavar="<number>",
+        help="Number of jobs to run simultaneously when pushing to DVC remote storage.",
     )
-    parser.add_argument(
-        "--temp",
+    experiments_push_parser.add_argument(
+        "--run-cache",
         action="store_true",
-        dest="tmp_dir",
-        help=(
-            "Run this experiment in a separate temporary directory instead of "
-            "your workspace."
-        ),
+        default=False,
+        help="Push run history for all stages.",
     )
-    parser.add_argument(
-        "--machine",
+    experiments_push_parser.add_argument(
+        "git_remote",
+        help="Git remote name or Git URL.",
+        metavar="<git_remote>",
+    )
+    experiments_push_parser.add_argument(
+        "experiment",
+        nargs="*",
         default=None,
-        help=argparse.SUPPRESS,
-        # help=(
-        #     "Run this experiment on the specified 'dvc machine' instance."
-        # )
-        # metavar="<name>",
-    )
+        help="Experiments to push.",
+        metavar="<experiment>",
+    ).complete = completion.EXPERIMENT
+    experiments_push_parser.set_defaults(func=CmdExperimentsPush)
```

### Comparing `dvc-2.9.5/dvc/commands/freeze.py` & `dvc-3.0.0a0/dvc/commands/freeze.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 from dvc.exceptions import DvcException
 
 logger = logging.getLogger(__name__)
 
 
 class CmdFreezeBase(CmdBase):
     def _run(self, func, name):
         ret = 0
         for target in self.args.targets:
             try:
                 func(target)
             except DvcException:
-                logger.exception(f"failed to {name} '{target}'")
+                logger.exception("failed to %s '%s'", name, target)
                 ret = 1
         return ret
 
 
 class CmdFreeze(CmdFreezeBase):
     def run(self):
         return self._run(self.repo.freeze, "freeze")
```

### Comparing `dvc-2.9.5/dvc/commands/gc.py` & `dvc-3.0.0a0/dvc/commands/gc.py`

 * *Files 23% similar despite different names*

```diff
@@ -6,38 +6,56 @@
 from dvc.cli.utils import append_doc_link
 from dvc.ui import ui
 
 logger = logging.getLogger(__name__)
 
 
 class CmdGC(CmdBase):
-    def run(self):
-        from dvc.repo.gc import _raise_error_if_all_disabled
+    def run(self):  # noqa: C901, PLR0912
+        from dvc.repo.gc import _validate_args
 
-        _raise_error_if_all_disabled(
+        _validate_args(
             all_branches=self.args.all_branches,
             all_tags=self.args.all_tags,
             all_commits=self.args.all_commits,
+            all_experiments=self.args.all_experiments,
+            commit_date=self.args.commit_date,
             workspace=self.args.workspace,
+            rev=self.args.rev,
+            num=self.args.num,
+            cloud=self.args.cloud,
+            not_in_remote=self.args.not_in_remote,
         )
 
+        if self.args.rev:
+            self.args.num = self.args.num or 1
+
         msg = "This will remove all cache except items used in "
 
         msg += "the workspace"
         if self.args.all_commits:
             msg += " and all git commits"
-        elif self.args.all_branches and self.args.all_tags:
-            msg += " and all git branches and tags"
-        elif self.args.all_branches:
-            msg += " and all git branches"
-        elif self.args.all_tags:
-            msg += " and all git tags"
-        elif self.args.all_experiments:
+        else:
+            if self.args.all_branches and self.args.all_tags:
+                msg += " and all git branches and tags"
+            elif self.args.all_branches:
+                msg += " and all git branches"
+            elif self.args.all_tags:
+                msg += " and all git tags"
+            if self.args.commit_date:
+                msg += f" and all git commits before date {self.args.commit_date}"
+            if self.args.rev:
+                msg += f" and last {self.args.num} commits from {self.args.rev}"
+
+        if self.args.all_experiments:
             msg += " and all experiments"
 
+        if self.args.not_in_remote:
+            msg += " that are present in the DVC remote"
+
         if self.args.repos:
             msg += " of the current and the following repos:"
 
             for repo_path in self.args.repos:
                 msg += "\n  - %s" % os.path.abspath(repo_path)
         else:
             msg += " of the current repo."
@@ -49,20 +67,24 @@
             return 1
 
         self.repo.gc(
             all_branches=self.args.all_branches,
             all_tags=self.args.all_tags,
             all_commits=self.args.all_commits,
             all_experiments=self.args.all_experiments,
+            commit_date=self.args.commit_date,
             cloud=self.args.cloud,
             remote=self.args.remote,
             force=self.args.force,
             jobs=self.args.jobs,
             repos=self.args.repos,
             workspace=self.args.workspace,
+            rev=self.args.rev,
+            num=self.args.num,
+            not_in_remote=self.args.not_in_remote,
         )
         return 0
 
 
 def add_parser(subparsers, parent_parser):
     GC_HELP = "Garbage collect unused objects from cache or remote storage."
     GC_DESCRIPTION = (
@@ -80,14 +102,34 @@
         "-w",
         "--workspace",
         action="store_true",
         default=False,
         help="Keep data files used in the current workspace.",
     )
     gc_parser.add_argument(
+        "--rev",
+        type=str,
+        default=None,
+        help="Keep data files used in the specified <commit>.",
+        metavar="<commit>",
+    )
+    gc_parser.add_argument(
+        "-n",
+        "--num",
+        type=int,
+        dest="num",
+        metavar="<num>",
+        help=(
+            "Keep data files used in the last `num` commits "
+            "starting from the `--rev` <commit>. "
+            "Only used if `--rev` is also provided. "
+            "Defaults to `1`."
+        ),
+    )
+    gc_parser.add_argument(
         "-a",
         "--all-branches",
         action="store_true",
         default=False,
         help="Keep data files for the tips of all Git branches.",
     )
     gc_parser.add_argument(
@@ -101,25 +143,43 @@
         "-A",
         "--all-commits",
         action="store_true",
         default=False,
         help="Keep data files for all Git commits.",
     )
     gc_parser.add_argument(
+        "--date",
+        type=str,
+        dest="commit_date",
+        metavar="<YYYY-MM-DD>",
+        default=None,
+        help=(
+            "Keep cached data referenced in the commits after ( inclusive )"
+            " a certain time. Date must match the extended ISO 8601 format "
+            "(YYYY-MM-DD)."
+        ),
+    )
+    gc_parser.add_argument(
         "--all-experiments",
         action="store_true",
         default=False,
         help="Keep data files for all experiments.",
     )
     gc_parser.add_argument(
+        "--not-in-remote",
+        action="store_true",
+        default=False,
+        help="Keep data files that are not present in the remote.",
+    )
+    gc_parser.add_argument(
         "-c",
         "--cloud",
         action="store_true",
         default=False,
-        help="Collect garbage in remote repository.",
+        help="Collect garbage in remote storage in addition to local cache.",
     )
     gc_parser.add_argument(
         "-r",
         "--remote",
         help="Remote storage to collect garbage in",
         metavar="<name>",
     )
@@ -133,23 +193,24 @@
     gc_parser.add_argument(
         "-j",
         "--jobs",
         type=int,
         help=(
             "Number of jobs to run simultaneously. "
             "The default value is 4 * cpu_count(). "
-            "For SSH remotes, the default is 4. "
         ),
         metavar="<number>",
     )
     gc_parser.add_argument(
         "-p",
         "--projects",
         dest="repos",
         type=str,
         nargs="*",
-        help="Keep data files required by these projects "
-        "in addition to the current one. "
-        "Useful if you share a single cache across repos.",
+        help=(
+            "Keep data files required by these projects "
+            "in addition to the current one. "
+            "Useful if you share a single cache across repos."
+        ),
         metavar="<paths>",
     )
     gc_parser.set_defaults(func=CmdGC)
```

### Comparing `dvc-2.9.5/dvc/commands/get.py` & `dvc-3.0.0a0/dvc/commands/get.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,58 +1,54 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBaseNoRepo
+from dvc.cli.utils import append_doc_link
 from dvc.exceptions import DvcException
 
-from ..cli.utils import append_doc_link
-from . import completion
-
 logger = logging.getLogger(__name__)
 
 
 class CmdGet(CmdBaseNoRepo):
     def _show_url(self):
         from dvc.api import get_url
         from dvc.ui import ui
 
-        try:
-            url = get_url(
-                self.args.path, repo=self.args.url, rev=self.args.rev
-            )
-            ui.write(url, force=True)
-        except DvcException:
-            logger.exception("failed to show URL")
-            return 1
+        url = get_url(self.args.path, repo=self.args.url, rev=self.args.rev)
+        ui.write(url, force=True)
 
         return 0
 
     def run(self):
         if self.args.show_url:
             return self._show_url()
 
         return self._get_file_from_repo()
 
     def _get_file_from_repo(self):
         from dvc.repo import Repo
+        from dvc.scm import CloneError
 
         try:
             Repo.get(
                 self.args.url,
                 path=self.args.path,
                 out=self.args.out,
                 rev=self.args.rev,
                 jobs=self.args.jobs,
+                force=self.args.force,
             )
             return 0
+        except CloneError:
+            logger.exception("failed to get '%s'", self.args.path)
+            return 1
         except DvcException:
             logger.exception(
-                "failed to get '{}' from '{}'".format(
-                    self.args.path, self.args.url
-                )
+                "failed to get '%s' from '%s'", self.args.path, self.args.url
             )
             return 1
 
 
 def add_parser(subparsers, parent_parser):
     GET_HELP = "Download file or directory tracked by DVC or by Git."
     get_parser = subparsers.add_parser(
@@ -80,22 +76,30 @@
         nargs="?",
         help="Git revision (e.g. SHA, branch, tag)",
         metavar="<commit>",
     )
     get_parser.add_argument(
         "--show-url",
         action="store_true",
-        help="Print the storage location (URL) the target data would be "
-        "downloaded from, and exit.",
+        help=(
+            "Print the storage location (URL) the target data would be "
+            "downloaded from, and exit."
+        ),
     )
     get_parser.add_argument(
         "-j",
         "--jobs",
         type=int,
         help=(
             "Number of jobs to run simultaneously. "
             "The default value is 4 * cpu_count(). "
-            "For SSH remotes, the default is 4. "
         ),
         metavar="<number>",
     )
+    get_parser.add_argument(
+        "-f",
+        "--force",
+        action="store_true",
+        default=False,
+        help="Override local file or folder if exists.",
+    )
     get_parser.set_defaults(func=CmdGet)
```

### Comparing `dvc-2.9.5/dvc/commands/get_url.py` & `dvc-3.0.0a0/dvc/commands/get_url.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,28 +1,32 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBaseNoRepo
+from dvc.cli.utils import append_doc_link
 from dvc.exceptions import DvcException
 
-from ..cli.utils import append_doc_link
-from . import completion
-
 logger = logging.getLogger(__name__)
 
 
 class CmdGetUrl(CmdBaseNoRepo):
     def run(self):
         from dvc.repo import Repo
 
         try:
-            Repo.get_url(self.args.url, out=self.args.out, jobs=self.args.jobs)
+            Repo.get_url(
+                self.args.url,
+                out=self.args.out,
+                jobs=self.args.jobs,
+                force=self.args.force,
+            )
             return 0
         except DvcException:
-            logger.exception(f"failed to get '{self.args.url}'")
+            logger.exception("failed to get '%s'", self.args.url)
             return 1
 
 
 def add_parser(subparsers, parent_parser):
     GET_HELP = "Download or copy files from URL."
     get_parser = subparsers.add_parser(
         "get-url",
@@ -40,12 +44,18 @@
     get_parser.add_argument(
         "-j",
         "--jobs",
         type=int,
         help=(
             "Number of jobs to run simultaneously. "
             "The default value is 4 * cpu_count(). "
-            "For SSH remotes, the default is 4. "
         ),
         metavar="<number>",
     )
+    get_parser.add_argument(
+        "-f",
+        "--force",
+        action="store_true",
+        default=False,
+        help="Override local file or folder if exists.",
+    )
     get_parser.set_defaults(func=CmdGetUrl)
```

### Comparing `dvc-2.9.5/dvc/commands/git_hook.py` & `dvc-3.0.0a0/dvc/commands/git_hook.py`

 * *Files 11% similar despite different names*

```diff
@@ -60,25 +60,25 @@
         from dvc.cli import main
 
         return main(["push"])
 
 
 class CmdMergeDriver(CmdHookBase):
     def _run(self):
-        from dvc.dvcfile import Dvcfile
+        from dvc.dvcfile import load_file
         from dvc.repo import Repo
 
         dvc = Repo()
 
         try:
-            ancestor = Dvcfile(dvc, self.args.ancestor, verify=False)
-            our = Dvcfile(dvc, self.args.our, verify=False)
-            their = Dvcfile(dvc, self.args.their, verify=False)
+            ancestor = load_file(dvc, self.args.ancestor, verify=False)
+            our = load_file(dvc, self.args.our, verify=False)
+            their = load_file(dvc, self.args.their, verify=False)
 
-            our.merge(ancestor, their)
+            our.merge(ancestor, their, allowed=["add", "remove", "change"])
 
             return 0
         finally:
             dvc.close()
 
 
 def add_parser(subparsers, parent_parser):
```

### Comparing `dvc-2.9.5/dvc/commands/imp.py` & `dvc-3.0.0a0/dvc/commands/imp.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,36 +1,41 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 from dvc.exceptions import DvcException
 
 logger = logging.getLogger(__name__)
 
 
 class CmdImport(CmdBase):
     def run(self):
+        from dvc.scm import CloneError
+
         try:
             self.repo.imp(
                 self.args.url,
                 self.args.path,
                 out=self.args.out,
                 fname=self.args.file,
                 rev=self.args.rev,
                 no_exec=self.args.no_exec,
-                desc=self.args.desc,
+                no_download=self.args.no_download,
                 jobs=self.args.jobs,
             )
+        except CloneError:
+            logger.exception("failed to import '%s'", self.args.path)
+            return 1
         except DvcException:
             logger.exception(
-                "failed to import '{}' from '{}'.".format(
-                    self.args.path, self.args.url
-                )
+                "failed to import '%s' from '%s'.",
+                self.args.path,
+                self.args.url,
             )
             return 1
         return 0
 
 
 def add_parser(subparsers, parent_parser):
     IMPORT_HELP = (
@@ -65,34 +70,34 @@
         metavar="<commit>",
     )
     import_parser.add_argument(
         "--file",
         help="Specify name of the .dvc file this command will generate.",
         metavar="<filename>",
     )
-    import_parser.add_argument(
+    no_download_exec_group = import_parser.add_mutually_exclusive_group()
+    no_download_exec_group.add_argument(
         "--no-exec",
         action="store_true",
         default=False,
-        help="Only create .dvc file without actually downloading it.",
+        help="Only create .dvc file without actually importing target data.",
     )
-    import_parser.add_argument(
-        "--desc",
-        type=str,
-        metavar="<text>",
+    no_download_exec_group.add_argument(
+        "--no-download",
+        action="store_true",
+        default=False,
         help=(
-            "User description of the data (optional). "
-            "This doesn't affect any DVC operations."
+            "Create .dvc file including target data hash value(s)"
+            " but do not actually download the file(s)."
         ),
     )
     import_parser.add_argument(
         "-j",
         "--jobs",
         type=int,
         help=(
             "Number of jobs to run simultaneously. "
             "The default value is 4 * cpu_count(). "
-            "For SSH remotes, the default is 4. "
         ),
         metavar="<number>",
     )
     import_parser.set_defaults(func=CmdImport)
```

### Comparing `dvc-2.9.5/dvc/commands/imp_url.py` & `dvc-3.0.0a0/dvc/commands/imp_url.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,107 +1,123 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 from dvc.exceptions import DvcException
 
 logger = logging.getLogger(__name__)
 
 
 class CmdImportUrl(CmdBase):
     def run(self):
         try:
             self.repo.imp_url(
                 self.args.url,
                 out=self.args.out,
                 fname=self.args.file,
                 no_exec=self.args.no_exec,
+                no_download=self.args.no_download,
                 remote=self.args.remote,
                 to_remote=self.args.to_remote,
-                desc=self.args.desc,
                 jobs=self.args.jobs,
+                force=self.args.force,
+                version_aware=self.args.version_aware,
             )
         except DvcException:
             logger.exception(
-                "failed to import {}. You could also try downloading "
-                "it manually, and adding it with `dvc add`.".format(
-                    self.args.url
-                )
+                (
+                    "failed to import %s. You could also try downloading "
+                    "it manually, and adding it with `dvc add`."
+                ),
+                self.args.url,
             )
             return 1
         return 0
 
 
 def add_parser(subparsers, parent_parser):
-    IMPORT_HELP = (
-        "Download or copy file from URL and take it under DVC control."
-    )
+    IMPORT_HELP = "Download or copy file from URL and take it under DVC control."
 
     import_parser = subparsers.add_parser(
         "import-url",
         parents=[parent_parser],
         description=append_doc_link(IMPORT_HELP, "import-url"),
         help=IMPORT_HELP,
         formatter_class=argparse.RawTextHelpFormatter,
     )
     import_parser.add_argument(
         "url",
-        help="Location of the data to download. Supported URLs:\n"
-        "/absolute/path/to/file/or/dir\n"
-        "relative/path/to/file/or/dir\n"
-        "C:\\\\path\\to\\file\\or\\dir\n"
-        "https://example.com/path/to/file\n"
-        "s3://bucket/key/path\n"
-        "gs://bucket/path/to/file/or/dir\n"
-        "hdfs://example.com/path/to/file\n"
-        "ssh://example.com/absolute/path/to/file/or/dir\n"
-        "remote://remote_name/path/to/file/or/dir (see `dvc remote`)",
+        help=(
+            "Location of the data to download. Supported URLs:\n"
+            "/absolute/path/to/file/or/dir\n"
+            "relative/path/to/file/or/dir\n"
+            "C:\\\\path\\to\\file\\or\\dir\n"
+            "https://example.com/path/to/file\n"
+            "s3://bucket/key/path\n"
+            "gs://bucket/path/to/file/or/dir\n"
+            "hdfs://example.com/path/to/file\n"
+            "ssh://example.com/absolute/path/to/file/or/dir\n"
+            "remote://remote_name/path/to/file/or/dir (see `dvc remote`)"
+        ),
     )
     import_parser.add_argument(
         "out", nargs="?", help="Destination path to put files to."
     ).complete = completion.DIR
     import_parser.add_argument(
         "--file",
         help="Specify name of the .dvc file this command will generate.",
         metavar="<filename>",
     ).complete = completion.DIR
     import_parser.add_argument(
-        "--no-exec",
-        action="store_true",
-        default=False,
-        help="Only create .dvc file without actually downloading it.",
-    )
-    import_parser.add_argument(
         "--to-remote",
         action="store_true",
         default=False,
         help="Download it directly to the remote",
     )
     import_parser.add_argument(
         "-r",
         "--remote",
         help="Remote storage to download to",
         metavar="<name>",
     )
+    no_download_exec_group = import_parser.add_mutually_exclusive_group()
+    no_download_exec_group.add_argument(
+        "--no-exec",
+        action="store_true",
+        default=False,
+        help="Only create .dvc file without actually importing target data.",
+    )
+    no_download_exec_group.add_argument(
+        "--no-download",
+        action="store_true",
+        default=False,
+        help=(
+            "Create .dvc file including target data hash value(s)"
+            " but do not actually download the file(s)."
+        ),
+    )
     import_parser.add_argument(
         "-j",
         "--jobs",
         type=int,
         help=(
             "Number of jobs to run simultaneously. "
             "The default value is 4 * cpu_count(). "
-            "For SSH remotes, the default is 4. "
         ),
         metavar="<number>",
     )
     import_parser.add_argument(
-        "--desc",
-        type=str,
-        metavar="<text>",
-        help=(
-            "User description of the data (optional). "
-            "This doesn't affect any DVC operations."
-        ),
+        "-f",
+        "--force",
+        action="store_true",
+        default=False,
+        help="Override local file or folder if exists.",
+    )
+    import_parser.add_argument(
+        "--version-aware",
+        action="store_true",
+        default=False,
+        help="Import using cloud versioning. Implied if the URL contains a version ID.",
     )
     import_parser.set_defaults(func=CmdImportUrl)
```

### Comparing `dvc-2.9.5/dvc/commands/init.py` & `dvc-3.0.0a0/dvc/commands/init.py`

 * *Files 2% similar despite different names*

```diff
@@ -71,25 +71,25 @@
         help=INIT_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
     init_parser.add_argument(
         "--no-scm",
         action="store_true",
         default=False,
-        help="Initiate DVC in directory that is "
-        "not tracked by any SCM tool (e.g. Git).",
+        help=(
+            "Initiate DVC in directory that is not tracked by any SCM tool (e.g. Git)."
+        ),
     )
     init_parser.add_argument(
         "-f",
         "--force",
         action="store_true",
         default=False,
         help=(
-            "Overwrite existing '.dvc/' directory. "
-            "This operation removes local cache."
+            "Overwrite existing '.dvc/' directory. This operation removes local cache."
         ),
     )
     init_parser.add_argument(
         "--subdir",
         action="store_true",
         default=False,
         help=(
```

### Comparing `dvc-2.9.5/dvc/commands/install.py` & `dvc-3.0.0a0/dvc/commands/install.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,11 +27,13 @@
         help=INSTALL_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
     install_parser.add_argument(
         "--use-pre-commit-tool",
         action="store_true",
         default=False,
-        help="Install DVC hooks using pre-commit "
-        "(https://pre-commit.com) if it is installed.",
+        help=(
+            "Install DVC hooks using pre-commit "
+            "(https://pre-commit.com) if it is installed."
+        ),
     )
     install_parser.set_defaults(func=CmdInstall)
```

### Comparing `dvc-2.9.5/dvc/commands/live.py` & `dvc-3.0.0a0/dvc/commands/ls/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,97 +1,88 @@
 import argparse
+import logging
 
-from dvc.cli.command import CmdBase
-from dvc.cli.utils import fix_subparsers
-from dvc.commands import completion
+from dvc.cli import completion
+from dvc.cli.command import CmdBaseNoRepo
+from dvc.cli.utils import append_doc_link
+from dvc.commands.ls.ls_colors import LsColors
+from dvc.exceptions import DvcException
 from dvc.ui import ui
 
+logger = logging.getLogger(__name__)
 
-class CmdLive(CmdBase):
-    UNINITIALIZED = True
 
-    def _run(self, target, revs=None):
-        from dvc.render.utils import match_renderers, render
+def _prettify(entries, with_color=False):
+    if with_color:
+        ls_colors = LsColors()
+        fmt = ls_colors.format
+    else:
 
-        metrics, plots = self.repo.live.show(target=target, revs=revs)
+        def fmt(entry):
+            return entry["path"]
 
-        if plots:
-            from pathlib import Path
+    return [fmt(entry) for entry in entries]
 
-            html_path = Path.cwd() / (self.args.target + "_html")
 
-            renderers = match_renderers(plots, self.repo.plots.templates)
-            index_path = render(self.repo, renderers, metrics, html_path)
-            ui.write(index_path.as_uri())
-            return 0
-        return 1
-
-
-class CmdLiveShow(CmdLive):
+class CmdList(CmdBaseNoRepo):
     def run(self):
-        return self._run(self.args.target)
-
-
-class CmdLiveDiff(CmdLive):
-    def run(self):
-        return self._run(self.args.target, self.args.revs)
-
+        from dvc.repo import Repo
 
-def shared_parent_parser():
-    parent_parser = argparse.ArgumentParser(add_help=False)
-    parent_parser.add_argument(
-        "target", help="Logs dir to produce summary from"
-    ).complete = completion.DIR
-    parent_parser.add_argument(
-        "-o",
-        "--out",
-        default=None,
-        help="Destination path to save plots to",
-        metavar="<path>",
-    ).complete = completion.DIR
-    return parent_parser
+        try:
+            entries = Repo.ls(
+                self.args.url,
+                self.args.path,
+                rev=self.args.rev,
+                recursive=self.args.recursive,
+                dvc_only=self.args.dvc_only,
+            )
+            if self.args.json:
+                ui.write_json(entries)
+            elif entries:
+                entries = _prettify(entries, with_color=True)
+                ui.write("\n".join(entries))
+            return 0
+        except DvcException:
+            logger.exception("failed to list '%s'", self.args.url)
+            return 1
 
 
 def add_parser(subparsers, parent_parser):
-    LIVE_DESCRIPTION = (
-        "Commands to visualize and compare dvclive-produced logs."
-    )
-    live_parser = subparsers.add_parser(
-        "live",
+    LIST_HELP = (
+        "List repository contents, including files"
+        " and directories tracked by DVC and by Git."
+    )
+    list_parser = subparsers.add_parser(
+        "list",
+        aliases=["ls"],
         parents=[parent_parser],
-        formatter_class=argparse.RawDescriptionHelpFormatter,
-        description=LIVE_DESCRIPTION,
-    )
-    live_subparsers = live_parser.add_subparsers(
-        dest="cmd",
-        help="Use `dvc live CMD --help` to display command-specific help.",
-    )
-
-    fix_subparsers(live_subparsers)
-
-    SHOW_HELP = "Visualize dvclive directory content."
-    live_show_parser = live_subparsers.add_parser(
-        "show",
-        parents=[parent_parser, shared_parent_parser()],
-        help=SHOW_HELP,
-        formatter_class=argparse.RawDescriptionHelpFormatter,
-    )
-    live_show_parser.set_defaults(func=CmdLiveShow)
-
-    DIFF_HELP = (
-        "Show multiple versions of dvclive data, "
-        "by plotting it in single view."
-    )
-    live_diff_parser = live_subparsers.add_parser(
-        "diff",
-        parents=[parent_parser, shared_parent_parser()],
-        help=DIFF_HELP,
-        formatter_class=argparse.RawDescriptionHelpFormatter,
-    )
-    live_diff_parser.add_argument(
-        "--revs",
-        nargs="*",
-        default=None,
+        description=append_doc_link(LIST_HELP, "list"),
+        help=LIST_HELP,
+        formatter_class=argparse.RawTextHelpFormatter,
+    )
+    list_parser.add_argument("url", help="Location of DVC repository to list")
+    list_parser.add_argument(
+        "-R",
+        "--recursive",
+        action="store_true",
+        help="Recursively list files.",
+    )
+    list_parser.add_argument(
+        "--dvc-only", action="store_true", help="Show only DVC outputs."
+    )
+    list_parser.add_argument(
+        "--json",
+        action="store_true",
+        help="Show output in JSON format.",
+    )
+    list_parser.add_argument(
+        "--rev",
+        nargs="?",
         help="Git revision (e.g. SHA, branch, tag)",
         metavar="<commit>",
     )
-    live_diff_parser.set_defaults(func=CmdLiveDiff)
+    list_parser.add_argument(
+        "path",
+        nargs="?",
+        help="Path to directory within the repository to list outputs for",
+    ).complete = completion.DIR
+    list_parser.set_defaults(func=CmdList)
```

### Comparing `dvc-2.9.5/dvc/commands/ls/__init__.py` & `dvc-3.0.0a0/dvc/commands/experiments/save.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,89 +1,85 @@
 import argparse
 import logging
 
-from dvc.cli.command import CmdBaseNoRepo
+from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
-from dvc.commands.ls.ls_colors import LsColors
 from dvc.exceptions import DvcException
 from dvc.ui import ui
 
 logger = logging.getLogger(__name__)
 
 
-def _prettify(entries, with_color=False):
-    if with_color:
-        ls_colors = LsColors()
-        fmt = ls_colors.format
-    else:
-
-        def fmt(entry):
-            return entry["path"]
-
-    return [fmt(entry) for entry in entries]
-
-
-class CmdList(CmdBaseNoRepo):
+class CmdExperimentsSave(CmdBase):
     def run(self):
-        from dvc.repo import Repo
-
         try:
-            entries = Repo.ls(
-                self.args.url,
-                self.args.path,
-                rev=self.args.rev,
-                recursive=self.args.recursive,
-                dvc_only=self.args.dvc_only,
+            ref = self.repo.experiments.save(
+                name=self.args.name,
+                force=self.args.force,
+                include_untracked=self.args.include_untracked,
+                message=self.args.message,
             )
-            if self.args.json:
-                ui.write_json(entries)
-            elif entries:
-                entries = _prettify(entries, with_color=True)
-                ui.write("\n".join(entries))
-            return 0
         except DvcException:
-            logger.exception(f"failed to list '{self.args.url}'")
+            logger.exception("failed to save experiment")
             return 1
 
+        if self.args.json:
+            ui.write_json({"ref": ref})
+        else:
+            name = self.repo.experiments.get_exact_name([ref])[ref]
+            ui.write(f"Experiment has been saved as: {name}")
+            ui.write(
+                "\nTo promote an experiment to a Git branch run:\n\n"
+                "\tdvc exp branch <exp> <branch>\n"
+            )
+
+        return 0
+
 
-def add_parser(subparsers, parent_parser):
-    LIST_HELP = (
-        "List repository contents, including files"
-        " and directories tracked by DVC and by Git."
-    )
-    list_parser = subparsers.add_parser(
-        "list",
-        aliases=["ls"],
+def add_parser(experiments_subparsers, parent_parser):
+    EXPERIMENTS_SAVE_HELP = "Save current workspace as an experiment."
+    save_parser = experiments_subparsers.add_parser(
+        "save",
         parents=[parent_parser],
-        description=append_doc_link(LIST_HELP, "list"),
-        help=LIST_HELP,
-        formatter_class=argparse.RawTextHelpFormatter,
-    )
-    list_parser.add_argument("url", help="Location of DVC repository to list")
-    list_parser.add_argument(
-        "-R",
-        "--recursive",
+        description=append_doc_link(EXPERIMENTS_SAVE_HELP, "exp/save"),
+        help=EXPERIMENTS_SAVE_HELP,
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+    save_parser.add_argument(
+        "-f",
+        "--force",
         action="store_true",
-        help="Recursively list files.",
+        default=False,
+        help="Replace experiment if it already exists.",
     )
-    list_parser.add_argument(
-        "--dvc-only", action="store_true", help="Show only DVC outputs."
-    )
-    list_parser.add_argument(
+    save_parser.add_argument(
         "--json",
-        "--show-json",
         action="store_true",
+        default=False,
         help="Show output in JSON format.",
     )
-    list_parser.add_argument(
-        "--rev",
-        nargs="?",
-        help="Git revision (e.g. SHA, branch, tag)",
-        metavar="<commit>",
-    )
-    list_parser.add_argument(
-        "path",
-        nargs="?",
-        help="Path to directory within the repository to list outputs for",
-    ).complete = completion.DIR
-    list_parser.set_defaults(func=CmdList)
+    save_parser.add_argument(
+        "-n",
+        "--name",
+        default=None,
+        help=(
+            "Human-readable experiment name. If not specified, a name will "
+            "be auto-generated."
+        ),
+        metavar="<name>",
+    )
+    save_parser.add_argument(
+        "-I",
+        "--include-untracked",
+        action="append",
+        default=[],
+        help="List of untracked paths to include in the experiment.",
+        metavar="<path>",
+    )
+    save_parser.add_argument(
+        "-M",
+        "--message",
+        type=str,
+        default=None,
+        help="Custom commit message to use when committing the experiment.",
+    )
+    save_parser.set_defaults(func=CmdExperimentsSave)
```

### Comparing `dvc-2.9.5/dvc/commands/ls/ls_colors.py` & `dvc-3.0.0a0/dvc/commands/ls/ls_colors.py`

 * *Files 0% similar despite different names*

```diff
@@ -16,15 +16,15 @@
             except ValueError:
                 continue
             if code.startswith("*."):
                 self._extensions[code[1:]] = color
             else:
                 self._codes[code] = color
 
-    def format(self, entry):
+    def format(self, entry):  # noqa: A003
         text = entry["path"]
 
         if entry.get("isout", False) and "out" in self._codes:
             return self._format(text, code="out")
 
         if entry.get("isdir", False):
             return self._format(text, code="di")
```

### Comparing `dvc-2.9.5/dvc/commands/machine.py` & `dvc-3.0.0a0/dvc/commands/machine.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,28 +1,27 @@
 import argparse
+from typing import Dict, List
 
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link, fix_subparsers
 from dvc.commands.config import CmdConfig
 from dvc.compare import TabularData
 from dvc.config import ConfigError
 from dvc.exceptions import DvcException
-from dvc.types import Dict, List
 from dvc.ui import ui
 from dvc.utils import format_link
 
 
 class MachineDisabledError(ConfigError):
     def __init__(self):
         super().__init__("Machine feature is disabled")
 
 
 class CmdMachineConfig(CmdConfig):
     def __init__(self, args):
-
         super().__init__(args)
         if not self.config["feature"].get("machine", False):
             raise MachineDisabledError
 
         if getattr(self.args, "name", None):
             self.args.name = self.args.name.lower()
 
@@ -159,28 +158,26 @@
         from dvc.machine import validate_name
 
         validate_name(self.args.new)
 
         all_config = self.config.load_config_to_level(None)
         if self.args.new in all_config.get("machine", {}):
             raise ConfigError(
-                "Rename failed. Machine '{}' already exists.".format(
-                    self.args.new
-                )
+                f"Rename failed. Machine '{self.args.new}' already exists."
             )
         ui.write(f"Rename machine '{self.args.name}' to '{self.args.new}'.")
 
     def run(self):
-
         self._check_before_rename()
 
         with self.config.edit(self.args.level) as conf:
             self._check_exists(conf)
             conf["machine"][self.args.new] = conf["machine"][self.args.name]
             try:
+                assert self.repo.machine
                 self.repo.machine.rename(self.args.name, self.args.new)
             except DvcException as error:
                 del conf["machine"][self.args.new]
                 raise ConfigError("terraform rename failed") from error
             del conf["machine"][self.args.name]
             self._rename_default(conf)
 
@@ -195,35 +192,32 @@
 
 
 class CmdMachineDefault(CmdMachineConfig):
     def run(self):
         if self.args.name is None and not self.args.unset:
             conf = self.config.read(self.args.level)
             try:
-                print(conf["core"]["machine"])
+                ui.write(conf["core"]["machine"])
             except KeyError:
                 ui.write("No default machine set")
                 return 1
         else:
             with self.config.edit(self.args.level) as conf:
                 if self.args.unset:
                     conf["core"].pop("machine", None)
                 else:
-                    merged_conf = self.config.load_config_to_level(
-                        self.args.level
-                    )
+                    merged_conf = self.config.load_config_to_level(self.args.level)
                     if (
                         self.args.name in conf["machine"]
                         or self.args.name in merged_conf["machine"]
                     ):
                         conf["core"]["machine"] = self.args.name
                     else:
                         raise ConfigError(
-                            "default machine must be present in machine "
-                            "list."
+                            "default machine must be present in machine list."
                         )
         return 0
 
 
 class CmdMachineCreate(CmdBase):
     def run(self):
         if self.repo.machine is None:
@@ -246,15 +240,14 @@
 
     def _add_row(
         self,
         name: str,
         all_status: List[Dict],
         td: TabularData,
     ):
-
         if not all_status:
             row = [
                 name,
                 self.FILL_VALUE,
                 "offline",
             ]  # back to `None` after #7167
             td.append(row)
@@ -305,30 +298,30 @@
         if self.repo.machine is None:
             raise MachineDisabledError
 
         self.repo.machine.run_shell(self.args.name)
         return 0
 
 
-def add_parser(subparsers, parent_parser):
+def add_parser(subparsers, parent_parser):  # noqa: PLR0915
     from dvc.commands.config import parent_config_parser
 
     machine_HELP = "Set up and manage cloud machines."
     machine_parser = subparsers.add_parser(
         "machine",
         parents=[parent_parser],
         description=append_doc_link(machine_HELP, "machine"),
         # NOTE: suppress help during development to hide command
         # help=machine_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
 
     machine_subparsers = machine_parser.add_subparsers(
         dest="cmd",
-        help="Use `dvc machine CMD --help` for " "command-specific help.",
+        help="Use `dvc machine CMD --help` for command-specific help.",
     )
 
     fix_subparsers(machine_subparsers)
 
     machine_ADD_HELP = "Add a new data machine."
     machine_add_parser = machine_subparsers.add_parser(
         "add",
@@ -338,16 +331,15 @@
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
     machine_add_parser.add_argument("name", help="Name of the machine")
     machine_add_parser.add_argument(
         "cloud",
         help="Machine cloud. See full list of supported clouds at {}".format(
             format_link(
-                "https://github.com/iterative/"
-                "terraform-provider-iterative#machine"
+                "https://github.com/iterative/terraform-provider-iterative#machine"
             )
         ),
     )
     machine_add_parser.add_argument(
         "-d",
         "--default",
         action="store_true",
@@ -367,17 +359,15 @@
     machine_default_parser = machine_subparsers.add_parser(
         "default",
         parents=[parent_config_parser, parent_parser],
         description=append_doc_link(machine_DEFAULT_HELP, "machine/default"),
         help=machine_DEFAULT_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
-    machine_default_parser.add_argument(
-        "name", nargs="?", help="Name of the machine"
-    )
+    machine_default_parser.add_argument("name", nargs="?", help="Name of the machine")
     machine_default_parser.add_argument(
         "-u",
         "--unset",
         action="store_true",
         default=False,
         help="Unset default machine.",
     )
@@ -409,17 +399,15 @@
         "modify",
         parents=[parent_config_parser, parent_parser],
         description=append_doc_link(machine_MODIFY_HELP, "machine/modify"),
         help=machine_MODIFY_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
     machine_modify_parser.add_argument("name", help="Name of the machine")
-    machine_modify_parser.add_argument(
-        "option", help="Name of the option to modify."
-    )
+    machine_modify_parser.add_argument("option", help="Name of the option to modify.")
     machine_modify_parser.add_argument(
         "value", nargs="?", help="(optional) Value of the option."
     )
     machine_modify_parser.add_argument(
         "-u",
         "--unset",
         default=False,
@@ -444,35 +432,29 @@
     machine_remove_parser = machine_subparsers.add_parser(
         "remove",
         parents=[parent_config_parser, parent_parser],
         description=append_doc_link(machine_REMOVE_HELP, "machine/remove"),
         help=machine_REMOVE_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
-    machine_remove_parser.add_argument(
-        "name", help="Name of the machine to remove."
-    )
+    machine_remove_parser.add_argument("name", help="Name of the machine to remove.")
     machine_remove_parser.set_defaults(func=CmdMachineRemove)
 
     machine_CREATE_HELP = "Create and start a machine instance."
     machine_create_parser = machine_subparsers.add_parser(
         "create",
         parents=[parent_parser],
         description=append_doc_link(machine_CREATE_HELP, "machine/create"),
         help=machine_CREATE_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
-    machine_create_parser.add_argument(
-        "name", help="Name of the machine to create."
-    )
+    machine_create_parser.add_argument("name", help="Name of the machine to create.")
     machine_create_parser.set_defaults(func=CmdMachineCreate)
 
-    machine_STATUS_HELP = (
-        "List the status of running instances for one/all machines."
-    )
+    machine_STATUS_HELP = "List the status of running instances for one/all machines."
     machine_status_parser = machine_subparsers.add_parser(
         "status",
         parents=[parent_parser],
         description=append_doc_link(machine_STATUS_HELP, "machine/status"),
         help=machine_STATUS_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
```

### Comparing `dvc-2.9.5/dvc/commands/metrics.py` & `dvc-3.0.0a0/dvc/commands/metrics.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link, fix_subparsers
-from dvc.commands import completion
 from dvc.exceptions import DvcException
 from dvc.ui import ui
 from dvc.utils.serialize import encode_exception
 
 logger = logging.getLogger(__name__)
 
 
@@ -136,22 +136,20 @@
         "--all-commits",
         action="store_true",
         default=False,
         help="Show metrics for all commits.",
     )
     metrics_show_parser.add_argument(
         "--json",
-        "--show-json",
         action="store_true",
         default=False,
         help="Show output in JSON format.",
     )
     metrics_show_parser.add_argument(
         "--md",
-        "--show-md",
         action="store_true",
         default=False,
         dest="markdown",
         help="Show tabulated output in the Markdown format (GFM).",
     )
     metrics_show_parser.add_argument(
         "-R",
@@ -219,22 +217,20 @@
         "--all",
         action="store_true",
         default=False,
         help="Show unchanged metrics as well.",
     )
     metrics_diff_parser.add_argument(
         "--json",
-        "--show-json",
         action="store_true",
         default=False,
         help="Show output in JSON format.",
     )
     metrics_diff_parser.add_argument(
         "--md",
-        "--show-md",
         action="store_true",
         default=False,
         dest="markdown",
         help="Show tabulated output in the Markdown format (GFM).",
     )
     metrics_diff_parser.add_argument(
         "--no-path",
```

### Comparing `dvc-2.9.5/dvc/commands/move.py` & `dvc-3.0.0a0/dvc/commands/move.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,26 +1,24 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 from dvc.exceptions import DvcException
 
 logger = logging.getLogger(__name__)
 
 
 class CmdMove(CmdBase):
     def run(self):
         try:
             self.repo.move(self.args.src, self.args.dst)
         except DvcException:
-            msg = "failed to move '{}' -> '{}'".format(
-                self.args.src, self.args.dst
-            )
+            msg = f"failed to move '{self.args.src}' -> '{self.args.dst}'"
             logger.exception(msg)
             return 1
         return 0
 
 
 def add_parser(subparsers, parent_parser):
     MOVE_HELP = "Rename or move a DVC controlled data file or a directory."
@@ -36,11 +34,9 @@
         description=append_doc_link(MOVE_DESCRIPTION, "move"),
         help=MOVE_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
     move_parser.add_argument(
         "src", help="Source path to a data file or directory."
     ).complete = completion.FILE
-    move_parser.add_argument(
-        "dst", help="Destination path."
-    ).complete = completion.FILE
+    move_parser.add_argument("dst", help="Destination path.").complete = completion.FILE
     move_parser.set_defaults(func=CmdMove)
```

### Comparing `dvc-2.9.5/dvc/commands/params.py` & `dvc-3.0.0a0/dvc/commands/params.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link, fix_subparsers
-from dvc.commands import completion
 from dvc.exceptions import DvcException
 from dvc.ui import ui
 
 logger = logging.getLogger(__name__)
 
 
 class CmdParamsDiff(CmdBase):
@@ -75,15 +75,15 @@
     )
     params_diff_parser.add_argument(
         "a_rev", nargs="?", help="Old Git commit to compare (defaults to HEAD)"
     )
     params_diff_parser.add_argument(
         "b_rev",
         nargs="?",
-        help=("New Git commit to compare (defaults to the current workspace)"),
+        help="New Git commit to compare (defaults to the current workspace)",
     )
     params_diff_parser.add_argument(
         "--targets",
         nargs="*",
         help=(
             "Specific params file(s) to compare "
             "(even if not found as `params` in `dvc.yaml`). "
@@ -101,22 +101,20 @@
         "--deps",
         action="store_true",
         default=False,
         help="Show only params that are stage dependencies.",
     )
     params_diff_parser.add_argument(
         "--json",
-        "--show-json",
         action="store_true",
         default=False,
         help="Show output in JSON format.",
     )
     params_diff_parser.add_argument(
         "--md",
-        "--show-md",
         action="store_true",
         default=False,
         dest="markdown",
         help="Show tabulated output in the Markdown format (GFM).",
     )
     params_diff_parser.add_argument(
         "--no-path",
```

### Comparing `dvc-2.9.5/dvc/commands/remote.py` & `dvc-3.0.0a0/dvc/commands/remote.py`

 * *Files 2% similar despite different names*

```diff
@@ -91,17 +91,15 @@
                 ui.write("No default remote set")
                 return 1
         else:
             with self.config.edit(self.args.level) as conf:
                 if self.args.unset:
                     conf["core"].pop("remote", None)
                 else:
-                    merged_conf = self.config.load_config_to_level(
-                        self.args.level
-                    )
+                    merged_conf = self.config.load_config_to_level(self.args.level)
                     if (
                         self.args.name in conf["remote"]
                         or self.args.name in merged_conf["remote"]
                     ):
                         conf["core"]["remote"] = self.args.name
                     else:
                         raise ConfigError(
@@ -109,16 +107,16 @@
                         )
         return 0
 
 
 class CmdRemoteList(CmdRemote):
     def run(self):
         conf = self.config.read(self.args.level)
-        for name, conf in conf["remote"].items():
-            ui.write(name, conf["url"], sep="\t")
+        for name, remote_conf in conf["remote"].items():
+            ui.write(name, remote_conf["url"], sep="\t")
         return 0
 
 
 class CmdRemoteRename(CmdRemote):
     def _rename_default(self, conf):
         if conf["core"].get("remote") == self.args.name:
             conf["core"]["remote"] = self.args.new
@@ -160,15 +158,15 @@
         description=append_doc_link(REMOTE_HELP, "remote"),
         help=REMOTE_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
 
     remote_subparsers = remote_parser.add_subparsers(
         dest="cmd",
-        help="Use `dvc remote CMD --help` for " "command-specific help.",
+        help="Use `dvc remote CMD --help` for command-specific help.",
     )
 
     fix_subparsers(remote_subparsers)
 
     REMOTE_ADD_HELP = "Add a new data remote."
     remote_add_parser = remote_subparsers.add_parser(
         "add",
@@ -204,17 +202,15 @@
     remote_default_parser = remote_subparsers.add_parser(
         "default",
         parents=[parent_config_parser, parent_parser],
         description=append_doc_link(REMOTE_DEFAULT_HELP, "remote/default"),
         help=REMOTE_DEFAULT_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
-    remote_default_parser.add_argument(
-        "name", nargs="?", help="Name of the remote"
-    )
+    remote_default_parser.add_argument("name", nargs="?", help="Name of the remote")
     remote_default_parser.add_argument(
         "-u",
         "--unset",
         action="store_true",
         default=False,
         help="Unset default remote.",
     )
@@ -225,17 +221,15 @@
         "modify",
         parents=[parent_config_parser, parent_parser],
         description=append_doc_link(REMOTE_MODIFY_HELP, "remote/modify"),
         help=REMOTE_MODIFY_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
     remote_modify_parser.add_argument("name", help="Name of the remote")
-    remote_modify_parser.add_argument(
-        "option", help="Name of the option to modify."
-    )
+    remote_modify_parser.add_argument("option", help="Name of the option to modify.")
     remote_modify_parser.add_argument(
         "value", nargs="?", help="(optional) Value of the option."
     )
     remote_modify_parser.add_argument(
         "-u",
         "--unset",
         default=False,
@@ -258,17 +252,15 @@
     remote_remove_parser = remote_subparsers.add_parser(
         "remove",
         parents=[parent_config_parser, parent_parser],
         description=append_doc_link(REMOTE_REMOVE_HELP, "remote/remove"),
         help=REMOTE_REMOVE_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
-    remote_remove_parser.add_argument(
-        "name", help="Name of the remote to remove."
-    )
+    remote_remove_parser.add_argument("name", help="Name of the remote to remove.")
     remote_remove_parser.set_defaults(func=CmdRemoteRemove)
     REMOTE_RENAME_HELP = "Rename a DVC remote"
     remote_rename_parser = remote_subparsers.add_parser(
         "rename",
         parents=[parent_config_parser, parent_parser],
         description=append_doc_link(REMOTE_RENAME_HELP, "remote/rename"),
         help=REMOTE_RENAME_HELP,
```

### Comparing `dvc-2.9.5/dvc/commands/remove.py` & `dvc-3.0.0a0/dvc/commands/remove.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 from dvc.exceptions import DvcException
 
 logger = logging.getLogger(__name__)
 
 
 class CmdRemove(CmdBase):
     def run(self):
@@ -18,16 +18,15 @@
                 logger.exception("")
                 return 1
         return 0
 
 
 def add_parser(subparsers, parent_parser):
     REMOVE_HELP = (
-        "Remove stages from dvc.yaml and/or"
-        " stop tracking files or directories."
+        "Remove stages from dvc.yaml and/or stop tracking files or directories."
     )
     remove_parser = subparsers.add_parser(
         "remove",
         parents=[parent_parser],
         description=append_doc_link(REMOVE_HELP, "remove"),
         help=REMOVE_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
```

### Comparing `dvc-2.9.5/dvc/commands/repro.py` & `dvc-3.0.0a0/dvc/commands/repro.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,104 +1,93 @@
 import argparse
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 from dvc.commands.status import CmdDataStatus
 
 
 class CmdRepro(CmdBase):
     def run(self):
         from dvc.ui import ui
 
-        stages = self.repo.reproduce(**self._repro_kwargs)
+        stages = self.repo.reproduce(**self._common_kwargs, **self._repro_kwargs)
         if len(stages) == 0:
             ui.write(CmdDataStatus.UP_TO_DATE_MSG)
         else:
-            ui.write(
-                "Use `dvc push` to send your updates to " "remote storage."
-            )
-
-        if self.args.metrics:
-            from dvc.compare import show_metrics
-
-            metrics = self.repo.metrics.show()
-            show_metrics(metrics)
+            ui.write("Use `dvc push` to send your updates to remote storage.")
 
         return 0
 
     @property
-    def _repro_kwargs(self):
+    def _common_kwargs(self):
         return {
             "targets": self.args.targets,
             "single_item": self.args.single_item,
             "force": self.args.force,
             "dry": self.args.dry,
             "interactive": self.args.interactive,
             "pipeline": self.args.pipeline,
             "all_pipelines": self.args.all_pipelines,
-            "run_cache": not self.args.no_run_cache,
-            "no_commit": self.args.no_commit,
             "downstream": self.args.downstream,
             "recursive": self.args.recursive,
             "force_downstream": self.args.force_downstream,
             "pull": self.args.pull,
+            "allow_missing": self.args.allow_missing,
+        }
+
+    @property
+    def _repro_kwargs(self):
+        return {
+            "run_cache": not self.args.no_run_cache,
+            "no_commit": self.args.no_commit,
             "glob": self.args.glob,
         }
 
 
 def add_arguments(repro_parser):
     repro_parser.add_argument(
         "targets",
         nargs="*",
-        help="Stages to reproduce. 'dvc.yaml' by default.",
+        help="""\
+Stages to reproduce. 'dvc.yaml' by default.
+The targets can be path to a dvc.yaml file or `.dvc` file,
+or a stage name from dvc.yaml file from
+current working directory. To run a stage from dvc.yaml
+from other directories, the target must be a path followed by colon `:`
+and then the stage name name.
+""",
     ).complete = completion.DVCFILES_AND_STAGE
     repro_parser.add_argument(
         "-f",
         "--force",
         action="store_true",
         default=False,
         help="Reproduce even if dependencies were not changed.",
     )
     repro_parser.add_argument(
-        "-s",
-        "--single-item",
-        action="store_true",
-        default=False,
-        help="Reproduce only single data item without recursive dependencies "
-        "check.",
-    )
-    repro_parser.add_argument(
-        "-m",
-        "--metrics",
-        action="store_true",
-        default=False,
-        help="Show metrics after reproduction.",
-    )
-    repro_parser.add_argument(
-        "--dry",
+        "-i",
+        "--interactive",
         action="store_true",
         default=False,
-        help="Only print the commands that would be executed without "
-        "actually executing.",
+        help="Ask for confirmation before reproducing each stage.",
     )
     repro_parser.add_argument(
-        "-i",
-        "--interactive",
+        "-s",
+        "--single-item",
         action="store_true",
         default=False,
-        help="Ask for confirmation before reproducing each stage.",
+        help="Reproduce only single data item without recursive dependencies check.",
     )
     repro_parser.add_argument(
         "-p",
         "--pipeline",
         action="store_true",
         default=False,
-        help="Reproduce the whole pipeline that the specified targets "
-        "belong to.",
+        help="Reproduce the whole pipeline that the specified targets belong to.",
     )
     repro_parser.add_argument(
         "-P",
         "--all-pipelines",
         action="store_true",
         default=False,
         help="Reproduce all pipelines in the repo.",
@@ -107,64 +96,80 @@
         "-R",
         "--recursive",
         action="store_true",
         default=False,
         help="Reproduce all stages in the specified directory.",
     )
     repro_parser.add_argument(
-        "--no-run-cache",
+        "--downstream",
         action="store_true",
         default=False,
-        help=(
-            "Execute stage commands even if they have already been run with "
-            "the same command/dependencies/outputs/etc before."
-        ),
+        help="Start from the specified stages when reproducing pipelines.",
     )
     repro_parser.add_argument(
         "--force-downstream",
         action="store_true",
         default=False,
-        help="Reproduce all descendants of a changed stage even if their "
-        "direct dependencies didn't change.",
+        help=(
+            "Reproduce all descendants of a changed stage even if their "
+            "direct dependencies didn't change."
+        ),
     )
     repro_parser.add_argument(
-        "--no-commit",
+        "--pull",
         action="store_true",
         default=False,
-        help="Don't put files/directories into cache.",
+        help=(
+            "Try automatically pulling missing cache for outputs restored "
+            "from the run-cache."
+        ),
     )
     repro_parser.add_argument(
-        "--downstream",
+        "--allow-missing",
         action="store_true",
         default=False,
-        help="Start from the specified stages when reproducing pipelines.",
+        help=("Skip stages with missing data but no other changes."),
     )
     repro_parser.add_argument(
-        "--pull",
+        "--dry",
         action="store_true",
         default=False,
         help=(
-            "Try automatically pulling missing cache for outputs restored "
-            "from the run-cache."
+            "Only print the commands that would be executed without actually executing."
         ),
     )
-    repro_parser.add_argument(
-        "--glob",
-        action="store_true",
-        default=False,
-        help="Allows targets containing shell-style wildcards.",
-    )
 
 
 def add_parser(subparsers, parent_parser):
-    REPRO_HELP = (
-        "Reproduce complete or partial pipelines by executing their stages."
-    )
+    REPRO_HELP = "Reproduce complete or partial pipelines by executing their stages."
     repro_parser = subparsers.add_parser(
         "repro",
         parents=[parent_parser],
         description=append_doc_link(REPRO_HELP, "repro"),
         help=REPRO_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
+    # repro/exp run shared args
     add_arguments(repro_parser)
+    # repro only args
+    repro_parser.add_argument(
+        "--glob",
+        action="store_true",
+        default=False,
+        help="Allows targets containing shell-style wildcards.",
+    )
+    repro_parser.add_argument(
+        "--no-commit",
+        action="store_true",
+        default=False,
+        help="Don't put files/directories into cache.",
+    )
+    repro_parser.add_argument(
+        "--no-run-cache",
+        action="store_true",
+        default=False,
+        help=(
+            "Execute stage commands even if they have already been run with "
+            "the same command/dependencies/outputs/etc before."
+        ),
+    )
     repro_parser.set_defaults(func=CmdRepro)
```

### Comparing `dvc-2.9.5/dvc/commands/root.py` & `dvc-3.0.0a0/dvc/commands/root.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/dvc/commands/run.py` & `dvc-3.0.0a0/dvc/commands/run.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,15 +35,15 @@
             return 1
 
         kwargs = vars(self.args)
         kwargs.update(
             {
                 "cmd": parse_cmd(self.args.command),
                 "fname": kwargs.pop("file"),
-                "no_exec": (self.args.no_exec or bool(self.args.checkpoints)),
+                "no_exec": self.args.no_exec or bool(self.args.checkpoints),
                 "run_cache": not kwargs.pop("no_run_cache"),
             }
         )
         try:
             self.repo.run(**kwargs)
         except DvcException:
             logger.exception("")
@@ -51,50 +51,46 @@
 
         return 0
 
 
 def add_parser(subparsers, parent_parser):
     from dvc.commands.stage import _add_common_args
 
-    RUN_HELP = (
-        "Generate a dvc.yaml file from a command and execute the command."
-    )
+    RUN_HELP = "Generate a dvc.yaml file from a command and execute the command."
     run_parser = subparsers.add_parser(
         "run",
         parents=[parent_parser],
         description=append_doc_link(RUN_HELP, "run"),
         help=RUN_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
     run_parser.add_argument("-n", "--name", help="Stage name.")
+    run_parser.add_argument("--file", metavar="<filename>", help=argparse.SUPPRESS)
     run_parser.add_argument(
-        "--file", metavar="<filename>", help=argparse.SUPPRESS
-    )
-    run_parser.add_argument(
-        "--no-exec",
+        "--single-stage",
         action="store_true",
         default=False,
-        help="Only create dvc.yaml without actually running it.",
+        help=argparse.SUPPRESS,
     )
+    _add_common_args(run_parser)
     run_parser.add_argument(
-        "--no-run-cache",
+        "--no-exec",
         action="store_true",
         default=False,
-        help=(
-            "Execute the command even if this stage has already been run "
-            "with the same command/dependencies/outputs/etc before."
-        ),
+        help="Only create dvc.yaml without actually running it.",
     )
     run_parser.add_argument(
         "--no-commit",
         action="store_true",
         default=False,
         help="Don't put files/directories into cache.",
     )
     run_parser.add_argument(
-        "--single-stage",
+        "--no-run-cache",
         action="store_true",
         default=False,
-        help=argparse.SUPPRESS,
+        help=(
+            "Execute the command even if this stage has already been run "
+            "with the same command/dependencies/outputs/etc before."
+        ),
     )
-    _add_common_args(run_parser)
     run_parser.set_defaults(func=CmdRun)
```

### Comparing `dvc-2.9.5/dvc/commands/stage.py` & `dvc-3.0.0a0/dvc/commands/stage.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 import argparse
 import logging
+from contextlib import contextmanager
 from itertools import chain, filterfalse
 from typing import TYPE_CHECKING, Dict, Iterable, List
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link, fix_subparsers
-from dvc.commands import completion
 from dvc.utils.cli_parse import parse_params
 from dvc.utils.humanize import truncate_text
 
 if TYPE_CHECKING:
     from dvc.output import Output
     from dvc.stage import Stage
 
@@ -40,48 +41,44 @@
     plots_and_metrics = list(filter(is_plot_or_metric, stage.outs))
     if plots_and_metrics:
         desc.append("Reports " + part_desc(plots_and_metrics))
 
     return "; ".join(desc)
 
 
-def prepare_description(
-    stage: "Stage", max_length: int = MAX_TEXT_LENGTH
-) -> str:
+def prepare_description(stage: "Stage", max_length: int = MAX_TEXT_LENGTH) -> str:
     desc = stage.short_description() or generate_description(stage)
     return truncate_text(desc, max_length)
 
 
 def prepare_stages_data(
     stages: Iterable["Stage"],
     description: bool = True,
     max_length: int = MAX_TEXT_LENGTH,
 ) -> Dict[str, str]:
     return {
-        stage.addressing: prepare_description(stage, max_length=max_length)
-        if description
-        else ""
+        stage.addressing: (
+            prepare_description(stage, max_length=max_length) if description else ""
+        )
         for stage in stages
     }
 
 
 class CmdStageList(CmdBase):
     def _get_stages(self) -> Iterable["Stage"]:
         if self.args.all:
-            stages: List["Stage"] = self.repo.index.stages  # type: ignore
+            stages: List["Stage"] = self.repo.index.stages
             logger.trace(  # type: ignore[attr-defined]
                 "%d no. of stages found", len(stages)
             )
             return stages
 
         # removing duplicates while maintaining order
         collected = chain.from_iterable(
-            self.repo.stage.collect(
-                target=target, recursive=self.args.recursive, accept_group=True
-            )
+            self.repo.stage.collect(target=target, recursive=self.args.recursive)
             for target in self.args.targets
         )
         return dict.fromkeys(collected).keys()
 
     def run(self):
         from dvc.ui import ui
 
@@ -90,18 +87,16 @@
                 raise exc
             logger.debug("Stages from %s failed to load", relpath)
 
         # silence stage collection error by default
         self.repo.stage_collection_error_handler = log_error
 
         stages = self._get_stages()
-        names_only = self.args.names_only
-
-        data = prepare_stages_data(stages, description=not names_only)
-        ui.table(data.items())
+        data = prepare_stages_data(stages, description=not self.args.name_only)
+        ui.table(list(data.items()))
 
         return 0
 
 
 def parse_cmd(commands: List[str]) -> str:
     """
     We need to take into account two cases:
@@ -121,62 +116,124 @@
         return arg
 
     if len(commands) < 2:
         return " ".join(commands)
     return " ".join(map(quote_argument, commands))
 
 
+@contextmanager
+def _disable_logging(highest_level=logging.CRITICAL):
+    previous_level = logging.root.manager.disable
+
+    logging.disable(highest_level)
+
+    try:
+        yield
+    finally:
+        logging.disable(previous_level)
+
+
 class CmdStageAdd(CmdBase):
     def run(self):
+        from dvc.repo import lock_repo
+
         kwargs = vars(self.args)
         kwargs.update(
             {
                 "cmd": parse_cmd(kwargs.pop("command")),
                 "params": parse_params(self.args.params),
             }
         )
-        self.repo.stage.add(**kwargs)
+
+        with self.repo.scm_context, lock_repo(self.repo):
+            with _disable_logging(logging.INFO):
+                stage = self.repo.stage.add(**kwargs)
+            logger.info("Added stage %r in %r", stage.addressing, stage.relpath)
+            if self.args.run:
+                stage.run()
+                stage.dump(update_pipeline=False)
+
         return 0
 
 
 def _add_common_args(parser):
     parser.add_argument(
+        "-f",
+        "--force",
+        action="store_true",
+        default=False,
+        help="Overwrite existing stage",
+    )
+    parser.add_argument(
         "-d",
         "--deps",
         action="append",
         default=[],
         help="Declare dependencies for reproducible cmd.",
         metavar="<path>",
     ).complete = completion.FILE
     parser.add_argument(
+        "-p",
+        "--params",
+        action="append",
+        default=[],
+        help="Declare parameter to use as additional dependency.",
+        metavar="[<filename>:]<params_list>",
+    ).complete = completion.FILE
+    parser.add_argument(
         "-o",
         "--outs",
         action="append",
         default=[],
         help="Declare output file or directory.",
         metavar="<filename>",
     ).complete = completion.FILE
     parser.add_argument(
         "-O",
         "--outs-no-cache",
         action="append",
         default=[],
-        help="Declare output file or directory "
-        "(do not put into DVC cache).",
+        help="Declare output file or directory (do not put into DVC cache).",
         metavar="<filename>",
     ).complete = completion.FILE
     parser.add_argument(
-        "-p",
-        "--params",
+        "-c",
+        "--checkpoints",
         action="append",
         default=[],
-        help="Declare parameter to use as additional dependency.",
-        metavar="[<filename>:]<params_list>",
+        help=(
+            "Declare checkpoint output file or directory for 'dvc exp run'. "
+            "Not compatible with 'dvc repro'."
+        ),
+        metavar="<filename>",
     ).complete = completion.FILE
     parser.add_argument(
+        "--external",
+        action="store_true",
+        default=False,
+        help="Allow outputs that are outside of the DVC repository.",
+    )
+    parser.add_argument(
+        "--outs-persist",
+        action="append",
+        default=[],
+        help="Declare output file or directory that will not be removed upon repro.",
+        metavar="<filename>",
+    )
+    parser.add_argument(
+        "--outs-persist-no-cache",
+        action="append",
+        default=[],
+        help=(
+            "Declare output file or directory that will not be "
+            "removed upon repro (do not put into DVC cache)."
+        ),
+        metavar="<filename>",
+    )
+    parser.add_argument(
         "-m",
         "--metrics",
         action="append",
         default=[],
         help="Declare output metrics file.",
         metavar="<path>",
     )
@@ -199,93 +256,41 @@
         "--plots-no-cache",
         action="append",
         default=[],
         help="Declare output plot file (do not put into DVC cache).",
         metavar="<path>",
     )
     parser.add_argument(
-        "--live", help="Declare output as dvclive.", metavar="<path>"
-    )
-    parser.add_argument(
-        "--live-no-cache",
-        help="Declare output as dvclive (do not put into DVC cache).",
-        metavar="<path>",
-    )
-    parser.add_argument(
-        "--live-no-summary",
-        action="store_true",
-        default=False,
-        help="Signal dvclive logger to not dump latest metrics file.",
-    )
-    parser.add_argument(
-        "--live-no-html",
-        action="store_true",
-        default=False,
-        help="Signal dvclive logger to not produce training report.",
-    )
-    parser.add_argument(
         "-w",
         "--wdir",
         help="Directory within your repo to run your command in.",
         metavar="<path>",
     )
     parser.add_argument(
-        "-f",
-        "--force",
-        action="store_true",
-        default=False,
-        help="Overwrite existing stage",
-    )
-    parser.add_argument(
-        "--outs-persist",
-        action="append",
-        default=[],
-        help="Declare output file or directory that will not be "
-        "removed upon repro.",
-        metavar="<filename>",
-    )
-    parser.add_argument(
-        "--outs-persist-no-cache",
-        action="append",
-        default=[],
-        help="Declare output file or directory that will not be "
-        "removed upon repro (do not put into DVC cache).",
-        metavar="<filename>",
-    )
-    parser.add_argument(
-        "-c",
-        "--checkpoints",
-        action="append",
-        default=[],
-        help="Declare checkpoint output file or directory for 'dvc exp run'. "
-        "Not compatible with 'dvc repro'.",
-        metavar="<filename>",
-    ).complete = completion.FILE
-    parser.add_argument(
         "--always-changed",
         action="store_true",
         default=False,
         help="Always consider this DVC-file as changed.",
     )
     parser.add_argument(
-        "--external",
-        action="store_true",
-        default=False,
-        help="Allow outputs that are outside of the DVC repository.",
-    )
-    parser.add_argument(
         "--desc",
         type=str,
         metavar="<text>",
         help=(
             "User description of the stage (optional). "
             "This doesn't affect any DVC operations."
         ),
     )
     parser.add_argument(
+        "--run",
+        action="store_true",
+        default=False,
+        help="Execute the stage after generating it.",
+    )
+    parser.add_argument(
         "command",
         nargs=argparse.REMAINDER,
         help="Command to execute.",
         metavar="command",
     )
 
 
@@ -354,13 +359,14 @@
         "-R",
         "--recursive",
         action="store_true",
         default=False,
         help="List all stages inside the specified directory.",
     )
     stage_list_parser.add_argument(
+        "--name-only",
         "--names-only",
         action="store_true",
         default=False,
         help="List only stage names.",
     )
     stage_list_parser.set_defaults(func=CmdStageList)
```

### Comparing `dvc-2.9.5/dvc/commands/status.py` & `dvc-3.0.0a0/dvc/commands/status.py`

 * *Files 6% similar despite different names*

```diff
@@ -77,15 +77,13 @@
                 self._show(st, indent)
                 return 0
 
             # additional hints for the user
             if not self.repo.index.stages:
                 ui.write(self.EMPTY_PROJECT_MSG)
             elif self.args.cloud or self.args.remote:
-                remote = self.args.remote or self.repo.config["core"].get(
-                    "remote"
-                )
+                remote = self.args.remote or self.repo.config["core"].get("remote")
                 ui.write(self.IN_SYNC_MSG.format(remote=remote))
             else:
                 ui.write(self.UP_TO_DATE_MSG)
 
         return 0
```

### Comparing `dvc-2.9.5/dvc/commands/unprotect.py` & `dvc-3.0.0a0/dvc/commands/unprotect.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 from dvc.exceptions import DvcException
 
 logger = logging.getLogger(__name__)
 
 
 class CmdUnprotect(CmdBase):
     def run(self):
```

### Comparing `dvc-2.9.5/dvc/commands/update.py` & `dvc-3.0.0a0/dvc/commands/update.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,38 +1,42 @@
 import argparse
 import logging
 
+from dvc.cli import completion
 from dvc.cli.command import CmdBase
 from dvc.cli.utils import append_doc_link
-from dvc.commands import completion
 from dvc.exceptions import DvcException
 
 logger = logging.getLogger(__name__)
 
 
 class CmdUpdate(CmdBase):
     def run(self):
         ret = 0
         try:
             self.repo.update(
                 targets=self.args.targets,
                 rev=self.args.rev,
                 recursive=self.args.recursive,
                 to_remote=self.args.to_remote,
+                no_download=self.args.no_download,
                 remote=self.args.remote,
                 jobs=self.args.jobs,
             )
         except DvcException:
             logger.exception("failed update data")
             ret = 1
         return ret
 
 
 def add_parser(subparsers, parent_parser):
-    UPDATE_HELP = "Update data artifacts imported from other DVC repositories."
+    UPDATE_HELP = (
+        "Update data artifact imported (via dvc import or dvc import-url) "
+        "from an external DVC repository or URL."
+    )
     update_parser = subparsers.add_parser(
         "update",
         parents=[parent_parser],
         description=append_doc_link(UPDATE_HELP, "update"),
         help=UPDATE_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
@@ -49,14 +53,23 @@
         "-R",
         "--recursive",
         action="store_true",
         default=False,
         help="Update all stages in the specified directory.",
     )
     update_parser.add_argument(
+        "--no-download",
+        action="store_true",
+        default=False,
+        help=(
+            "Update .dvc file git revision/hash value(s)"
+            " but do not download the file(s)."
+        ),
+    )
+    update_parser.add_argument(
         "--to-remote",
         action="store_true",
         default=False,
         help="Update data directly on the remote",
     )
     update_parser.add_argument(
         "-r",
@@ -67,12 +80,11 @@
     update_parser.add_argument(
         "-j",
         "--jobs",
         type=int,
         help=(
             "Number of jobs to run simultaneously. "
             "The default value is 4 * cpu_count(). "
-            "For SSH remotes, the default is 4. "
         ),
         metavar="<number>",
     )
     update_parser.set_defaults(func=CmdUpdate)
```

### Comparing `dvc-2.9.5/dvc/commands/version.py` & `dvc-3.0.0a0/dvc/commands/version.py`

 * *Files 12% similar despite different names*

```diff
@@ -17,17 +17,15 @@
         ui.write(dvc_info, force=True)
 
         notify_updates()
         return 0
 
 
 def add_parser(subparsers, parent_parser):
-    VERSION_HELP = (
-        "Display the DVC version and system/environment information."
-    )
+    VERSION_HELP = "Display the DVC version and system/environment information."
     version_parser = subparsers.add_parser(
         "version",
         parents=[parent_parser],
         description=append_doc_link(VERSION_HELP, "version"),
         help=VERSION_HELP,
         formatter_class=argparse.RawDescriptionHelpFormatter,
         aliases=["doctor"],
```

### Comparing `dvc-2.9.5/dvc/compare.py` & `dvc-3.0.0a0/dvc/compare.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,28 +18,27 @@
     Union,
     overload,
 )
 
 from funcy import reraise
 
 if TYPE_CHECKING:
-    from dvc.types import StrPath
     from dvc.ui.table import CellT
 
 
 class Column(List["CellT"]):
     pass
 
 
 def with_value(value, default):
     return default if value is None else value
 
 
 class TabularData(MutableSequence[Sequence["CellT"]]):
-    def __init__(self, columns: Sequence[str], fill_value: str = ""):
+    def __init__(self, columns: Sequence[str], fill_value: Optional[str] = ""):
         self._columns: Dict[str, Column] = {name: Column() for name in columns}
         self._keys: List[str] = list(columns)
         self._fill_value = fill_value
         self._protected: Set[str] = set()
 
     @property
     def columns(self) -> List[Column]:
@@ -54,23 +53,21 @@
     def unprotect(self, *col_names: str):
         self._protected = self._protected.difference(col_names)
 
     def column(self, name: str) -> Column:
         return self._columns[name]
 
     def items(self) -> ItemsView[str, Column]:
-        projection = {k: self.column(k) for k in self.keys()}
+        projection = {k: self.column(k) for k in self.keys()}  # noqa: SIM118
         return projection.items()
 
     def keys(self) -> List[str]:
         return self._keys
 
-    def _iter_col_row(
-        self, row: Sequence["CellT"]
-    ) -> Iterator[Tuple["CellT", Column]]:
+    def _iter_col_row(self, row: Sequence["CellT"]) -> Iterator[Tuple["CellT", Column]]:
         for val, col in zip_longest(row, self.columns):
             if col is None:
                 break
             yield with_value(val, self._fill_value), col
 
     def append(self, value: Sequence["CellT"]) -> None:
         for val, col in self._iter_col_row(value):
@@ -99,26 +96,23 @@
         return list(it)
 
     @overload
     def __setitem__(self, item: int, value: Sequence["CellT"]) -> None:
         ...
 
     @overload
-    def __setitem__(
-        self, item: slice, value: Iterable[Sequence["CellT"]]
-    ) -> None:
+    def __setitem__(self, item: slice, value: Iterable[Sequence["CellT"]]) -> None:
         ...
 
     def __setitem__(self, item, value) -> None:
         it = value
         if isinstance(item, slice):
             n = len(self.columns)
             normalized_rows = (
-                chain(val, repeat(self._fill_value, n - len(val)))
-                for val in value
+                chain(val, repeat(self._fill_value, n - len(val))) for val in value
             )
             # we need to transpose those rows into columnar format
             # as we work in terms of column-based arrays
             it = zip(*normalized_rows)
 
         for i, col in self._iter_col_row(it):
             col[item] = i
@@ -162,121 +156,97 @@
         writer = csv.writer(buff)
         writer.writerow(self.keys())
 
         for row in self:
             writer.writerow(row)
         return buff.getvalue()
 
-    def to_parallel_coordinates(
-        self, output_path: "StrPath", color_by: str = None
-    ) -> str:
-        from dvc.render.html import write
-        from dvc.render.plotly import ParallelCoordinatesRenderer
-
-        index_path = write(
-            output_path,
-            renderers=[
-                ParallelCoordinatesRenderer(self, color_by, self._fill_value)
-            ],
-        )
-        return index_path.as_uri()
-
     def add_column(self, name: str) -> None:
         self._columns[name] = Column([self._fill_value] * len(self))
         self._keys.append(name)
 
     def row_from_dict(self, d: Mapping[str, "CellT"]) -> None:
         keys = self.keys()
         for key in d:
             if key not in keys:
                 self.add_column(key)
 
         row: List["CellT"] = [
-            with_value(d.get(key), self._fill_value) for key in self.keys()
+            with_value(d.get(key), self._fill_value)
+            for key in self.keys()  # noqa: SIM118
         ]
         self.append(row)
 
     def render(self, **kwargs: Any):
         from dvc.ui import ui
 
         if kwargs.pop("csv", False):
             ui.write(self.to_csv(), end="")
         else:
             ui.table(self, headers=self.keys(), **kwargs)
 
     def as_dict(
-        self, cols: Iterable[str] = None
+        self, cols: Optional[Iterable[str]] = None
     ) -> Iterable[Dict[str, "CellT"]]:
         keys = self.keys() if cols is None else set(cols)
-        return [
-            {k: self._columns[k][i] for k in keys} for i in range(len(self))
-        ]
+        return [{k: self._columns[k][i] for k in keys} for i in range(len(self))]
 
-    def dropna(
+    def dropna(  # noqa: C901, PLR0912
         self,
         axis: str = "rows",
         how="any",
         subset: Optional[Iterable[str]] = None,
     ):
         if axis not in ["rows", "cols"]:
             raise ValueError(
-                f"Invalid 'axis' value {axis}."
-                "Choose one of ['rows', 'cols']"
+                f"Invalid 'axis' value {axis}.Choose one of ['rows', 'cols']"
             )
         if how not in ["any", "all"]:
-            raise ValueError(
-                f"Invalid 'how' value {how}." "Choose one of ['any', 'all']"
-            )
+            raise ValueError(f"Invalid 'how' value {how}. Choose one of ['any', 'all']")
 
         match_line: Set = set()
         match_any = True
         if how == "all":
             match_any = False
 
         for n_row, row in enumerate(self):
             for n_col, col in enumerate(row):
                 if subset and self.keys()[n_col] not in subset:
                     continue
                 if (col == self._fill_value) is match_any:
                     if axis == "rows":
                         match_line.add(n_row)
                         break
-                    else:
-                        match_line.add(self.keys()[n_col])
+                    match_line.add(self.keys()[n_col])
 
         to_drop = match_line
         if how == "all":
             if axis == "rows":
                 to_drop = set(range(len(self)))
             else:
                 to_drop = set(self.keys())
             to_drop -= match_line
 
         if axis == "rows":
-            for name in self.keys():
+            for name in self.keys():  # noqa: SIM118
                 self._columns[name] = Column(
-                    [
-                        x
-                        for n, x in enumerate(self._columns[name])
-                        if n not in to_drop
-                    ]
+                    [x for n, x in enumerate(self._columns[name]) if n not in to_drop]
                 )
         else:
             self.drop(*to_drop)
 
-    def drop_duplicates(
+    def drop_duplicates(  # noqa: C901
         self,
         axis: str = "rows",
         subset: Optional[Iterable[str]] = None,
         ignore_empty: bool = True,
     ):
         if axis not in ["rows", "cols"]:
             raise ValueError(
-                f"Invalid 'axis' value {axis}."
-                "Choose one of ['rows', 'cols']"
+                f"Invalid 'axis' value {axis}.Choose one of ['rows', 'cols']"
             )
 
         if axis == "cols":
             cols_to_drop: List[str] = []
             for n_col, col in enumerate(self.columns):
                 if subset and self.keys()[n_col] not in subset:
                     continue
@@ -301,35 +271,36 @@
 
                 tuple_row = tuple(row)
                 if tuple_row in unique_rows:
                     rows_to_drop.append(n_row)
                 else:
                     unique_rows.append(tuple_row)
 
-            for name in self.keys():
+            for name in self.keys():  # noqa: SIM118
                 self._columns[name] = Column(
                     [
                         x
                         for n, x in enumerate(self._columns[name])
                         if n not in rows_to_drop
                     ]
                 )
 
 
 def _normalize_float(val: float, precision: int):
     return f"{val:.{precision}g}"
 
 
 def _format_field(
-    val: Any, precision: int = None, round_digits: bool = False
+    val: Any, precision: Optional[int] = None, round_digits: bool = False
 ) -> str:
     def _format(_val):
         if isinstance(_val, float) and precision:
-            func = round if round_digits else _normalize_float
-            return func(_val, precision)
+            if round_digits:
+                return round(_val, precision)
+            return _normalize_float(_val, precision)
         if isinstance(_val, abc.Mapping):
             return {k: _format(v) for k, v in _val.items()}
         if isinstance(_val, list):
             return [_format(x) for x in _val]
         return _val
 
     return str(_format(val))
@@ -337,33 +308,31 @@
 
 def diff_table(
     diff,
     title: str,
     old: bool = True,
     no_path: bool = False,
     show_changes: bool = True,
-    precision: int = None,
+    precision: Optional[int] = None,
     round_digits: bool = False,
-    on_empty_diff: str = None,
-    a_rev: str = None,
-    b_rev: str = None,
+    on_empty_diff: Optional[str] = None,
+    a_rev: Optional[str] = None,
+    b_rev: Optional[str] = None,
 ) -> TabularData:
     a_rev = a_rev or "HEAD"
     b_rev = b_rev or "workspace"
     headers: List[str] = ["Path", title, a_rev, b_rev, "Change"]
     fill_value = "-"
     td = TabularData(headers, fill_value=fill_value)
 
     for fname, diff_in_file in diff.items():
         for item, change in sorted(diff_in_file.items()):
             old_value = with_value(change.get("old"), fill_value)
             new_value = with_value(change.get("new"), fill_value)
-            diff_value = with_value(
-                change.get("diff", on_empty_diff), fill_value
-            )
+            diff_value = with_value(change.get("diff", on_empty_diff), fill_value)
             td.append(
                 [
                     fname,
                     str(item),
                     _format_field(old_value, precision, round_digits),
                     _format_field(new_value, precision, round_digits),
                     _format_field(diff_value, precision, round_digits),
@@ -379,26 +348,26 @@
     if not old:
         td.drop(a_rev)
         td.rename(b_rev, "Value")
 
     return td
 
 
-def show_diff(
+def show_diff(  # noqa: PLR0913
     diff,
     title: str,
     old: bool = True,
     no_path: bool = False,
     show_changes: bool = True,
-    precision: int = None,
+    precision: Optional[int] = None,
     round_digits: bool = False,
-    on_empty_diff: str = None,
+    on_empty_diff: Optional[str] = None,
     markdown: bool = False,
-    a_rev: str = None,
-    b_rev: str = None,
+    a_rev: Optional[str] = None,
+    b_rev: Optional[str] = None,
 ) -> None:
     td = diff_table(
         diff,
         title=title,
         old=old,
         no_path=no_path,
         show_changes=show_changes,
@@ -412,15 +381,15 @@
 
 
 def metrics_table(
     metrics,
     all_branches: bool = False,
     all_tags: bool = False,
     all_commits: bool = False,
-    precision: int = None,
+    precision: Optional[int] = None,
     round_digits: bool = False,
 ):
     from dvc.utils.diff import format_dict
     from dvc.utils.flatten import flatten
 
     td = TabularData(["Revision", "Path"], fill_value="-")
 
@@ -452,15 +421,15 @@
 
 def show_metrics(
     metrics,
     markdown: bool = False,
     all_branches: bool = False,
     all_tags: bool = False,
     all_commits: bool = False,
-    precision: int = None,
+    precision: Optional[int] = None,
     round_digits: bool = False,
 ) -> None:
     td = metrics_table(
         metrics,
         all_branches=all_branches,
         all_tags=all_tags,
         all_commits=all_commits,
```

### Comparing `dvc-2.9.5/dvc/config.py` & `dvc-3.0.0a0/dvc/config.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,25 @@
 """DVC config objects."""
 import logging
 import os
 import re
 from contextlib import contextmanager
 from functools import partial
+from typing import TYPE_CHECKING, Dict, Optional
 
-from funcy import cached_property, compact, memoize, re_find
+from funcy import compact, memoize, re_find
 
 from dvc.exceptions import DvcException, NotDvcRepoError
 
+from .utils.objects import cached_property
+
+if TYPE_CHECKING:
+    from dvc.fs import FileSystem
+    from dvc.types import DictStrAny, StrPath
+
 logger = logging.getLogger(__name__)
 
 
 class ConfigError(DvcException):
     """DVC config exception."""
 
     def __init__(self, msg):
@@ -68,86 +75,90 @@
         validate (bool): optional flag to tell dvc if it should validate the
             config or just load it as is. 'True' by default.
 
     Raises:
         ConfigError: thrown if config has an invalid format.
     """
 
-    APPNAME = "dvc"
-    APPAUTHOR = "iterative"
-
     SYSTEM_LEVELS = ("system", "global")
     REPO_LEVELS = ("repo", "local")
     # In the order they shadow each other
     LEVELS = SYSTEM_LEVELS + REPO_LEVELS
 
     CONFIG = "config"
     CONFIG_LOCAL = "config.local"
 
     def __init__(
-        self, dvc_dir=None, validate=True, fs=None, config=None
+        self,
+        dvc_dir: Optional["StrPath"] = None,
+        validate: bool = True,
+        fs: Optional["FileSystem"] = None,
+        config: Optional["DictStrAny"] = None,
     ):  # pylint: disable=super-init-not-called
-        from dvc.fs.local import LocalFileSystem
+        from dvc.fs import LocalFileSystem
 
         self.dvc_dir = dvc_dir
-
-        if not dvc_dir:
-            try:
-                from dvc.repo import Repo
-
-                self.dvc_dir = Repo.find_dvc_dir()
-            except NotDvcRepoError:
-                self.dvc_dir = None
-        else:
-            self.dvc_dir = os.path.abspath(os.path.realpath(dvc_dir))
-
-        self.wfs = LocalFileSystem(url=self.dvc_dir)
+        self.wfs = LocalFileSystem()
         self.fs = fs or self.wfs
 
+        if dvc_dir:
+            self.dvc_dir = self.fs.path.abspath(self.fs.path.realpath(dvc_dir))
+
         self.load(validate=validate, config=config)
 
     @classmethod
+    def from_cwd(cls, fs: Optional["FileSystem"] = None, **kwargs):
+        from dvc.repo import Repo
+
+        try:
+            dvc_dir = Repo.find_dvc_dir(fs=fs)
+        except NotDvcRepoError:
+            dvc_dir = None
+
+        return cls(dvc_dir=dvc_dir, fs=fs, **kwargs)
+
+    @classmethod
     def get_dir(cls, level):
-        from appdirs import site_config_dir, user_config_dir
+        from dvc.dirs import global_config_dir, system_config_dir
 
         assert level in ("global", "system")
 
         if level == "global":
-            return user_config_dir(cls.APPNAME, cls.APPAUTHOR)
+            return global_config_dir()
         if level == "system":
-            return site_config_dir(cls.APPNAME, cls.APPAUTHOR)
+            return system_config_dir()
 
     @cached_property
-    def files(self):
+    def files(self) -> Dict[str, str]:
         files = {
             level: os.path.join(self.get_dir(level), self.CONFIG)
             for level in ("system", "global")
         }
 
         if self.dvc_dir is not None:
-            files["repo"] = os.path.join(self.dvc_dir, self.CONFIG)
-            files["local"] = os.path.join(self.dvc_dir, self.CONFIG_LOCAL)
+            files["repo"] = self.fs.path.join(self.dvc_dir, self.CONFIG)
+            files["local"] = self.fs.path.join(self.dvc_dir, self.CONFIG_LOCAL)
 
         return files
 
     @staticmethod
     def init(dvc_dir):
         """Initializes dvc config.
 
         Args:
             dvc_dir (str): path to .dvc directory.
 
         Returns:
             dvc.config.Config: config object.
         """
         config_file = os.path.join(dvc_dir, Config.CONFIG)
-        open(config_file, "w+", encoding="utf-8").close()
-        return Config(dvc_dir)
+        with open(config_file, "w+", encoding="utf-8"):
+            return Config(dvc_dir)
 
-    def load(self, validate=True, config=None):
+    def load(self, validate: bool = True, config: Optional["DictStrAny"] = None):
         """Loads config from all the config files.
 
         Raises:
             ConfigError: thrown if config has an invalid format.
         """
         conf = self.load_config_to_level()
 
@@ -156,43 +167,44 @@
 
         if validate:
             conf = self.validate(conf)
 
         self.clear()
         self.update(conf)
 
-        # Add resolved default cache.dir
-        if not self["cache"].get("dir") and self.dvc_dir:
-            self["cache"]["dir"] = os.path.join(self.dvc_dir, "cache")
-
     def _get_fs(self, level):
         # NOTE: this might be a Gitfs, which doesn't see things outside of
         # the repo.
         return self.fs if level == "repo" else self.wfs
 
     def _load_config(self, level):
-        from configobj import ConfigObj
+        from configobj import ConfigObj, ConfigObjError
 
         filename = self.files[level]
         fs = self._get_fs(level)
 
         if fs.exists(filename):
             with fs.open(filename) as fobj:
-                conf_obj = ConfigObj(fobj)
+                try:
+                    conf_obj = ConfigObj(fobj)
+                except UnicodeDecodeError as exc:
+                    raise ConfigError(str(exc)) from exc
+                except ConfigObjError as exc:
+                    raise ConfigError(str(exc)) from exc
         else:
             conf_obj = ConfigObj()
         return _parse_named(_lower_keys(conf_obj.dict()))
 
     def _save_config(self, level, conf_dict):
         from configobj import ConfigObj
 
         filename = self.files[level]
         fs = self._get_fs(level)
 
-        logger.debug(f"Writing '{filename}'.")
+        logger.debug("Writing '%s'.", filename)
 
         fs.makedirs(os.path.dirname(filename))
 
         config = ConfigObj(_pack_named(conf_dict))
         with fs.open(filename, "wb") as fobj:
             config.write(fobj)
         config.filename = filename
@@ -224,15 +236,15 @@
 
             return RelPath(os.path.join(abs_conf_dir, path))
 
         return Config._map_dirs(conf, resolve)
 
     @staticmethod
     def _to_relpath(conf_dir, path):
-        from dvc.fs.local import localfs
+        from dvc.fs import localfs
         from dvc.utils import relpath
 
         from .config_schema import RelPath
 
         if re.match(r"\w+://", path):
             return path
 
@@ -272,15 +284,15 @@
                     "setup_script": func,
                 }
             },
         }
         return Schema(dirs_schema, extra=ALLOW_EXTRA)(conf)
 
     def load_config_to_level(self, level=None):
-        merged_conf = {}
+        merged_conf: Dict = {}
         for merge_level in self.LEVELS:
             if merge_level == level:
                 break
             if merge_level in self.files:
                 merge(merged_conf, self.load_one(merge_level))
         return merged_conf
 
@@ -318,15 +330,15 @@
         try:
             return get_compiled_schema()(data)
         except Invalid as exc:
             raise ConfigError(str(exc)) from None
 
 
 def _parse_named(conf):
-    result = {"remote": {}, "machine": {}}
+    result: Dict[str, Dict] = {"remote": {}, "machine": {}}
 
     for section, val in conf.items():
         match = re_find(r'^\s*(remote|machine)\s*"(.*)"\s*$', section)
         if match:
             key, name = match
             result[key][name] = val
         else:
@@ -355,10 +367,9 @@
             merge(into[key], val)
         else:
             into[key] = val
 
 
 def _lower_keys(data):
     return {
-        k.lower(): _lower_keys(v) if isinstance(v, dict) else v
-        for k, v in data.items()
+        k.lower(): _lower_keys(v) if isinstance(v, dict) else v for k, v in data.items()
     }
```

### Comparing `dvc-2.9.5/dvc/config_schema.py` & `dvc-3.0.0a0/dvc/config_schema.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,22 +1,26 @@
+import logging
 import os
 from urllib.parse import urlparse
 
-from funcy import walk_values
+from funcy import once, walk_values
 from voluptuous import (
+    REMOVE_EXTRA,
     All,
     Any,
     Coerce,
     Invalid,
     Lower,
     Optional,
     Range,
     Schema,
 )
 
+logger = logging.getLogger(__name__)
+
 Bool = All(
     Lower,
     Any("true", "false"),
     lambda v: v == "true",
     msg="expected true or false",
 )
 
@@ -30,59 +34,82 @@
     if types is None:
         return None
     if isinstance(types, str):
         types = [typ.strip() for typ in types.split(",")]
 
     unsupported = set(types) - {"reflink", "hardlink", "symlink", "copy"}
     if unsupported:
-        raise Invalid(
-            "Unsupported cache type(s): {}".format(", ".join(unsupported))
-        )
+        raise Invalid("Unsupported cache type(s): {}".format(", ".join(unsupported)))
 
     return types
 
 
-def Choices(*choices):
+def Choices(*choices):  # noqa: N802
     """Checks that value belongs to the specified set of values
 
     Args:
         *choices: pass allowed values as arguments, or pass a list or
             tuple as a single argument
     """
     return Any(*choices, msg="expected one of {}".format(", ".join(choices)))
 
 
-def ByUrl(mapping):
+def ByUrl(mapping):  # noqa: N802
     schemas = walk_values(Schema, mapping)
 
     def validate(data):
         if "url" not in data:
             raise Invalid("expected 'url'")
 
         parsed = urlparse(data["url"])
         # Windows absolute paths should really have scheme == "" (local)
-        if os.name == "nt" and len(parsed.scheme) == 1 and parsed.netloc == "":
+        if os.name == "nt" and len(parsed.scheme) == 1 and not parsed.netloc:
+            return schemas[""](data)
+        if not parsed.netloc:
             return schemas[""](data)
         if parsed.scheme not in schemas:
             raise Invalid(f"Unsupported URL type {parsed.scheme}://")
 
         return schemas[parsed.scheme](data)
 
     return validate
 
 
 class RelPath(str):
     pass
 
 
+class FeatureSchema(Schema):
+    def __init__(self, schema, required=False):
+        super().__init__(schema, required=required, extra=REMOVE_EXTRA)
+
+    @staticmethod
+    @once
+    def _log_deprecated(keys):
+        # only run this once per session
+        message = "%s config option%s unsupported"
+        paths = ", ".join(f"'feature.{key}'" for key in keys)
+        pluralize = " is" if len(keys) == 1 else "s are"
+        logger.warning(message, paths, pluralize)
+
+    def __call__(self, data):
+        ret = super().__call__(data)
+        extra_keys = data.keys() - ret.keys()
+        if extra_keys:
+            self._log_deprecated(sorted(extra_keys))
+        return ret
+
+
 REMOTE_COMMON = {
     "url": str,
     "checksum_jobs": All(Coerce(int), Range(1)),
     "jobs": All(Coerce(int), Range(1)),
+    Optional("worktree"): Bool,
     Optional("no_traverse"): Bool,  # obsoleted
+    Optional("version_aware"): Bool,
 }
 LOCAL_COMMON = {
     "type": supported_cache_type,
     Optional("protected", default=False): Bool,  # obsoleted
     "shared": All(Lower, Choices("group")),
     Optional("slow_link_warning", default=True): Bool,
     Optional("verify", default=False): Bool,
@@ -91,21 +118,24 @@
     "auth": All(Lower, Choices("basic", "digest", "custom")),
     "custom_auth_header": str,
     "user": str,
     "password": str,
     "ask_password": Bool,
     "ssl_verify": Any(Bool, str),
     "method": str,
+    "connect_timeout": All(Coerce(float), Range(0, min_included=True)),
+    "read_timeout": All(Coerce(float), Range(0, min_included=True)),
     Optional("verify", default=False): Bool,
 }
 WEBDAV_COMMON = {
     "user": str,
     "password": str,
     "ask_password": Bool,
     "token": str,
+    "custom_auth_header": str,
     "cert_path": str,
     "key_path": str,
     "timeout": Coerce(int),
     "ssl_verify": Any(Bool, str),
     Optional("verify", default=False): Bool,
 }
 
@@ -116,14 +146,15 @@
         Optional("interactive", default=False): Bool,
         Optional("analytics", default=True): Bool,
         Optional("hardlink_lock", default=False): Bool,
         Optional("no_scm", default=False): Bool,
         Optional("autostage", default=False): Bool,
         Optional("experiments"): Bool,  # obsoleted
         Optional("check_update", default=True): Bool,
+        "site_cache_dir": str,
         "machine": Lower,
     },
     "cache": {
         "local": str,
         "s3": str,
         "gs": str,
         "hdfs": str,
@@ -148,41 +179,47 @@
                     "secret_access_key": str,
                     "session_token": str,
                     Optional("listobjects", default=False): Bool,  # obsoleted
                     Optional("use_ssl", default=True): Bool,
                     "ssl_verify": Any(Bool, str),
                     "sse": str,
                     "sse_kms_key_id": str,
+                    "sse_customer_algorithm": str,
+                    "sse_customer_key": str,
                     "acl": str,
                     "grant_read": str,
                     "grant_read_acp": str,
                     "grant_write_acp": str,
                     "grant_full_control": str,
                     "cache_regions": bool,
                     "read_timeout": Coerce(int),
                     "connect_timeout": Coerce(int),
                     Optional("verify", default=False): Bool,
                     **REMOTE_COMMON,
                 },
                 "gs": {
                     "projectname": str,
                     "credentialpath": str,
+                    "endpointurl": str,
                     Optional("verify", default=False): Bool,
                     **REMOTE_COMMON,
                 },
                 "ssh": {
                     "type": supported_cache_type,
                     "port": Coerce(int),
                     "user": str,
                     "password": str,
                     "ask_password": Bool,
+                    "passphrase": str,
+                    "ask_passphrase": Bool,
                     "keyfile": str,
                     "timeout": Coerce(int),
                     "gss_auth": Bool,
                     "allow_agent": Bool,
+                    "max_sessions": Coerce(int),
                     Optional("verify", default=False): Bool,
                     **REMOTE_COMMON,
                 },
                 "hdfs": {"user": str, "kerb_ticket": str, **REMOTE_COMMON},
                 "webhdfs": {
                     "kerberos": Bool,
                     "kerberos_principal": str,
@@ -213,70 +250,87 @@
                     "oss_key_id": str,
                     "oss_key_secret": str,
                     "oss_endpoint": str,
                     Optional("verify", default=True): Bool,
                     **REMOTE_COMMON,
                 },
                 "gdrive": {
+                    "profile": str,
                     "gdrive_use_service_account": Bool,
                     "gdrive_client_id": str,
                     "gdrive_client_secret": str,
                     "gdrive_user_credentials_file": str,
                     "gdrive_service_account_user_email": str,
                     "gdrive_service_account_json_file_path": str,
                     Optional("gdrive_trash_only", default=False): Bool,
+                    Optional("gdrive_acknowledge_abuse", default=False): Bool,
                     Optional("verify", default=True): Bool,
                     **REMOTE_COMMON,
                 },
                 "http": {**HTTP_COMMON, **REMOTE_COMMON},
                 "https": {**HTTP_COMMON, **REMOTE_COMMON},
                 "webdav": {**WEBDAV_COMMON, **REMOTE_COMMON},
                 "webdavs": {**WEBDAV_COMMON, **REMOTE_COMMON},
                 "remote": {str: object},  # Any of the above options are valid
             }
         )
     },
     "state": {
-        "dir": str,
+        "dir": str,  # obsoleted
         "row_limit": All(Coerce(int), Range(1)),  # obsoleted
         "row_cleanup_quota": All(Coerce(int), Range(0, 100)),  # obsoleted
     },
     "index": {
-        "dir": str,
+        "dir": str,  # obsoleted
     },
     "machine": {
         str: {
             "cloud": All(Lower, Choices("aws", "azure")),
-            "region": All(
-                Lower, Choices("us-west", "us-east", "eu-west", "eu-north")
-            ),
+            "region": All(Lower, Choices("us-west", "us-east", "eu-west", "eu-north")),
             "image": str,
             "spot": Bool,
             "spot_price": Coerce(float),
             "instance_hdd_size": Coerce(int),
             "instance_type": Lower,
             "instance_gpu": Lower,
             "ssh_private": str,
             "startup_script": str,
             "setup_script": str,
         },
     },
     # section for experimental features
-    "feature": {
-        Optional("machine", default=False): Bool,
-        # enabled by default. It's of no use, kept for backward compatibility.
-        Optional("parametrization", default=True): Bool,
-    },
+    # only specified keys are validated, others get logged and then ignored/removed
+    "feature": FeatureSchema(
+        {
+            Optional("machine", default=False): Bool,
+        },
+    ),
     "plots": {
         "html_template": str,
         Optional("auto_open", default=False): Bool,
+        "out_dir": str,
     },
     "exp": {
         "code": str,
         "data": str,
         "models": str,
         "metrics": str,
         "params": str,
         "plots": str,
         "live": str,
     },
+    "parsing": {
+        "bool": All(Lower, Choices("store_true", "boolean_optional")),
+        "list": All(Lower, Choices("nargs", "append")),
+    },
+    "hydra": {
+        Optional("enabled", default=False): Bool,
+        "config_dir": str,
+        "config_name": str,
+    },
+    "studio": {
+        "token": str,
+        "url": str,
+        "repo_url": str,
+        Optional("offline", default=False): Bool,
+    },
 }
```

### Comparing `dvc-2.9.5/dvc/daemon.py` & `dvc-3.0.0a0/dvc/daemon.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,113 +1,138 @@
 """Launch `dvc daemon` command in a separate detached process."""
 
 import inspect
 import logging
 import os
 import platform
 import sys
-from subprocess import Popen
+from subprocess import Popen  # nosec B404
+from typing import List
 
 from dvc.env import DVC_DAEMON
 from dvc.utils import fix_env, is_binary
 
 logger = logging.getLogger(__name__)
 
 
-def _popen(cmd, **kwargs):
+def _suppress_resource_warning(popen: Popen):
+    """Sets the returncode to avoid ResourceWarning when popen is garbage collected."""
+    # only use for daemon processes.
+    # See https://bugs.python.org/issue38890.
+    popen.returncode = 0
+
+
+def _popen(cmd, **kwargs) -> Popen:
     prefix = [sys.executable]
     if not is_binary():
         main_entrypoint = os.path.join(
             os.path.abspath(os.path.dirname(__file__)), "__main__.py"
         )
         prefix += [main_entrypoint]
-    return Popen(prefix + cmd, close_fds=True, shell=False, **kwargs)
+    return Popen(prefix + cmd, close_fds=True, shell=False, **kwargs)  # nosec B603
 
 
 def _spawn_windows(cmd, env):
-    from subprocess import (
-        CREATE_NEW_PROCESS_GROUP,
-        CREATE_NO_WINDOW,
-        STARTF_USESHOWWINDOW,
-        STARTUPINFO,
-    )
-
-    # https://stackoverflow.com/a/7006424
-    # https://bugs.python.org/issue41619
-    creationflags = CREATE_NEW_PROCESS_GROUP | CREATE_NO_WINDOW
+    if sys.platform == "win32":
+        from subprocess import (  # nosec B404
+            CREATE_NEW_PROCESS_GROUP,
+            CREATE_NO_WINDOW,
+            STARTF_USESHOWWINDOW,
+            STARTUPINFO,
+        )
 
-    startupinfo = STARTUPINFO()
-    startupinfo.dwFlags |= STARTF_USESHOWWINDOW
+        # https://stackoverflow.com/a/7006424
+        # https://bugs.python.org/issue41619
+        creationflags = CREATE_NEW_PROCESS_GROUP | CREATE_NO_WINDOW
 
-    _popen(cmd, env=env, creationflags=creationflags, startupinfo=startupinfo)
+        startupinfo = STARTUPINFO()
+        startupinfo.dwFlags |= STARTF_USESHOWWINDOW
+
+        popen = _popen(
+            cmd, env=env, creationflags=creationflags, startupinfo=startupinfo
+        )
+        _suppress_resource_warning(popen)
 
 
 def _spawn_posix(cmd, env):
     from dvc.cli import main
 
+    # `fork` will copy buffers, so we need to flush them before forking.
+    # Otherwise, we will get duplicated outputs.
+    if sys.stdout and not sys.stdout.closed:
+        sys.stdout.flush()
+    if sys.stderr and not sys.stderr.closed:
+        sys.stderr.flush()
+
     # NOTE: using os._exit instead of sys.exit, because dvc built
     # with PyInstaller has trouble with SystemExit exception and throws
     # errors such as "[26338] Failed to execute script __main__"
     try:
-        pid = os.fork()  # pylint: disable=no-member
+        # pylint: disable-next=no-member
+        pid = os.fork()  # type: ignore[attr-defined]
         if pid > 0:
             return
     except OSError:
         logger.exception("failed at first fork")
         os._exit(1)  # pylint: disable=protected-access
 
-    os.setsid()  # pylint: disable=no-member
+    os.setsid()  # type: ignore[attr-defined]  # pylint: disable=no-member
 
     try:
-        pid = os.fork()  # pylint: disable=no-member
+        # pylint: disable-next=no-member
+        pid = os.fork()  # type: ignore[attr-defined]
         if pid > 0:
             os._exit(0)  # pylint: disable=protected-access
     except OSError:
         logger.exception("failed at second fork")
         os._exit(1)  # pylint: disable=protected-access
 
     sys.stdin.close()
     sys.stdout.close()
     sys.stderr.close()
+    os.closerange(0, 3)
 
     if platform.system() == "Darwin":
         # workaround for MacOS bug
         # https://github.com/iterative/dvc/issues/4294
         _popen(cmd, env=env).communicate()
     else:
         os.environ.update(env)
         main(cmd)
 
     os._exit(0)  # pylint: disable=protected-access
 
 
 def _spawn(cmd, env):
-    logger.debug(f"Trying to spawn '{cmd}'")
+    logger.debug("Trying to spawn '%s'", cmd)
 
     if os.name == "nt":
         _spawn_windows(cmd, env)
     elif os.name == "posix":
         _spawn_posix(cmd, env)
     else:
         raise NotImplementedError
 
-    logger.debug(f"Spawned '{cmd}'")
+    logger.debug("Spawned '%s'", cmd)
 
 
 def daemon(args):
     """Launch a `dvc daemon` command in a detached process.
 
     Args:
         args (list): list of arguments to append to `dvc daemon` command.
     """
+    daemonize(["daemon", "-q", *args])
+
+
+def daemonize(cmd: List[str]):
     if os.environ.get(DVC_DAEMON):
         logger.debug("skipping launching a new daemon.")
         return
 
-    cmd = ["daemon", "-q"] + args
-
     env = fix_env()
-    file_path = os.path.abspath(inspect.stack()[0][1])
-    env["PYTHONPATH"] = os.path.dirname(os.path.dirname(file_path))
+    if not is_binary():
+        file_path = os.path.abspath(inspect.stack()[0][1])
+        env["PYTHONPATH"] = os.path.dirname(os.path.dirname(file_path))
     env[DVC_DAEMON] = "1"
 
     _spawn(cmd, env)
```

### Comparing `dvc-2.9.5/dvc/dagascii.py` & `dvc-3.0.0a0/dvc/dagascii.py`

 * *Files 2% similar despite different names*

```diff
@@ -55,16 +55,15 @@
         self.lines = lines
 
         self.canvas = [[" "] * cols for line in range(lines)]
 
     def draw(self):
         """Draws ASCII canvas on the screen."""
         lines = map("".join, self.canvas)
-        joined_lines = os.linesep.join(lines)
-        return joined_lines
+        return os.linesep.join(lines)
 
     def point(self, x, y, char):
         """Create a point on ASCII canvas.
 
         Args:
             x (int): x coordinate. Should be >= 0 and < number of columns in
                 the canvas.
@@ -77,15 +76,15 @@
         assert x >= 0
         assert x < self.cols
         assert y >= 0
         assert y < self.lines
 
         self.canvas[y][x] = char
 
-    def line(self, x0, y0, x1, y1, char):
+    def line(self, x0, y0, x1, y1, char):  # noqa: C901, PLR0912
         """Create a line on ASCII canvas.
 
         Args:
             x0 (int): x coordinate where the line should start.
             y0 (int): y coordinate where the line should start.
             x1 (int): x coordinate where the line should end.
             y1 (int): y coordinate where the line should end.
@@ -210,16 +209,16 @@
     Args:
         vertices (list): list of graph vertices.
         edges (list): list of graph edges.
     """
     # pylint: disable=too-many-locals
     # NOTE: coordinates might me negative, so we need to shift
     # everything to the positive plane before we actually draw it.
-    Xs = []  # pylint: disable=invalid-name
-    Ys = []  # pylint: disable=invalid-name
+    Xs = []  # noqa: N806, pylint: disable=invalid-name
+    Ys = []  # noqa: N806, pylint: disable=invalid-name
 
     sug = _build_sugiyama_layout(vertices, edges)
 
     for vertex in sug.g.sV:
         # NOTE: moving boxes w/2 to the left
         Xs.append(vertex.view.xy[0] - vertex.view.w / 2.0)
         Xs.append(vertex.view.xy[0] + vertex.view.w / 2.0)
@@ -269,12 +268,10 @@
         canvas.box(
             int(round(x - minx)),
             int(round(y - miny)),
             vertex.view.w,
             vertex.view.h,
         )
 
-        canvas.text(
-            int(round(x - minx)) + 1, int(round(y - miny)) + 1, vertex.data
-        )
+        canvas.text(int(round(x - minx)) + 1, int(round(y - miny)) + 1, vertex.data)
 
     return canvas.draw()
```

### Comparing `dvc-2.9.5/dvc/data/db/__init__.py` & `dvc-3.0.0a0/dvc/cachemgr.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,87 +1,71 @@
-from typing import TYPE_CHECKING
+import os
 
-from dvc.scheme import Schemes
+from dvc.fs import GitFileSystem, Schemes
+from dvc_data.hashfile.db import get_odb
 
-if TYPE_CHECKING:
-    from .index import ObjectDBIndexBase
 
-
-def get_odb(fs, fs_path, **config):
-    from dvc.objects.db import ObjectDB
-
-    from .local import LocalObjectDB
-
-    if fs.scheme == Schemes.LOCAL:
-        return LocalObjectDB(fs, fs_path, **config)
-
-    return ObjectDB(fs, fs_path, **config)
-
-
-def _get_odb(repo, settings):
+def _get_odb(repo, settings, fs=None):
     from dvc.fs import get_cloud_fs
 
     if not settings:
         return None
 
     cls, config, fs_path = get_cloud_fs(repo, **settings)
-    config["tmp_dir"] = repo.tmp_dir
-    return get_odb(cls(**config), fs_path, state=repo.state, **config)
+    fs = fs or cls(**config)
+    return get_odb(fs, fs_path, state=repo.state, **config)
 
 
-def get_index(odb) -> "ObjectDBIndexBase":
-    import hashlib
-
-    from .index import ObjectDBIndex, ObjectDBIndexNoop
-
-    cls = ObjectDBIndex if odb.tmp_dir else ObjectDBIndexNoop
-    return cls(
-        odb.tmp_dir,
-        hashlib.sha256(
-            odb.fs.unstrip_protocol(odb.fs_path).encode("utf-8")
-        ).hexdigest(),
-    )
-
-
-class ODBManager:
+class CacheManager:
     CACHE_DIR = "cache"
     CLOUD_SCHEMES = [
         Schemes.S3,
         Schemes.GS,
         Schemes.SSH,
         Schemes.HDFS,
         Schemes.WEBHDFS,
     ]
 
     def __init__(self, repo):
-        self.repo = repo
+        self._repo = repo
         self.config = config = repo.config["cache"]
         self._odb = {}
 
+        default = None
+        if repo and repo.local_dvc_dir:
+            default = os.path.join(repo.local_dvc_dir, self.CACHE_DIR)
+
         local = config.get("local")
 
         if local:
             settings = {"name": local}
-        elif "dir" not in config:
+        elif "dir" not in config and not default:
             settings = None
         else:
             from dvc.config_schema import LOCAL_COMMON
 
-            settings = {"url": config["dir"]}
-            for opt in LOCAL_COMMON.keys():
+            url = config.get("dir") or default
+            settings = {"url": url}
+            for opt in LOCAL_COMMON:
                 if opt in config:
                     settings[str(opt)] = config.get(opt)
 
-        self._odb[Schemes.LOCAL] = _get_odb(repo, settings)
+        kwargs = {}
+        if not isinstance(repo.fs, GitFileSystem):
+            kwargs["fs"] = repo.fs
+
+        odb = _get_odb(repo, settings, **kwargs)
+        self._odb["repo"] = odb
+        self._odb[Schemes.LOCAL] = odb
 
     def _init_odb(self, schemes):
         for scheme in schemes:
             remote = self.config.get(scheme)
             settings = {"name": remote} if remote else None
-            self._odb[scheme] = _get_odb(self.repo, settings)
+            self._odb[scheme] = _get_odb(self._repo, settings)
 
     def __getattr__(self, name):
         if name not in self._odb and name in self.CLOUD_SCHEMES:
             self._init_odb([name])
 
         try:
             return self._odb[name]
```

### Comparing `dvc-2.9.5/dvc/dependency/param.py` & `dvc-3.0.0a0/dvc/dependency/param.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,18 +1,19 @@
-import errno
 import logging
 import os
+import typing
 from collections import defaultdict
+from typing import Dict
 
-import dpath.util
+import dpath
 from voluptuous import Any
 
 from dvc.exceptions import DvcException
-from dvc.hash_info import HashInfo
-from dvc.utils.serialize import LOADERS, ParseError
+from dvc.utils.serialize import ParseError, load_path
+from dvc_data.hashfile.hash_info import HashInfo
 
 from .base import Dependency
 
 logger = logging.getLogger(__name__)
 
 
 class MissingParamsError(DvcException):
@@ -33,158 +34,148 @@
 
 class ParamsDependency(Dependency):
     PARAM_PARAMS = "params"
     PARAM_SCHEMA = {PARAM_PARAMS: Any(dict, list, None)}
     DEFAULT_PARAMS_FILE = "params.yaml"
 
     def __init__(self, stage, path, params=None, repo=None):
-        info = {}
-        self.params = params or []
-        if params:
-            if isinstance(params, list):
-                self.params = params
-            else:
-                assert isinstance(params, dict)
-                self.params = list(params.keys())
-                info = {self.PARAM_PARAMS: params}
-
-        super().__init__(
-            stage,
-            path
-            or os.path.join(stage.repo.root_dir, self.DEFAULT_PARAMS_FILE),
-            info=info,
-            repo=repo,
-        )
+        self.params = list(params) if params else []
+        hash_info = HashInfo()
+        if isinstance(params, dict):
+            hash_info = HashInfo(
+                self.PARAM_PARAMS,
+                params,  # type: ignore[arg-type]
+            )
+        repo = repo or stage.repo
+        path = path or os.path.join(repo.root_dir, self.DEFAULT_PARAMS_FILE)
+        super().__init__(stage, path, repo=repo)
+        self.hash_info = hash_info
 
-    def dumpd(self):
+    def dumpd(self, **kwargs):
         ret = super().dumpd()
         if not self.hash_info:
-            ret[self.PARAM_PARAMS] = self.params
+            ret[self.PARAM_PARAMS] = self.params or {}
         return ret
 
     def fill_values(self, values=None):
         """Load params values dynamically."""
-        if not values:
+        if values is None:
             return
+
         info = {}
+        if not self.params:
+            info.update(values)
         for param in self.params:
             if param in values:
                 info[param] = values[param]
-        self.hash_info = HashInfo(self.PARAM_PARAMS, info)
+        self.hash_info = HashInfo(
+            self.PARAM_PARAMS,
+            info,  # type: ignore[arg-type]
+        )
+
+    def read_params(
+        self, flatten: bool = True, **kwargs: typing.Any
+    ) -> Dict[str, typing.Any]:
+        try:
+            config = self.read_file()
+        except MissingParamsFile:
+            config = {}
+
+        if not self.params:
+            return config
+
+        ret = {}
+        if flatten:
+            for param in self.params:
+                try:
+                    ret[param] = dpath.get(config, param, separator=".")
+                except KeyError:
+                    continue
+            return ret
+
+        from dpath import merge
+
+        for param in self.params:
+            merge(
+                ret,
+                dpath.search(config, param, separator="."),
+                separator=".",
+            )
+        return ret
 
     def workspace_status(self):
-        status = super().workspace_status()
+        if not self.exists:
+            return {str(self): "deleted"}
+        if self.hash_info.value is None:
+            return {str(self): "new"}
 
-        if status.get(str(self)) == "deleted":
-            return status
+        from funcy import ldistinct
 
-        status = defaultdict(dict)
+        status: Dict[str, Any] = defaultdict(dict)
+        assert isinstance(self.hash_info.value, dict)
         info = self.hash_info.value if self.hash_info else {}
         actual = self.read_params()
-        for param in self.params:
-            if param not in actual.keys():
+
+        # NOTE: we want to preserve the order of params as specified in the
+        # status. In case of tracking the whole file, the order is top-level
+        # keys in the file and then the keys in the `info` from `dvc.lock`
+        # (which are alphabetically sorted).
+        params = self.params or ldistinct([*actual.keys(), *info.keys()])
+        for param in params:
+            if param not in actual:
                 st = "deleted"
             elif param not in info:
                 st = "new"
             elif actual[param] != info[param]:
+                if (
+                    isinstance(actual[param], tuple)
+                    and list(actual[param]) == info[param]
+                ):
+                    continue
                 st = "modified"
             else:
-                assert actual[param] == info[param]
                 continue
 
             status[str(self)][param] = st
 
         return status
 
     def status(self):
         return self.workspace_status()
 
     def validate_filepath(self):
         if not self.exists:
-            raise FileNotFoundError(
-                errno.ENOENT, os.strerror(errno.ENOENT), str(self)
-            )
+            raise MissingParamsFile(f"Parameters file '{self}' does not exist")
         if self.isdir():
-            raise IsADirectoryError(
-                errno.EISDIR, os.strerror(errno.EISDIR), str(self)
-            )
-
-    def read_file(self):
-        _, ext = os.path.splitext(self.fs_path)
-        loader = LOADERS[ext]
-
-        try:
-            self.validate_filepath()
-        except FileNotFoundError as exc:
-            raise MissingParamsFile(
-                f"Parameters file '{self}' does not exist"
-            ) from exc
-        except IsADirectoryError as exc:
             raise ParamsIsADirectoryError(
                 f"'{self}' is a directory, expected a parameters file"
-            ) from exc
+            )
 
+    def read_file(self):
+        self.validate_filepath()
         try:
-            return loader(self.fs_path, fs=self.repo.fs)
+            return load_path(self.fs_path, self.repo.fs)
         except ParseError as exc:
-            raise BadParamFileError(
-                f"Unable to read parameters from '{self}'"
-            ) from exc
-
-    def _read(self):
-        try:
-            return self.read_file()
-        except MissingParamsFile:
-            return {}
-
-    def read_params_d(self, **kwargs):
-        config = self._read()
-
-        ret = {}
-        for param in self.params:
-            dpath.util.merge(
-                ret,
-                dpath.util.search(config, param, separator="."),
-                separator=".",
-            )
-        return ret
-
-    def read_params(self):
-        config = self._read()
-
-        ret = {}
-        for param in self.params:
-            try:
-                ret[param] = dpath.util.get(config, param, separator=".")
-            except KeyError:
-                pass
-        return ret
+            raise BadParamFileError(f"Unable to read parameters from '{self}'") from exc
 
     def get_hash(self):
         info = self.read_params()
 
         missing_params = set(self.params) - set(info.keys())
         if missing_params:
             raise MissingParamsError(
                 "Parameters '{}' are missing from '{}'.".format(
                     ", ".join(missing_params), self
                 )
             )
 
-        return HashInfo(self.PARAM_PARAMS, info)
+        return HashInfo(self.PARAM_PARAMS, info)  # type: ignore[arg-type]
 
     def save(self):
         if not self.exists:
             raise self.DoesNotExistError(self)
 
-        if not self.isfile and not self.isdir:
+        if not self.isfile() and not self.isdir():
             raise self.IsNotFileOrDirError(self)
 
-        if self.is_empty:
-            logger.warning(f"'{self}' is empty.")
-
         self.ignore()
-
-        if self.metric or self.plot:
-            self.verify_metric()
-
         self.hash_info = self.get_hash()
```

### Comparing `dvc-2.9.5/dvc/dvcfile.py` & `dvc-3.0.0a0/dvc/dvcfile.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,35 +1,48 @@
 import contextlib
 import logging
 import os
-from typing import TYPE_CHECKING, Any, Callable, Tuple, TypeVar, Union
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Callable,
+    Dict,
+    List,
+    Optional,
+    Tuple,
+    TypeVar,
+    Union,
+)
 
 from dvc.exceptions import DvcException
-from dvc.parsing.versions import LOCKFILE_VERSION, SCHEMA_KWD
 from dvc.stage import serialize
 from dvc.stage.exceptions import (
     StageFileBadNameError,
     StageFileDoesNotExistError,
     StageFileIsNotDvcFileError,
 )
-from dvc.types import AnyPath
 from dvc.utils import relpath
 from dvc.utils.collections import apply_diff
+from dvc.utils.objects import cached_property
 from dvc.utils.serialize import dump_yaml, modify_yaml
 
 if TYPE_CHECKING:
     from dvc.repo import Repo
+    from dvc.types import StrOrBytesPath
+
+    from .parsing import DataResolver
+    from .stage import Stage
 
 logger = logging.getLogger(__name__)
 _T = TypeVar("_T")
 
 DVC_FILE = "Dvcfile"
 DVC_FILE_SUFFIX = ".dvc"
-PIPELINE_FILE = "dvc.yaml"
-PIPELINE_LOCK = "dvc.lock"
+PROJECT_FILE = "dvc.yaml"
+LOCK_FILE = "dvc.lock"
 
 
 class FileIsGitIgnored(DvcException):
     def __init__(self, path, pipeline_file=False):
         super().__init__(
             "{}'{}' is git-ignored.".format(
                 "bad DVC file name " if pipeline_file else "", path
@@ -40,46 +53,42 @@
 class ParametrizedDumpError(DvcException):
     pass
 
 
 def is_valid_filename(path):
     return path.endswith(DVC_FILE_SUFFIX) or os.path.basename(path) in [
         DVC_FILE,
-        PIPELINE_FILE,
+        PROJECT_FILE,
     ]
 
 
 def is_dvc_file(path):
-    return os.path.isfile(path) and (
-        is_valid_filename(path) or is_lock_file(path)
-    )
+    return os.path.isfile(path) and (is_valid_filename(path) or is_lock_file(path))
 
 
 def is_lock_file(path):
-    return os.path.basename(path) == PIPELINE_LOCK
+    return os.path.basename(path) == LOCK_FILE
 
 
 def is_git_ignored(repo, path):
-    from dvc.fs.local import LocalFileSystem
+    from dvc.fs import LocalFileSystem
     from dvc.scm import NoSCMError
 
     try:
-        return isinstance(repo.fs, LocalFileSystem) and repo.scm.is_ignored(
-            path
-        )
+        return isinstance(repo.fs, LocalFileSystem) and repo.scm.is_ignored(path)
     except NoSCMError:
         return False
 
 
 def check_dvcfile_path(repo, path):
     if not is_valid_filename(path):
         raise StageFileBadNameError(
             "bad DVC file name '{}'. DVC files should be named "
             "'{}' or have a '.dvc' suffix (e.g. '{}.dvc').".format(
-                relpath(path), PIPELINE_FILE, os.path.basename(path)
+                relpath(path), PROJECT_FILE, os.path.basename(path)
             )
         )
 
     if is_git_ignored(repo, path):
         raise FileIsGitIgnored(relpath(path), True)
 
 
@@ -134,167 +143,204 @@
         # it raises the proper exceptions by priority:
         # 1. when the file doesn't exists
         # 2. filename is not a DVC file
         # 3. path doesn't represent a regular file
         # 4. when the file is git ignored
         if not self.exists():
             dvc_ignored = self.repo.dvcignore.is_ignored_file(self.path)
-            raise StageFileDoesNotExistError(
-                self.path, dvc_ignored=dvc_ignored
-            )
+            raise StageFileDoesNotExistError(self.path, dvc_ignored=dvc_ignored)
 
         self._verify_filename()
         if not self.repo.fs.isfile(self.path):
             raise StageFileIsNotDvcFileError(self.path)
 
         self._check_gitignored()
         return self._load_yaml(**kwargs)
 
     @classmethod
-    def validate(cls, d: _T, fname: str = None) -> _T:
+    def validate(cls, d: _T, fname: Optional[str] = None) -> _T:
         from dvc.utils.strictyaml import validate
 
         return validate(d, cls.SCHEMA, path=fname)  # type: ignore[arg-type]
 
     def _load_yaml(self, **kwargs: Any) -> Tuple[Any, str]:
         from dvc.utils import strictyaml
 
         return strictyaml.load(
             self.path,
             self.SCHEMA,  # type: ignore[arg-type]
             self.repo.fs,
             **kwargs,
         )
 
-    def remove(self, force=False):  # pylint: disable=unused-argument
+    # pylint: disable-next=unused-argument
+    def remove(self, force=False):  # noqa: ARG002
         with contextlib.suppress(FileNotFoundError):
             os.unlink(self.path)
 
     def dump(self, stage, **kwargs):
         raise NotImplementedError
 
-    def merge(self, ancestor, other):
+    def merge(self, ancestor, other, allowed=None):
         raise NotImplementedError
 
 
 class SingleStageFile(FileMixin):
     from dvc.schema import COMPILED_SINGLE_STAGE_SCHEMA as SCHEMA
-    from dvc.stage.loader import SingleStageLoader as LOADER
+    from dvc.stage.loader import SingleStageLoader as LOADER  # noqa: N814
+
+    metrics: List[str] = []
+    plots: Any = {}
+    params: List[str] = []
+    artifacts: Dict[str, Optional[Dict[str, Any]]] = {}
 
     @property
-    def stage(self):
+    def stage(self) -> "Stage":
         data, raw = self._load()
         return self.LOADER.load_stage(self, data, raw)
 
     @property
-    def stages(self):
+    def stages(self) -> LOADER:
         data, raw = self._load()
         return self.LOADER(self, data, raw)
 
-    def dump(self, stage, **kwargs):
+    def dump(self, stage, **kwargs) -> None:
         """Dumps given stage appropriately in the dvcfile."""
         from dvc.stage import PipelineStage
 
         assert not isinstance(stage, PipelineStage)
         if self.verify:
             check_dvcfile_path(self.repo, self.path)
-        logger.debug(f"Saving information to '{relpath(self.path)}'.")
-        dump_yaml(self.path, serialize.to_single_stage_file(stage))
+        logger.debug("Saving information to '%s'.", relpath(self.path))
+        dump_yaml(self.path, serialize.to_single_stage_file(stage, **kwargs))
         self.repo.scm_context.track_file(self.relpath)
 
-    def remove_stage(self, stage):  # pylint: disable=unused-argument
+    # pylint: disable-next=unused-argument
+    def remove_stage(self, stage):  # noqa: ARG002
         self.remove()
 
-    def merge(self, ancestor, other):
+    def merge(self, ancestor, other, allowed=None):
         assert isinstance(ancestor, SingleStageFile)
         assert isinstance(other, SingleStageFile)
 
         stage = self.stage
-        stage.merge(ancestor.stage, other.stage)
+        stage.merge(ancestor.stage, other.stage, allowed=allowed)
         self.dump(stage)
 
 
-class PipelineFile(FileMixin):
+class ProjectFile(FileMixin):
     """Abstraction for pipelines file, .yaml + .lock combined."""
 
     from dvc.schema import COMPILED_MULTI_STAGE_SCHEMA as SCHEMA
-    from dvc.stage.loader import StageLoader as LOADER
+    from dvc.stage.loader import StageLoader as LOADER  # noqa: N814
 
     @property
     def _lockfile(self):
         return Lockfile(self.repo, os.path.splitext(self.path)[0] + ".lock")
 
+    def _reset(self):
+        self.__dict__.pop("contents", None)
+        self.__dict__.pop("lockfile_contents", None)
+        self.__dict__.pop("resolver", None)
+        self.__dict__.pop("stages", None)
+
     def dump(
         self, stage, update_pipeline=True, update_lock=True, **kwargs
     ):  # pylint: disable=arguments-differ
         """Dumps given stage appropriately in the dvcfile."""
         from dvc.stage import PipelineStage
 
         assert isinstance(stage, PipelineStage)
         if self.verify:
             check_dvcfile_path(self.repo, self.path)
 
         if update_pipeline and not stage.is_data_source:
             self._dump_pipeline_file(stage)
 
         if update_lock:
-            self._dump_lockfile(stage)
+            self._dump_lockfile(stage, **kwargs)
 
-    def _dump_lockfile(self, stage):
-        self._lockfile.dump(stage)
+    def _dump_lockfile(self, stage, **kwargs):
+        self._lockfile.dump(stage, **kwargs)
 
     @staticmethod
-    def _check_if_parametrized(stage):
+    def _check_if_parametrized(stage, action: str = "dump") -> None:
         if stage.raw_data.parametrized:
-            raise ParametrizedDumpError(f"cannot dump a parametrized {stage}")
+            raise ParametrizedDumpError(f"cannot {action} a parametrized {stage}")
 
     def _dump_pipeline_file(self, stage):
         self._check_if_parametrized(stage)
         stage_data = serialize.to_pipeline_file(stage)
 
         with modify_yaml(self.path, fs=self.repo.fs) as data:
             if not data:
                 logger.info("Creating '%s'", self.relpath)
 
             data["stages"] = data.get("stages", {})
             existing_entry = stage.name in data["stages"]
             action = "Modifying" if existing_entry else "Adding"
-            logger.info(
-                "%s stage '%s' in '%s'", action, stage.name, self.relpath
-            )
+            logger.info("%s stage '%s' in '%s'", action, stage.name, self.relpath)
 
             if existing_entry:
                 orig_stage_data = data["stages"][stage.name]
                 apply_diff(stage_data[stage.name], orig_stage_data)
             else:
                 data["stages"].update(stage_data)
 
         self.repo.scm_context.track_file(self.relpath)
 
     @property
     def stage(self):
-        raise DvcException(
-            "PipelineFile has multiple stages. Please specify it's name."
-        )
+        raise DvcException("ProjectFile has multiple stages. Please specify it's name.")
+
+    @cached_property
+    def contents(self) -> Dict[str, Any]:
+        return self._load()[0]
+
+    @cached_property
+    def lockfile_contents(self) -> Dict[str, Any]:
+        return self._lockfile.load()
+
+    @cached_property
+    def resolver(self) -> "DataResolver":
+        from .parsing import DataResolver
+
+        wdir = self.repo.fs.path.parent(self.path)
+        return DataResolver(self.repo, wdir, self.contents)
+
+    @cached_property
+    def stages(self) -> LOADER:
+        return self.LOADER(self, self.contents, self.lockfile_contents)
 
     @property
-    def stages(self):
-        data, _ = self._load()
-        lockfile_data = self._lockfile.load()
-        return self.LOADER(self, data, lockfile_data)
+    def metrics(self) -> List[str]:
+        return self.contents.get("metrics", [])
+
+    @property
+    def plots(self) -> Any:
+        return self.contents.get("plots", {})
+
+    @property
+    def params(self) -> List[str]:
+        return self.contents.get("params", [])
+
+    @property
+    def artifacts(self) -> Dict[str, Optional[Dict[str, Any]]]:
+        return self.contents.get("artifacts", {})
 
     def remove(self, force=False):
         if not force:
             logger.warning("Cannot remove pipeline file.")
             return
 
         super().remove()
         self._lockfile.remove()
 
     def remove_stage(self, stage):
+        self._check_if_parametrized(stage, "remove")
         self._lockfile.remove_stage(stage)
         if not self.exists():
             return
 
         d, _ = self._load_yaml(round_trip=True)
         if stage.name not in d.get("stages", {}):
             return
@@ -303,84 +349,41 @@
         del d["stages"][stage.name]
 
         if d["stages"]:
             dump_yaml(self.path, d)
         else:
             super().remove()
 
-    def merge(self, ancestor, other):
+    def merge(self, ancestor, other, allowed=None):
         raise NotImplementedError
 
 
-def get_lockfile_schema(d):
-    from dvc.schema import (
-        COMPILED_LOCKFILE_V1_SCHEMA,
-        COMPILED_LOCKFILE_V2_SCHEMA,
-    )
-
-    schema = {
-        LOCKFILE_VERSION.V1: COMPILED_LOCKFILE_V1_SCHEMA,
-        LOCKFILE_VERSION.V2: COMPILED_LOCKFILE_V2_SCHEMA,
-    }
-
-    version = LOCKFILE_VERSION.from_dict(d)
-    return schema[version]
-
-
-def migrate_lock_v1_to_v2(d, version_info):
-    stages = {k: v for k, v in d.items()}
-
-    for key in stages:
-        d.pop(key)
-
-    # forcing order, meta should always be at the top
-    d.update(version_info)
-    d["stages"] = stages
-
-
-def lockfile_schema(data: _T) -> _T:
-    schema = get_lockfile_schema(data)
-    return schema(data)
-
-
 class Lockfile(FileMixin):
-    SCHEMA = staticmethod(lockfile_schema)  # type: ignore[assignment]
+    from dvc.schema import COMPILED_LOCKFILE_SCHEMA as SCHEMA
 
     def _verify_filename(self):
         pass  # lockfile path is hardcoded, so no need to verify here
 
     def _load(self, **kwargs: Any):
         try:
             return super()._load(**kwargs)
         except StageFileDoesNotExistError:
             # we still need to account for git-ignored dvc.lock file
             # even though it may not exist or have been .dvcignored
             self._check_gitignored()
             return {}, ""
 
-    @property
-    def latest_version_info(self):
-        version = LOCKFILE_VERSION.V2.value  # pylint:disable=no-member
-        return {SCHEMA_KWD: version}
-
     def dump(self, stage, **kwargs):
-        stage_data = serialize.to_lockfile(stage)
+        stage_data = serialize.to_lockfile(stage, **kwargs)
 
         with modify_yaml(self.path, fs=self.repo.fs) as data:
-            version = LOCKFILE_VERSION.from_dict(data)
-            if version == LOCKFILE_VERSION.V1:
-                logger.info(
-                    "Migrating lock file '%s' from v1 to v2", self.relpath
-                )
-                migrate_lock_v1_to_v2(data, self.latest_version_info)
-            else:
-                if not data:
-                    data.update(self.latest_version_info)
-                    # order is important, meta should always be at the top
-                    logger.info("Generating lock file '%s'", self.relpath)
+            if not data:
+                data.update({"schema": "2.0"})
+                # order is important, meta should always be at the top
+                logger.info("Generating lock file '%s'", self.relpath)
 
             data["stages"] = data.get("stages", {})
             modified = data["stages"].get(stage.name, {}) != stage_data.get(
                 stage.name, {}
             )
             if modified:
                 logger.info("Updating lock file '%s'", self.relpath)
@@ -391,41 +394,30 @@
             self.repo.scm_context.track_file(self.relpath)
 
     def remove_stage(self, stage):
         if not self.exists():
             return
 
         d, _ = self._load_yaml(round_trip=True)
-        version = LOCKFILE_VERSION.from_dict(d)
-        data = d if version == LOCKFILE_VERSION.V1 else d.get("stages", {})
+        data = d.get("stages", {})
         if stage.name not in data:
             return
 
         logger.debug("Removing '%s' from '%s'", stage.name, self.path)
         del data[stage.name]
 
         if data:
             dump_yaml(self.path, d)
         else:
             self.remove()
 
-    def merge(self, ancestor, other):
+    def merge(self, ancestor, other, allowed=None):
         raise NotImplementedError
 
 
-class Dvcfile:
-    def __new__(cls, repo: "Repo", path: AnyPath, **kwargs: Any):
-        assert path
-        assert repo
-
-        return make_dvcfile(repo, path, **kwargs)
-
-
-DVCFile = Union["PipelineFile", "SingleStageFile"]
-
-
-def make_dvcfile(repo: "Repo", path: AnyPath, **kwargs: Any) -> DVCFile:
-    _, ext = os.path.splitext(str(path))
-    if ext in [".yaml", ".yml"]:
-        return PipelineFile(repo, path, **kwargs)
-    # fallback to single stage file for better error messages
+def load_file(
+    repo: "Repo", path: "StrOrBytesPath", **kwargs: Any
+) -> Union[ProjectFile, SingleStageFile]:
+    _, ext = os.path.splitext(path)
+    if ext in (".yaml", ".yml"):
+        return ProjectFile(repo, path, **kwargs)
     return SingleStageFile(repo, path, **kwargs)
```

### Comparing `dvc-2.9.5/dvc/exceptions.py` & `dvc-3.0.0a0/dvc/exceptions.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,12 @@
 """Exceptions raised by the dvc."""
+import errno
+from typing import Dict, List
+
+from dvc.utils import format_link
 
 
 class DvcException(Exception):
     """Base class for all dvc exceptions."""
 
     def __init__(self, msg, *args):
         assert msg
@@ -24,22 +28,27 @@
     """
 
     def __init__(self, output, stages):
         from funcy import first
 
         assert isinstance(output, str)
         assert all(hasattr(stage, "relpath") for stage in stages)
+        msg = ""
+        stage_names = [s.addressing for s in stages]
+        stages_str = " ".join(stage_names)
         if len(stages) == 1:
-            msg = "output '{}' is already specified in {}.".format(
-                output, first(stages)
-            )
+            stage_name = first(stages)
+            msg = f"output '{output}' is already specified in {stage_name}."
         else:
             msg = "output '{}' is already specified in stages:\n{}".format(
-                output, "\n".join(f"\t- {s.addressing}" for s in stages)
+                output, "\n".join(f"\t- {s}" for s in stage_names)
             )
+        msg += (
+            f"\nUse `dvc remove {stages_str}` to stop tracking the overlapping output."
+        )
         super().__init__(msg)
         self.stages = stages
         self.output = output
 
 
 class OutputNotFoundError(DvcException):
     """Thrown if a file/directory is not found as an output in any pipeline.
@@ -127,25 +136,33 @@
 class NotDvcRepoError(DvcException):
     """Thrown if a directory is not a DVC repo"""
 
 
 class CyclicGraphError(DvcException):
     def __init__(self, stages):
         assert isinstance(stages, list)
-        msg = "Pipeline has a cycle involving: {}.".format(
-            ", ".join(s.addressing for s in stages)
+        stage_part = "stage" if len(stages) == 1 else "stages"
+        msg = (
+            "Same item(s) are defined as both a dependency and an output "
+            "in {stage_part}: {stage}."
+        )
+        super().__init__(
+            msg.format(
+                stage_part=stage_part,
+                stage=", ".join(s.addressing for s in stages),
+            )
         )
-        super().__init__(msg)
 
 
 class ConfirmRemoveError(DvcException):
     def __init__(self, path):
         super().__init__(
-            "unable to remove '{}' without a confirmation. Use "
-            "`-f` to force.".format(path)
+            "unable to remove '{}' without a confirmation. Use `-f` to force.".format(
+                path
+            )
         )
 
 
 class InitError(DvcException):
     pass
 
 
@@ -163,17 +180,15 @@
                 paths=", ".join(f"'{path}'" for path in paths)
             )
         )
 
 
 class RecursiveAddingWhileUsingFilename(DvcException):
     def __init__(self):
-        super().__init__(
-            "cannot use `fname` with multiple targets or `-R|--recursive`"
-        )
+        super().__init__("cannot use `fname` with multiple targets or `-R|--recursive`")
 
 
 class OverlappingOutputPathsError(DvcException):
     def __init__(self, parent, overlapping_out, message):
         self.parent = parent
         self.overlapping_out = overlapping_out
         super().__init__(message)
@@ -188,29 +203,31 @@
     def __init__(self, etag, cached_etag):
         super().__init__(
             "ETag mismatch detected when copying file to cache! "
             "(expected: '{}', actual: '{}')".format(etag, cached_etag)
         )
 
 
-class FileMissingError(DvcException):
+class FileExistsLocallyError(FileExistsError, DvcException):
     def __init__(self, path, hint=None):
+        import os.path
+
         self.path = path
         hint = "" if hint is None else f". {hint}"
-        super().__init__(
-            f"Can't find '{path}' neither locally nor on remote{hint}"
-        )
+        path_typ = "directory" if os.path.isdir(path) else "file"
+        msg = f"The {path_typ} '{path}' already exists locally{hint}"
+        super().__init__(msg)
+        self.errno = errno.EEXIST
 
 
-class DvcIgnoreInCollectedDirError(DvcException):
-    def __init__(self, ignore_dirname):
-        super().__init__(
-            ".dvcignore file should not be in collected dir path: "
-            "'{}'".format(ignore_dirname)
-        )
+class FileMissingError(DvcException):
+    def __init__(self, path, hint=None):
+        self.path = path
+        hint = "" if hint is None else f". {hint}"
+        super().__init__(f"Can't find '{path}' neither locally nor on remote{hint}")
 
 
 class FileTransferError(DvcException):
     _METHOD = "transfer"
 
     def __init__(self, amount):
         self.amount = amount
@@ -223,15 +240,15 @@
 
 
 class UploadError(FileTransferError):
     _METHOD = "upload"
 
 
 class CheckoutError(DvcException):
-    def __init__(self, target_infos, stats=None):
+    def __init__(self, target_infos: List[str], stats: Dict[str, List[str]]):
         from dvc.utils import error_link
 
         self.target_infos = target_infos
         self.stats = stats
         targets = [str(t) for t in target_infos]
         m = (
             "Checkout failed for following targets:\n{}\nIs your "
@@ -244,17 +261,15 @@
 
 class CollectCacheError(DvcException):
     pass
 
 
 class NoRemoteInExternalRepoError(DvcException):
     def __init__(self, url):
-        super().__init__(
-            f"No DVC remote is specified in target repository '{url}'."
-        )
+        super().__init__(f"No DVC remote is specified in target repository '{url}'.")
 
 
 class NoOutputInExternalRepoError(DvcException):
     def __init__(self, path, external_repo_path, external_repo_url):
         from dvc.utils import relpath
 
         super().__init__(
@@ -271,28 +286,30 @@
 
 class PathMissingError(DvcException):
     default_msg = (
         "The path '{}' does not exist in the target repository '{}'"
         " neither as a DVC output nor as a Git-tracked file."
     )
     default_msg_dvc_only = (
-        "The path '{}' does not exist in the target repository '{}'"
-        " as an DVC output."
+        "The path '{}' does not exist in the target repository '{}' as an DVC output."
     )
 
     def __init__(self, path, repo, dvc_only=False):
         msg = self.default_msg if not dvc_only else self.default_msg_dvc_only
         super().__init__(msg.format(path, repo))
         self.dvc_only = dvc_only
 
 
+class URLMissingError(DvcException):
+    def __init__(self, url):
+        super().__init__(f"The path '{url}' does not exist")
+
+
 class RemoteCacheRequiredError(DvcException):
     def __init__(self, scheme, fs_path):
-        from dvc.utils import format_link
-
         super().__init__(
             (
                 "Current operation was unsuccessful because '{}' requires "
                 "existing cache on '{}' remote. See {} for information on how "
                 "to set up remote cache."
             ).format(
                 fs_path,
@@ -309,44 +326,31 @@
 class NoOutputOrStageError(DvcException):
     """
     Raised when the target is neither an output nor a stage name in dvc.yaml
     """
 
     def __init__(self, target, file):
         super().__init__(
-            f"'{target}' "
-            f"does not exist as an output or a stage name in '{file}'"
+            f"'{target}' does not exist as an output or a stage name in '{file}'"
         )
 
 
 class MergeError(DvcException):
     pass
 
 
 class CacheLinkError(DvcException):
-    from dvc.utils import format_link
-
     SUPPORT_LINK = "See {} for more information.".format(
-        format_link(
-            "https://dvc.org/doc/user-guide/troubleshooting#cache-types"
-        )
+        format_link("https://dvc.org/doc/user-guide/troubleshooting#cache-types")
     )
 
     def __init__(self, fs_paths):
         msg = "No possible cache link types for '{}'. {}".format(
             ", ".join(fs_paths), self.SUPPORT_LINK
         )
         super().__init__(msg)
         self.fs_paths = fs_paths
 
 
-class CircularImportError(DvcException):
-    def __init__(self, dep, a, b):
-        super().__init__(
-            f"'{dep}' contains invalid circular import. "
-            f"DVC repo '{a}' already imports from '{b}'."
-        )
-
-
 class PrettyDvcException(DvcException):
     def __pretty_exc__(self, **kwargs):
         """Print prettier exception message."""
```

### Comparing `dvc-2.9.5/dvc/external_repo.py` & `dvc-3.0.0a0/dvc/repo/open_repo.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,111 +1,102 @@
 import logging
 import os
 import tempfile
 import threading
 from contextlib import contextmanager
-from typing import Dict
+from typing import TYPE_CHECKING, Dict, Iterator, Optional, Tuple
 
 from funcy import retry, wrap_with
 
-from dvc.exceptions import (
-    FileMissingError,
-    NoOutputInExternalRepoError,
-    NoRemoteInExternalRepoError,
-    NotDvcRepoError,
-    OutputNotFoundError,
-    PathMissingError,
-)
+from dvc.exceptions import NotDvcRepoError
 from dvc.repo import Repo
 from dvc.scm import CloneError, map_scm_exception
 from dvc.utils import relpath
 
+if TYPE_CHECKING:
+    from dvc.scm import Git
+
 logger = logging.getLogger(__name__)
 
 
 @contextmanager
 @map_scm_exception()
-def external_repo(
-    url, rev=None, for_write=False, cache_dir=None, cache_types=None, **kwargs
-):
-    from scmrepo.git import Git
-
-    from dvc.config import NoRemoteError
-    from dvc.fs.git import GitFileSystem
-
+def _external_repo(
+    url,
+    rev: Optional[str] = None,
+    for_write: bool = False,
+    **kwargs,
+) -> Iterator["Repo"]:
     logger.debug("Creating external repo %s@%s", url, rev)
     path = _cached_clone(url, rev, for_write=for_write)
     # Local HEAD points to the tip of whatever branch we first cloned from
     # (which may not be the default branch), use origin/HEAD here to get
     # the tip of the default branch
     rev = rev or "refs/remotes/origin/HEAD"
 
-    cache_config = {
-        "cache": {"dir": cache_dir or _get_cache_dir(url), "type": cache_types}
-    }
-
     config = _get_remote_config(url) if os.path.isdir(url) else {}
-    config.update(cache_config)
+    config.update({"cache": {"dir": _get_cache_dir(url)}})
+    config.update(kwargs.pop("config", None) or {})
 
+    main_root = "/"
     if for_write:
-        root_dir = path
-        fs = None
-    else:
-        root_dir = os.path.realpath(path)
-        scm = Git(root_dir)
-        fs = GitFileSystem(scm=scm, rev=rev)
+        # we already checked out needed revision
+        rev = None
+        main_root = path
 
     repo_kwargs = dict(
-        root_dir=root_dir,
+        root_dir=path,
         url=url,
-        fs=fs,
         config=config,
-        repo_factory=erepo_factory(url, cache_config),
+        repo_factory=erepo_factory(url, main_root, {"cache": config["cache"]}),
+        rev=rev,
         **kwargs,
     )
 
-    if "subrepos" not in repo_kwargs:
-        repo_kwargs["subrepos"] = True
-
-    if "uninitialized" not in repo_kwargs:
-        repo_kwargs["uninitialized"] = True
-
     repo = Repo(**repo_kwargs)
 
     try:
         yield repo
-    except NoRemoteError as exc:
-        raise NoRemoteInExternalRepoError(url) from exc
-    except OutputNotFoundError as exc:
-        if exc.repo is repo:
-            raise NoOutputInExternalRepoError(
-                exc.output, repo.root_dir, url
-            ) from exc
-        raise
-    except FileMissingError as exc:
-        raise PathMissingError(exc.path, url) from exc
     finally:
         repo.close()
         if for_write:
             _remove(path)
 
 
-def erepo_factory(url, cache_config, *args, **kwargs):
-    def make_repo(path, **_kwargs):
+def open_repo(url, *args, **kwargs):
+    if url is None:
+        url = os.getcwd()
+
+    if os.path.exists(url):
+        try:
+            config = _get_remote_config(url)
+            config.update(kwargs.get("config") or {})
+            kwargs["config"] = config
+            return Repo(url, *args, **kwargs)
+        except NotDvcRepoError:
+            pass  # fallthrough to _external_repo
+
+    return _external_repo(url, *args, **kwargs)
+
+
+def erepo_factory(url, root_dir, cache_config):
+    from dvc.fs import localfs
+
+    def make_repo(path, fs=None, **_kwargs):
         _config = cache_config.copy()
         if os.path.isdir(url):
-            rel = os.path.relpath(path, _kwargs["fs"].fs_args["scm"].root_dir)
-            repo_path = os.path.join(url, rel)
+            fs = fs or localfs
+            repo_path = os.path.join(url, *fs.path.relparts(path, root_dir))
             _config.update(_get_remote_config(repo_path))
-        return Repo(path, config=_config, **_kwargs)
+        return Repo(path, fs=fs, config=_config, **_kwargs)
 
     return make_repo
 
 
-CLONES: Dict[str, str] = {}
+CLONES: Dict[str, Tuple[str, bool]] = {}
 CACHE_DIRS: Dict[str, str] = {}
 
 
 @wrap_with(threading.Lock())
 def _get_cache_dir(url):
     try:
         cache_dir = CACHE_DIRS[url]
@@ -134,15 +125,15 @@
         name = repo.config["core"].get("remote")
         if not name:
             # Fill the empty upstream entry with a new remote pointing to the
             # original repo's cache location.
             name = "auto-generated-upstream"
             return {
                 "core": {"remote": name},
-                "remote": {name: {"url": repo.odb.local.cache_dir}},
+                "remote": {name: {"url": repo.cache.local.path}},
             }
 
         # Use original remote to make sure that we are using correct url,
         # credential paths, etc if they are relative to the config location.
         return {"remote": {name: repo.config["remote"][name]}}
     finally:
         repo.close()
@@ -174,22 +165,22 @@
         _git_checkout(repo_path, rev)
     else:
         CLONES[url] = (repo_path, shallow)
     return repo_path
 
 
 @wrap_with(threading.Lock())
-def _clone_default_branch(url, rev, for_write=False):
+def _clone_default_branch(url, rev, for_write=False):  # noqa: C901, PLR0912
     """Get or create a clean clone of the url.
 
     The cloned is reactualized with git pull unless rev is a known sha.
     """
-    from scmrepo.git import Git
+    from dvc.scm import Git
 
-    clone_path, shallow = CLONES.get(url, (None, False))
+    clone_path, shallow = CLONES.get(url) or (None, False)
 
     git = None
     try:
         if clone_path:
             git = Git(clone_path)
             # Do not pull for known shas, branches and tags might move
             if not Git.is_sha(rev) or not git.has_rev(rev):
@@ -197,62 +188,72 @@
                     # If we are missing a rev in a shallow clone, fallback to
                     # a full (unshallowed) clone. Since fetching specific rev
                     # SHAs is only available in certain git versions, if we
                     # have need to reference multiple specific revs for a
                     # given repo URL it is easier/safer for us to work with
                     # full clones in this case.
                     logger.debug("erepo: unshallowing clone for '%s'", url)
-                    _unshallow(git)
+                    _pull(git, unshallow=True)
                     shallow = False
                     CLONES[url] = (clone_path, shallow)
                 else:
                     logger.debug("erepo: git pull '%s'", url)
-                    git.pull()
+                    _pull(git)
         else:
             from dvc.scm import clone
 
             logger.debug("erepo: git clone '%s' to a temporary dir", url)
             clone_path = tempfile.mkdtemp("dvc-clone")
             if not for_write and rev and not Git.is_sha(rev):
                 # If rev is a tag or branch name try shallow clone first
 
                 try:
                     git = clone(url, clone_path, shallow_branch=rev)
-                    shallow = True
-                    logger.debug(
-                        "erepo: using shallow clone for branch '%s'", rev
+                    shallow = os.path.exists(
+                        os.path.join(clone_path, Git.GIT_DIR, "shallow")
                     )
+                    if shallow:
+                        logger.debug("erepo: using shallow clone for branch '%s'", rev)
                 except CloneError:
-                    pass
+                    git_dir = os.path.join(clone_path, ".git")
+                    if os.path.exists(git_dir):
+                        _remove(git_dir)
             if not git:
                 git = clone(url, clone_path)
                 shallow = False
             CLONES[url] = (clone_path, shallow)
     finally:
         if git:
             git.close()
 
     return clone_path, shallow
 
 
-def _unshallow(git):
-    if git.gitpython.repo.head.is_detached:
-        # If this is a detached head (i.e. we shallow cloned a tag) switch to
-        # the default branch
-        origin_refs = git.gitpython.repo.remotes["origin"].refs
-        ref = origin_refs["HEAD"].reference
-        branch_name = ref.name.split("/")[-1]
-        branch = git.gitpython.repo.create_head(branch_name, ref)
-        branch.set_tracking_branch(ref)
-        branch.checkout()
-    git.pull(unshallow=True)
+def _pull(git: "Git", unshallow: bool = False):
+    from dvc.repo.experiments.utils import fetch_all_exps
+
+    git.fetch(unshallow=unshallow)
+    _merge_upstream(git)
+    fetch_all_exps(git, "origin")
+
+
+def _merge_upstream(git: "Git"):
+    from scmrepo.exceptions import SCMError
+
+    try:
+        branch = git.active_branch()
+        upstream = f"refs/remotes/origin/{branch}"
+        if git.get_ref(upstream):
+            git.merge(upstream)
+    except SCMError:
+        pass
 
 
 def _git_checkout(repo_path, rev):
-    from scmrepo.git import Git
+    from dvc.scm import Git
 
     logger.debug("erepo: git checkout %s@%s", repo_path, rev)
     git = Git(repo_path)
     try:
         git.checkout(rev)
     finally:
         git.close()
@@ -263,12 +264,10 @@
 
     if os.name == "nt":
         # git.exe may hang for a while not permitting to remove temp dir
         os_retry = retry(5, errors=OSError, timeout=0.1)
         try:
             os_retry(remove)(path)
         except PermissionError:
-            logger.warning(
-                "Failed to remove '%s'", relpath(path), exc_info=True
-            )
+            logger.warning("Failed to remove '%s'", relpath(path), exc_info=True)
     else:
         remove(path)
```

### Comparing `dvc-2.9.5/dvc/fs/fsspec_wrapper.py` & `dvc-3.0.0a0/dvc/testing/tmp_dir.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,257 +1,276 @@
+"""
+The goal of this module is making dvc functional tests setup a breeze. This
+includes a temporary dir, initializing git and DVC repos and bootstrapping some
+file structure.
+
+The cornerstone of these fixtures is `tmp_dir`, which creates a temporary dir
+and changes path to it, it might be combined with `scm` and `dvc` to initialize
+empty git and DVC repos. `tmp_dir` returns a Path instance, which should save
+you from using `open()`, `os` and `os.path` utils many times:
+
+    (tmp_dir / "some_file").write_text("some text")
+    # ...
+    assert "some text" == (tmp_dir / "some_file").read_text()
+    assert (tmp_dir / "some_file").exists()
+
+Additionally it provides `.gen()`, `.scm_gen()` and `.dvc_gen()` methods to
+bootstrap a required file structure in a single call:
+
+    # Generate a dir with files
+    tmp_dir.gen({"dir": {"file": "file text", "second_file": "..."}})
+
+    # Generate a single file, dirs will be created along the way
+    tmp_dir.gen("dir/file", "file text")
+
+    # Generate + git add
+    tmp_dir.scm_gen({"file1": "...", ...})
+
+    # Generate + git add + git commit
+    tmp_dir.scm_gen({"file1": "...", ...}, commit="add files")
+
+    # Generate + dvc add
+    tmp_dir.dvc_gen({"file1": "...", ...})
+
+    # Generate + dvc add + git commit -am "..."
+    # This commits stages to git not the generated files.
+    tmp_dir.dvc_gen({"file1": "...", ...}, commit="add files")
+
+Making it easier to bootstrap things has a supergoal of incentivizing a move
+from global repo template to creating everything inplace, which:
+
+    - makes all path references local to test, enhancing readability
+    - allows using telling filenames, e.g. "git_tracked_file" instead of "foo"
+    - does not create unnecessary files
+"""
+
+# pylint: disable=redefined-outer-name, attribute-defined-outside-init
+
 import os
-import shutil
-from typing import IO, TYPE_CHECKING, Any, Dict, Iterator, Optional, overload
+import pathlib
+import sys
+from contextlib import contextmanager
+from functools import partialmethod
 
-from funcy import cached_property
-from tqdm.utils import CallbackIOWrapper
+from dvc.utils import serialize
 
-from ._callback import DEFAULT_CALLBACK
-from .base import FileSystem
 
-FSPath = str
-AnyFSPath = str
-
-if TYPE_CHECKING:
-    from typing_extensions import Literal
-
-
-# An info() entry, might evolve to a TypedDict
-# in the future (e.g for properly type 'size' etc).
-Entry = Dict[str, Any]
-
-
-# pylint: disable=no-member
-class FSSpecWrapper(FileSystem):
-    TRAVERSE_PREFIX_LEN = 2
-
-    def __init__(self, **kwargs):
-        super().__init__(**kwargs)
-        self.fs_args = {"skip_instance_cache": True}
-        self.fs_args.update(self._prepare_credentials(**kwargs))
-
-    @staticmethod
-    def _get_kwargs_from_urls(urlpath: str) -> "Dict[str, Any]":
-        from fsspec.utils import infer_storage_options
-
-        options = infer_storage_options(urlpath)
-        options.pop("path", None)
-        options.pop("protocol", None)
-        return options
-
-    @cached_property
-    def fs(self):
-        raise NotImplementedError
-
-    def _prepare_credentials(
-        self, **config: Dict[str, Any]  # pylint: disable=unused-argument
-    ) -> Dict[str, Any]:
-        """Prepare the arguments for authentication to the
-        host filesystem"""
-        return {}
-
-    def isdir(self, path: AnyFSPath) -> bool:
-        return self.fs.isdir(path)
-
-    def isfile(self, path: AnyFSPath) -> bool:
-        return self.fs.isfile(path)
-
-    def is_empty(self, path: AnyFSPath) -> bool:
-        entry = self.info(path)
-        if entry["type"] == "directory":
-            return not self.fs.ls(path)
-        return entry["size"] == 0
-
-    def open(
-        self,
-        path: AnyFSPath,
-        mode: str = "r",
-        encoding: Optional[str] = None,
-        **kwargs,
-    ) -> "IO":  # pylint: disable=arguments-differ
-        return self.fs.open(path, mode=mode)
-
-    def checksum(self, path: AnyFSPath) -> str:
-        return self.fs.checksum(path)
-
-    def copy(self, from_info: AnyFSPath, to_info: AnyFSPath) -> None:
-        self.makedirs(self.path.parent(to_info))
-        self.fs.copy(from_info, to_info)
-
-    def exists(self, path: AnyFSPath) -> bool:
-        return self.fs.exists(path)
-
-    @overload
-    def ls(
-        self, path: AnyFSPath, detail: "Literal[True]"
-    ) -> "Iterator[Entry]":
-        ...
-
-    @overload
-    def ls(self, path: AnyFSPath, detail: "Literal[False]") -> Iterator[str]:
-        ...
-
-    def ls(self, path, detail=False):
-        yield from self.fs.ls(path, detail=detail)
-
-    def find(self, path, prefix=None):
-        yield from self.fs.find(path)
-
-    def move(self, from_info: AnyFSPath, to_info: AnyFSPath) -> None:
-        self.fs.move(from_info, to_info)
-
-    def remove(self, path: AnyFSPath) -> None:
-        self.fs.rm_file(path)
-
-    def info(self, path: AnyFSPath) -> "Entry":
-        return self.fs.info(path)
-
-    def makedirs(self, path: AnyFSPath, **kwargs) -> None:
-        self.fs.makedirs(path, exist_ok=kwargs.pop("exist_ok", True))
-
-    def put_file(
-        self,
-        from_file: AnyFSPath,
-        to_info: AnyFSPath,
-        callback: Any = DEFAULT_CALLBACK,
-        **kwargs,
-    ) -> None:
-        self.fs.put_file(from_file, to_info, callback=callback, **kwargs)
-        self.fs.invalidate_cache(self.path.parent(to_info))
-
-    def get_file(
-        self,
-        from_info: AnyFSPath,
-        to_info: AnyFSPath,
-        callback: Any = DEFAULT_CALLBACK,
-        **kwargs,
-    ) -> None:
-        self.fs.get_file(from_info, to_info, callback=callback, **kwargs)
-
-    def upload_fobj(self, fobj: IO, to_info: AnyFSPath, **kwargs) -> None:
-        self.makedirs(self.path.parent(to_info))
-        with self.open(to_info, "wb") as fdest:
-            shutil.copyfileobj(
-                fobj,
-                fdest,
-                length=getattr(fdest, "blocksize", None),  # type: ignore
-            )
+class TmpDir(pathlib.Path):
+    scheme = "local"
 
-    def walk(
-        self,
-        top: AnyFSPath,
-        topdown: bool = True,
-        **kwargs: Any,
-    ):
-        return self.fs.walk(top, topdown=topdown, **kwargs)
-
-
-# pylint: disable=abstract-method
-class ObjectFSWrapper(FSSpecWrapper):
-    TRAVERSE_PREFIX_LEN = 3
-
-    def makedirs(self, path: AnyFSPath, **kwargs) -> None:
-        # For object storages make this method a no-op. The original
-        # fs.makedirs() method will only check if the bucket exists
-        # and create if it doesn't though we don't want to support
-        # that behavior, and the check will cost some time so we'll
-        # simply ignore all mkdir()/makedirs() calls.
-        return None
-
-    def _isdir(self, path: AnyFSPath) -> bool:
-        # Directory in object storages are interpreted differently
-        # among different fsspec providers, so this logic is a temporary
-        # measure for us to adapt as of now. It checks whether it is a
-        # directory (as in a prefix with contents) or whether it is an empty
-        # file where it's name ends with a forward slash
-
-        entry = self.info(path)
-        return entry["type"] == "directory" or (
-            entry["size"] == 0
-            and entry["type"] == "file"
-            and entry["name"].endswith("/")
-        )
+    @property
+    def fs_path(self):
+        return os.fspath(self)
 
-    def isdir(self, path: AnyFSPath) -> bool:
-        try:
-            return self._isdir(path)
-        except FileNotFoundError:
-            return False
+    @property
+    def url(self):
+        return self.fs_path
 
-    def isfile(self, path: AnyFSPath) -> bool:
+    @property
+    def config(self):
+        return {"url": self.url}
+
+    def __new__(cls, *args, **kwargs):
+        if cls is TmpDir:
+            cls = (  # pylint: disable=self-cls-assignment
+                WindowsTmpDir if os.name == "nt" else PosixTmpDir
+            )
+        # init parameter and `_init` method has been removed in Python 3.10.
+        kw = {"init": False} if sys.version_info < (3, 10) else {}
+        # pylint: disable-next=unexpected-keyword-arg
+        self = cls._from_parts(args, **kw)  # type: ignore[attr-defined]
+        if not self._flavour.is_supported:
+            raise NotImplementedError(
+                f"cannot instantiate {cls.__name__!r} on your system"
+            )
+        if sys.version_info < (3, 10):
+            self._init()  # pylint: disable=no-member
+        return self
+
+    def init(self, *, scm=False, dvc=False, subdir=False):
+        from dvc.repo import Repo
+        from dvc.scm import Git
+
+        assert not scm or not hasattr(self, "scm")
+        assert not dvc or not hasattr(self, "dvc")
+
+        if scm:
+            Git.init(self.fs_path).close()
+        if dvc:
+            self.dvc = Repo.init(
+                self.fs_path,
+                no_scm=not scm and not hasattr(self, "scm"),
+                subdir=subdir,
+            )
+        if scm:
+            self.scm = self.dvc.scm if hasattr(self, "dvc") else Git(self.fs_path)
+        if dvc and hasattr(self, "scm"):
+            self.scm.commit("init dvc")
+
+    def close(self):
+        if hasattr(self, "scm"):
+            self.scm.close()
+        if hasattr(self, "dvc"):
+            self.dvc.close()
+
+    def _require(self, name):
+        if not hasattr(self, name):
+            raise TypeError(
+                "Can't use {name} for this temporary dir. "
+                'Did you forget to use "{name}" fixture?'.format(name=name)
+            )
+
+    # Bootstrapping methods
+    def gen(self, struct, text=""):
+        if isinstance(struct, (str, bytes, pathlib.PurePath)):
+            struct = {struct: text}
+
+        return self._gen(struct)
+
+    def _gen(self, struct, prefix=None):
+        paths = []
+        for name, contents in struct.items():
+            path = (prefix or self) / name
+
+            if isinstance(contents, dict):
+                if not contents:
+                    os.makedirs(path, exist_ok=True)
+                else:
+                    self._gen(contents, prefix=path)
+            else:
+                os.makedirs(path.parent, exist_ok=True)
+                if isinstance(contents, bytes):
+                    path.write_bytes(contents)
+                else:
+                    path.write_text(contents, encoding="utf-8")
+            paths.append(path)
+        return paths
+
+    def dvc_gen(self, struct, text="", commit=None):
+        paths = self.gen(struct, text)
+        return self.dvc_add(paths, commit=commit)
+
+    def scm_gen(self, struct, text="", commit=None, force=False):
+        paths = self.gen(struct, text)
+        return self.scm_add(paths, commit=commit, force=force)
+
+    def commit(self, output_paths, msg, force=False):
+        def to_gitignore(stage_path):
+            from dvc.scm import Git
+
+            return os.path.join(os.path.dirname(stage_path), Git.GITIGNORE)
+
+        gitignores = [
+            to_gitignore(s) for s in output_paths if os.path.exists(to_gitignore(s))
+        ]
+        return self.scm_add(output_paths + gitignores, commit=msg, force=force)
+
+    def dvc_add(self, filenames, commit=None):
+        self._require("dvc")
+        filenames = _coerce_filenames(filenames)
+
+        stages = self.dvc.add(filenames)
+        if commit:
+            self.commit([s.path for s in stages], msg=commit)
+        return stages
+
+    def scm_add(self, filenames, commit=None, force=False):
+        from dvc.scm import Git
+
+        self._require("scm")
+        filenames = _coerce_filenames(filenames)
+        assert isinstance(self.scm, Git)
+        self.scm.add(filenames, force=force)
+        if commit:
+            self.scm.commit(commit)
+
+    def add_remote(self, *, url=None, config=None, name="upstream", default=True):
+        self._require("dvc")
+
+        assert bool(url) ^ bool(config)
+
+        if url:
+            config = {"url": url}
+
+        with self.dvc.config.edit() as conf:
+            conf["remote"][name] = config
+            if default:
+                conf["core"]["remote"] = name
+
+        if hasattr(self, "scm"):
+            self.scm.add(self.dvc.config.files["repo"])
+            self.scm.commit(f"add '{name}' remote")
+
+        return url or config["url"]
+
+    # contexts
+    @contextmanager
+    def chdir(self):
+        old = os.getcwd()
+        try:
+            os.chdir(self)
+            yield
+        finally:
+            os.chdir(old)
+
+    @contextmanager
+    def branch(self, name, new=False):
+        self._require("scm")
+        old = self.scm.active_branch()
         try:
-            return not self._isdir(path)
-        except FileNotFoundError:
-            return False
-
-    def find(self, path, prefix=None):
-        if prefix:
-            with_prefix = self.path.parent(path)
-            files = self.fs.find(with_prefix, prefix=self.path.parts(path)[-1])
-        else:
-            with_prefix = path
-            files = self.fs.find(path)
-
-        # When calling find() on a file, it returns the same file in a list.
-        # For object-based storages, the same behavior applies to empty
-        # directories since they are represented as files. This condition
-        # checks whether we should yield an empty list (if it is an empty
-        # directory) or just yield the file itself.
-        if len(files) == 1 and files[0] == with_prefix and self.isdir(path):
-            return None
-
-        yield from files
-
-
-# pylint: disable=arguments-differ
-class NoDirectoriesMixin:
-    def isdir(self, *args, **kwargs):
-        return False
-
-    def isfile(self, *args, **kwargs):
-        return True
-
-    def find(self, *args, **kwargs):
-        raise NotImplementedError
-
-    def walk(self, *args, **kwargs):
-        raise NotImplementedError
-
-    def ls(self, *args, **kwargs):
-        raise NotImplementedError
-
-
-class CallbackMixin:
-    """Provides callback support for the filesystem that don't support yet."""
-
-    def put_file(
-        self,
-        from_file,
-        to_info,
-        callback=DEFAULT_CALLBACK,
-        **kwargs,
-    ):
-        """Add compatibility support for Callback."""
-        # pylint: disable=protected-access
-        self.makedirs(self.path.parent(to_info))
-        size = os.path.getsize(from_file)
-        with open(from_file, "rb") as fobj:
-            callback.set_size(size)
-            wrapped = CallbackIOWrapper(callback.relative_update, fobj)
-            self.upload_fobj(wrapped, to_info)
-            self.fs.invalidate_cache(self.path.parent(to_info))
-
-    def get_file(
-        self,
-        from_info,
-        to_info,
-        callback=DEFAULT_CALLBACK,
-        **kwargs,
-    ):
-        # pylint: disable=protected-access
-        total: int = self.getsize(from_info)
-        if total:
-            callback.set_size(total)
-
-        with self.open(from_info, "rb") as fobj, open(to_info, "wb") as fdest:
-            wrapped = CallbackIOWrapper(callback.relative_update, fobj)
-            shutil.copyfileobj(wrapped, fdest, length=fobj.blocksize)
+            self.scm.checkout(name, create_new=new)
+            yield
+        finally:
+            self.scm.checkout(old)
+
+    def read_text(self, *args, **kwargs):  # pylint: disable=signature-differs
+        # NOTE: on windows we'll get PermissionError instead of
+        # IsADirectoryError when we try to `open` a directory, so we can't
+        # rely on exception flow control
+        if self.is_dir():
+            return {
+                path.name: path.read_text(*args, **kwargs) for path in self.iterdir()
+            }
+        kwargs.setdefault("encoding", "utf-8")  # type: ignore[call-overload]
+        return super().read_text(*args, **kwargs)
+
+    def oid_to_path(self, hash_):
+        return str(self / hash_[0:2] / hash_[2:])
+
+    def dump(self, *args, **kwargs):
+        return serialize.DUMPERS[self.suffix](self, *args, **kwargs)
+
+    def parse(self, *args, **kwargs):
+        return serialize.LOADERS[self.suffix](self, *args, **kwargs)
+
+    def modify(self, *args, **kwargs):
+        return serialize.MODIFIERS[self.suffix](self, *args, **kwargs)
+
+    load_yaml = partialmethod(serialize.load_yaml)
+    dump_yaml = partialmethod(serialize.dump_yaml)
+    load_json = partialmethod(serialize.load_json)
+    dump_json = partialmethod(serialize.dump_json)
+    load_toml = partialmethod(serialize.load_toml)
+    dump_toml = partialmethod(serialize.dump_toml)
+
+
+def make_subrepo(dir_: TmpDir, scm, config=None):
+    dir_.mkdir(parents=True, exist_ok=True)
+    with dir_.chdir():
+        dir_.scm = scm
+        dir_.init(dvc=True, subdir=True)
+        if config:
+            dir_.add_remote(config=config)
+
+
+def _coerce_filenames(filenames):
+    if isinstance(filenames, (str, bytes, pathlib.PurePath)):
+        filenames = [filenames]
+    return list(map(os.fspath, filenames))
+
+
+class WindowsTmpDir(TmpDir, pathlib.PureWindowsPath):
+    pass
+
+
+class PosixTmpDir(TmpDir, pathlib.PurePosixPath):
+    pass
```

### Comparing `dvc-2.9.5/dvc/fs/git.py` & `dvc-3.0.0a0/dvc/fs/git.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,33 +1,31 @@
-import os
-import threading
-from typing import TYPE_CHECKING, Any
+import functools
+from typing import TYPE_CHECKING, Any, Optional
 
-from funcy import cached_property, wrap_prop
-
-from .fsspec_wrapper import FSSpecWrapper
+from . import FileSystem
 
 if TYPE_CHECKING:
     from scmrepo.fs import GitFileSystem as FsspecGitFileSystem
-    from scmrepo.git import Git
     from scmrepo.git.objects import GitTrie
 
+    from dvc.scm import Git
+
 
-class GitFileSystem(FSSpecWrapper):  # pylint:disable=abstract-method
+class GitFileSystem(FileSystem):  # pylint:disable=abstract-method
     """Proxies the repo file access methods to Git objects"""
 
-    sep = os.sep
-    scheme = "local"
+    protocol = "local"
+    PARAM_CHECKSUM = "md5"
 
     def __init__(
         self,
-        path: str = None,
-        rev: str = None,
-        scm: "Git" = None,
-        trie: "GitTrie" = None,
+        path: Optional[str] = None,
+        rev: Optional[str] = None,
+        scm: Optional["Git"] = None,
+        trie: Optional["GitTrie"] = None,
         **kwargs: Any,
     ) -> None:
         from dvc.scm import resolve_rev
 
         super().__init__()
         self.fs_args.update(
             {
@@ -36,17 +34,25 @@
                 "scm": scm,
                 "trie": trie,
                 "rev_resolver": resolve_rev,
                 **kwargs,
             }
         )
 
-    @wrap_prop(threading.Lock())
-    @cached_property
-    def fs(self) -> "FsspecGitFileSystem":
+    @functools.cached_property
+    def fs(  # pylint: disable=invalid-overridden-method
+        self,
+    ) -> "FsspecGitFileSystem":
         from scmrepo.fs import GitFileSystem as FsspecGitFileSystem
 
         return FsspecGitFileSystem(**self.fs_args)
 
+    @functools.cached_property
+    def path(self):  # pylint: disable=invalid-overridden-method
+        return self.fs.path
+
     @property
     def rev(self) -> str:
         return self.fs.rev
+
+    def ls(self, path, detail=True, **kwargs):
+        return self.fs.ls(path, detail=detail, **kwargs) or []
```

### Comparing `dvc-2.9.5/dvc/fs/hdfs.py` & `dvc-3.0.0a0/dvc/utils/threadpool.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,99 +1,75 @@
-import os
-import re
-import subprocess
-import threading
-
-from funcy import cached_property, wrap_prop
-
-from dvc.scheme import Schemes
-from dvc.utils import fix_env
-
-from .fsspec_wrapper import CallbackMixin, FSSpecWrapper
-
-CHECKSUM_REGEX = re.compile(r".*\t.*\t(?P<checksum>.*)")
-
-
-# pylint: disable=abstract-method
-class HDFSFileSystem(CallbackMixin, FSSpecWrapper):
-    scheme = Schemes.HDFS
-    REQUIRES = {"fsspec": "fsspec", "pyarrow": "pyarrow"}
-    PARAM_CHECKSUM = "checksum"
-
-    @classmethod
-    def _strip_protocol(cls, path: str) -> str:
-        from fsspec.utils import infer_storage_options
-
-        return infer_storage_options(path)["path"].lstrip("/")
-
-    def unstrip_protocol(self, path: str) -> str:
-        host = self.fs_args["host"]
-        port = self.fs_args.get("port")
-        netloc = host + (f":{port}" if port else "")
-        return "hdfs://" + netloc + "/" + path.lstrip("/")
-
-    @staticmethod
-    def _get_kwargs_from_urls(urlpath):
-        from fsspec.implementations.arrow import HadoopFileSystem
-
-        # pylint:disable=protected-access
-        return HadoopFileSystem._get_kwargs_from_urls(urlpath)
-
-    def _prepare_credentials(self, **config):
-        return config
-
-    @wrap_prop(threading.Lock())
-    @cached_property
-    def fs(self):
-        from fsspec.implementations.arrow import HadoopFileSystem
-
-        return HadoopFileSystem(**self.fs_args)
-
-    def checksum(self, path):
-        return self._checksum(path)
-
-    def _checksum(self, path, **kwargs):
-        # PyArrow doesn't natively support retrieving the
-        # checksum, so we have to use hadoop fs
-
-        url = self.unstrip_protocol(path)
-
-        result = self._run_command(
-            f"checksum {url}",
-            env=fix_env(os.environ),
-            user=self.fs_args["user"],
-        )
-        if result is None:
-            return None
-
-        match = CHECKSUM_REGEX.match(result)
-        if match is None:
-            return None
-
-        return match.group("checksum")
-
-    def _run_command(self, cmd, env=None, user=None):
-        cmd = "hadoop fs -" + cmd
-        if user:
-            cmd = f"HADOOP_USER_NAME={user} " + cmd
-
-        # NOTE: close_fds doesn't work with redirected stdin/stdout/stderr.
-        # See https://github.com/iterative/dvc/issues/1197.
-        close_fds = os.name != "nt"
-
-        executable = os.getenv("SHELL") if os.name != "nt" else None
-        p = subprocess.Popen(
-            cmd,
-            shell=True,
-            close_fds=close_fds,
-            executable=executable,
-            env=env or os.environ,
-            stdin=subprocess.PIPE,
-            stdout=subprocess.PIPE,
-            stderr=subprocess.PIPE,
-        )
-        out, err = p.communicate()
-
-        if p.returncode != 0:
-            raise subprocess.CalledProcessError(p.returncode, cmd, out, err)
+import queue
+import sys
+from concurrent import futures
+from itertools import islice
+from typing import Any, Callable, Iterable, Iterator, Optional, Set, TypeVar
+
+_T = TypeVar("_T")
+
+
+class ThreadPoolExecutor(futures.ThreadPoolExecutor):
+    _max_workers: int
+
+    def __init__(
+        self,
+        max_workers: Optional[int] = None,
+        cancel_on_error: bool = False,
+        **kwargs,
+    ):
+        super().__init__(max_workers=max_workers, **kwargs)
+        self._cancel_on_error = cancel_on_error
+
+    @property
+    def max_workers(self) -> int:
+        return self._max_workers
+
+    def imap_unordered(
+        self, fn: Callable[..., _T], *iterables: Iterable[Any]
+    ) -> Iterator[_T]:
+        """Lazier version of map that does not preserve ordering of results.
+
+        It does not create all the futures at once to reduce memory usage.
+        """
+
+        def create_taskset(n: int) -> Set[futures.Future]:
+            return {self.submit(fn, *args) for args in islice(it, n)}
+
+        it = zip(*iterables)
+        tasks = create_taskset(self.max_workers * 5)
+        while tasks:
+            done, tasks = futures.wait(tasks, return_when=futures.FIRST_COMPLETED)
+            for fut in done:
+                yield fut.result()
+            tasks.update(create_taskset(len(done)))
+
+    def shutdown(self, wait=True, *, cancel_futures=False):
+        if sys.version_info > (3, 9):  # pylint: disable=no-else-return
+            # pylint: disable=unexpected-keyword-arg
+            return super().shutdown(wait=wait, cancel_futures=cancel_futures)
+        else:
+            with self._shutdown_lock:
+                self._shutdown = True
+                if cancel_futures:
+                    # Drain all work items from the queue, and then cancel
+                    # their associated futures.
+                    while True:
+                        try:
+                            work_item = self._work_queue.get_nowait()
+                        except queue.Empty:
+                            break
+                        if work_item is not None:
+                            work_item.future.cancel()
+
+                # Send a wake-up to prevent threads calling
+                # _work_queue.get(block=True) from permanently blocking.
+                self._work_queue.put(None)  # type: ignore[arg-type]
+            if wait:
+                for t in self._threads:
+                    t.join()
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        if self._cancel_on_error:
+            self.shutdown(wait=True, cancel_futures=exc_val is not None)
         else:
-            return out.decode("utf-8")
+            self.shutdown(wait=True)
+        return False
```

### Comparing `dvc-2.9.5/dvc/fs/repo.py` & `dvc-3.0.0a0/dvc/fs/dvc.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,512 +1,422 @@
+import errno
+import functools
 import logging
+import ntpath
 import os
+import posixpath
 import threading
-from itertools import takewhile
-from typing import TYPE_CHECKING, Callable, Optional, Tuple, Type, Union
+from contextlib import suppress
+from typing import TYPE_CHECKING, Any, Callable, Dict, Optional, Tuple, Type, Union
 
-from funcy import lfilter, wrap_with
+from fsspec.spec import AbstractFileSystem
+from funcy import wrap_with
 
-from ._callback import DEFAULT_CALLBACK
-from .base import FileSystem
-from .dvc import DvcFileSystem
+from dvc_objects.fs.base import FileSystem
+from dvc_objects.fs.path import Path
+
+from .data import DataFileSystem
 
 if TYPE_CHECKING:
     from dvc.repo import Repo
+    from dvc.types import StrPath
 
 logger = logging.getLogger(__name__)
 
-RepoFactory = Union[Callable[[str], "Repo"], Type["Repo"]]
+RepoFactory = Union[Callable[..., "Repo"], Type["Repo"]]
+Key = Tuple[str, ...]
 
 
-def _wrap_walk(dvc_fs, *args, **kwargs):
-    for root, dnames, fnames in dvc_fs.walk(*args, **kwargs):
-        yield dvc_fs.path.join(dvc_fs.repo.root_dir, root), dnames, fnames
+def as_posix(path: str) -> str:
+    return path.replace(ntpath.sep, posixpath.sep)
 
 
-class RepoFileSystem(FileSystem):  # pylint:disable=abstract-method
-    """DVC + git-tracked files fs.
+# NOT the same as dvc.dvcfile.is_dvc_file()!
+def _is_dvc_file(fname):
+    from dvc.dvcfile import is_valid_filename
+    from dvc.ignore import DvcIgnore
 
-    Args:
-        repo: DVC or git repo.
-        subrepos: traverse to subrepos (by default, it ignores subrepos)
-        repo_factory: A function to initialize subrepo with, default is Repo.
-        kwargs: Additional keyword arguments passed to the `DvcFileSystem()`.
-    """
+    return is_valid_filename(fname) or fname == DvcIgnore.DVCIGNORE_FILE
 
-    sep = os.sep
 
-    scheme = "local"
-    PARAM_CHECKSUM = "md5"
-    PARAM_REPO_URL = "repo_url"
-    PARAM_REPO_ROOT = "repo_root"
-    PARAM_REV = "rev"
-    PARAM_CACHE_DIR = "cache_dir"
-    PARAM_CACHE_TYPES = "cache_types"
-    PARAM_SUBREPOS = "subrepos"
+def _merge_info(repo, fs_info, dvc_info):
+    from . import utils
+
+    ret = {"repo": repo}
+
+    if dvc_info:
+        ret["dvc_info"] = dvc_info
+        ret["type"] = dvc_info["type"]
+        ret["size"] = dvc_info["size"]
+        if not fs_info and "md5" in dvc_info:
+            ret["md5"] = dvc_info["md5"]
+
+    if fs_info:
+        ret["type"] = fs_info["type"]
+        ret["size"] = fs_info["size"]
+        isexec = False
+        if fs_info["type"] == "file":
+            isexec = utils.is_exec(fs_info["mode"])
+        ret["isexec"] = isexec
+
+    return ret
+
+
+def _get_dvc_path(dvc_fs, subkey):
+    return dvc_fs.path.join(*subkey) if subkey else ""
+
+
+class _DVCFileSystem(AbstractFileSystem):  # pylint:disable=abstract-method
+    cachable = False
+    root_marker = "/"
 
     def __init__(
         self,
+        url: Optional[str] = None,
+        rev: Optional[str] = None,
         repo: Optional["Repo"] = None,
-        subrepos=False,
-        repo_factory: RepoFactory = None,
-        **kwargs,
-    ):
-        super().__init__()
+        subrepos: bool = False,
+        repo_factory: Optional[RepoFactory] = None,
+        **repo_kwargs: Any,
+    ) -> None:
+        """DVC + git-tracked files fs.
 
-        from dvc.utils.collections import PathStringTrie
+        Args:
+            path (str, optional): URL or path to a DVC/Git repository.
+                Defaults to a DVC repository in the current working directory.
+                Both HTTP and SSH protocols are supported for remote Git repos
+                (e.g. [user@]server:project.git).
+            rev (str, optional): Any Git revision such as a branch or tag name,
+                a commit hash or a dvc experiment name.
+                Defaults to the default branch in case of remote repositories.
+                In case of a local repository, if rev is unspecified, it will
+                default to the working directory.
+                If the repo is not a Git repo, this option is ignored.
+            repo (:obj:`Repo`, optional): `Repo` instance.
+            subrepos (bool): traverse to subrepos.
+                By default, it ignores subrepos.
+            repo_factory (callable): A function to initialize subrepo with.
+                The default is `Repo`.
+
+        Examples:
+            - Opening a filesystem from repo in current working directory
+
+            >>> fs = DVCFileSystem()
+
+            - Opening a filesystem from local repository
+
+            >>> fs = DVCFileSystem("path/to/local/repository")
+
+            - Opening a remote repository
+
+            >>> fs = DVCFileSystem(
+            ...    "https://github.com/iterative/example-get-started",
+            ...    rev="main",
+            ... )
+        """
+        from pygtrie import Trie
 
+        super().__init__()
         if repo is None:
-            repo, repo_factory = self._repo_from_fs_config(
-                subrepos=subrepos, **kwargs
-            )
+            repo = self._make_repo(url=url, rev=rev, subrepos=subrepos, **repo_kwargs)
+            assert repo is not None
+            # pylint: disable=protected-access
+            repo_factory = repo._fs_conf["repo_factory"]
 
         if not repo_factory:
             from dvc.repo import Repo
 
             self.repo_factory: RepoFactory = Repo
         else:
             self.repo_factory = repo_factory
 
-        self._main_repo = repo
+        def _getcwd():
+            relparts = ()
+            assert repo is not None
+            if repo.fs.path.isin(repo.fs.path.getcwd(), repo.root_dir):
+                relparts = repo.fs.path.relparts(repo.fs.path.getcwd(), repo.root_dir)
+            return self.root_marker + self.sep.join(relparts)
+
+        self.path = Path(self.sep, getcwd=_getcwd)
+        self.repo = repo
         self.hash_jobs = repo.fs.hash_jobs
-        self.root_dir: str = repo.root_dir
         self._traverse_subrepos = subrepos
 
-        self._subrepos_trie = PathStringTrie()
+        self._subrepos_trie = Trie()
         """Keeps track of each and every path with the corresponding repo."""
 
-        self._subrepos_trie[self.root_dir] = repo
+        key = self._get_key(self.repo.root_dir)
+        self._subrepos_trie[key] = repo
 
-        self._dvcfss = {}
-        """Keep a dvcfs instance of each repo."""
+        self._datafss = {}
+        """Keep a datafs instance of each repo."""
 
         if hasattr(repo, "dvc_dir"):
-            self._dvcfss[repo.root_dir] = DvcFileSystem(repo=repo)
+            self._datafss[key] = DataFileSystem(index=repo.index.data["repo"])
 
-    @property
-    def repo_url(self):
-        if self._main_repo is None:
-            return None
-        return self._main_repo.url
+    def _get_key(self, path: "StrPath") -> Key:
+        parts = self.repo.fs.path.relparts(path, self.repo.root_dir)
+        if parts == (os.curdir,):
+            return ()
+        return parts
+
+    def _get_key_from_relative(self, path) -> Key:
+        parts = self.path.relparts(path, self.root_marker)
+        if parts and parts[0] == os.curdir:
+            return parts[1:]
+        return parts
+
+    def _from_key(self, parts: Key) -> str:
+        return self.repo.fs.path.join(self.repo.root_dir, *parts)
 
     @property
-    def config(self):
-        return {
-            self.PARAM_REPO_URL: self.repo_url,
-            self.PARAM_REPO_ROOT: self.root_dir,
-            self.PARAM_REV: getattr(self._main_repo.fs, "rev", None),
-            self.PARAM_CACHE_DIR: os.path.abspath(
-                self._main_repo.odb.local.cache_dir
-            ),
-            self.PARAM_CACHE_TYPES: self._main_repo.odb.local.cache_types,
-            self.PARAM_SUBREPOS: self._traverse_subrepos,
-        }
+    def repo_url(self):
+        return self.repo.url
 
     @classmethod
-    def _repo_from_fs_config(
-        cls, **config
-    ) -> Tuple["Repo", Optional["RepoFactory"]]:
-        from dvc.external_repo import erepo_factory, external_repo
+    def _make_repo(cls, **kwargs) -> "Repo":
         from dvc.repo import Repo
 
-        url = config.get(cls.PARAM_REPO_URL)
-        root = config.get(cls.PARAM_REPO_ROOT)
-        assert url or root
-
-        def _open(*args, **kwargs):
-            # NOTE: if original repo was an erepo (and has a URL),
-            # we cannot use Repo.open() since it will skip erepo
-            # cache/remote setup for local URLs
-            if url is None:
-                return Repo.open(*args, **kwargs)
-            return external_repo(*args, **kwargs)
-
-        cache_dir = config.get(cls.PARAM_CACHE_DIR)
-        cache_config = (
-            {}
-            if not cache_dir
-            else {
-                "cache": {
-                    "dir": cache_dir,
-                    "type": config.get(cls.PARAM_CACHE_TYPES),
-                }
-            }
-        )
-        repo_kwargs: dict = {
-            "rev": config.get(cls.PARAM_REV),
-            "subrepos": config.get(cls.PARAM_SUBREPOS, False),
-            "uninitialized": True,
-        }
-        factory: Optional["RepoFactory"] = None
-        if url is None:
-            repo_kwargs["config"] = cache_config
-        else:
-            repo_kwargs["cache_dir"] = cache_dir
-            factory = erepo_factory(url, cache_config)
-
-        with _open(
-            url if url else root,
-            **repo_kwargs,
-        ) as repo:
-            return repo, factory
+        with Repo.open(uninitialized=True, **kwargs) as repo:
+            return repo
 
-    def _get_repo(self, path: str) -> Optional["Repo"]:
+    def _get_repo(self, key: Key) -> "Repo":
         """Returns repo that the path falls in, using prefix.
 
         If the path is already tracked/collected, it just returns the repo.
 
         Otherwise, it collects the repos that might be in the path's parents
         and then returns the appropriate one.
         """
-        repo = self._subrepos_trie.get(path)
+        repo = self._subrepos_trie.get(key)
         if repo:
             return repo
 
-        prefix, repo = self._subrepos_trie.longest_prefix(path)
-        if not prefix:
-            return None
-
-        parents = (parent for parent in self.path.parents(path))
-        dirs = [path] + list(takewhile(lambda p: p != prefix, parents))
-        dirs.reverse()
-        self._update(dirs, starting_repo=repo)
-        return self._subrepos_trie.get(path)
+        prefix_key, repo = self._subrepos_trie.longest_prefix(key)
+        dir_keys = (key[:i] for i in range(len(prefix_key) + 1, len(key) + 1))
+        self._update(dir_keys, starting_repo=repo)
+        return self._subrepos_trie.get(key) or self.repo
 
     @wrap_with(threading.Lock())
-    def _update(self, dirs, starting_repo):
+    def _update(self, dir_keys, starting_repo):
         """Checks for subrepo in directories and updates them."""
         repo = starting_repo
-        for d in dirs:
+        for key in dir_keys:
+            d = self._from_key(key)
             if self._is_dvc_repo(d):
                 repo = self.repo_factory(
                     d,
-                    fs=self._main_repo.fs,
+                    fs=self.repo.fs,
+                    scm=self.repo.scm,
                     repo_factory=self.repo_factory,
                 )
-                self._dvcfss[repo.root_dir] = DvcFileSystem(repo=repo)
-            self._subrepos_trie[d] = repo
+                self._datafss[key] = DataFileSystem(index=repo.index.data["repo"])
+            self._subrepos_trie[key] = repo
 
     def _is_dvc_repo(self, dir_path):
         """Check if the directory is a dvc repo."""
         if not self._traverse_subrepos:
             return False
 
         from dvc.repo import Repo
 
-        repo_path = os.path.join(dir_path, Repo.DVC_DIR)
-        return self._main_repo.fs.isdir(repo_path)
+        repo_path = self.repo.fs.path.join(dir_path, Repo.DVC_DIR)
+        return self.repo.fs.isdir(repo_path)
 
-    def _get_fs_pair(
-        self, path
-    ) -> Tuple[FileSystem, Optional[DvcFileSystem], str]:
+    def _get_subrepo_info(
+        self, key: Key
+    ) -> Tuple["Repo", Optional[DataFileSystem], Key]:
         """
-        Returns a pair of fss based on repo the path falls in, using prefix.
+        Returns information about the subrepo the key is part of.
         """
-        path = os.path.abspath(path)
-
-        # fallback to the top-level repo if repo was not found
-        # this can happen if the path is outside of the repo
-        repo = self._get_repo(path) or self._main_repo
-
-        dvc_fs = self._dvcfss.get(repo.root_dir)
-
-        if path.startswith(repo.root_dir):
-            dvc_path = path[len(repo.root_dir) + 1 :]
+        repo = self._get_repo(key)
+        repo_key: Key
+        if repo is self.repo:
+            repo_key = ()
+            subkey = key
         else:
-            dvc_path = path
+            repo_key = self._get_key(repo.root_dir)
+            subkey = key[len(repo_key) :]
 
-        return repo.fs, dvc_fs, dvc_path
+        dvc_fs = self._datafss.get(repo_key)
+        return repo, dvc_fs, subkey
 
-    def open(
-        self, path, mode="r", encoding="utf-8", **kwargs
-    ):  # pylint: disable=arguments-renamed
-        if "b" in mode:
-            encoding = None
+    def _open(
+        self, path, mode="rb", **kwargs
+    ):  # pylint: disable=arguments-renamed, arguments-differ
+        if mode != "rb":
+            raise OSError(errno.EROFS, os.strerror(errno.EROFS))
 
-        fs, dvc_fs, dvc_path = self._get_fs_pair(path)
+        key = self._get_key_from_relative(path)
+        fs_path = self._from_key(key)
         try:
-            return fs.open(path, mode=mode, encoding=encoding)
+            return self.repo.fs.open(fs_path, mode=mode)
         except FileNotFoundError:
+            repo, dvc_fs, subkey = self._get_subrepo_info(key)
             if not dvc_fs:
                 raise
 
-        return dvc_fs.open(dvc_path, mode=mode, encoding=encoding, **kwargs)
-
-    def exists(self, path) -> bool:
-        path = os.path.abspath(path)
-
-        fs, dvc_fs, dvc_path = self._get_fs_pair(path)
-
-        if not dvc_fs:
-            return fs.exists(path)
-
-        if dvc_fs.repo.dvcignore.is_ignored(fs, path):
-            return False
-
-        if fs.exists(path):
-            return True
+        dvc_path = _get_dvc_path(dvc_fs, subkey)
+        kw = {}
+        if kwargs.get("cache_remote_stream", False):
+            kw["cache_odb"] = repo.cache.local
+        return dvc_fs.open(dvc_path, mode=mode, **kw)
+
+    def isdvc(self, path, **kwargs) -> bool:
+        """Is this entry dvc-tracked?"""
+        key = self._get_key_from_relative(path)
+        _, dvc_fs, subkey = self._get_subrepo_info(key)
+        dvc_path = _get_dvc_path(dvc_fs, subkey)
+        return dvc_fs is not None and dvc_fs.isdvc(dvc_path, **kwargs)
 
-        if not dvc_fs.exists(dvc_path):
-            return False
+    def ls(  # pylint: disable=arguments-differ # noqa: C901
+        self, path, detail=True, dvc_only=False, **kwargs
+    ):
+        key = self._get_key_from_relative(path)
+        repo, dvc_fs, subkey = self._get_subrepo_info(key)
 
-        for p in self.path.parents(path):
+        dvc_exists = False
+        dvc_infos = {}
+        if dvc_fs:
+            dvc_path = _get_dvc_path(dvc_fs, subkey)
+            with suppress(FileNotFoundError):
+                for info in dvc_fs.ls(dvc_path, detail=True):
+                    dvc_infos[dvc_fs.path.name(info["name"])] = info
+            dvc_exists = bool(dvc_infos) or dvc_fs.exists(dvc_path)
+
+        fs_exists = False
+        fs_infos = {}
+        ignore_subrepos = kwargs.get("ignore_subrepos", True)
+        if not dvc_only:
+            fs = self.repo.fs
+            fs_path = self._from_key(key)
             try:
-                if fs.info(p)["type"] != "directory":
-                    return False
-            except FileNotFoundError:
-                continue
-
-        return True
-
-    def isdir(self, path):  # pylint: disable=arguments-renamed
-        path = os.path.abspath(path)
-
-        fs, dvc_fs, dvc_path = self._get_fs_pair(path)
-
-        if dvc_fs and dvc_fs.repo.dvcignore.is_ignored_dir(path):
-            return False
-
-        try:
-            info = fs.info(path)
-            return info["type"] == "directory"
-        except (OSError, ValueError):
-            # from CPython's os.path.isdir()
-            pass
-
-        if not dvc_fs:
-            return False
-
-        try:
-            info = dvc_fs.info(dvc_path)
-        except FileNotFoundError:
-            return False
+                for info in repo.dvcignore.ls(
+                    fs, fs_path, detail=True, ignore_subrepos=ignore_subrepos
+                ):
+                    fs_infos[fs.path.name(info["name"])] = info
+            except (FileNotFoundError, NotADirectoryError):
+                pass
+
+            fs_exists = bool(fs_infos) or fs.exists(fs_path)
+
+        dvcfiles = kwargs.get("dvcfiles", False)
+
+        infos = []
+        paths = []
+        names = set(dvc_infos.keys()) | set(fs_infos.keys())
+
+        if not names and (dvc_exists or fs_exists):
+            # broken symlink or TreeError
+            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path)
 
-        for p in self.path.parents(path):
-            try:
-                if fs.info(p)["type"] != "directory":
-                    return False
-            except FileNotFoundError:
+        for name in names:
+            if not dvcfiles and _is_dvc_file(name):
                 continue
 
-        return info["type"] == "directory"
-
-    def isdvc(self, path, **kwargs):
-        _, dvc_fs, dvc_path = self._get_fs_pair(path)
-        return dvc_fs is not None and dvc_fs.isdvc(dvc_path, **kwargs)
-
-    def isfile(self, path):  # pylint: disable=arguments-renamed
-        path = os.path.abspath(path)
+            entry_path = self.path.join(path, name)
+            info = _merge_info(repo, fs_infos.get(name), dvc_infos.get(name))
+            info["name"] = entry_path
+            infos.append(info)
+            paths.append(entry_path)
+
+        if not detail:
+            return paths
+
+        return infos
+
+    def info(self, path, **kwargs):
+        key = self._get_key_from_relative(path)
+        ignore_subrepos = kwargs.get("ignore_subrepos", True)
+        return self._info(key, path, ignore_subrepos=ignore_subrepos)
 
-        fs, dvc_fs, dvc_path = self._get_fs_pair(path)
-
-        if dvc_fs and dvc_fs.repo.dvcignore.is_ignored_file(path):
-            return False
-
-        try:
-            info = fs.info(path)
-            return info["type"] == "file"
-        except (OSError, ValueError):
-            # from CPython's os.path.isfile()
-            pass
-
-        if not dvc_fs:
-            return False
-
-        try:
-            info = dvc_fs.info(dvc_path)
-        except FileNotFoundError:
-            return False
+    def _info(  # noqa: C901, PLR0912
+        self, key, path, ignore_subrepos=True, check_ignored=True
+    ):
+        repo, dvc_fs, subkey = self._get_subrepo_info(key)
 
-        for p in self.path.parents(path):
+        dvc_info = None
+        if dvc_fs:
             try:
-                if fs.info(p)["type"] != "directory":
-                    return False
+                dvc_info = dvc_fs.fs.index.info(subkey)
+                dvc_path = _get_dvc_path(dvc_fs, subkey)
+                dvc_info["name"] = dvc_path
             except FileNotFoundError:
-                continue
-
-        return info["type"] == "file"
+                pass
 
-    def _dvc_walk(self, walk):
-        try:
-            root, dirs, files = next(walk)
-        except StopIteration:
-            return
-        yield root, dirs, files
-        for _ in dirs:
-            yield from self._dvc_walk(walk)
-
-    def _subrepo_walk(self, dir_path, **kwargs):
-        """Walk into a new repo.
-
-        NOTE: subrepo will only be discovered when walking if
-        ignore_subrepos is set to False.
-        """
-        fs, dvc_fs, dvc_path = self._get_fs_pair(dir_path)
-        fs_walk = fs.walk(dir_path, topdown=True)
-        if dvc_fs:
-            dvc_walk = _wrap_walk(dvc_fs, dvc_path, topdown=True, **kwargs)
-        else:
-            dvc_walk = None
-        yield from self._walk(fs_walk, dvc_walk, **kwargs)
-
-    def _walk(self, repo_walk, dvc_walk=None, dvcfiles=False):
-        from dvc.dvcfile import is_valid_filename
-        from dvc.ignore import DvcIgnore
-
-        assert repo_walk
-        try:
-            _, dvc_dirs, dvc_fnames = (
-                next(dvc_walk) if dvc_walk else (None, [], [])
-            )
-            repo_root, repo_dirs, repo_fnames = next(repo_walk)
-        except StopIteration:
-            return
-
-        # separate subdirs into shared dirs, dvc-only dirs, repo-only dirs
-        dvc_set = set(dvc_dirs)
-        repo_set = set(repo_dirs)
-        dvc_only = list(dvc_set - repo_set)
-        repo_only = list(repo_set - dvc_set)
-        shared = list(dvc_set & repo_set)
-        dirs = shared + dvc_only + repo_only
-
-        def _func(fname):
-            if dvcfiles:
-                return True
-
-            return not (
-                is_valid_filename(fname) or fname == DvcIgnore.DVCIGNORE_FILE
-            )
-
-        # merge file lists
-        files = set(filter(_func, dvc_fnames + repo_fnames))
-
-        yield repo_root, dirs, list(files)
-
-        def is_dvc_repo(d):
-            return self._is_dvc_repo(os.path.join(repo_root, d))
-
-        # remove subrepos to prevent it from being traversed
-        subrepos = set(filter(is_dvc_repo, repo_only))
-        # set dir order for next recursion level - shared dirs first so that
-        # next() for both generators recurses into the same shared directory
-        dvc_dirs[:] = [dirname for dirname in dirs if dirname in dvc_set]
-        repo_dirs[:] = lfilter(lambda d: d in (repo_set - subrepos), dirs)
-
-        for dirname in dirs:
-            if dirname in subrepos:
-                dir_path = os.path.join(repo_root, dirname)
-                yield from self._subrepo_walk(dir_path, dvcfiles=dvcfiles)
-            elif dirname in shared:
-                yield from self._walk(repo_walk, dvc_walk, dvcfiles=dvcfiles)
-            elif dirname in dvc_set:
-                yield from self._dvc_walk(dvc_walk)
-            elif dirname in repo_set:
-                yield from self._walk(repo_walk, None, dvcfiles=dvcfiles)
-
-    def walk(self, top, topdown=True, **kwargs):
-        """Walk and merge both DVC and repo fss.
-
-        Args:
-            top: path to walk from
-            topdown: if True, fs will be walked from top down.
-            dvcfiles: if True, dvcfiles will be included in the files list
-                for walked directories.
-
-        Any kwargs will be passed into methods used for fetching and/or
-        streaming DVC outs from remotes.
-        """
-        assert topdown
-
-        if not self.exists(top):
-            return
-
-        if not self.isdir(top):
-            return
-
-        repo = self._get_repo(os.path.abspath(top))
-        dvcfiles = kwargs.pop("dvcfiles", False)
-
-        fs, dvc_fs, dvc_path = self._get_fs_pair(top)
-        repo_exists = fs.exists(top)
-
-        repo_walk = repo.dvcignore.walk(fs, top, topdown=topdown, **kwargs)
-
-        if not dvc_fs or (repo_exists and dvc_fs.isdvc(dvc_path)):
-            yield from self._walk(repo_walk, None, dvcfiles=dvcfiles)
-            return
-
-        if not repo_exists:
-            yield from _wrap_walk(dvc_fs, dvc_path, topdown=topdown, **kwargs)
-
-        dvc_walk = None
-        if dvc_fs.exists(dvc_path):
-            dvc_walk = _wrap_walk(dvc_fs, dvc_path, topdown=topdown, **kwargs)
-
-        yield from self._walk(repo_walk, dvc_walk, dvcfiles=dvcfiles)
-
-    def find(self, path, prefix=None):
-        for root, _, files in self.walk(path):
-            for fname in files:
-                yield self.path.join(root, fname)
+        fs_info = None
+        fs = self.repo.fs
+        fs_path = self._from_key(key)
+        try:
+            fs_info = fs.info(fs_path)
+            if check_ignored and repo.dvcignore.is_ignored(
+                fs, fs_path, ignore_subrepos=ignore_subrepos
+            ):
+                fs_info = None
+        except (FileNotFoundError, NotADirectoryError):
+            if not dvc_info:
+                raise
 
-    def get_file(
-        self, from_info, to_file, callback=DEFAULT_CALLBACK, **kwargs
-    ):
-        fs, dvc_fs, dvc_path = self._get_fs_pair(from_info)
+        # NOTE: if some parent in fs_path turns out to be a file, it means
+        # that the whole repofs branch doesn't exist.
+        if dvc_info and not fs_info:
+            for parent in fs.path.parents(fs_path):
+                try:
+                    if fs.info(parent)["type"] != "directory":
+                        dvc_info = None
+                        break
+                except FileNotFoundError:
+                    continue
+
+        if not dvc_info and not fs_info:
+            raise FileNotFoundError
+
+        info = _merge_info(repo, fs_info, dvc_info)
+        info["name"] = path
+        return info
+
+    def get_file(self, rpath, lpath, **kwargs):  # pylint: disable=arguments-differ
+        key = self._get_key_from_relative(rpath)
+        fs_path = self._from_key(key)
         try:
-            fs.get_file(  # pylint: disable=protected-access
-                from_info, to_file, callback=callback, **kwargs
-            )
-            return
+            return self.repo.fs.get_file(fs_path, lpath, **kwargs)
         except FileNotFoundError:
+            _, dvc_fs, subkey = self._get_subrepo_info(key)
             if not dvc_fs:
                 raise
 
-        dvc_fs.get_file(  # pylint: disable=protected-access
-            dvc_path, to_file, callback=callback, **kwargs
-        )
+        dvc_path = _get_dvc_path(dvc_fs, subkey)
+        return dvc_fs.get_file(dvc_path, lpath, **kwargs)
 
-    def info(self, path):
-        fs, dvc_fs, dvc_path = self._get_fs_pair(path)
 
-        try:
-            dvc_info = dvc_fs.info(dvc_path)
-        except FileNotFoundError:
-            dvc_info = None
+class DVCFileSystem(FileSystem):
+    protocol = "local"
+    PARAM_CHECKSUM = "md5"
 
-        try:
-            from dvc.utils import is_exec
+    def _prepare_credentials(self, **config) -> Dict[str, Any]:
+        return config
 
-            fs_info = fs.info(path)
-            fs_info["repo"] = dvc_fs.repo
-            fs_info["isout"] = (
-                dvc_info.get("isout", False) if dvc_info else False
-            )
-            fs_info["outs"] = dvc_info["outs"] if dvc_info else None
-            fs_info["isdvc"] = dvc_info["isdvc"] if dvc_info else False
-            fs_info["meta"] = dvc_info.get("meta") if dvc_info else None
-
-            isexec = False
-            if dvc_info:
-                isexec = dvc_info["isexec"]
-            elif fs_info["type"] == "file":
-                isexec = is_exec(fs_info["mode"])
-            fs_info["isexec"] = isexec
-            return fs_info
+    @functools.cached_property
+    # pylint: disable-next=invalid-overridden-method
+    def fs(self) -> "DVCFileSystem":
+        return _DVCFileSystem(**self.fs_args)
 
-        except FileNotFoundError:
-            if not dvc_info:
-                raise
+    def isdvc(self, path, **kwargs) -> bool:
+        return self.fs.isdvc(path, **kwargs)
 
-            dvc_info["repo"] = dvc_fs.repo
-            dvc_info["isdvc"] = True
-            return dvc_info
+    @property
+    def path(self) -> Path:  # pylint: disable=invalid-overridden-method
+        return self.fs.path
 
-    def checksum(self, path):
-        fs, dvc_fs, dvc_path = self._get_fs_pair(path)
+    @property
+    def repo(self) -> "Repo":
+        return self.fs.repo
 
-        try:
-            return fs.checksum(path)
-        except FileNotFoundError:
-            return dvc_fs.checksum(dvc_path)
+    @property
+    def repo_url(self) -> str:
+        return self.fs.repo_url
+
+    def from_os_path(self, path: str) -> str:
+        if os.path.isabs(path):
+            path = os.path.relpath(path, self.repo.root_dir)
+
+        return as_posix(path)
```

### Comparing `dvc-2.9.5/dvc/ignore.py` & `dvc-3.0.0a0/dvc/ignore.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,94 +1,94 @@
 import logging
 import os
 import re
 from collections import namedtuple
-from itertools import groupby, takewhile
+from itertools import chain, groupby, takewhile
+from typing import TYPE_CHECKING, List, Optional
 
 from pathspec.patterns import GitWildMatchPattern
 from pathspec.util import normalize_file
+from pygtrie import Trie
 
-from dvc.fs.base import FileSystem
-from dvc.fs.local import localfs
+from dvc.fs import Schemes, localfs
 from dvc.pathspec_math import PatternInfo, merge_patterns
-from dvc.scheme import Schemes
-from dvc.types import AnyPath, List, Optional
-from dvc.utils import relpath
-from dvc.utils.collections import PathStringTrie
+
+if TYPE_CHECKING:
+    from dvc.fs import AnyFSPath, FileSystem
 
 logger = logging.getLogger(__name__)
 
 
 class DvcIgnore:
     DVCIGNORE_FILE = ".dvcignore"
 
     def __call__(self, root, dirs, files):
         raise NotImplementedError
 
 
 class DvcIgnorePatterns(DvcIgnore):
-    def __init__(self, pattern_list, dirname):
-        if pattern_list:
-            if isinstance(pattern_list[0], str):
-                pattern_list = [
-                    PatternInfo(pattern, "") for pattern in pattern_list
-                ]
+    def __init__(self, pattern_list, dirname, sep):
+        from pathspec.patterns.gitwildmatch import _DIR_MARK
+
+        if pattern_list and isinstance(pattern_list[0], str):
+            pattern_list = [PatternInfo(pattern, "") for pattern in pattern_list]
 
+        self.sep = sep
         self.pattern_list = pattern_list
         self.dirname = dirname
-        self.prefix = self.dirname + os.sep
 
-        self.regex_pattern_list = [
-            GitWildMatchPattern.pattern_to_regex(pattern_info.patterns)
-            for pattern_info in pattern_list
-        ]
+        self.regex_pattern_list = []
+        for count, pattern in enumerate(pattern_list):
+            pattern, group = GitWildMatchPattern.pattern_to_regex(pattern.patterns)
+            if pattern:
+                pattern = pattern.replace(f"<{_DIR_MARK}>", f"<{_DIR_MARK}{count}>")
+                self.regex_pattern_list.append((pattern, group))
 
         self.ignore_spec = [
             (ignore, re.compile("|".join(item[0] for item in group)))
-            for ignore, group in groupby(
-                self.regex_pattern_list, lambda x: x[1]
-            )
+            for ignore, group in groupby(self.regex_pattern_list, lambda x: x[1])
             if ignore is not None
         ]
 
     @classmethod
     def from_file(cls, path, fs, name):
-        assert os.path.isabs(path)
-        dirname = os.path.normpath(os.path.dirname(path))
+        assert fs.path.isabs(path)
+        dirname = fs.path.normpath(fs.path.dirname(path))
         with fs.open(path, encoding="utf-8") as fobj:
             path_spec_lines = [
                 PatternInfo(line, f"{name}:{line_no + 1}:{line}")
-                for line_no, line in enumerate(
-                    map(str.strip, fobj.readlines())
-                )
+                for line_no, line in enumerate(map(str.strip, fobj.readlines()))
                 if line and not (line.strip().startswith("#"))
             ]
 
-        return cls(path_spec_lines, dirname)
+        return cls(path_spec_lines, dirname, fs.sep)
 
     def __call__(self, root: List[str], dirs: List[str], files: List[str]):
         files = [f for f in files if not self.matches(root, f)]
         dirs = [d for d in dirs if not self.matches(root, d, True)]
 
         return dirs, files
 
     def _get_normalize_path(self, dirname, basename):
         # NOTE: `relpath` is too slow, so we have to assume that both
         # `dirname` and `self.dirname` are relative or absolute together.
+
+        prefix = self.dirname.rstrip(self.sep) + self.sep
+
         if dirname == self.dirname:
             path = basename
-        elif dirname.startswith(self.prefix):
-            rel = dirname[len(self.prefix) :]
+        elif dirname.startswith(prefix):
+            rel = dirname[len(prefix) :]
             # NOTE: `os.path.join` is ~x5.5 slower
-            path = f"{rel}{os.sep}{basename}"
+            path = f"{rel}{self.sep}{basename}"
         else:
             return False
 
         if os.name == "nt":
-            path = normalize_file(path)
+            return normalize_file(path)
         return path
 
     def matches(self, dirname, basename, is_dir=False, details: bool = False):
         path = self._get_normalize_path(dirname, basename)
         if not path:
             return False
 
@@ -144,17 +144,15 @@
             == [pattern.patterns for pattern in other.pattern_list]
         )
 
     def __bool__(self):
         return bool(self.pattern_list)
 
 
-CheckIgnoreResult = namedtuple(
-    "CheckIgnoreResult", ["file", "match", "patterns"]
-)
+CheckIgnoreResult = namedtuple("CheckIgnoreResult", ["file", "match", "patterns"])
 
 
 def _no_match(path):
     return CheckIgnoreResult(path, False, ["::"])
 
 
 class DvcIgnoreFilter:
@@ -166,226 +164,264 @@
             ".git/",
             ".git",
             f"{Repo.DVC_DIR}/",
         ]
 
         self.fs = fs
         self.root_dir = root_dir
-        self.ignores_trie_fs = PathStringTrie()
-        self._ignores_trie_subrepos = PathStringTrie()
-        self.ignores_trie_fs[root_dir] = DvcIgnorePatterns(
-            default_ignore_patterns, root_dir
+        self.ignores_trie_fs = Trie()
+        self._ignores_trie_subrepos = Trie()
+
+        key = self._get_key(root_dir)
+        self.ignores_trie_fs[key] = DvcIgnorePatterns(
+            default_ignore_patterns,
+            root_dir,
+            fs.sep,
         )
-        self._ignores_trie_subrepos[root_dir] = self.ignores_trie_fs[root_dir]
+        self._ignores_trie_subrepos[key] = self.ignores_trie_fs[key]
         self._update(
             self.root_dir,
             self._ignores_trie_subrepos,
             dnames=None,
             ignore_subrepos=False,
         )
         self._update(
             self.root_dir,
             self.ignores_trie_fs,
             dnames=None,
             ignore_subrepos=True,
         )
 
-    def _update_trie(self, dirname: str, trie: PathStringTrie) -> None:
-        old_pattern = trie.longest_prefix(dirname).value
+    def _get_key(self, path):
+        parts = self.fs.path.relparts(path, self.root_dir)
+        if parts == (os.curdir,):
+            return ()
+        return parts
+
+    def _update_trie(self, dirname: str, trie: Trie) -> None:
+        key = self._get_key(dirname)
+        old_pattern = trie.longest_prefix(key).value
         matches = old_pattern.matches(dirname, DvcIgnore.DVCIGNORE_FILE, False)
 
-        path = os.path.join(dirname, DvcIgnore.DVCIGNORE_FILE)
+        path = self.fs.path.join(dirname, DvcIgnore.DVCIGNORE_FILE)
         if not matches and self.fs.exists(path):
-            name = os.path.relpath(path, self.root_dir)
+            name = self.fs.path.relpath(path, self.root_dir)
             new_pattern = DvcIgnorePatterns.from_file(path, self.fs, name)
             if old_pattern:
-                trie[dirname] = DvcIgnorePatterns(
-                    *merge_patterns(
-                        old_pattern.pattern_list,
-                        old_pattern.dirname,
-                        new_pattern.pattern_list,
-                        new_pattern.dirname,
-                    )
+                plist, prefix = merge_patterns(
+                    self.fs.path.flavour,
+                    old_pattern.pattern_list,
+                    old_pattern.dirname,
+                    new_pattern.pattern_list,
+                    new_pattern.dirname,
                 )
+                trie[key] = DvcIgnorePatterns(plist, prefix, self.fs.sep)
             else:
-                trie[dirname] = new_pattern
+                trie[key] = new_pattern
         elif old_pattern:
-            trie[dirname] = old_pattern
+            trie[key] = old_pattern
 
     def _update(
         self,
         dirname: str,
-        ignore_trie: PathStringTrie,
+        ignore_trie: Trie,
         dnames: Optional["List"],
         ignore_subrepos: bool,
     ) -> None:
         self._update_trie(dirname, ignore_trie)
 
         if ignore_subrepos:
             if dnames is None:
                 try:
                     _, dnames, _ = next(self.fs.walk(dirname))
                 except StopIteration:
                     dnames = []
 
             for dname in dnames:
-                self._update_sub_repo(
-                    os.path.join(dirname, dname), ignore_trie
-                )
+                self._update_sub_repo(self.fs.path.join(dirname, dname), ignore_trie)
 
-    def _update_sub_repo(self, path, ignore_trie: PathStringTrie):
+    def _update_sub_repo(self, path, ignore_trie: Trie):
         from dvc.repo import Repo
 
         if path == self.root_dir:
             return
 
-        dvc_dir = os.path.join(path, Repo.DVC_DIR)
-        if not os.path.exists(dvc_dir):
+        dvc_dir = self.fs.path.join(path, Repo.DVC_DIR)
+        if not self.fs.exists(dvc_dir):
             return
 
-        root, dname = os.path.split(path)
+        root, dname = self.fs.path.split(path)
+        key = self._get_key(root)
         pattern_info = PatternInfo(f"/{dname}/", f"in sub_repo:{dname}")
-        new_pattern = DvcIgnorePatterns([pattern_info], root)
-        old_pattern = ignore_trie.longest_prefix(root).value
+        new_pattern = DvcIgnorePatterns([pattern_info], root, self.fs.sep)
+        old_pattern = ignore_trie.longest_prefix(key).value
         if old_pattern:
-            ignore_trie[root] = DvcIgnorePatterns(
-                *merge_patterns(
-                    old_pattern.pattern_list,
-                    old_pattern.dirname,
-                    new_pattern.pattern_list,
-                    new_pattern.dirname,
-                )
+            plist, prefix = merge_patterns(
+                self.fs.path.flavour,
+                old_pattern.pattern_list,
+                old_pattern.dirname,
+                new_pattern.pattern_list,
+                new_pattern.dirname,
             )
+            ignore_trie[key] = DvcIgnorePatterns(plist, prefix, self.fs.sep)
         else:
-            ignore_trie[root] = new_pattern
+            ignore_trie[key] = new_pattern
 
     def __call__(self, root, dirs, files, ignore_subrepos=True):
-        abs_root = os.path.abspath(root)
+        abs_root = self.fs.path.abspath(root)
         ignore_pattern = self._get_trie_pattern(
             abs_root, dnames=dirs, ignore_subrepos=ignore_subrepos
         )
         if ignore_pattern:
             dirs, files = ignore_pattern(abs_root, dirs, files)
         return dirs, files
 
-    def walk(self, fs: FileSystem, path: AnyPath, **kwargs):
+    def ls(self, fs, path, detail=True, **kwargs):
+        fs_dict = {}
+        dirs = []
+        nondirs = []
+
+        for entry in fs.ls(path, detail=True, **kwargs):
+            name = fs.path.name(entry["name"])
+            fs_dict[name] = entry
+            if entry["type"] == "directory":
+                dirs.append(name)
+            else:
+                nondirs.append(name)
+
+        dirs, nondirs = self(path, dirs, nondirs, **kwargs)
+
+        if not detail:
+            return dirs + nondirs
+
+        return [fs_dict[name] for name in chain(dirs, nondirs)]
+
+    def walk(self, fs: "FileSystem", path: "AnyFSPath", **kwargs):
+        detail = kwargs.get("detail", False)
         ignore_subrepos = kwargs.pop("ignore_subrepos", True)
-        if fs.scheme == Schemes.LOCAL:
+        if fs.protocol == Schemes.LOCAL:
             for root, dirs, files in fs.walk(path, **kwargs):
-                dirs[:], files[:] = self(
-                    root, dirs, files, ignore_subrepos=ignore_subrepos
-                )
+                if detail:
+                    all_dnames = set(dirs.keys())
+                    all_fnames = set(files.keys())
+                    dnames, fnames = self(
+                        root,
+                        all_dnames,
+                        all_fnames,
+                        ignore_subrepos=ignore_subrepos,
+                    )
+                    list(map(dirs.pop, all_dnames - set(dnames)))
+                    list(map(files.pop, all_fnames - set(fnames)))
+                else:
+                    dirs[:], files[:] = self(
+                        root, dirs, files, ignore_subrepos=ignore_subrepos
+                    )
                 yield root, dirs, files
         else:
             yield from fs.walk(path, **kwargs)
 
-    def find(self, fs: FileSystem, path: AnyPath, **kwargs):
-        if fs.scheme == Schemes.LOCAL:
+    def find(self, fs: "FileSystem", path: "AnyFSPath", **kwargs):
+        if fs.protocol == Schemes.LOCAL:
             for root, _, files in self.walk(fs, path, **kwargs):
                 for file in files:
                     # NOTE: os.path.join is ~5.5 times slower
-                    yield f"{root}{os.sep}{file}"
+                    yield f"{root}{fs.sep}{file}"
         else:
             yield from fs.find(path)
 
     def _get_trie_pattern(
         self, dirname, dnames: Optional["List"] = None, ignore_subrepos=True
     ) -> Optional["DvcIgnorePatterns"]:
         if ignore_subrepos:
             ignores_trie = self.ignores_trie_fs
         else:
             ignores_trie = self._ignores_trie_subrepos
 
-        ignore_pattern = ignores_trie.get(dirname)
+        if not self.fs.path.isin_or_eq(dirname, self.root_dir):
+            # outside of the repo
+            return None
+
+        key = self._get_key(dirname)
+
+        ignore_pattern = ignores_trie.get(key)
         if ignore_pattern:
             return ignore_pattern
 
-        prefix = ignores_trie.longest_prefix(dirname).key
-        if not prefix:
-            # outside of the repo
-            return None
+        prefix_key = ignores_trie.longest_prefix(key).key or ()
+        prefix = self.fs.path.join(self.root_dir, *prefix_key)
 
         dirs = list(
             takewhile(
                 lambda path: path != prefix,
                 (parent for parent in localfs.path.parents(dirname)),
             )
         )
         dirs.reverse()
         dirs.append(dirname)
 
         for parent in dirs:
             self._update(parent, ignores_trie, dnames, ignore_subrepos)
 
-        return ignores_trie.get(dirname)
+        return ignores_trie.get(key)
 
     def _is_ignored(
         self, path: str, is_dir: bool = False, ignore_subrepos: bool = True
     ):
         if self._outside_repo(path):
             return False
-        dirname, basename = os.path.split(os.path.normpath(path))
+        dirname, basename = self.fs.path.split(self.fs.path.normpath(path))
         ignore_pattern = self._get_trie_pattern(dirname, None, ignore_subrepos)
         if ignore_pattern:
             return ignore_pattern.matches(dirname, basename, is_dir)
         return False
 
     def is_ignored_dir(self, path: str, ignore_subrepos: bool = True) -> bool:
-        "Only used in LocalFileSystem"
-        path = os.path.abspath(path)
+        # only used in LocalFileSystem
+        path = self.fs.path.abspath(path)
         if path == self.root_dir:
             return False
 
         return self._is_ignored(path, True, ignore_subrepos=ignore_subrepos)
 
-    def is_ignored_file(self, path: str) -> bool:
-        "Only used in LocalFileSystem"
-        path = os.path.abspath(path)
-        return self._is_ignored(path, False)
+    def is_ignored_file(self, path: str, ignore_subrepos: bool = True) -> bool:
+        # only used in LocalFileSystem
+        path = self.fs.path.abspath(path)
+        return self._is_ignored(path, False, ignore_subrepos=ignore_subrepos)
 
     def _outside_repo(self, path):
-        # paths outside of the repo should be ignored
-        path = relpath(path, self.root_dir)
-        if path.startswith("..") or (
-            os.name == "nt"
-            and not os.path.commonprefix(
-                [os.path.abspath(path), self.root_dir]
-            )
-        ):
-            return True
-        return False
+        return not self.fs.path.isin_or_eq(path, self.root_dir)
 
     def check_ignore(self, target):
         # NOTE: can only be used in `dvc check-ignore`, see
         # https://github.com/iterative/dvc/issues/5046
-        full_target = os.path.abspath(target)
+        full_target = self.fs.path.abspath(target)
         if not self._outside_repo(full_target):
-            dirname, basename = os.path.split(os.path.normpath(full_target))
+            dirname, basename = self.fs.path.split(self.fs.path.normpath(full_target))
             pattern = self._get_trie_pattern(dirname)
             if pattern:
                 matches = pattern.matches(
-                    dirname, basename, os.path.isdir(full_target), True
+                    dirname, basename, self.fs.isdir(full_target), True
                 )
 
                 if matches:
                     return CheckIgnoreResult(target, True, matches)
         return _no_match(target)
 
     def is_ignored(
-        self, fs: FileSystem, path: str, ignore_subrepos: bool = True
+        self, fs: "FileSystem", path: str, ignore_subrepos: bool = True
     ) -> bool:
         # NOTE: can't use self.check_ignore(path).match for now, see
         # https://github.com/iterative/dvc/issues/4555
-        if fs.scheme != Schemes.LOCAL:
+        if fs.protocol != Schemes.LOCAL:
             return False
         if fs.isfile(path):
-            return self.is_ignored_file(path)
+            return self.is_ignored_file(path, ignore_subrepos)
         if fs.isdir(path):
             return self.is_ignored_dir(path, ignore_subrepos)
-        return self.is_ignored_file(path) or self.is_ignored_dir(
+        return self.is_ignored_file(path, ignore_subrepos) or self.is_ignored_dir(
             path, ignore_subrepos
         )
 
 
 def init(path):
     dvcignore = os.path.join(path, DvcIgnore.DVCIGNORE_FILE)
     if os.path.exists(dvcignore):
```

### Comparing `dvc-2.9.5/dvc/info.py` & `dvc-3.0.0a0/dvc/info.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,127 +1,161 @@
+import importlib.metadata as importlib_metadata
 import itertools
 import os
 import pathlib
 import platform
 
 import psutil
 
 from dvc import __version__
 from dvc.exceptions import NotDvcRepoError
-from dvc.fs import FS_MAP, get_fs_cls, get_fs_config
-from dvc.fs.utils import test_links
+from dvc.fs import Schemes, generic, get_fs_cls, get_fs_config, registry
 from dvc.repo import Repo
 from dvc.scm import SCMError
 from dvc.utils import error_link
 from dvc.utils.pkg import PKG
 
-try:
-    import importlib.metadata as importlib_metadata
-except ImportError:  # < 3.8
-    import importlib_metadata  # type: ignore[no-redef]
-
-
-package = "" if PKG is None else f"({PKG})"
+SUBPROJECTS = (
+    "dvc_data",
+    "dvc_objects",
+    "dvc_render",
+    "dvc_task",
+    "scmrepo",
+)
+package = "" if PKG is None else f" ({PKG})"
 
 
 def get_dvc_info():
+    dvc_version = f"DVC version: {__version__}{package}"
     info = [
-        f"DVC version: {__version__} {package}",
-        "---------------------------------",
-        f"Platform: Python {platform.python_version()} on "
-        f"{platform.platform()}",
+        dvc_version,
+        "-" * len(dvc_version),
+        f"Platform: Python {platform.python_version()} on {platform.platform()}",
+        f"Subprojects:{_get_subprojects()}",
         f"Supports:{_get_supported_remotes()}",
+        f"Config:{_get_config_dirs()}",
     ]
 
     try:
         with Repo() as repo:
             # cache_dir might not exist yet (e.g. after `dvc init`), and we
             # can't auto-create it, as it might cause issues if the user
             # later decides to enable shared cache mode with
             # `dvc config cache.shared group`.
-            if os.path.exists(repo.odb.local.cache_dir):
+            if os.path.exists(repo.cache.local.path):
                 info.append(f"Cache types: {_get_linktype_support_info(repo)}")
-                fs_type = get_fs_type(repo.odb.local.cache_dir)
+                fs_type = _get_fs_type(repo.cache.local.path)
                 info.append(f"Cache directory: {fs_type}")
             else:
                 info.append("Cache types: " + error_link("no-dvc-cache"))
 
-            info.append(f"Caches: {_get_caches(repo.odb)}")
+            info.append(f"Caches: {_get_caches(repo.cache)}")
             info.append(f"Remotes: {_get_remotes(repo.config)}")
 
             root_directory = repo.root_dir
-            fs_root = get_fs_type(os.path.abspath(root_directory))
+            fs_root = _get_fs_type(os.path.abspath(root_directory))
             info.append(f"Workspace directory: {fs_root}")
             info.append(f"Repo: {_get_dvc_repo_info(repo)}")
+            info.append(f"Repo.site_cache_dir: {repo.site_cache_dir}")
     except NotDvcRepoError:
         pass
     except SCMError:
         info.append("Repo: dvc, git (broken)")
 
     return "\n".join(info)
 
 
 def _get_caches(cache):
     caches = (
         cache_type
         for cache_type, cache_instance in cache.by_scheme()
-        if cache_instance
+        if cache_instance and cache_type != "repo"
     )
 
     # Caches will be always non-empty including the local cache
     return ", ".join(caches)
 
 
 def _get_remotes(config):
     schemes = (
-        get_fs_cls(get_fs_config(None, config, name=remote)).scheme
+        get_fs_cls(get_fs_config(config, name=remote)).protocol
         for remote in config["remote"]
     )
 
     return ", ".join(schemes) or "None"
 
 
 def _get_linktype_support_info(repo):
-    odb = repo.odb.local
+    odb = repo.cache.local
 
-    links = test_links(
+    links = generic.test_links(
         ["reflink", "hardlink", "symlink"],
         odb.fs,
-        odb.fs_path,
+        odb.path,
         repo.fs,
         repo.root_dir,
     )
 
     return ", ".join(links)
 
 
+def _get_subprojects():
+    subprojects = []
+    for subproject in SUBPROJECTS:
+        try:
+            version = importlib_metadata.version(subproject)
+            subprojects.append(f"{subproject} = {version}")
+        except ImportError:
+            pass
+
+    return "\n\t" + "\n\t".join(subprojects)
+
+
 def _get_supported_remotes():
     supported_remotes = []
-    for scheme, fs_cls in FS_MAP.items():
+    for scheme in registry:
+        if scheme in [Schemes.LOCAL, Schemes.MEMORY, "dvc", "git"]:
+            continue
+
+        try:
+            fs_cls = registry[scheme]
+        except ImportError:
+            continue
+
         if not fs_cls.get_missing_deps():
             dependencies = []
             for requirement in fs_cls.REQUIRES:
                 dependencies.append(
-                    f"{requirement} = "
-                    f"{importlib_metadata.version(requirement)}"
+                    f"{requirement} = {importlib_metadata.version(requirement)}"
                 )
 
             remote_info = scheme
             if dependencies:
                 remote_info += " (" + ", ".join(dependencies) + ")"
             supported_remotes.append(remote_info)
 
     assert len(supported_remotes) >= 1
     return "\n\t" + ",\n\t".join(supported_remotes)
 
 
-def get_fs_type(path):
+def _get_config_dirs():
+    from dvc.config import Config
+
+    dirs = [
+        f"Global: {Config.get_dir('global')}",
+        f"System: {Config.get_dir('system')}",
+    ]
+
+    return "\n\t" + "\n\t".join(dirs)
+
+
+def _get_fs_type(path):
     partition = {}
     for part in psutil.disk_partitions(all=True):
-        if part.fstype != "":
+        if part.fstype:
             try:
                 mountpoint = pathlib.Path(part.mountpoint).resolve()
                 partition[mountpoint] = part.fstype + " on " + part.device
             except PermissionError:
                 pass
 
     # need to follow the symlink: https://github.com/iterative/dvc/issues/5065
@@ -129,15 +163,15 @@
 
     for parent in itertools.chain([path], path.parents):
         if parent in partition:
             return partition[parent]
     return ("unknown", "none")
 
 
-def _get_dvc_repo_info(self):
-    if self.config.get("core", {}).get("no_scm", False):
+def _get_dvc_repo_info(repo):
+    if repo.config.get("core", {}).get("no_scm", False):
         return "dvc (no_scm)"
 
-    if self.root_dir != self.scm.root_dir:
+    if repo.root_dir != repo.scm.root_dir:
         return "dvc (subdir), git"
 
     return "dvc, git"
```

### Comparing `dvc-2.9.5/dvc/lock.py` & `dvc-3.0.0a0/dvc/lock.py`

 * *Files 8% similar despite different names*

```diff
@@ -16,17 +16,15 @@
 DEFAULT_TIMEOUT = 3
 
 
 FAILED_TO_LOCK_MESSAGE = (
     "Unable to acquire lock. Most likely another DVC process is running or "
     "was terminated abruptly. Check the page {} for other possible reasons "
     "and to learn how to resolve this."
-).format(
-    format_link("https://dvc.org/doc/user-guide/troubleshooting#lock-issue")
-)
+).format(format_link("https://dvc.org/doc/user-guide/troubleshooting#lock-issue"))
 
 
 class LockError(DvcException):
     """Thrown when unable to acquire the lock for DVC repo."""
 
 
 class LockBase(ABC):
@@ -57,23 +55,23 @@
 
     @abstractmethod
     def __exit__(self, typ, value, tbck):
         pass
 
 
 class LockNoop(LockBase):
-    def __init__(
-        self, *args, **kwargs
-    ):  # pylint: disable=super-init-not-called
+    def __init__(self, *args, **kwargs):  # pylint: disable=super-init-not-called
         self._lock = False
 
     def lock(self):
         self._lock = True
 
     def unlock(self):
+        if not self.is_locked:
+            raise DvcException("Unlock called on an unlocked lock")
         self._lock = False
 
     @property
     def is_locked(self):
         return self._lock
 
     def __enter__(self):
@@ -89,41 +87,49 @@
     Uses zc.lockfile as backend.
     """
 
     def __init__(self, lockfile, friendly=False, **kwargs):
         super().__init__(lockfile)
         self._friendly = friendly
         self._lock = None
+        self._lock_failed = False
 
     @property
     def files(self):
         return [self._lockfile]
 
     def _do_lock(self):
         try:
+            self._lock_failed = False
             with Tqdm(
                 bar_format="{desc}",
                 disable=not self._friendly,
-                desc=(
-                    "If DVC froze, see `hardlink_lock` in {}".format(
-                        format_link("https://man.dvc.org/config#core")
-                    )
+                desc="If DVC froze, see `hardlink_lock` in {}".format(
+                    format_link("https://man.dvc.org/config#core")
                 ),
             ):
                 self._lock = zc.lockfile.LockFile(self._lockfile)
         except zc.lockfile.LockError:
-            raise LockError(FAILED_TO_LOCK_MESSAGE)
+            self._lock_failed = True
+            raise LockError(FAILED_TO_LOCK_MESSAGE)  # noqa: B904
 
     def lock(self):
         retries = 6
         delay = DEFAULT_TIMEOUT / retries
         lock_retry = retry(retries, LockError, timeout=delay)(self._do_lock)
         lock_retry()
 
     def unlock(self):
+        if self._lock_failed:
+            assert self._lock is None
+            return
+
+        if not self.is_locked:
+            raise DvcException("Unlock called on an unlocked lock")
+        assert self._lock
         self._lock.close()
         self._lock = None
 
     @property
     def is_locked(self):
         return bool(self._lock)
 
@@ -170,21 +176,24 @@
         self._owned = True
         self._retry_errnos = []
 
     def lock(self):  # pylint: disable=arguments-differ
         try:
             super().lock(timedelta(seconds=DEFAULT_TIMEOUT))
         except flufl.lock.TimeOutError:
-            raise LockError(FAILED_TO_LOCK_MESSAGE)
+            raise LockError(FAILED_TO_LOCK_MESSAGE)  # noqa: B904
 
     def _set_claimfile(self):
         super()._set_claimfile()
 
         if self._tmp_dir is not None:
             # Under Windows file path length is limited so we hash it
-            filename = hashlib.md5(self._claimfile.encode()).hexdigest()
+            hasher = hashlib.md5(  # nosec B324, B303  # noqa: S324
+                self._claimfile.encode()
+            )
+            filename = hasher.hexdigest()
             self._claimfile = os.path.join(self._tmp_dir, filename + ".lock")
 
 
 def make_lock(lockfile, tmp_dir=None, friendly=False, hardlink_lock=False):
     cls = HardlinkLock if hardlink_lock else Lock
     return cls(lockfile, tmp_dir=tmp_dir, friendly=friendly)
```

### Comparing `dvc-2.9.5/dvc/logger.py` & `dvc-3.0.0a0/dvc/logger.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,148 +1,144 @@
 """Manages logging configuration for DVC repo."""
 
+import logging
 import logging.config
 import logging.handlers
+import os
+import sys
 
 import colorama
 
+from dvc.env import DVC_SHOW_TRACEBACK
 from dvc.progress import Tqdm
 
-FOOTER = (
-    "\n{yellow}Having any troubles?{nc}"
-    " Hit us up at {blue}https://dvc.org/support{nc},"
-    " we are always happy to help!"
-).format(
-    blue=colorama.Fore.BLUE,
-    nc=colorama.Fore.RESET,
-    yellow=colorama.Fore.YELLOW,
-)
 
-
-def addLoggingLevel(levelName, levelNum, methodName=None):
+def add_logging_level(level_name, level_num, method_name=None):
     """
     Adds a new logging level to the `logging` module and the
     currently configured logging class.
 
-    Uses the existing numeric levelNum if already defined.
+    Uses the existing numeric level_num if already defined.
 
     Based on https://stackoverflow.com/questions/2183233
     """
-    if methodName is None:
-        methodName = levelName.lower()
+    if method_name is None:
+        method_name = level_name.lower()
 
     # If the level name is already defined as a top-level `logging`
     # constant, then adopt the existing numeric level.
-    if hasattr(logging, levelName):
-        existingLevelNum = getattr(logging, levelName)
-        assert isinstance(existingLevelNum, int)
-        levelNum = existingLevelNum
+    if hasattr(logging, level_name):
+        existing_level_num = getattr(logging, level_name)
+        assert isinstance(existing_level_num, int)
+        level_num = existing_level_num
 
-    def logForLevel(self, message, *args, **kwargs):
-        if self.isEnabledFor(levelNum):
+    def log_for_level(self, message, *args, **kwargs):
+        if self.isEnabledFor(level_num):
             # pylint: disable=protected-access
-            self._log(levelNum, message, args, **kwargs)
+            self._log(level_num, message, args, **kwargs)
 
-    def logToRoot(message, *args, **kwargs):
-        logging.log(levelNum, message, *args, **kwargs)
+    def log_to_root(message, *args, **kwargs):
+        logging.log(level_num, message, *args, **kwargs)
 
     # getLevelName resolves the numeric log level if already defined,
     # otherwise returns a string
-    if not isinstance(logging.getLevelName(levelName), int):
-        logging.addLevelName(levelNum, levelName)
+    if not isinstance(logging.getLevelName(level_name), int):
+        logging.addLevelName(level_num, level_name)
 
-    if not hasattr(logging, levelName):
-        setattr(logging, levelName, levelNum)
+    if not hasattr(logging, level_name):
+        setattr(logging, level_name, level_num)
 
-    if not hasattr(logging.getLoggerClass(), methodName):
-        setattr(logging.getLoggerClass(), methodName, logForLevel)
+    if not hasattr(logging.getLoggerClass(), method_name):
+        setattr(logging.getLoggerClass(), method_name, log_for_level)
 
-    if not hasattr(logging, methodName):
-        setattr(logging, methodName, logToRoot)
+    if not hasattr(logging, method_name):
+        setattr(logging, method_name, log_to_root)
 
 
 class LoggingException(Exception):
     def __init__(self, record):
         msg = f"failed to log {str(record)}"
         super().__init__(msg)
 
 
-def excludeFilter(level):
-    class ExcludeLevelFilter(logging.Filter):
-        def filter(self, record):
-            return record.levelno < level
+def exclude_filter(level: int):
+    def filter_fn(record: "logging.LogRecord") -> bool:
+        return record.levelno < level
 
-    return ExcludeLevelFilter
+    return filter_fn
 
 
 class ColorFormatter(logging.Formatter):
     """Spit out colored text in supported terminals.
 
     colorama__ makes ANSI escape character sequences work under Windows.
     See the colorama documentation for details.
 
     __ https://pypi.python.org/pypi/colorama
 
     If record has an extra `tb_only` attribute, it will not show the
     exception cause, just the message and the traceback.
     """
 
-    color_code = {
+    reset = colorama.Fore.RESET
+    color_codes = {
         "TRACE": colorama.Fore.GREEN,
         "DEBUG": colorama.Fore.BLUE,
         "WARNING": colorama.Fore.YELLOW,
         "ERROR": colorama.Fore.RED,
         "CRITICAL": colorama.Fore.RED,
     }
 
-    def format(self, record):
+    def __init__(self, log_colors: bool = True, show_traceback: bool = False) -> None:
+        super().__init__()
+        self.log_colors = log_colors
+        self.show_traceback = show_traceback
+
+    def format(self, record) -> str:  # noqa: A003, C901
         record.message = record.getMessage()
         msg = self.formatMessage(record)
 
-        if record.levelname == "INFO":
+        if record.levelno == logging.INFO:
             return msg
 
-        if record.exc_info:
-            if getattr(record, "tb_only", False):
-                cause = ""
-            else:
-                cause = ": ".join(_iter_causes(record.exc_info[1]))
-
-            msg = "{message}{separator}{cause}".format(
-                message=msg or "",
-                separator=" - " if msg and cause else "",
-                cause=cause,
-            )
-
-            if _is_verbose():
-                msg += _stack_trace(record.exc_info)
-
-        return "{asctime}{color}{levelname}{nc}: {msg}".format(
-            asctime=self.formatTime(record, self.datefmt),
-            color=self.color_code[record.levelname],
-            nc=colorama.Fore.RESET,
-            levelname=record.levelname,
-            msg=msg,
-        )
-
-    def formatTime(self, record, datefmt=None):
-        # only show if current level is set to DEBUG
-        # also, skip INFO as it is used for UI
-        if not _is_verbose() or record.levelno == logging.INFO:
-            return ""
-
-        return "{green}{date}{nc} ".format(
-            green=colorama.Fore.GREEN,
-            date=super().formatTime(record, datefmt),
-            nc=colorama.Fore.RESET,
-        )
+        ei = record.exc_info
+        if ei:
+            cause = ""
+            if not getattr(record, "tb_only", False):
+                cause = ": ".join(_iter_causes(ei[1]))
+            sep = " - " if msg and cause else ""
+            msg = msg + sep + cause
+
+        asctime = ""
+        verbose = _is_verbose()
+        if verbose:
+            asctime = self.formatTime(record, self.datefmt)
+        if verbose or self.show_traceback:
+            if ei and not record.exc_text:
+                record.exc_text = self.formatException(ei)
+            if record.exc_text:
+                if msg[-1:] != "\n":
+                    msg = msg + "\n"
+                msg = msg + record.exc_text + "\n"
+            if record.stack_info:
+                if msg[-1:] != "\n":
+                    msg = msg + "\n"
+                msg = msg + self.formatStack(record.stack_info) + "\n"
+
+        level = record.levelname
+        if self.log_colors:
+            color = self.color_codes[level]
+            if asctime:
+                asctime = color + asctime + self.reset
+            level = color + level + self.reset
+        return asctime + (" " if asctime else "") + level + ": " + msg
 
 
 class LoggerHandler(logging.StreamHandler):
-    def handleError(self, record):
+    def handleError(self, record):  # noqa: N802
         super().handleError(record)
         raise LoggingException(record)
 
     def emit_pretty_exception(self, exc, verbose: bool = False):
         return exc.__pretty_exc__(verbose=verbose)
 
     def emit(self, record):
@@ -151,126 +147,83 @@
             if record.exc_info:
                 _, exc, *_ = record.exc_info
                 if hasattr(exc, "__pretty_exc__"):
                     try:
                         self.emit_pretty_exception(exc, verbose=_is_verbose())
                         if not _is_verbose():
                             return
-                    except Exception:  # noqa, pylint: disable=broad-except
+                    # pylint: disable-next=broad-except
+                    except Exception:  # noqa: BLE001, S110  # nosec B110
                         pass
 
             msg = self.format(record)
-            Tqdm.write(
-                msg, file=self.stream, end=getattr(self, "terminator", "\n")
-            )
+            Tqdm.write(msg, file=self.stream, end=getattr(self, "terminator", "\n"))
             self.flush()
-        except RecursionError:
+        except (BrokenPipeError, RecursionError):
             raise
         except Exception:  # noqa, pylint: disable=broad-except
             self.handleError(record)
 
 
 def _is_verbose():
     return (
-        logging.NOTSET
-        < logging.getLogger("dvc").getEffectiveLevel()
-        <= logging.DEBUG
+        logging.NOTSET < logging.getLogger("dvc").getEffectiveLevel() <= logging.DEBUG
     )
 
 
 def _iter_causes(exc):
     while exc:
         yield str(exc)
         exc = exc.__cause__
 
 
-def _stack_trace(exc_info):
-    import traceback
-
-    return (
-        "\n"
-        "{red}{line}{nc}\n"
-        "{trace}"
-        "{red}{line}{nc}".format(
-            red=colorama.Fore.RED,
-            line="-" * 60,
-            trace="".join(traceback.format_exception(*exc_info)),
-            nc=colorama.Fore.RESET,
-        )
-    )
+def set_loggers_level(level: int = logging.INFO) -> None:
+    for name in ["dvc", "dvc_objects", "dvc_data"]:
+        logging.getLogger(name).setLevel(level)
 
 
-def disable_other_loggers():
-    logging.captureWarnings(True)
-    loggerDict = logging.root.manager.loggerDict  # pylint: disable=no-member
-    for logger_name, logger in loggerDict.items():
-        if logger_name != "dvc" and not logger_name.startswith("dvc."):
-            logger.disabled = True
+def setup(level: int = logging.INFO, log_colors: bool = True) -> None:
+    colorama.init()
 
+    formatter = ColorFormatter(log_colors=log_colors and sys.stdout.isatty())
 
-def setup(level=logging.INFO):
-    colorama.init()
+    console_info = LoggerHandler(sys.stdout)
+    console_info.setLevel(logging.INFO)
+    console_info.setFormatter(formatter)
+    console_info.addFilter(exclude_filter(logging.WARNING))
+
+    console_debug = LoggerHandler(sys.stdout)
+    console_debug.setLevel(logging.DEBUG)
+    console_debug.setFormatter(formatter)
+    console_debug.addFilter(exclude_filter(logging.INFO))
+
+    add_logging_level("TRACE", logging.DEBUG - 5)
+
+    console_trace = LoggerHandler(sys.stdout)
+    console_trace.setLevel(logging.TRACE)  # type: ignore[attr-defined]
+    console_trace.setFormatter(formatter)
+    console_trace.addFilter(exclude_filter(logging.DEBUG))
+
+    show_traceback = bool(os.environ.get(DVC_SHOW_TRACEBACK))
+    err_formatter = ColorFormatter(
+        log_colors=log_colors and sys.stderr.isatty(), show_traceback=show_traceback
+    )
+    console_errors = LoggerHandler(sys.stderr)
+    console_errors.setLevel(logging.WARNING)
+    console_errors.setFormatter(err_formatter)
+
+    for name in ["dvc", "dvc_objects", "dvc_data"]:
+        logger = logging.getLogger(name)
+        logger.setLevel(level)
+        for handler in [console_info, console_debug, console_trace, console_errors]:
+            logger.addHandler(handler)
 
     if level >= logging.DEBUG:
         # Unclosed session errors for asyncio/aiohttp are only available
         # on the tracing mode for extensive debug purposes. They are really
         # noisy, and this is potentially somewhere in the client library
         # not managing their own session. Even though it is the best practice
         # for them to do so, we can be assured that these errors raised when
         # the object is getting deallocated, so no need to take any extensive
         # action.
         logging.getLogger("asyncio").setLevel(logging.CRITICAL)
         logging.getLogger("aiohttp").setLevel(logging.CRITICAL)
-
-    addLoggingLevel("TRACE", logging.DEBUG - 5)
-    logging.config.dictConfig(
-        {
-            "version": 1,
-            "filters": {
-                "exclude_errors": {"()": excludeFilter(logging.WARNING)},
-                "exclude_info": {"()": excludeFilter(logging.INFO)},
-                "exclude_debug": {"()": excludeFilter(logging.DEBUG)},
-            },
-            "formatters": {"color": {"()": ColorFormatter}},
-            "handlers": {
-                "console_info": {
-                    "class": "dvc.logger.LoggerHandler",
-                    "level": "INFO",
-                    "formatter": "color",
-                    "stream": "ext://sys.stdout",
-                    "filters": ["exclude_errors"],
-                },
-                "console_debug": {
-                    "class": "dvc.logger.LoggerHandler",
-                    "level": "DEBUG",
-                    "formatter": "color",
-                    "stream": "ext://sys.stdout",
-                    "filters": ["exclude_info"],
-                },
-                "console_trace": {
-                    "class": "dvc.logger.LoggerHandler",
-                    "level": "TRACE",
-                    "formatter": "color",
-                    "stream": "ext://sys.stdout",
-                    "filters": ["exclude_debug"],
-                },
-                "console_errors": {
-                    "class": "dvc.logger.LoggerHandler",
-                    "level": "WARNING",
-                    "formatter": "color",
-                    "stream": "ext://sys.stderr",
-                },
-            },
-            "loggers": {
-                "dvc": {
-                    "level": level,
-                    "handlers": [
-                        "console_info",
-                        "console_debug",
-                        "console_trace",
-                        "console_errors",
-                    ],
-                }
-            },
-            "disable_existing_loggers": False,
-        }
-    )
```

### Comparing `dvc-2.9.5/dvc/machine/__init__.py` & `dvc-3.0.0a0/dvc/machine/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,18 +9,18 @@
     Mapping,
     Optional,
     Tuple,
     Type,
 )
 
 from dvc.exceptions import DvcException
-from dvc.types import StrPath
 
 if TYPE_CHECKING:
     from dvc.repo import Repo
+    from dvc.types import StrPath
 
     from .backend.base import BaseMachineBackend
 
     BackendCls = Type[BaseMachineBackend]
 
 logger = logging.getLogger(__name__)
 
@@ -63,24 +63,22 @@
     }
 
     def __getitem__(self, key: str) -> "BaseMachineBackend":
         """Lazily initialize backends and cache it afterwards"""
         initialized = self.initialized.get(key)
         if not initialized:
             backend = self.backends[key]
-            initialized = backend(
-                os.path.join(self.tmp_dir, key), **self.kwargs
-            )
+            initialized = backend(os.path.join(self.tmp_dir, key), **self.kwargs)
             self.initialized[key] = initialized
         return initialized
 
     def __init__(
         self,
         selected: Optional[Iterable[str]],
-        tmp_dir: StrPath,
+        tmp_dir: "StrPath",
         **kwargs,
     ) -> None:
         selected = selected or list(self.DEFAULT)
         self.backends: Dict[str, "BackendCls"] = {}
         for key in selected:
             cls = self.DEFAULT.get(key)
             if cls is None:
@@ -123,14 +121,15 @@
         "azure": "terraform",
     }
 
     def __init__(
         self, repo: "Repo", backends: Optional[Iterable[str]] = None, **kwargs
     ):
         self.repo = repo
+        assert self.repo.tmp_dir
         tmp_dir = os.path.join(self.repo.tmp_dir, "machine")
         self.backends = MachineBackends(backends, tmp_dir=tmp_dir, **kwargs)
 
     def get_config_and_backend(
         self,
         name: Optional[str] = None,
     ) -> Tuple[dict, "BaseMachineBackend"]:
@@ -163,27 +162,31 @@
         if name:
             try:
                 conf = config["machine"][name.lower()]
                 conf["name"] = name
             except KeyError:
                 from dvc.config import MachineNotFoundError
 
-                raise MachineNotFoundError(f"machine '{name}' doesn't exist")
+                raise MachineNotFoundError(  # noqa: B904
+                    f"machine '{name}' doesn't exist"
+                )
         else:
             conf = kwargs
         return conf
 
     def _get_backend(self, cloud: str) -> "BaseMachineBackend":
         from dvc.config import NoMachineError
 
         try:
             backend = self.CLOUD_BACKENDS[cloud]
             return self.backends[backend]
         except KeyError:
-            raise NoMachineError(f"Machine platform '{cloud}' unsupported")
+            raise NoMachineError(  # noqa: B904
+                f"Machine platform '{cloud}' unsupported"
+            )
 
     def create(self, name: Optional[str]):
         """Create and start the specified machine instance."""
         config, backend = self.get_config_and_backend(name)
         if "startup_script" in config:
             with open(config["startup_script"], encoding="utf-8") as fobj:
                 startup_script = fobj.read()
```

### Comparing `dvc-2.9.5/dvc/machine/backend/base.py` & `dvc-3.0.0a0/dvc/machine/backend/base.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 from abc import ABC, abstractmethod
 from contextlib import contextmanager
 from typing import TYPE_CHECKING, Iterator, Optional
 
 if TYPE_CHECKING:
-    from dvc.fs.ssh import SSHFileSystem
+    from dvc_ssh import SSHFileSystem
+
     from dvc.types import StrPath
 
 
 class BaseMachineBackend(ABC):
     def __init__(self, tmp_dir: "StrPath", **kwargs):
         self.tmp_dir = tmp_dir
 
@@ -16,20 +17,18 @@
         """Create and start an instance of the specified machine."""
 
     @abstractmethod
     def destroy(self, name: Optional[str] = None, **config):
         """Stop and destroy all instances of the specified machine."""
 
     @abstractmethod
-    def instances(
-        self, name: Optional[str] = None, **config
-    ) -> Iterator[dict]:
+    def instances(self, name: Optional[str] = None, **config) -> Iterator[dict]:
         """Iterate over status of all instances of the specified machine."""
 
-    def close(self):
+    def close(self):  # noqa: B027
         pass
 
     @abstractmethod
     def run_shell(self, name: Optional[str] = None, **config):
         """Spawn an interactive SSH shell for the specified machine."""
 
     @abstractmethod
```

### Comparing `dvc-2.9.5/dvc/machine/backend/terraform.py` & `dvc-3.0.0a0/dvc/machine/backend/terraform.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import os
 from contextlib import contextmanager
 from functools import partial, partialmethod
 from typing import TYPE_CHECKING, Iterator, Optional
 
+from dvc_ssh import DEFAULT_PORT, SSHFileSystem
+
 from dvc.exceptions import DvcException
-from dvc.fs.ssh import SSHFileSystem
-from dvc.utils.fs import makedirs
 
 from .base import BaseMachineBackend
 
 if TYPE_CHECKING:
     from dvc.types import StrPath
 
 
@@ -25,24 +25,24 @@
         )
         yield fs
 
 
 class TerraformBackend(BaseMachineBackend):
     def __init__(self, tmp_dir: "StrPath", **kwargs):
         super().__init__(tmp_dir, **kwargs)
-        makedirs(tmp_dir, exist_ok=True)
+        os.makedirs(tmp_dir, exist_ok=True)
 
     @contextmanager
     def make_tpi(self, name: str):
         from tpi import TPIError
         from tpi.terraform import TerraformBackend as TPIBackend
 
         try:
             working_dir = os.path.join(self.tmp_dir, name)
-            makedirs(working_dir, exist_ok=True)
+            os.makedirs(working_dir, exist_ok=True)
             yield TPIBackend(working_dir=working_dir)
         except TPIError as exc:
             raise DvcException("TPI operation failed") from exc
 
     def _tpi_func(self, fname, name: Optional[str] = None, **config):
         from tpi import TPIError
 
@@ -52,27 +52,23 @@
             try:
                 return func(name=name, **config)
             except TPIError as exc:
                 raise DvcException(f"TPI {fname} failed") from exc
 
     create = partialmethod(_tpi_func, "create")  # type: ignore[assignment]
     destroy = partialmethod(_tpi_func, "destroy")  # type: ignore[assignment]
-    instances = partialmethod(
-        _tpi_func, "instances"
-    )  # type: ignore[assignment]
-    run_shell = partialmethod(
-        _tpi_func, "run_shell"
-    )  # type: ignore[assignment]
+    instances = partialmethod(_tpi_func, "instances")  # type: ignore[assignment]
+    run_shell = partialmethod(_tpi_func, "run_shell")  # type: ignore[assignment]
 
     def get_executor_kwargs(self, name: str, **config) -> dict:
         with self.make_tpi(name) as tpi:
             resource = tpi.default_resource(name)
         return {
             "host": resource["instance_ip"],
-            "port": SSHFileSystem.DEFAULT_PORT,
+            "port": DEFAULT_PORT,
             "username": "ubuntu",
             "fs_factory": partial(_sshfs, dict(resource)),
         }
 
     @contextmanager
     def get_sshfs(  # pylint: disable=unused-argument
         self, name: Optional[str] = None, **config
```

### Comparing `dvc-2.9.5/dvc/output.py` & `dvc-3.0.0a0/dvc/repo/experiments/queue/base.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,1064 +1,792 @@
 import logging
 import os
-from collections import defaultdict
-from typing import TYPE_CHECKING, Dict, Optional, Set, Type
-from urllib.parse import urlparse
-
-from funcy import collecting, project
-from voluptuous import And, Any, Coerce, Length, Lower, Required, SetTo
-
-from dvc import prompt
-from dvc.exceptions import (
-    CheckoutError,
-    CollectCacheError,
-    DvcException,
-    MergeError,
-    RemoteCacheRequiredError,
+from abc import ABC, abstractmethod
+from dataclasses import asdict, dataclass
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Collection,
+    Dict,
+    Generator,
+    Iterable,
+    List,
+    Mapping,
+    NamedTuple,
+    Optional,
+    Type,
+    Union,
 )
 
-from .data import check as ocheck
-from .data import load as oload
-from .data.checkout import checkout
-from .data.meta import Meta
-from .data.stage import stage as ostage
-from .data.transfer import transfer as otransfer
-from .data.tree import Tree
-from .fs import get_cloud_fs
-from .fs.hdfs import HDFSFileSystem
-from .fs.local import LocalFileSystem
-from .fs.s3 import S3FileSystem
-from .hash_info import HashInfo
-from .istextfile import istextfile
-from .objects.errors import ObjectFormatError
-from .scheme import Schemes
-from .utils import relpath
-from .utils.fs import path_isin
+from dvc_studio_client.post_live_metrics import get_studio_config
+from funcy import retry
 
-if TYPE_CHECKING:
-    from .objects.db import ObjectDB
-
-logger = logging.getLogger(__name__)
-
-
-CHECKSUM_SCHEMA = Any(
-    None,
-    And(str, Length(max=0), SetTo(None)),
-    And(Any(str, And(int, Coerce(str))), Length(min=3), Lower),
+from dvc.dependency import ParamsDependency
+from dvc.env import DVC_EXP_BASELINE_REV, DVC_EXP_NAME, DVCLIVE_RESUME
+from dvc.exceptions import DvcException
+from dvc.lock import LockError
+from dvc.repo.experiments.exceptions import CheckpointExistsError, ExperimentExistsError
+from dvc.repo.experiments.executor.base import BaseExecutor
+from dvc.repo.experiments.executor.local import WorkspaceExecutor
+from dvc.repo.experiments.refs import ExpRefInfo
+from dvc.repo.experiments.stash import ExpStash, ExpStashEntry
+from dvc.repo.experiments.utils import (
+    EXEC_PID_DIR,
+    EXEC_TMP_DIR,
+    exp_refs_by_rev,
+    get_exp_rwlock,
+    get_random_exp_name,
 )
+from dvc.ui import ui
+from dvc.utils.objects import cached_property
+from dvc.utils.studio import config_to_env
 
-CASE_SENSITIVE_CHECKSUM_SCHEMA = Any(
-    None,
-    And(str, Length(max=0), SetTo(None)),
-    And(Any(str, And(int, Coerce(str))), Length(min=3)),
-)
-
-# NOTE: currently there are only 3 possible checksum names:
-#
-#    1) md5 (LOCAL, SSH);
-#    2) etag (S3, GS, OSS, AZURE, HTTP);
-#    3) checksum (HDFS);
-#
-# so when a few types of outputs share the same name, we only need
-# specify it once.
-CHECKSUMS_SCHEMA = {
-    LocalFileSystem.PARAM_CHECKSUM: CHECKSUM_SCHEMA,
-    HDFSFileSystem.PARAM_CHECKSUM: CHECKSUM_SCHEMA,
-    S3FileSystem.PARAM_CHECKSUM: CASE_SENSITIVE_CHECKSUM_SCHEMA,
-}
-
-
-def _get(stage, path, **kwargs):
-    return Output(stage, path, **kwargs)
-
-
-def loadd_from(stage, d_list):
-    ret = []
-    for d in d_list:
-        p = d.pop(Output.PARAM_PATH)
-        cache = d.pop(Output.PARAM_CACHE, True)
-        metric = d.pop(Output.PARAM_METRIC, False)
-        plot = d.pop(Output.PARAM_PLOT, False)
-        persist = d.pop(Output.PARAM_PERSIST, False)
-        checkpoint = d.pop(Output.PARAM_CHECKPOINT, False)
-        desc = d.pop(Output.PARAM_DESC, False)
-        live = d.pop(Output.PARAM_LIVE, False)
-        remote = d.pop(Output.PARAM_REMOTE, None)
-        ret.append(
-            _get(
-                stage,
-                p,
-                info=d,
-                cache=cache,
-                metric=metric,
-                plot=plot,
-                persist=persist,
-                checkpoint=checkpoint,
-                desc=desc,
-                live=live,
-                remote=remote,
-            )
-        )
-    return ret
-
-
-def loads_from(
-    stage,
-    s_list,
-    use_cache=True,
-    metric=False,
-    plot=False,
-    persist=False,
-    checkpoint=False,
-    live=False,
-    remote=None,
-):
-    return [
-        _get(
-            stage,
-            s,
-            info={},
-            cache=use_cache,
-            metric=metric,
-            plot=plot,
-            persist=persist,
-            checkpoint=checkpoint,
-            live=live,
-            remote=remote,
-        )
-        for s in s_list
-    ]
-
-
-def _split_dict(d, keys):
-    return project(d, keys), project(d, d.keys() - keys)
-
-
-def _merge_data(s_list):
-    d = defaultdict(dict)
-    for key in s_list:
-        if isinstance(key, str):
-            d[key].update({})
-            continue
-        if not isinstance(key, dict):
-            raise ValueError(f"'{type(key).__name__}' not supported.")
-
-        for k, flags in key.items():
-            if not isinstance(flags, dict):
-                raise ValueError(
-                    f"Expected dict for '{k}', got: '{type(flags).__name__}'"
-                )
-            d[k].update(flags)
-    return d
-
-
-@collecting
-def load_from_pipeline(stage, data, typ="outs"):
-    if typ not in (
-        stage.PARAM_OUTS,
-        stage.PARAM_METRICS,
-        stage.PARAM_PLOTS,
-        stage.PARAM_LIVE,
-    ):
-        raise ValueError(f"'{typ}' key is not allowed for pipeline files.")
-
-    metric = typ == stage.PARAM_METRICS
-    plot = typ == stage.PARAM_PLOTS
-    live = typ == stage.PARAM_LIVE
-    if live:
-        # `live` is single object
-        data = [data]
-
-    d = _merge_data(data)
-
-    for path, flags in d.items():
-        plt_d, live_d = {}, {}
-        if plot:
-            from dvc.schema import PLOT_PROPS
-
-            plt_d, flags = _split_dict(flags, keys=PLOT_PROPS.keys())
-        if live:
-            from dvc.schema import LIVE_PROPS
-
-            live_d, flags = _split_dict(flags, keys=LIVE_PROPS.keys())
-        extra = project(
-            flags,
-            [
-                Output.PARAM_CACHE,
-                Output.PARAM_PERSIST,
-                Output.PARAM_CHECKPOINT,
-                Output.PARAM_REMOTE,
-            ],
-        )
-
-        yield _get(
-            stage,
-            path,
-            info={},
-            plot=plt_d or plot,
-            metric=metric,
-            live=live_d or live,
-            **extra,
-        )
-
-
-class OutputDoesNotExistError(DvcException):
-    def __init__(self, path):
-        msg = f"output '{path}' does not exist"
-        super().__init__(msg)
-
-
-class OutputIsNotFileOrDirError(DvcException):
-    def __init__(self, path):
-        msg = f"output '{path}' is not a file or directory"
-        super().__init__(msg)
-
-
-class OutputAlreadyTrackedError(DvcException):
-    def __init__(self, path):
-        msg = f""" output '{path}' is already tracked by SCM (e.g. Git).
-    You can remove it from Git, then add to DVC.
-        To stop tracking from Git:
-            git rm -r --cached '{path}'
-            git commit -m "stop tracking {path}" """
-        super().__init__(msg)
-
-
-class OutputIsStageFileError(DvcException):
-    def __init__(self, path):
-        super().__init__(f"DVC file '{path}' cannot be an output.")
-
-
-class OutputIsIgnoredError(DvcException):
-    def __init__(self, match):
-        lines = "\n".join(match.patterns)
-        super().__init__(f"Path '{match.file}' is ignored by\n{lines}")
-
-
-class Output:
-    IS_DEPENDENCY = False
-
-    PARAM_PATH = "path"
-    PARAM_CACHE = "cache"
-    PARAM_CHECKPOINT = "checkpoint"
-    PARAM_METRIC = "metric"
-    PARAM_METRIC_TYPE = "type"
-    PARAM_METRIC_XPATH = "xpath"
-    PARAM_PLOT = "plot"
-    PARAM_PLOT_TEMPLATE = "template"
-    PARAM_PLOT_X = "x"
-    PARAM_PLOT_Y = "y"
-    PARAM_PLOT_X_LABEL = "x_label"
-    PARAM_PLOT_Y_LABEL = "y_label"
-    PARAM_PLOT_TITLE = "title"
-    PARAM_PLOT_HEADER = "header"
-    PARAM_PERSIST = "persist"
-    PARAM_DESC = "desc"
-    PARAM_LIVE = "live"
-    PARAM_LIVE_SUMMARY = "summary"
-    PARAM_LIVE_HTML = "html"
-    PARAM_REMOTE = "remote"
-
-    METRIC_SCHEMA = Any(
-        None,
-        bool,
-        {
-            PARAM_METRIC_TYPE: Any(str, None),
-            PARAM_METRIC_XPATH: Any(str, None),
-        },
-    )
-
-    DoesNotExistError = OutputDoesNotExistError  # type: Type[DvcException]
-    IsNotFileOrDirError = OutputIsNotFileOrDirError  # type: Type[DvcException]
-    IsStageFileError = OutputIsStageFileError  # type: Type[DvcException]
-    IsIgnoredError = OutputIsIgnoredError  # type: Type[DvcException]
-
-    def __init__(
-        self,
-        stage,
-        path,
-        info=None,
-        cache=True,
-        metric=False,
-        plot=False,
-        persist=False,
-        checkpoint=False,
-        live=False,
-        desc=None,
-        remote=None,
-        repo=None,
-    ):
-        self.repo = stage.repo if not repo and stage else repo
-        fs_cls, fs_config, fs_path = get_cloud_fs(self.repo, url=path)
-        self.fs = fs_cls(**fs_config)
-
-        if (
-            self.fs.scheme == "local"
-            and stage
-            and path_isin(path, stage.repo.root_dir)
-        ):
-            self.def_path = relpath(path, stage.wdir)
-        else:
-            self.def_path = path
-
-        self._validate_output_path(path, stage)
-        # This output (and dependency) objects have too many paths/urls
-        # here is a list and comments:
-        #
-        #   .def_path - path from definition in DVC file
-        #   .fspath - local only, resolved
-        #   .__str__ - for presentation purposes, def_path/relpath
-        #
-        # By resolved path, which contains actual location,
-        # should be absolute and don't contain remote:// refs.
-        self.stage = stage
-        self.meta = Meta.from_dict(info)
-        self.hash_info = HashInfo.from_dict(info)
-        self.use_cache = False if self.IS_DEPENDENCY else cache
-        self.metric = False if self.IS_DEPENDENCY else metric
-        self.plot = False if self.IS_DEPENDENCY else plot
-        self.persist = persist
-        self.checkpoint = checkpoint
-        self.live = live
-        self.desc = desc
-
-        self.fs_path = self._parse_path(self.fs, fs_path)
-        self.obj = None
-
-        self.remote = remote
-
-    def _parse_path(self, fs, fs_path):
-        if fs.scheme != "local":
-            return fs_path
-
-        parsed = urlparse(self.def_path)
-        if parsed.scheme != "remote":
-            # NOTE: we can path either from command line or .dvc file,
-            # so we should expect both posix and windows style paths.
-            # paths accepts both, i.e. / works everywhere, \ only on win.
-            #
-            # FIXME: if we have Windows path containing / or posix one with \
-            # then we have #2059 bug and can't really handle that.
-            if self.stage and not os.path.isabs(fs_path):
-                fs_path = fs.path.join(self.stage.wdir, fs_path)
-
-        abs_p = os.path.abspath(os.path.normpath(fs_path))
-        return abs_p
-
-    def __repr__(self):
-        return "{class_name}: '{def_path}'".format(
-            class_name=type(self).__name__, def_path=self.def_path
-        )
-
-    def __str__(self):
-        if self.fs.scheme != "local":
-            return self.def_path
-
-        if (
-            not self.repo
-            or urlparse(self.def_path).scheme == "remote"
-            or os.path.isabs(self.def_path)
-        ):
-            return str(self.def_path)
-
-        cur_dir = os.getcwd()
-        if path_isin(cur_dir, self.repo.root_dir):
-            return relpath(self.fs_path, cur_dir)
-
-        return relpath(self.fs_path, self.repo.root_dir)
-
-    @property
-    def scheme(self):
-        return self.fs.scheme
-
-    @property
-    def is_in_repo(self):
-        if self.fs.scheme != "local":
-            return False
-
-        if urlparse(self.def_path).scheme == "remote":
-            return False
-
-        if os.path.isabs(self.def_path):
-            return False
+from .utils import get_remote_executor_refs
 
-        return self.repo and path_isin(
-            os.path.realpath(self.fs_path), self.repo.root_dir
-        )
+if TYPE_CHECKING:
+    from dvc.repo import Repo
+    from dvc.repo.experiments import Experiments
+    from dvc.repo.experiments.executor.base import ExecutorResult
+    from dvc.repo.experiments.serialize import ExpRange
+    from dvc.scm import Git
 
-    @property
-    def use_scm_ignore(self):
-        if not self.is_in_repo:
-            return False
+logger = logging.getLogger(__name__)
 
-        return self.use_cache or self.stage.is_repo_import
 
-    @property
-    def odb(self):
-        odb = getattr(self.repo.odb, self.scheme)
-        if self.use_cache and odb is None:
-            raise RemoteCacheRequiredError(self.fs.scheme, self.fs_path)
-        return odb
+@dataclass(frozen=True)
+class QueueEntry:
+    dvc_root: str
+    scm_root: str
+    stash_ref: str
+    stash_rev: str
+    baseline_rev: str
+    branch: Optional[str]
+    name: Optional[str]
+    head_rev: Optional[str] = None
 
-    @property
-    def cache_path(self):
-        return self.odb.fs.unstrip_protocol(
-            self.odb.hash_to_path(self.hash_info.value)
+    def __eq__(self, other: object):
+        return (
+            isinstance(other, QueueEntry)
+            and self.dvc_root == other.dvc_root
+            and self.scm_root == other.scm_root
+            and self.stash_ref == other.stash_ref
+            and self.stash_rev == other.stash_rev
         )
 
-    def get_hash(self):
-        if self.use_cache:
-            odb = self.odb
-            name = self.odb.fs.PARAM_CHECKSUM
-        else:
-            odb = self.repo.odb.local
-            name = self.fs.PARAM_CHECKSUM
-        _, _, obj = ostage(
-            odb,
-            self.fs_path,
-            self.fs,
-            name,
-            dvcignore=self.dvcignore,
-            dry_run=not self.use_cache,
-        )
-        return obj.hash_info
-
-    @property
-    def is_dir_checksum(self):
-        return self.hash_info.isdir
+    def asdict(self) -> Dict[str, Any]:
+        return asdict(self)
 
-    def _is_path_dvcignore(self, path) -> bool:
-        if not self.IS_DEPENDENCY and self.dvcignore:
-            if self.dvcignore.is_ignored(self.fs, path, ignore_subrepos=False):
-                return True
-        return False
-
-    @property
-    def exists(self):
-        if self._is_path_dvcignore(self.fs_path):
-            return False
+    @classmethod
+    def from_dict(cls, d: Dict[str, Any]) -> "QueueEntry":
+        return cls(**d)
 
-        return self.fs.exists(self.fs_path)
 
-    def changed_checksum(self):
-        return self.hash_info != self.get_hash()
+class QueueGetResult(NamedTuple):
+    entry: QueueEntry
+    executor: BaseExecutor
 
-    def changed_cache(self, filter_info=None):
-        if not self.use_cache or not self.hash_info:
-            return True
 
-        obj = self.get_obj(filter_info=filter_info)
-        if not obj:
-            return True
+class QueueDoneResult(NamedTuple):
+    entry: QueueEntry
+    result: Optional["ExecutorResult"]
 
-        try:
-            ocheck(self.odb, obj)
-            return False
-        except (FileNotFoundError, ObjectFormatError):
-            return True
 
-    def workspace_status(self):
-        if not self.exists:
-            return {str(self): "deleted"}
+class ExpRefAndQueueEntry(NamedTuple):
+    exp_ref_info: Optional["ExpRefInfo"]
+    queue_entry: Optional["QueueEntry"]
 
-        if self.changed_checksum():
-            return {str(self): "modified"}
 
-        if not self.hash_info:
-            return {str(self): "new"}
+class BaseStashQueue(ABC):
+    """Naive Git-stash based experiment queue.
 
-        return {}
+    Maps queued experiments to (Git) stash reflog entries.
+    """
 
-    def status(self):
-        if self.hash_info and self.use_cache and self.changed_cache():
-            return {str(self): "not in cache"}
+    def __init__(self, repo: "Repo", ref: str, failed_ref: Optional[str] = None):
+        """Construct a queue.
 
-        return self.workspace_status()
-
-    def changed(self):
-        status = self.status()
-        logger.debug(str(status))
-        return bool(status)
-
-    @property
-    def dvcignore(self):
-        if self.fs.scheme == "local":
-            return self.repo.dvcignore
-        return None
+        Arguments:
+            scm: Git SCM instance for this queue.
+            ref: Git stash ref for this queue.
+            failed_ref: Failed run Git stash ref for this queue.
+        """
+        self.repo = repo
+        assert self.repo.tmp_dir
+        self.ref = ref
+        self.failed_ref = failed_ref
 
     @property
-    def is_empty(self):
-        return self.fs.is_empty(self.fs_path)
-
-    def isdir(self):
-        if self._is_path_dvcignore(self.fs_path):
-            return False
-        return self.fs.isdir(self.fs_path)
+    def scm(self) -> "Git":
+        from dvc.scm import Git
 
-    def isfile(self):
-        if self._is_path_dvcignore(self.fs_path):
-            return False
-        return self.fs.isfile(self.fs_path)
+        assert isinstance(self.repo.scm, Git)
+        return self.repo.scm
 
-    # pylint: disable=no-member
+    @cached_property
+    def stash(self) -> ExpStash:
+        return ExpStash(self.scm, self.ref)
 
-    def ignore(self):
-        if not self.use_scm_ignore:
-            return
+    @cached_property
+    def failed_stash(self) -> Optional[ExpStash]:
+        return ExpStash(self.scm, self.failed_ref) if self.failed_ref else None
 
-        if self.repo.scm.is_tracked(self.fspath):
-            raise OutputAlreadyTrackedError(self)
+    @cached_property
+    def pid_dir(self) -> str:
+        assert self.repo.tmp_dir is not None
+        return os.path.join(self.repo.tmp_dir, EXEC_TMP_DIR, EXEC_PID_DIR)
 
-        self.repo.scm_context.ignore(self.fspath)
+    @cached_property
+    def args_file(self) -> str:
+        assert self.repo.tmp_dir is not None
+        return os.path.join(self.repo.tmp_dir, BaseExecutor.PACKED_ARGS_FILE)
 
-    def ignore_remove(self):
-        if not self.use_scm_ignore:
-            return
+    @abstractmethod
+    def put(self, *args, **kwargs) -> QueueEntry:
+        """Stash an experiment and add it to the queue."""
 
-        self.repo.scm_context.ignore_remove(self.fspath)
+    @abstractmethod
+    def get(self) -> QueueGetResult:
+        """Pop and return the first item in the queue."""
 
-    # pylint: enable=no-member
-
-    def save(self):
-        if not self.exists:
-            raise self.DoesNotExistError(self)
-
-        if not self.isfile and not self.isdir:
-            raise self.IsNotFileOrDirError(self)
-
-        if self.is_empty:
-            logger.warning(f"'{self}' is empty.")
+    def remove(
+        self,
+        revs: Collection[str],
+        all_: bool = False,
+        queued: bool = False,
+        **kwargs,
+    ) -> List[str]:
+        """Remove the specified entries from the queue.
 
-        self.ignore()
+        Arguments:
+            revs: Stash revisions or queued exp names to be removed.
+            queued: Remove all queued tasks.
+            all: Remove all tasks.
+
+        Returns:
+            Revisions (or names) which were removed.
+        """
+
+        if all_ or queued:
+            return self.clear()
+
+        name_to_remove: List[str] = []
+        entry_to_remove: List[ExpStashEntry] = []
+        queue_entries = self.match_queue_entry_by_name(revs, self.iter_queued())
+        for name, entry in queue_entries.items():
+            if entry:
+                entry_to_remove.append(self.stash.stash_revs[entry.stash_rev])
+                name_to_remove.append(name)
+
+        self.stash.remove_revs(entry_to_remove)
+        return name_to_remove
+
+    def clear(self, **kwargs) -> List[str]:
+        """Remove all entries from the queue."""
+        stash_revs = self.stash.stash_revs
+        name_to_remove = list(stash_revs)
+        self.stash.remove_revs(list(stash_revs.values()))
+
+        return name_to_remove
+
+    def status(self) -> List[Dict[str, Any]]:
+        """Show the status of exp tasks in queue"""
+        from datetime import datetime
+
+        result: List[Dict[str, Optional[str]]] = []
+
+        def _get_timestamp(rev: str) -> datetime:
+            commit = self.scm.resolve_commit(rev)
+            return datetime.fromtimestamp(commit.commit_time)
+
+        def _format_entry(
+            entry: QueueEntry,
+            exp_result: Optional["ExecutorResult"] = None,
+            status: str = "Unknown",
+        ) -> Dict[str, Any]:
+            name = entry.name
+            if not name and exp_result and exp_result.ref_info:
+                name = exp_result.ref_info.name
+            # NOTE: We fallback to Unknown status for experiments
+            # generated in prior (incompatible) DVC versions
+            return {
+                "rev": entry.stash_rev,
+                "name": name,
+                "timestamp": _get_timestamp(entry.stash_rev),
+                "status": status,
+            }
+
+        result.extend(
+            _format_entry(queue_entry, status="Running")
+            for queue_entry in self.iter_active()
+        )
+        result.extend(
+            _format_entry(queue_entry, status="Queued")
+            for queue_entry in self.iter_queued()
+        )
+        result.extend(
+            _format_entry(queue_entry, status="Failed")
+            for queue_entry, _ in self.iter_failed()
+        )
+        result.extend(
+            _format_entry(queue_entry, exp_result=exp_result, status="Success")
+            for queue_entry, exp_result in self.iter_success()
+        )
+        return result
+
+    @abstractmethod
+    def iter_queued(self) -> Generator[QueueEntry, None, None]:
+        """Iterate over items in the queue."""
+
+    @abstractmethod
+    def iter_active(self) -> Generator[QueueEntry, None, None]:
+        """Iterate over items which are being actively processed."""
+
+    @abstractmethod
+    def iter_done(self) -> Generator[QueueDoneResult, None, None]:
+        """Iterate over items which been processed."""
+
+    @abstractmethod
+    def iter_success(self) -> Generator[QueueDoneResult, None, None]:
+        """Iterate over items which been success."""
+
+    @abstractmethod
+    def iter_failed(self) -> Generator[QueueDoneResult, None, None]:
+        """Iterate over items which been failed."""
+
+    @abstractmethod
+    def reproduce(
+        self, copy_paths: Optional[List[str]] = None, message: Optional[str] = None
+    ) -> Mapping[str, Mapping[str, str]]:
+        """Reproduce queued experiments sequentially."""
+
+    @abstractmethod
+    def get_result(self, entry: QueueEntry) -> Optional["ExecutorResult"]:
+        """Return result of the specified item.
+
+        This method blocks until the specified item has been collected.
+        """
+
+    @abstractmethod
+    def kill(self, revs: str) -> None:
+        """Kill the specified running entries in the queue.
+
+        Arguments:
+            revs: Stash revs or running exp name to be killed.
+        """
+
+    @abstractmethod
+    def shutdown(self, kill: bool = False):
+        """Shutdown the queue worker.
+
+        Arguments:
+            kill: If True, the any active experiments will be killed and the
+                worker will shutdown immediately. If False, the worker will
+                finish any active experiments before shutting down.
+        """
 
-        if self.metric or self.plot:
-            self.verify_metric()
+    @abstractmethod
+    def logs(
+        self,
+        rev: str,
+        encoding: Optional[str] = None,
+        follow: bool = False,
+    ):
+        """Print redirected output logs for an exp process.
 
-        if not self.use_cache:
-            _, self.meta, obj = ostage(
-                self.repo.odb.local,
-                self.fs_path,
-                self.fs,
-                self.fs.PARAM_CHECKSUM,
-                dvcignore=self.dvcignore,
-                dry_run=True,
-            )
-            self.hash_info = obj.hash_info
-            if not self.IS_DEPENDENCY:
-                logger.debug(
-                    "Output '%s' doesn't use cache. Skipping saving.", self
-                )
-            return
+        Args:
+            rev: Stash rev or exp name.
+            encoding: Text encoding for redirected output. Defaults to
+                `locale.getpreferredencoding()`.
+            follow: Attach to running exp process and follow additional
+                output.
+        """
 
-        assert not self.IS_DEPENDENCY
+    def _stash_exp(  # noqa: PLR0915, C901
+        self,
+        *args,
+        params: Optional[Dict[str, List[str]]] = None,
+        resume_rev: Optional[str] = None,
+        baseline_rev: Optional[str] = None,
+        branch: Optional[str] = None,
+        name: Optional[str] = None,
+        **kwargs,
+    ) -> QueueEntry:
+        """Stash changes from the workspace as an experiment.
 
-        _, self.meta, self.obj = ostage(
-            self.odb,
-            self.fs_path,
-            self.fs,
-            self.odb.fs.PARAM_CHECKSUM,
-            dvcignore=self.dvcignore,
+        Args:
+            params: Dict mapping paths to `Hydra Override`_ patterns,
+                provided via `exp run --set-param`.
+            resume_rev: Optional checkpoint resume rev.
+            baseline_rev: Optional baseline rev for this experiment, defaults
+                to the current SCM rev.
+            branch: Optional experiment branch name. If specified, the
+                experiment will be added to `branch` instead of creating
+                a new branch.
+            name: Optional experiment name. If specified this will be used as
+                the human-readable name in the experiment branch ref. Has no
+                effect of branch is specified.
+
+        .. _Hydra Override:
+            https://hydra.cc/docs/next/advanced/override_grammar/basic/
+        """
+        with self.scm.stash_workspace(reinstate_index=True) as workspace:
+            with self.scm.detach_head(client="dvc") as orig_head:
+                stash_head = orig_head
+                if baseline_rev is None:
+                    baseline_rev = orig_head
+
+                try:
+                    if workspace:
+                        self.stash.apply(workspace)
+
+                    if resume_rev:
+                        # move HEAD to the resume rev so that the stashed diff
+                        # only contains changes relative to resume rev
+                        stash_head = resume_rev
+                        self.scm.set_ref(
+                            "HEAD",
+                            resume_rev,
+                            message=f"dvc: resume from HEAD {resume_rev[:7]}",
+                        )
+                        self.scm.reset()
+
+                    # update experiment params from command line
+                    if params:
+                        self._update_params(params)
+
+                    # DVC commit data deps to preserve state across workspace
+                    # & tempdir runs
+                    self._stash_commit_deps(*args, **kwargs)
+
+                    if resume_rev:
+                        if branch:
+                            branch_name = ExpRefInfo.from_ref(branch).name
+                        else:
+                            branch_name = f"{resume_rev[:7]}"
+                        if self.scm.is_dirty(untracked_files=False):
+                            ui.write(
+                                (
+                                    "Modified checkpoint experiment based on "
+                                    f"'{branch_name}' will be created"
+                                ),
+                            )
+                            branch = None
+                        elif not branch or self.scm.get_ref(branch) != resume_rev:
+                            err_msg = [
+                                "Nothing to do for unchanged checkpoint "
+                                f"'{resume_rev[:7]}'. "
+                            ]
+                            if branch:
+                                err_msg.append(
+                                    "To resume from the head of this "
+                                    "experiment, use "
+                                    f"'dvc exp apply {branch_name}'."
+                                )
+                            else:
+                                names = [
+                                    ref_info.name
+                                    for ref_info in exp_refs_by_rev(
+                                        self.scm, resume_rev
+                                    )
+                                ]
+                                if len(names) > 3:
+                                    names[3:] = [f"... ({len(names) - 3} more)"]
+                                err_msg.append(
+                                    "To resume an experiment containing this "
+                                    "checkpoint, apply one of these heads:\n"
+                                    "\t{}".format(", ".join(names))
+                                )
+                            raise DvcException("".join(err_msg))
+                        else:
+                            ui.write(
+                                "Existing checkpoint experiment "
+                                f"'{branch_name}' will be resumed"
+                            )
+                        if name:
+                            logger.warning(
+                                (
+                                    "Ignoring option '--name %s' for resumed "
+                                    "experiment. Existing experiment name will"
+                                    "be preserved instead."
+                                ),
+                                name,
+                            )
+
+                    # save additional repro command line arguments
+                    run_env = {
+                        DVC_EXP_BASELINE_REV: baseline_rev,
+                    }
+                    if not name:
+                        name = get_random_exp_name(self.scm, baseline_rev)
+                    run_env[DVC_EXP_NAME] = name
+                    if resume_rev:
+                        run_env[DVCLIVE_RESUME] = "1"
+
+                    # save studio config to read later by dvc and dvclive
+                    studio_config = get_studio_config(
+                        dvc_studio_config=self.repo.config.get("studio")
+                    )
+                    run_env = {**config_to_env(studio_config), **run_env}
+                    self._pack_args(*args, run_env=run_env, **kwargs)
+                    # save experiment as a stash commit
+                    msg = self._stash_msg(
+                        stash_head,
+                        baseline_rev=baseline_rev,
+                        branch=branch,
+                        name=name,
+                    )
+                    stash_rev = self.stash.push(message=msg)
+                    assert stash_rev
+                    logger.debug(
+                        (
+                            "Stashed experiment '%s' with baseline '%s' "
+                            "for future execution."
+                        ),
+                        stash_rev[:7],
+                        baseline_rev[:7],
+                    )
+                finally:
+                    if resume_rev:
+                        # NOTE: this set_ref + reset() is equivalent to
+                        # `git reset orig_head` (our SCM reset() only operates
+                        # on HEAD rather than any arbitrary commit)
+                        self.scm.set_ref("HEAD", orig_head, message="dvc: restore HEAD")
+                        self.scm.reset()
+                    # Revert any of our changes before prior unstashing
+                    self.scm.reset(hard=True)
+
+        return QueueEntry(
+            self.repo.root_dir,
+            self.scm.root_dir,
+            self.ref,
+            stash_rev,
+            baseline_rev,
+            branch,
+            name,
+            stash_head,
         )
-        self.hash_info = self.obj.hash_info
 
-    def set_exec(self):
-        if self.isfile() and self.meta.isexec:
-            self.odb.set_exec(self.fs_path)
-
-    def commit(self, filter_info=None):
-        if not self.exists:
-            raise self.DoesNotExistError(self)
-
-        assert self.hash_info
-
-        if self.use_cache:
-            granular = (
-                self.is_dir_checksum
-                and filter_info
-                and filter_info != self.fs_path
-            )
-            if granular:
-                obj = self._commit_granular_dir(filter_info)
-            else:
-                staging, _, obj = ostage(
-                    self.odb,
-                    filter_info or self.fs_path,
-                    self.fs,
-                    self.odb.fs.PARAM_CHECKSUM,
-                    dvcignore=self.dvcignore,
+    def _stash_commit_deps(self, *args, **kwargs):
+        if len(args):
+            targets = args[0]
+        else:
+            targets = kwargs.get("targets")
+        if isinstance(targets, str):
+            targets = [targets]
+        elif not targets:
+            targets = [None]
+        for target in targets:
+            self.repo.commit(
+                target,
+                with_deps=True,
+                recursive=kwargs.get("recursive", False),
+                force=True,
+                allow_missing=True,
+                data_only=True,
+                relink=False,
+            )
+
+    @staticmethod
+    def _stash_msg(
+        rev: str,
+        baseline_rev: str,
+        branch: Optional[str] = None,
+        name: Optional[str] = None,
+    ) -> str:
+        if not baseline_rev:
+            baseline_rev = rev
+        msg = ExpStash.format_message(rev, baseline_rev, name)
+        if branch:
+            return f"{msg}:{branch}"
+        return msg
+
+    def _pack_args(self, *args, **kwargs) -> None:
+        import pickle  # nosec B403
+
+        if os.path.exists(self.args_file) and self.scm.is_tracked(self.args_file):
+            logger.warning(
+                (
+                    "Temporary DVC file '.dvc/tmp/%s' exists and was "
+                    "likely committed to Git by mistake. It should be removed "
+                    "with:\n"
+                    "\tgit rm .dvc/tmp/%s"
+                ),
+                BaseExecutor.PACKED_ARGS_FILE,
+                BaseExecutor.PACKED_ARGS_FILE,
+            )
+            with open(self.args_file, "rb") as fobj:
+                try:
+                    data = pickle.load(fobj)  # noqa: S301  # nosec B301
+                except Exception:  # noqa: BLE001, pylint: disable=broad-except
+                    data = {}
+            extra = int(data.get("extra", 0)) + 1
+        else:
+            extra = None
+        BaseExecutor.pack_repro_args(self.args_file, *args, extra=extra, **kwargs)
+        self.scm.add(self.args_file, force=True)
+
+    @staticmethod
+    def _format_new_params_msg(new_params, config_path):
+        """Format an error message for when new parameters are identified"""
+        new_param_count = len(new_params)
+        pluralise = "s are" if new_param_count > 1 else " is"
+        param_list = ", ".join(new_params)
+        return (
+            f"{new_param_count} parameter{pluralise} missing "
+            f"from '{config_path}': {param_list}"
+        )
+
+    def _update_params(self, params: Dict[str, List[str]]):
+        """Update param files with the provided `Hydra Override`_ patterns.
+
+        Args:
+            params: Dict mapping paths to `Hydra Override`_ patterns,
+                provided via `exp run --set-param`.
+
+        .. _Hydra Override:
+            https://hydra.cc/docs/advanced/override_grammar/basic/
+        """
+        from dvc.utils.hydra import apply_overrides, compose_and_dump
+
+        logger.debug("Using experiment params '%s'", params)
+
+        hydra_config = self.repo.config.get("hydra", {})
+        hydra_enabled = hydra_config.get("enabled", False)
+        hydra_output_file = ParamsDependency.DEFAULT_PARAMS_FILE
+        for path, overrides in params.items():
+            if hydra_enabled and path == hydra_output_file:
+                config_dir = os.path.join(
+                    self.repo.root_dir, hydra_config.get("config_dir", "conf")
                 )
-                otransfer(
-                    staging,
-                    self.odb,
-                    {obj.hash_info},
-                    shallow=False,
-                    hardlink=True,
+                config_name = hydra_config.get("config_name", "config")
+                compose_and_dump(
+                    path,
+                    config_dir,
+                    config_name,
+                    overrides,
                 )
-            checkout(
-                filter_info or self.fs_path,
-                self.fs,
-                obj,
-                self.odb,
-                relink=True,
-                dvcignore=self.dvcignore,
-                state=self.repo.state,
-            )
-            self.set_exec()
-
-    def _commit_granular_dir(self, filter_info):
-        prefix = self.fs.path.parts(
-            self.fs.path.relpath(filter_info, self.fs_path)
-        )
-        staging, _, save_obj = ostage(
-            self.odb,
-            self.fs_path,
-            self.fs,
-            self.odb.fs.PARAM_CHECKSUM,
-            dvcignore=self.dvcignore,
-        )
-        save_obj = save_obj.filter(prefix)
-        checkout_obj = save_obj.get(self.odb, prefix)
-        otransfer(
-            staging,
-            self.odb,
-            {save_obj.hash_info} | {oid for _, _, oid in save_obj},
-            shallow=True,
-            hardlink=True,
-        )
-        return checkout_obj
+            else:
+                apply_overrides(path, overrides)
 
-    def dumpd(self):
-        ret = {**self.hash_info.to_dict(), **self.meta.to_dict()}
+        # Force params file changes to be staged in git
+        # Otherwise in certain situations the changes to params file may be
+        # ignored when we `git stash` them since mtime is used to determine
+        # whether the file is dirty
+        self.scm.add(list(params.keys()))
+
+    @staticmethod
+    @retry(180, errors=LockError, timeout=1)
+    def get_stash_entry(
+        exp: "Experiments",
+        queue_entry: QueueEntry,
+    ) -> "ExpStashEntry":
+        stash = ExpStash(exp.scm, queue_entry.stash_ref)
+        stash_rev = queue_entry.stash_rev
+        with get_exp_rwlock(exp.repo, writes=[queue_entry.stash_ref]):
+            stash_entry = stash.stash_revs.get(
+                stash_rev,
+                ExpStashEntry(None, stash_rev, stash_rev, None, None),
+            )
+            if stash_entry.stash_index is not None:
+                stash.drop(stash_entry.stash_index)
+        return stash_entry
+
+    @classmethod
+    def init_executor(
+        cls,
+        exp: "Experiments",
+        queue_entry: QueueEntry,
+        executor_cls: Type[BaseExecutor] = WorkspaceExecutor,
+        **kwargs,
+    ) -> BaseExecutor:
+        stash_entry = cls.get_stash_entry(exp, queue_entry)
 
-        if self.is_in_repo:
-            path = self.fs.path.as_posix(
-                relpath(self.fs_path, self.stage.wdir)
-            )
-        else:
-            path = self.def_path
+        executor = executor_cls.from_stash_entry(exp.repo, stash_entry, **kwargs)
 
-        ret[self.PARAM_PATH] = path
+        stash_rev = queue_entry.stash_rev
+        infofile = exp.celery_queue.get_infofile_path(stash_rev)
+        executor.init_git(
+            exp.repo,
+            exp.repo.scm,
+            stash_rev,
+            stash_entry,
+            infofile,
+            branch=stash_entry.branch,
+        )
 
-        if self.IS_DEPENDENCY:
-            return ret
+        executor.init_cache(exp.repo, stash_rev)
 
-        if self.desc:
-            ret[self.PARAM_DESC] = self.desc
+        return executor
 
-        if not self.use_cache:
-            ret[self.PARAM_CACHE] = self.use_cache
+    def get_infofile_path(self, name: str) -> str:
+        return os.path.join(
+            self.pid_dir,
+            name,
+            f"{name}{BaseExecutor.INFOFILE_EXT}",
+        )
 
-        if isinstance(self.metric, dict):
-            if (
-                self.PARAM_METRIC_XPATH in self.metric
-                and not self.metric[self.PARAM_METRIC_XPATH]
+    @staticmethod
+    @retry(180, errors=LockError, timeout=1)
+    def collect_git(
+        exp: "Experiments",
+        executor: BaseExecutor,
+        exec_result: "ExecutorResult",
+    ) -> Dict[str, str]:
+        results = {}
+
+        def on_diverged(ref: str, checkpoint: bool):
+            ref_info = ExpRefInfo.from_ref(ref)
+            if checkpoint:
+                raise CheckpointExistsError(ref_info.name)
+            raise ExperimentExistsError(ref_info.name)
+
+        refs = get_remote_executor_refs(exp.scm, executor.git_url)
+
+        with get_exp_rwlock(exp.repo, writes=refs):
+            for ref in executor.fetch_exps(
+                exp.scm,
+                refs,
+                force=exec_result.force,
+                on_diverged=on_diverged,
             ):
-                del self.metric[self.PARAM_METRIC_XPATH]
-
-        if self.metric:
-            ret[self.PARAM_METRIC] = self.metric
-
-        if self.plot:
-            ret[self.PARAM_PLOT] = self.plot
-
-        if self.persist:
-            ret[self.PARAM_PERSIST] = self.persist
-
-        if self.checkpoint:
-            ret[self.PARAM_CHECKPOINT] = self.checkpoint
-
-        if self.live:
-            ret[self.PARAM_LIVE] = self.live
-
-        return ret
-
-    def verify_metric(self):
-        if self.fs.scheme != "local":
-            raise DvcException(
-                f"verify metric is not supported for {self.scheme}"
-            )
-
-        if not self.metric or self.plot:
-            return
+                exp_rev = exp.scm.get_ref(ref)
+                if exp_rev:
+                    assert exec_result.exp_hash
+                    logger.debug("Collected experiment '%s'.", exp_rev[:7])
+                    results[exp_rev] = exec_result.exp_hash
+
+        return results
+
+    @classmethod
+    def collect_executor(
+        cls,
+        exp: "Experiments",
+        executor: BaseExecutor,
+        exec_result: "ExecutorResult",
+    ) -> Dict[str, str]:
+        results = cls.collect_git(exp, executor, exec_result)
 
-        if not os.path.exists(self.fs_path):
-            return
+        if exec_result.ref_info is not None:
+            executor.collect_cache(exp.repo, exec_result.ref_info)
 
-        name = "metrics" if self.metric else "plot"
-        if os.path.isdir(self.fs_path):
-            msg = "directory '%s' cannot be used as %s."
-            logger.debug(msg, str(self), name)
-            return
-
-        if not istextfile(self.fs_path, self.fs):
-            msg = "binary file '{}' cannot be used as {}."
-            raise DvcException(msg.format(self.fs_path, name))
-
-    def download(self, to, jobs=None):
-        self.fs.download(self.fs_path, to.fs_path, jobs=jobs)
-
-    def get_obj(self, filter_info=None, **kwargs):
-        if self.obj:
-            obj = self.obj
-        elif self.hash_info:
-            try:
-                obj = oload(self.odb, self.hash_info)
-            except FileNotFoundError:
-                return None
-        else:
-            return None
+        return results
 
-        fs_path = self.fs.path
-        if filter_info and filter_info != self.fs_path:
-            prefix = fs_path.relparts(filter_info, self.fs_path)
-            obj = obj.get(self.odb, prefix)
-
-        return obj
+    def match_queue_entry_by_name(
+        self,
+        exp_names: Collection[str],
+        *entries: Iterable[Union[QueueEntry, QueueDoneResult]],
+    ) -> Dict[str, Optional[QueueEntry]]:
+        from funcy import concat
+
+        entry_name_dict: Dict[str, QueueEntry] = {}
+        entry_rev_dict: Dict[str, QueueEntry] = {}
+        for entry in concat(*entries):
+            if isinstance(entry, QueueDoneResult):
+                queue_entry: QueueEntry = entry.entry
+                if entry.result is not None and entry.result.ref_info is not None:
+                    name: Optional[str] = entry.result.ref_info.name
+                else:
+                    name = queue_entry.name
+            else:
+                queue_entry = entry
+                name = queue_entry.name
+            if name:
+                entry_name_dict[name] = queue_entry
+            entry_rev_dict[queue_entry.stash_rev] = queue_entry
+
+        result: Dict[str, Optional[QueueEntry]] = {}
+        for exp_name in exp_names:
+            result[exp_name] = None
+            if exp_name in entry_name_dict:
+                result[exp_name] = entry_name_dict[exp_name]
+                continue
+            if self.scm.is_sha(exp_name):
+                for rev in entry_rev_dict:
+                    if rev.startswith(exp_name.lower()):
+                        result[exp_name] = entry_rev_dict[rev]
+                        break
+
+        return result
+
+    def stash_failed(self, entry: QueueEntry) -> None:
+        """Add an entry to the failed exp stash.
+
+        Arguments:
+            entry: Failed queue entry to add. ``entry.stash_rev`` must be a
+                valid Git stash commit.
+        """
+        if self.failed_stash is not None:
+            assert entry.head_rev
+            logger.debug("Stashing failed exp '%s'", entry.stash_rev[:7])
+            msg = self.failed_stash.format_message(
+                entry.head_rev,
+                baseline_rev=entry.baseline_rev,
+                name=entry.name,
+                branch=entry.branch,
+            )
+            self.scm.set_ref(
+                self.failed_stash.ref,
+                entry.stash_rev,
+                message=f"commit: {msg}",
+            )
+
+    @abstractmethod
+    def get_running_exps(self, fetch_refs: bool = True) -> Dict[str, Dict]:
+        """Get the execution info of the currently running experiments
+
+        Args:
+            fetch_ref (bool): fetch completed checkpoints or not.
+        """
 
-    def checkout(
+    @abstractmethod
+    def collect_active_data(
         self,
-        force=False,
-        progress_callback=None,
-        relink=False,
-        filter_info=None,
-        allow_missing=False,
-        checkpoint_reset=False,
+        baseline_revs: Optional[Collection[str]],
+        fetch_refs: bool = False,
         **kwargs,
-    ):
-        if not self.use_cache:
-            if progress_callback:
-                progress_callback(
-                    self.fs_path, self.get_files_number(filter_info)
-                )
-            return None
-
-        obj = self.get_obj(filter_info=filter_info)
-        if not obj and (filter_info and filter_info != self.fs_path):
-            # backward compatibility
-            return None
-
-        if self.checkpoint and checkpoint_reset:
-            if self.exists:
-                self.remove()
-            return None
-
-        added = not self.exists
-
-        try:
-            modified = checkout(
-                filter_info or self.fs_path,
-                self.fs,
-                obj,
-                self.odb,
-                force=force,
-                progress_callback=progress_callback,
-                relink=relink,
-                state=self.repo.state,
-                **kwargs,
-            )
-        except CheckoutError:
-            if allow_missing or self.checkpoint:
-                return None
-            raise
-        self.set_exec()
-        return added, False if added else modified
-
-    def remove(self, ignore_remove=False):
-        self.fs.remove(self.fs_path)
-        if self.scheme != Schemes.LOCAL:
-            return
-
-        if ignore_remove:
-            self.ignore_remove()
-
-    def move(self, out):
-        # pylint: disable=no-member
-        if self.scheme == "local" and self.use_scm_ignore:
-            self.repo.scm_context.ignore_remove(self.fspath)
-
-        self.fs.move(self.fs_path, out.fs_path)
-        self.def_path = out.def_path
-        self.fs_path = out.fs_path
-        self.save()
-        self.commit()
+    ) -> Dict[str, List["ExpRange"]]:
+        """Collect data for active (running) experiments.
 
-        if self.scheme == "local" and self.use_scm_ignore:
-            self.repo.scm_context.ignore(self.fspath)
+        Args:
+            baseline_revs: Optional resolved baseline Git SHAs. If set, only experiments
+                derived from the specified revisions will be collected. Defaults to
+                collecting all experiments.
+            fetch_refs: Whether or not to fetch completed checkpoint commits from Git
+                remote.
+
+        Returns:
+            Dict mapping baseline revision to list of active experiments.
+        """
 
-    def transfer(
-        self, source, odb=None, jobs=None, update=False, no_progress_bar=False
-    ):
-        if odb is None:
-            odb = self.odb
-
-        cls, config, from_info = get_cloud_fs(self.repo, url=source)
-        from_fs = cls(**config)
-
-        # When running import-url --to-remote / add --to-remote/-o ... we
-        # assume that it is unlikely that the odb will contain majority of the
-        # hashes, so we transfer everything as is (even if that file might
-        # already be in the cache) and don't waste an upload to scan the layout
-        # of the source location. But when doing update --to-remote, there is
-        # a high probability that the odb might contain some of the hashes, so
-        # we first calculate all the hashes (but don't transfer anything) and
-        # then only update the missing cache files.
-
-        upload = not (update and from_fs.isdir(from_info))
-        jobs = jobs or min((from_fs.jobs, odb.fs.jobs))
-        staging, self.meta, obj = ostage(
-            odb,
-            from_info,
-            from_fs,
-            "md5",
-            upload=upload,
-            jobs=jobs,
-            no_progress_bar=no_progress_bar,
-        )
-        otransfer(
-            staging,
-            odb,
-            {obj.hash_info},
-            jobs=jobs,
-            hardlink=False,
-            shallow=False,
-        )
-
-        self.hash_info = obj.hash_info
-        return obj
-
-    def get_files_number(self, filter_info=None):
-        if not self.use_cache or not self.hash_info:
-            return 0
-
-        if not self.hash_info.isdir:
-            return 1
-
-        if not filter_info or filter_info == self.fs_path:
-            return self.meta.nfiles or 0
-
-        obj = self.get_obj(filter_info=filter_info)
-        return len(obj) if obj else 0
-
-    def unprotect(self):
-        if self.exists:
-            self.odb.unprotect(self.fs_path)
-
-    def get_dir_cache(self, **kwargs):
-        if not self.is_dir_checksum:
-            raise DvcException("cannot get dir cache for file checksum")
-
-        obj = self.odb.get(self.hash_info)
-        try:
-            ocheck(self.odb, obj)
-        except FileNotFoundError:
-            if self.remote:
-                kwargs["remote"] = self.remote
-            self.repo.cloud.pull([obj.hash_info], **kwargs)
-
-        if self.obj:
-            return self.obj
-
-        try:
-            self.obj = oload(self.odb, self.hash_info)
-        except (FileNotFoundError, ObjectFormatError):
-            self.obj = None
-
-        return self.obj
-
-    def _collect_used_dir_cache(
-        self, remote=None, force=False, jobs=None, filter_info=None
-    ) -> Optional["Tree"]:
-        """Fetch dir cache and return used object IDs for this out."""
-
-        try:
-            self.get_dir_cache(jobs=jobs, remote=remote)
-        except DvcException:
-            logger.debug(f"failed to pull cache for '{self}'")
-
-        try:
-            ocheck(self.odb, self.odb.get(self.hash_info))
-        except FileNotFoundError:
-            msg = (
-                "Missing cache for directory '{}'. "
-                "Cache for files inside will be lost. "
-                "Would you like to continue? Use '-f' to force."
-            )
-            if not force and not prompt.confirm(msg.format(self.fs_path)):
-                raise CollectCacheError(
-                    "unable to fully collect used cache"
-                    " without cache for directory '{}'".format(self)
-                )
-            return None
-
-        obj = self.get_obj()
-        if filter_info and filter_info != self.fs_path:
-            prefix = self.fs.path.parts(
-                self.fs.path.relpath(filter_info, self.fs_path)
-            )
-            obj = obj.filter(prefix)
-        return obj
-
-    def get_used_objs(
-        self, **kwargs
-    ) -> Dict[Optional["ObjectDB"], Set["HashInfo"]]:
-        """Return filtered set of used object IDs for this out."""
-
-        if not self.use_cache:
-            return {}
-
-        if self.stage.is_repo_import:
-            return self.get_used_external(**kwargs)
-
-        if not self.hash_info:
-            msg = (
-                "Output '{}'({}) is missing version info. "
-                "Cache for it will not be collected. "
-                "Use `dvc repro` to get your pipeline up to date.".format(
-                    self, self.stage
-                )
-            )
-            if self.exists:
-                msg += (
-                    "\n"
-                    "You can also use `dvc commit {stage.addressing}` "
-                    "to associate existing '{out}' with {stage}.".format(
-                        out=self, stage=self.stage
-                    )
-                )
-            logger.warning(msg)
-            return {}
-
-        if self.is_dir_checksum:
-            obj = self._collect_used_dir_cache(**kwargs)
-        else:
-            obj = self.get_obj(filter_info=kwargs.get("filter_info"))
-            if not obj:
-                obj = self.odb.get(self.hash_info)
-
-        if not obj:
-            return {}
-
-        if self.remote:
-            remote = self.repo.cloud.get_remote_odb(name=self.remote)
-        else:
-            remote = None
-
-        return {remote: self._named_obj_ids(obj)}
-
-    def _named_obj_ids(self, obj):
-        name = str(self)
-        obj.hash_info.obj_name = name
-        oids = {obj.hash_info}
-        if isinstance(obj, Tree):
-            for key, _, oid in obj:
-                oid.obj_name = self.fs.sep.join([name, *key])
-                oids.add(oid)
-        return oids
-
-    def get_used_external(
-        self, **kwargs
-    ) -> Dict[Optional["ObjectDB"], Set["HashInfo"]]:
-        if not self.use_cache or not self.stage.is_repo_import:
-            return {}
-
-        (dep,) = self.stage.deps
-        return dep.get_used_objs(**kwargs)
-
-    def _validate_output_path(self, path, stage=None):
-        from dvc.dvcfile import is_valid_filename
-
-        if is_valid_filename(path):
-            raise self.IsStageFileError(path)
-
-        if stage:
-            abs_path = os.path.join(stage.wdir, path)
-            if self._is_path_dvcignore(abs_path):
-                check = stage.repo.dvcignore.check_ignore(abs_path)
-                raise self.IsIgnoredError(check)
-
-    def _check_can_merge(self, out):
-        if self.scheme != out.scheme:
-            raise MergeError("unable to auto-merge outputs of different types")
-
-        my = self.dumpd()
-        other = out.dumpd()
-
-        ignored = [
-            self.fs.PARAM_CHECKSUM,
-            Meta.PARAM_SIZE,
-            Meta.PARAM_NFILES,
-        ]
-
-        for opt in ignored:
-            my.pop(opt, None)
-            other.pop(opt, None)
-
-        if my != other:
-            raise MergeError(
-                "unable to auto-merge outputs with different options"
-            )
-
-        if not out.is_dir_checksum:
-            raise MergeError(
-                "unable to auto-merge outputs that are not directories"
-            )
-
-    def merge(self, ancestor, other):
-        from dvc.data.tree import du, merge
-
-        assert other
-
-        if ancestor:
-            self._check_can_merge(ancestor)
-            ancestor_info = ancestor.hash_info
-        else:
-            ancestor_info = None
-
-        self._check_can_merge(self)
-        self._check_can_merge(other)
-
-        merged = merge(
-            self.odb, ancestor_info, self.hash_info, other.hash_info
-        )
-        self.odb.add(merged.fs_path, merged.fs, merged.hash_info)
-
-        self.hash_info = merged.hash_info
-        self.meta = Meta(
-            size=du(self.odb, merged),
-            nfiles=len(merged),
-        )
-
-    @property
-    def fspath(self):
-        return self.fs_path
-
-    @property
-    def is_decorated(self) -> bool:
-        return self.is_metric or self.is_plot
-
-    @property
-    def is_metric(self) -> bool:
-        return bool(self.metric) or bool(self.live)
+    @abstractmethod
+    def collect_queued_data(
+        self,
+        baseline_revs: Optional[Collection[str]],
+        **kwargs,
+    ) -> Dict[str, List["ExpRange"]]:
+        """Collect data for queued experiments.
 
-    @property
-    def is_plot(self) -> bool:
-        return bool(self.plot)
+        Args:
+            baseline_revs: Optional resolved baseline Git SHAs. If set, only experiments
+                derived from the specified revisions will be collected. Defaults to
+                collecting all experiments.
+
+        Returns:
+            Dict mapping baseline revision to list of queued experiments.
+        """
 
+    @abstractmethod
+    def collect_failed_data(
+        self,
+        baseline_revs: Optional[Collection[str]],
+        **kwargs,
+    ) -> Dict[str, List["ExpRange"]]:
+        """Collect data for failed experiments.
 
-ARTIFACT_SCHEMA = {
-    **CHECKSUMS_SCHEMA,
-    Required(Output.PARAM_PATH): str,
-    Output.PARAM_PLOT: bool,
-    Output.PARAM_PERSIST: bool,
-    Output.PARAM_CHECKPOINT: bool,
-    Meta.PARAM_SIZE: int,
-    Meta.PARAM_NFILES: int,
-    Meta.PARAM_ISEXEC: bool,
-}
-
-SCHEMA = {
-    **ARTIFACT_SCHEMA,
-    Output.PARAM_CACHE: bool,
-    Output.PARAM_METRIC: Output.METRIC_SCHEMA,
-    Output.PARAM_DESC: str,
-    Output.PARAM_REMOTE: str,
-}
+        Args:
+            baseline_revs: Optional resolved baseline Git SHAs. If set, only experiments
+                derived from the specified revisions will be collected. Defaults to
+                collecting all experiments.
+
+        Returns:
+            Dict mapping baseline revision to list of queued experiments.
+        """
```

### Comparing `dvc-2.9.5/dvc/parsing/__init__.py` & `dvc-3.0.0a0/dvc/parsing/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,62 +1,65 @@
 import logging
+import os
 from collections.abc import Mapping, Sequence
 from copy import deepcopy
 from typing import (
     TYPE_CHECKING,
     Any,
     Dict,
     List,
     NamedTuple,
     Optional,
     Tuple,
     Type,
     Union,
 )
 
-from funcy import cached_property, collecting, first, isa, join, reraise
+from funcy import collecting, first, isa, join, reraise
 
 from dvc.exceptions import DvcException
 from dvc.parsing.interpolate import ParseError
-from dvc.utils import relpath
+from dvc.utils.objects import cached_property
 
 from .context import (
     Context,
     ContextError,
     KeyNotInContext,
     MergeError,
     Node,
-    SeqOrMap,
     VarsAlreadyLoaded,
 )
 from .interpolate import (
     check_recursive_parse_errors,
     is_interpolated_string,
     recurse,
     to_str,
 )
 
 if TYPE_CHECKING:
-    from typing_extensions import NoReturn
+    from typing import NoReturn
 
     from dvc.repo import Repo
+    from dvc.types import DictStrAny
+
+    from .context import SeqOrMap
+
 
 logger = logging.getLogger(__name__)
 
 STAGES_KWD = "stages"
 VARS_KWD = "vars"
 WDIR_KWD = "wdir"
 PARAMS_KWD = "params"
 FOREACH_KWD = "foreach"
 DO_KWD = "do"
 
 DEFAULT_PARAMS_FILE = "params.yaml"
 
 JOIN = "@"
-DictStr = Dict[str, Any]
 
 
 class ResolveError(DvcException):
     pass
 
 
 class EntryNotFound(DvcException):
@@ -65,36 +68,34 @@
 
 def _format_preamble(msg: str, path: str, spacing: str = " ") -> str:
     return f"failed to parse {msg} in '{path}':{spacing}"
 
 
 def format_and_raise(exc: Exception, msg: str, path: str) -> "NoReturn":
     spacing = (
-        "\n"
-        if isinstance(exc, (ParseError, MergeError, VarsAlreadyLoaded))
-        else " "
+        "\n" if isinstance(exc, (ParseError, MergeError, VarsAlreadyLoaded)) else " "
     )
     message = _format_preamble(msg, path, spacing) + str(exc)
 
     # FIXME: cannot reraise because of how we log "cause" of the exception
     # the error message is verbose, hence need control over the spacing
     _reraise_err(ResolveError, message, from_exc=exc)
 
 
 def _reraise_err(
-    exc_cls: Type[Exception], *args, from_exc: Exception = None
+    exc_cls: Type[Exception], *args, from_exc: Optional[Exception] = None
 ) -> "NoReturn":
     err = exc_cls(*args)
     if from_exc and logger.isEnabledFor(logging.DEBUG):
         raise err from from_exc
     raise err
 
 
 def check_syntax_errors(
-    definition: DictStr, name: str, path: str, where: str = "stages"
+    definition: "DictStrAny", name: str, path: str, where: str = "stages"
 ):
     for key, d in definition.items():
         try:
             check_recursive_parse_errors(d)
         except ParseError as exc:
             format_and_raise(exc, f"'{where}.{name}.{key}'", path)
 
@@ -105,42 +106,46 @@
 
 
 def split_foreach_name(name: str) -> Tuple[str, Optional[str]]:
     group, *keys = name.rsplit(JOIN, maxsplit=1)
     return group, first(keys)
 
 
-def check_interpolations(data: DictStr, where: str, path: str):
-    def func(s: DictStr) -> None:
+def check_interpolations(data: "DictStrAny", where: str, path: str):
+    def func(s: "DictStrAny") -> None:
         if is_interpolated_string(s):
             raise ResolveError(
-                _format_preamble(f"'{where}'", path)
-                + "interpolating is not allowed"
+                _format_preamble(f"'{where}'", path) + "interpolating is not allowed"
             )
 
     return recurse(func)(data)
 
 
 Definition = Union["ForeachDefinition", "EntryDefinition"]
 
 
 def make_definition(
-    resolver: "DataResolver", name: str, definition: DictStr, **kwargs
+    resolver: "DataResolver", name: str, definition: "DictStrAny", **kwargs
 ) -> Definition:
     args = resolver, resolver.context, name, definition
     if FOREACH_KWD in definition:
         return ForeachDefinition(*args, **kwargs)
     return EntryDefinition(*args, **kwargs)
 
 
 class DataResolver:
     def __init__(self, repo: "Repo", wdir: str, d: dict):
         self.fs = fs = repo.fs
+
+        if os.path.isabs(wdir):
+            wdir = fs.path.relpath(wdir)
+            wdir = "" if wdir == os.curdir else wdir
+
         self.wdir = wdir
-        self.relpath = relpath(fs.path.join(self.wdir, "dvc.yaml"))
+        self.relpath = fs.path.normpath(fs.path.join(self.wdir, "dvc.yaml"))
 
         vars_ = d.get(VARS_KWD, [])
         check_interpolations(vars_, VARS_KWD, self.relpath)
         self.context: Context = Context()
 
         try:
             args = fs, vars_, wdir  # load from `vars` section
@@ -176,32 +181,30 @@
 
         assert key
         return definition.resolve_one(key)
 
     def resolve(self):
         """Used for testing purposes, otherwise use resolve_one()."""
         data = join(map(self.resolve_one, self.get_keys()))
-        logger.trace(  # type: ignore[attr-defined]
-            "Resolved dvc.yaml:\n%s", data
-        )
+        logger.trace("Resolved dvc.yaml:\n%s", data)  # type: ignore[attr-defined]
         return {STAGES_KWD: data}
 
     def has_key(self, key: str):
         return self._has_group_and_key(*split_foreach_name(key))
 
-    def _has_group_and_key(self, group: str, key: str = None):
+    def _has_group_and_key(self, group: str, key: Optional[str] = None):
         try:
             definition = self.definitions[group]
         except KeyError:
             return False
 
         if key:
-            return isinstance(
-                definition, ForeachDefinition
-            ) and definition.has_member(key)
+            return isinstance(definition, ForeachDefinition) and definition.has_member(
+                key
+            )
         return not isinstance(definition, ForeachDefinition)
 
     @collecting
     def get_keys(self):
         for name, definition in self.definitions.items():
             if isinstance(definition, ForeachDefinition):
                 yield from definition.get_generated_names()
@@ -214,27 +217,27 @@
 
 class EntryDefinition:
     def __init__(
         self,
         resolver: DataResolver,
         context: Context,
         name: str,
-        definition: DictStr,
+        definition: "DictStrAny",
         where: str = STAGES_KWD,
     ):
         self.resolver = resolver
         self.wdir = self.resolver.wdir
         self.relpath = self.resolver.relpath
         self.context = context
         self.name = name
         self.definition = definition
         self.where = where
 
     def _resolve_wdir(
-        self, context: Context, name: str, wdir: str = None
+        self, context: Context, name: str, wdir: Optional[str] = None
     ) -> str:
         if not wdir:
             return self.wdir
 
         try:
             wdir = to_str(context.resolve_str(wdir))
         except (ContextError, ParseError) as exc:
@@ -243,15 +246,15 @@
 
     def resolve(self, **kwargs):
         try:
             return self.resolve_stage(**kwargs)
         except ContextError as exc:
             format_and_raise(exc, f"stage '{self.name}'", self.relpath)
 
-    def resolve_stage(self, skip_checks: bool = False) -> DictStr:
+    def resolve_stage(self, skip_checks: bool = False) -> "DictStrAny":
         context = self.context
         name = self.name
         if not skip_checks:
             # we can check for syntax errors as we go for interpolated entries,
             # but for foreach-generated ones, once is enough, which it does
             # that itself. See `ForeachDefinition.do_definition`.
             check_syntax_errors(self.definition, name, self.relpath)
@@ -291,37 +294,35 @@
             }
 
         self.resolver.track_vars(name, tracked_data)
         return {name: resolved}
 
     def _resolve(
         self, context: "Context", value: Any, key: str, skip_checks: bool
-    ) -> DictStr:
+    ) -> "DictStrAny":
         try:
             return context.resolve(
-                value, skip_interpolation_checks=skip_checks
+                value, skip_interpolation_checks=skip_checks, key=key
             )
         except (ParseError, KeyNotInContext) as exc:
-            format_and_raise(
-                exc, f"'{self.where}.{self.name}.{key}'", self.relpath
-            )
+            format_and_raise(exc, f"'{self.where}.{self.name}.{key}'", self.relpath)
 
 
 class IterationPair(NamedTuple):
     key: str = "key"
     value: str = "item"
 
 
 class ForeachDefinition:
     def __init__(
         self,
         resolver: DataResolver,
         context: Context,
         name: str,
-        definition: DictStr,
+        definition: "DictStrAny",
         where: str = STAGES_KWD,
     ):
         self.resolver = resolver
         self.relpath = self.resolver.relpath
         self.context = context
         self.name = name
 
@@ -338,21 +339,19 @@
         check_syntax_errors(self._do_definition, self.name, self.relpath)
         return self._do_definition
 
     @cached_property
     def resolved_iterable(self):
         return self._resolve_foreach_data()
 
-    def _resolve_foreach_data(self) -> SeqOrMap:
+    def _resolve_foreach_data(self) -> "SeqOrMap":
         try:
             iterable = self.context.resolve(self.foreach_data, unwrap=False)
         except (ContextError, ParseError) as exc:
-            format_and_raise(
-                exc, f"'{self.where}.{self.name}.foreach'", self.relpath
-            )
+            format_and_raise(exc, f"'{self.where}.{self.name}.foreach'", self.relpath)
 
         # foreach data can be a resolved dictionary/list.
         self._check_is_map_or_seq(iterable)
         # foreach stages will have `item` and `key` added to the context
         # so, we better warn them if they have them already in the context
         # from the global vars. We could add them in `set_temporarily`, but
         # that'd make it display for each iteration.
@@ -369,16 +368,18 @@
             )
 
     def _warn_if_overwriting(self, keys: List[str]):
         warn_for = [k for k in keys if k in self.context]
         if warn_for:
             linking_verb = "is" if len(warn_for) == 1 else "are"
             logger.warning(
-                "%s %s already specified, "
-                "will be overwritten for stages generated from '%s'",
+                (
+                    "%s %s already specified, "
+                    "will be overwritten for stages generated from '%s'"
+                ),
                 " and ".join(warn_for),
                 linking_verb,
                 self.name,
             )
 
     def _inserted_keys(self, iterable) -> List[str]:
         keys = [self.pair.value]
@@ -387,15 +388,15 @@
         return keys
 
     @cached_property
     def normalized_iterable(self):
         """Convert sequence to Mapping with keys normalized."""
         iterable = self.resolved_iterable
         if isinstance(iterable, Mapping):
-            return iterable
+            return {to_str(k): v for k, v in iterable.items()}
 
         assert isinstance(iterable, Sequence)
         if any(map(is_map_or_seq, iterable)):
             # if the list contains composite data, index are the keys
             return {to_str(idx): value for idx, value in enumerate(iterable)}
 
         # for simple lists, eg: ["foo", "bar"],  contents are the key itself
@@ -406,21 +407,21 @@
 
     def get_generated_names(self):
         return list(map(self._generate_name, self.normalized_iterable))
 
     def _generate_name(self, key: str) -> str:
         return f"{self.name}{JOIN}{key}"
 
-    def resolve_all(self) -> DictStr:
+    def resolve_all(self) -> "DictStrAny":
         return join(map(self.resolve_one, self.normalized_iterable))
 
-    def resolve_one(self, key: str) -> DictStr:
+    def resolve_one(self, key: str) -> "DictStrAny":
         return self._each_iter(key)
 
-    def _each_iter(self, key: str) -> DictStr:
+    def _each_iter(self, key: str) -> "DictStrAny":
         err_message = f"Could not find '{key}' in foreach group '{self.name}'"
         with reraise(KeyError, EntryNotFound(err_message)):
             value = self.normalized_iterable[key]
 
         # NOTE: we need to use resolved iterable/foreach-data,
         # not the normalized ones to figure out whether to make item/key
         # available
```

### Comparing `dvc-2.9.5/dvc/parsing/context.py` & `dvc-3.0.0a0/dvc/parsing/context.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 import logging
-import os
 from abc import ABC, abstractmethod
 from collections import defaultdict
 from collections.abc import Mapping, MutableMapping, MutableSequence, Sequence
 from contextlib import contextmanager
 from copy import deepcopy
 from dataclasses import dataclass, field, replace
 from typing import Any, Dict, List, Optional, Union
@@ -14,16 +13,16 @@
 from dvc.parsing.interpolate import (
     get_expression,
     get_matches,
     is_exact_string,
     normalize_key,
     recurse,
     str_interpolate,
+    validate_value,
 )
-from dvc.utils import relpath
 
 logger = logging.getLogger(__name__)
 SeqOrMap = Union[Sequence, Mapping]
 DictStr = Dict[str, Any]
 
 
 class ContextError(DvcException):
@@ -45,17 +44,15 @@
 
 
 class MergeError(ContextError):
     def __init__(self, key, new, into):
         self.key = key
         to_node = into[key]
         if not isinstance(to_node, Node) or not isinstance(new, Node):
-            super().__init__(
-                f"cannot merge '{key}' as it already exists in {into}"
-            )
+            super().__init__(f"cannot merge '{key}' as it already exists in {into}")
             return
 
         assert isinstance(to_node, Node)
         assert isinstance(new, Node)
         preexisting = to_node.meta.source
         new_src = new.meta.source
         path = new.meta.path()
@@ -135,17 +132,15 @@
     def value(self):
         pass
 
 
 @dataclass
 class Value(Node):
     _value: Any
-    meta: Meta = field(
-        compare=False, default_factory=_default_meta, repr=False
-    )
+    meta: Meta = field(compare=False, default_factory=_default_meta, repr=False)
 
     def __repr__(self):
         return repr(self._value)
 
     def __str__(self) -> str:
         return str(self._value)
 
@@ -169,28 +164,25 @@
         self.meta = meta or _default_meta()
 
     def _convert(self, key, value):
         meta = Meta.update_path(self.meta, key)
         return self._convert_with_meta(value, meta)
 
     @staticmethod
-    def _convert_with_meta(value, meta: Meta = None):
+    def _convert_with_meta(value, meta: Optional[Meta] = None):
         if value is None or isinstance(value, PRIMITIVES):
             assert meta
             return Value(value, meta=meta)
         if isinstance(value, Node):
             return value
         if isinstance(value, (list, dict)):
             assert meta
             container = CtxDict if isinstance(value, dict) else CtxList
             return container(value, meta=meta)
-        msg = (
-            "Unsupported value of type "
-            f"'{type(value).__name__}' in '{meta}'"
-        )
+        msg = f"Unsupported value of type '{type(value).__name__}' in '{meta}'"
         raise TypeError(msg)
 
     def __repr__(self):
         return repr(self.data)
 
     def __getitem__(self, key):
         return self.data[key]
@@ -216,36 +208,34 @@
     def select(self, key: str):
         index, *rems = key.split(sep=".", maxsplit=1)
         index = index.strip()
         index = self._key_transform(index)
         try:
             d = self[index]
         except LookupError as exc:
-            raise ValueError(
-                f"Could not find '{index}' in {self.data}"
-            ) from exc
+            raise ValueError(f"Could not find '{index}' in {self.data}") from exc
 
         if not rems:
             return d
 
         rem = rems[0]
         if not isinstance(d, Container):
-            raise ValueError(
+            raise ValueError(  # noqa: TRY004
                 f"{index} is a primitive value, cannot get '{rem}'"
             )
         return d.select(rem)
 
     def get_sources(self):
         return {}
 
 
 class CtxList(Container, MutableSequence):
     _key_transform = staticmethod(int)
 
-    def __init__(self, values: Sequence, meta: Meta = None):
+    def __init__(self, values: Sequence, meta: Optional[Meta] = None):
         super().__init__(meta=meta)
         self.data: list = []
         self.extend(values)
 
     def insert(self, index: int, value):
         self.data.insert(index, self._convert(index, value))
 
@@ -260,15 +250,20 @@
         # optimization: we don't support overriding a list
         new = CtxList([])
         new.data = self.data[:]  # Short-circuiting __setitem__
         return new
 
 
 class CtxDict(Container, MutableMapping):
-    def __init__(self, mapping: Mapping = None, meta: Meta = None, **kwargs):
+    def __init__(
+        self,
+        mapping: Optional[Mapping] = None,
+        meta: Optional[Meta] = None,
+        **kwargs,
+    ):
         super().__init__(meta=meta)
 
         self.data: dict = {}
         if mapping:
             self.update(mapping)
         self.update(kwargs)
 
@@ -298,15 +293,15 @@
 class Context(CtxDict):
     def __init__(self, *args, **kwargs):
         """
         Top level mutable dict, with some helpers to create context and track
         """
         super().__init__(*args, **kwargs)
         self._track = False
-        self._tracked_data = defaultdict(dict)
+        self._tracked_data: Dict[str, Dict] = defaultdict(dict)
         self.imports = {}
         self._reserved_keys = {}
 
     @contextmanager
     def track(self):
         self._track = True
         yield self._tracked_data
@@ -351,99 +346,93 @@
 
         assert isinstance(node, Node)
         self._track_data(node)
         return node.value if unwrap else node
 
     @classmethod
     def load_from(
-        cls, fs, path: str, select_keys: List[str] = None
+        cls, fs, path: str, select_keys: Optional[List[str]] = None
     ) -> "Context":
-        from dvc.utils.serialize import LOADERS
+        from dvc.utils.serialize import load_path
 
-        file = relpath(path)
         if not fs.exists(path):
-            raise ParamsLoadError(f"'{file}' does not exist")
+            raise ParamsLoadError(f"'{path}' does not exist")
         if fs.isdir(path):
-            raise ParamsLoadError(f"'{file}' is a directory")
-
-        _, ext = os.path.splitext(file)
-        loader = LOADERS[ext]
+            raise ParamsLoadError(f"'{path}' is a directory")
 
-        data = loader(path, fs=fs)
+        data = load_path(path, fs)
         if not isinstance(data, Mapping):
             typ = type(data).__name__
             raise ParamsLoadError(
-                f"expected a dictionary, got '{typ}' in file '{file}'"
+                f"expected a dictionary, got '{typ}' in file '{path}'"
             )
 
         if select_keys:
             try:
                 data = {key: data[key] for key in select_keys}
             except KeyError as exc:
                 key, *_ = exc.args
-                raise ParamsLoadError(
-                    f"could not find '{key}' in '{file}'"
-                ) from exc
+                raise ParamsLoadError(f"could not find '{key}' in '{path}'") from exc
 
-        meta = Meta(source=file, local=False)
+        meta = Meta(source=path, local=False)
         ctx = cls(data, meta=meta)
-        ctx.imports[os.path.abspath(path)] = select_keys
+        ctx.imports[path] = select_keys
         return ctx
 
     def merge_update(self, other: "Context", overwrite=False):
         matches = select(lambda key: key in other, self._reserved_keys.keys())
         if matches:
             raise ReservedKeyError(matches)
         return super().merge_update(other, overwrite=overwrite)
 
     def merge_from(self, fs, item: str, wdir: str, overwrite=False):
         path, _, keys_str = item.partition(":")
-        select_keys = lfilter(bool, keys_str.split(",")) if keys_str else None
+        path = fs.path.normpath(fs.path.join(wdir, path))
 
-        abspath = os.path.abspath(fs.path.join(wdir, path))
-        if abspath in self.imports:
-            if not select_keys and self.imports[abspath] is None:
+        select_keys = lfilter(bool, keys_str.split(",")) if keys_str else None
+        if path in self.imports:
+            if not select_keys and self.imports[path] is None:
                 return  # allow specifying complete filepath multiple times
-            self.check_loaded(abspath, item, select_keys)
+            self.check_loaded(path, item, select_keys)
 
-        ctx = Context.load_from(fs, abspath, select_keys)
+        ctx = Context.load_from(fs, path, select_keys)
 
         try:
             self.merge_update(ctx, overwrite=overwrite)
         except ReservedKeyError as exc:
             raise ReservedKeyError(exc.keys, item) from exc
 
-        cp = ctx.imports[abspath]
-        if abspath not in self.imports:
-            self.imports[abspath] = cp
+        cp = ctx.imports[path]
+        if path not in self.imports:
+            self.imports[path] = cp
         elif cp:
-            self.imports[abspath].extend(cp)
+            self.imports[path].extend(cp)
 
     def check_loaded(self, path, item, keys):
-        if not keys and isinstance(self.imports[path], list):
+        imported = self.imports[path]
+        if not keys and isinstance(imported, list):
             raise VarsAlreadyLoaded(
                 f"cannot load '{item}' as it's partially loaded already"
             )
-        elif keys and self.imports[path] is None:
+        if keys and imported is None:
             raise VarsAlreadyLoaded(
                 f"cannot partially load '{item}' as it's already loaded."
             )
-        elif isinstance(self.imports[path], list):
-            if not set(keys).isdisjoint(set(self.imports[path])):
-                raise VarsAlreadyLoaded(
-                    f"cannot load '{item}' as it's partially loaded already"
-                )
+        if isinstance(imported, list) and set(keys) & set(imported):
+            raise VarsAlreadyLoaded(
+                f"cannot load '{item}' as it's partially loaded already"
+            )
 
     def load_from_vars(
         self,
         fs,
         vars_: List,
         wdir: str,
-        stage_name: str = None,
-        default: str = None,
+        stage_name: Optional[str] = None,
+        default: Optional[str] = None,
     ):
         if default:
             to_import = fs.path.join(wdir, default)
             if fs.exists(to_import):
                 self.merge_from(fs, default, wdir)
             else:
                 msg = "%s does not exist, it won't be used in parametrization"
@@ -476,22 +465,20 @@
         """Allow reserving some keys so that they cannot be overwritten.
 
         Ideally, we should delegate this to a separate container
         and support proper namespacing so that we could support `env` features.
         But for now, just `item` and `key`, this should do.
         """
         # using dict to make the error messages ordered
-        new = dict.fromkeys(
-            [key for key in keys if key not in self._reserved_keys]
-        )
+        new = dict.fromkeys([key for key in keys if key not in self._reserved_keys])
         self._reserved_keys.update(new)
         try:
             yield
         finally:
-            for key in new.keys():
+            for key in new:
                 self._reserved_keys.pop(key)
 
     @contextmanager
     def set_temporarily(self, to_set: DictStr, reserve: bool = False):
         cm = self.reserved(*to_set) if reserve else nullcontext()
 
         non_existing = frozenset(to_set.keys() - self.keys())
@@ -504,15 +491,15 @@
                 yield
         finally:
             self.update(prev)
             for key in non_existing:
                 self.data.pop(key, None)
 
     def resolve(
-        self, src, unwrap=True, skip_interpolation_checks=False
+        self, src, unwrap=True, skip_interpolation_checks=False, key=None
     ) -> Any:
         """Recursively resolves interpolation and returns resolved data.
 
         Args:
             src: Data (str/list/dict etc.) to resolve
             unwrap: Unwrap CtxDict/CtxList/Value to it's original data if
                 inside `src`. Defaults to True.
@@ -520,39 +507,39 @@
                 The callee is responsible to check for errors in advance.
 
         >>> c = Context({"three": 3})
         >>> c.resolve({"lst": [1, 2, "${three}"]})
         {'lst': [1, 2, 3]}
         """
         func = recurse(self.resolve_str)
-        return func(src, unwrap, skip_interpolation_checks)
+        return func(src, unwrap, skip_interpolation_checks, key)
 
     def resolve_str(
-        self, src: str, unwrap=True, skip_interpolation_checks=False
+        self, src: str, unwrap=True, skip_interpolation_checks=False, key=None
     ) -> str:
         """Resolves interpolated string to it's original value,
         or in case of multiple interpolations, a combined string.
 
         >>> c = Context({"enabled": True})
         >>> c.resolve_str("${enabled}")
         True
         >>> c.resolve_str("enabled? ${enabled}")
         'enabled? true'
         """
         matches = get_matches(src)
         if is_exact_string(src, matches):
             # replace "${enabled}", if `enabled` is a boolean, with it's actual
             # value rather than it's string counterparts.
-            expr = get_expression(
-                matches[0], skip_checks=skip_interpolation_checks
-            )
-            return self.select(expr, unwrap=unwrap)
+            expr = get_expression(matches[0], skip_checks=skip_interpolation_checks)
+            value = self.select(expr, unwrap=unwrap)
+            validate_value(value, key)
+            return value
         # but not "${num} days"
         return str_interpolate(
-            src, matches, self, skip_checks=skip_interpolation_checks
+            src, matches, self, skip_checks=skip_interpolation_checks, key=key
         )
 
 
 if __name__ == "__main__":
     import doctest
 
     doctest.testmod()
```

### Comparing `dvc-2.9.5/dvc/parsing/interpolate.py` & `dvc-3.0.0a0/dvc/analytics.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,177 +1,156 @@
-import re
-import typing
-from collections.abc import Mapping
-from functools import singledispatch
+import json
+import logging
+import os
 
-from funcy import memoize, rpartial
+from .env import DVC_NO_ANALYTICS
 
-from dvc.exceptions import DvcException
+logger = logging.getLogger(__name__)
 
-if typing.TYPE_CHECKING:
-    from typing import List, Match
 
-    from pyparsing import ParseException
-    from typing_extensions import NoReturn
+def collect_and_send_report(args=None, return_code=None):
+    """
+    Collect information from the runtime/environment and the command
+    being executed into a report and send it over the network.
 
-    from .context import Context
+    To prevent analytics from blocking the execution of the main thread,
+    sending the report is done in a separate process.
 
-BRACE_OPEN = "${"
-BRACE_CLOSE = "}"
-LBRACK = "["
-RBRACK = "]"
-PERIOD = "."
-KEYCRE = re.compile(
-    r"""
-    (?<!\\)                            # escape \${}
-    \${                                # starts with ${
-    (?P<inner>.*?)                     # match every char inside
-    }                                  # end with {
-""",
-    re.VERBOSE,
-)
+    The inter-process communication happens through a file containing the
+    report as a JSON, where the _collector_ generates it and the _sender_
+    removes it after sending it.
+    """
+    import tempfile
 
+    from dvc.daemon import daemon
 
-@memoize
-def get_parser():
-    from pyparsing import CharsNotIn, ParserElement, Suppress, ZeroOrMore
+    report = {}
 
-    ParserElement.enablePackrat()
+    # Include command execution information on the report only when available.
+    if args and hasattr(args, "func"):
+        report.update({"cmd_class": args.func.__name__})
 
-    word = CharsNotIn(f"{PERIOD}{LBRACK}{RBRACK}")
-    idx = Suppress(LBRACK) + word + Suppress(RBRACK)
-    attr = Suppress(PERIOD) + word
-    parser = word + ZeroOrMore(attr ^ idx)
-    parser.setParseAction(PERIOD.join)
+    if return_code is not None:
+        report.update({"cmd_return_code": return_code})
 
-    return parser
+    with tempfile.NamedTemporaryFile(delete=False, mode="w") as fobj:
+        json.dump(report, fobj)
+    daemon(["analytics", fobj.name])
 
 
-class ParseError(DvcException):
-    pass
+def is_enabled():
+    from dvc.config import Config, to_bool
+    from dvc.utils import env2bool
 
+    if env2bool("DVC_TEST"):
+        return False
 
-def get_matches(template: str):
-    return list(KEYCRE.finditer(template))
+    enabled = not os.getenv(DVC_NO_ANALYTICS)
+    if enabled:
+        enabled = to_bool(
+            Config.from_cwd(validate=False).get("core", {}).get("analytics", "true")
+        )
 
+    logger.debug("Analytics is %sabled.", "en" if enabled else "dis")
 
-def is_interpolated_string(val):
-    return isinstance(val, str) and bool(get_matches(val))
+    return enabled
 
 
-def normalize_key(key: str):
-    return key.replace(LBRACK, PERIOD).replace(RBRACK, "")
+def send(path):
+    """
+    Side effect: Removes the report after sending it.
 
+    The report is generated and stored in a temporary file, see:
+    `collect_and_send_report`. Sending happens on another process,
+    thus, the need of removing such file afterwards.
+    """
+    import requests
 
-def format_and_raise_parse_error(exc) -> "NoReturn":
-    raise ParseError(_format_exc_msg(exc))
+    url = "https://analytics.dvc.org"
+    headers = {"content-type": "application/json"}
 
+    with open(path, encoding="utf-8") as fobj:
+        report = json.load(fobj)
 
-def embrace(s: str):
-    return BRACE_OPEN + s + BRACE_CLOSE
+    report.update(_runtime_info())
 
+    try:
+        requests.post(url, json=report, headers=headers, timeout=5)
+    except requests.exceptions.RequestException:
+        logger.debug("failed to send analytics report", exc_info=True)
 
-@singledispatch
-def to_str(obj) -> str:
-    return str(obj)
-
-
-@to_str.register(bool)
-def _(obj: bool):
-    return "true" if obj else "false"
-
-
-def _format_exc_msg(exc: "ParseException"):
-    from pyparsing import ParseException
-
-    from dvc.utils import colorize
-
-    exc.loc += 2  # 2 because we append `${` at the start of expr below
-
-    expr = exc.pstr
-    exc.pstr = embrace(exc.pstr)
-    error = ParseException.explain(exc, depth=0)
-
-    _, pointer, *explains = error.splitlines()
-    pstr = "{brace_open}{expr}{brace_close}".format(
-        brace_open=colorize(BRACE_OPEN, color="blue"),
-        expr=colorize(expr, color="magenta"),
-        brace_close=colorize(BRACE_CLOSE, color="blue"),
-    )
-    msg = "\n".join(explains)
-    pointer = colorize(pointer, color="red")
-    return "\n".join([pstr, pointer, colorize(msg, color="red", style="bold")])
-
-
-def recurse(f):
-    seq = (list, tuple, set)
-
-    def wrapper(data, *args):
-        g = rpartial(wrapper, *args)
-        if isinstance(data, Mapping):
-            return {g(k): g(v) for k, v in data.items()}
-        if isinstance(data, seq):
-            return type(data)(map(g, data))
-        if isinstance(data, str):
-            return f(data, *args)
-        return data
-
-    return wrapper
-
-
-def check_recursive_parse_errors(data):
-    func = recurse(check_expression)
-    return func(data)
-
+    os.remove(path)
 
-def check_expression(s: str):
-    matches = get_matches(s)
-    for match in matches:
-        get_expression(match)
 
+def _scm_in_use():
+    from dvc.exceptions import NotDvcRepoError
+    from dvc.repo import Repo
+    from dvc.scm import NoSCM
 
-def parse_expr(s: str):
-    from pyparsing import ParseException
+    from .scm import SCM, SCMError
 
     try:
-        result = get_parser().parseString(s, parseAll=True)
-    except ParseException as exc:
-        format_and_raise_parse_error(exc)
-        raise AssertionError("unreachable")
-
-    joined = result.asList()
-    assert len(joined) == 1
-    return joined[0]
-
-
-def get_expression(match: "Match", skip_checks: bool = False):
-    inner = match["inner"]
-    return inner if skip_checks else parse_expr(inner)
-
-
-def str_interpolate(
-    template: str,
-    matches: "List[Match]",
-    context: "Context",
-    skip_checks: bool = False,
-):
-    from .context import PRIMITIVES
-
-    index, buf = 0, ""
-    for match in matches:
-        start, end = match.span(0)
-        expr = get_expression(match, skip_checks=skip_checks)
-        value = context.select(expr, unwrap=True)
-        if value is not None and not isinstance(value, PRIMITIVES):
-            raise ParseError(
-                f"Cannot interpolate data of type '{type(value).__name__}'"
-            )
-        buf += template[index:start] + to_str(value)
-        index = end
-    buf += template[index:]
-    # regex already backtracks and avoids any `${` starting with
-    # backslashes(`\`). We just need to replace those by `${`.
-    return buf.replace(r"\${", BRACE_OPEN)
-
+        scm = SCM(root_dir=Repo.find_root())
+        return type(scm).__name__
+    except SCMError:
+        return NoSCM.__name__
+    except NotDvcRepoError:
+        pass
+
+
+def _runtime_info():
+    """
+    Gather information from the environment where DVC runs to fill a report.
+    """
+    from iterative_telemetry import _generate_ci_id, find_or_create_user_id
+
+    from dvc import __version__
+    from dvc.utils import is_binary
+
+    ci_id = _generate_ci_id()
+    if ci_id:
+        group_id, user_id = ci_id
+    else:
+        group_id, user_id = None, find_or_create_user_id()
+
+    return {
+        "dvc_version": __version__,
+        "is_binary": is_binary(),
+        "scm_class": _scm_in_use(),
+        "system_info": _system_info(),
+        "user_id": user_id,
+        "group_id": group_id,
+    }
+
+
+def _system_info():
+    import platform
+    import sys
+
+    import distro
+
+    system = platform.system()
+
+    if system == "Windows":
+        version = sys.getwindowsversion()  # type: ignore[attr-defined]
+
+        return {
+            "os": "windows",
+            "windows_version_build": version.build,
+            "windows_version_major": version.major,
+            "windows_version_minor": version.minor,
+            "windows_version_service_pack": version.service_pack,
+        }
+
+    if system == "Darwin":
+        return {"os": "mac", "mac_version": platform.mac_ver()[0]}
+
+    if system == "Linux":
+        return {
+            "os": "linux",
+            "linux_distro": distro.id(),
+            "linux_distro_like": distro.like(),
+            "linux_distro_version": distro.version(),
+        }
 
-def is_exact_string(src: str, matches: "List[Match]"):
-    return len(matches) == 1 and src == matches[0].group(0)
+    # We don't collect data for any other system.
+    raise NotImplementedError
```

### Comparing `dvc-2.9.5/dvc/pathspec_math.py` & `dvc-3.0.0a0/dvc/pathspec_math.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,12 +1,11 @@
 # Path Specification Pattern Math
 # Including changing base dir of path specification patterns and merging
 # of two path specification patterns with different base
 # All the operations follow the documents of `gitignore`
-import os
 from collections import namedtuple
 
 from pathspec.util import normalize_file
 
 from dvc.utils import relpath
 
 PatternInfo = namedtuple("PatternInfo", ["patterns", "file_info"])
@@ -47,16 +46,15 @@
         rule = f"/{rule}"
     else:
         rule = f"/**/{rule}"
     if not_ignore:
         rule = f"!/{rel}{rule}"
     else:
         rule = f"/{rel}{rule}"
-    rule = normalize_file(rule)
-    return rule
+    return normalize_file(rule)
 
 
 def _change_dirname(dirname, pattern_list, new_dirname):
     if new_dirname == dirname:
         return pattern_list
     rel = relpath(dirname, new_dirname)
     if rel.startswith(".."):
@@ -64,28 +62,28 @@
 
     return [
         PatternInfo(change_rule(rule.patterns, rel), rule.file_info)
         for rule in pattern_list
     ]
 
 
-def merge_patterns(pattern_a, prefix_a, pattern_b, prefix_b):
+def merge_patterns(flavour, pattern_a, prefix_a, pattern_b, prefix_b):
     """
     Merge two path specification patterns.
 
     This implementation merge two path specification patterns on different
     bases. It returns the longest common parent directory, and the patterns
     based on this new base directory.
     """
     if not pattern_a:
         return pattern_b, prefix_b
     if not pattern_b:
         return pattern_a, prefix_a
 
-    longest_common_dir = os.path.commonpath([prefix_a, prefix_b])
+    longest_common_dir = flavour.commonpath([prefix_a, prefix_b])
     new_pattern_a = _change_dirname(prefix_a, pattern_a, longest_common_dir)
     new_pattern_b = _change_dirname(prefix_b, pattern_b, longest_common_dir)
 
     if len(prefix_a) <= len(prefix_b):
         merged_pattern = new_pattern_a + new_pattern_b
     else:
         merged_pattern = new_pattern_b + new_pattern_a
```

### Comparing `dvc-2.9.5/dvc/progress.py` & `dvc-3.0.0a0/dvc/progress.py`

 * *Files 7% similar despite different names*

```diff
@@ -24,33 +24,31 @@
     )
     # nested bars should have fixed bar widths to align nicely
     BAR_FMT_DEFAULT_NESTED = (
         "{percentage:3.0f}%|{bar:10}|{desc:{ncols_desc}.{ncols_desc}}"
         "{postfix[info]}{n_fmt}/{total_fmt}"
         " [{elapsed}<{remaining}, {rate_fmt:>11}]"
     )
-    BAR_FMT_NOTOTAL = (
-        "{desc}{bar:b}|{postfix[info]}{n_fmt} [{elapsed}, {rate_fmt:>11}]"
-    )
+    BAR_FMT_NOTOTAL = "{desc}{bar:b}|{postfix[info]}{n_fmt} [{elapsed}, {rate_fmt:>11}]"
     BYTES_DEFAULTS = {
         "unit": "B",
         "unit_scale": True,
         "unit_divisor": 1024,
         "miniters": 1,
     }
 
-    def __init__(
+    def __init__(  # noqa: PLR0913
         self,
         iterable=None,
         disable=None,
         level=logging.ERROR,
         desc=None,
         leave=False,
         bar_format=None,
-        bytes=False,  # pylint: disable=redefined-builtin
+        bytes=False,  # noqa: A002, pylint: disable=redefined-builtin
         file=None,
         total=None,
         postfix=None,
         **kwargs,
     ):
         """
         bytes   : shortcut for
@@ -71,19 +69,15 @@
             kwargs.setdefault("unit_scale", total > 999 if total else True)
         if file is None:
             file = sys.stderr
         # auto-disable based on `logger.level`
         if not disable:
             disable = logger.getEffectiveLevel() > level
         # auto-disable based on TTY
-        if (
-            not disable
-            and not env2bool(DVC_IGNORE_ISATTY)
-            and hasattr(file, "isatty")
-        ):
+        if not disable and not env2bool(DVC_IGNORE_ISATTY) and hasattr(file, "isatty"):
             disable = not file.isatty()
         super().__init__(
             iterable=iterable,
             disable=disable,
             leave=leave,
             desc=desc,
             bar_format="!",
@@ -91,17 +85,15 @@
             total=total,
             **kwargs,
         )
         self.postfix = postfix or {"info": ""}
         if bar_format is None:
             if self.__len__():
                 self.bar_format = (
-                    self.BAR_FMT_DEFAULT_NESTED
-                    if self.pos
-                    else self.BAR_FMT_DEFAULT
+                    self.BAR_FMT_DEFAULT_NESTED if self.pos else self.BAR_FMT_DEFAULT
                 )
             else:
                 self.bar_format = self.BAR_FMT_NOTOTAL
         else:
             self.bar_format = bar_format
         self.refresh()
 
@@ -131,35 +123,36 @@
         def wrapped(*args, **kwargs):
             res = fn(*args, **kwargs)
             callback()
             return res
 
         return wrapped
 
-    def as_callback(self):
-        from dvc.fs._callback import FsspecCallback
-
-        return FsspecCallback(self)
-
     def close(self):
         self.postfix["info"] = ""
         # remove ETA (either unknown or zero); remove completed bar
         self.bar_format = self.bar_format.replace("<{remaining}", "").replace(
             "|{bar:10}|", " "
         )
         super().close()
 
     @property
     def format_dict(self):
         """inject `ncols_desc` to fill the display width (`ncols`)"""
         d = super().format_dict
-        ncols = d["ncols"] or 80
+        ncols: int = d["ncols"] or 80
         # assumes `bar_format` has max one of ("ncols_desc" & "ncols_info")
         ncols_left = (
-            ncols - len(self.format_meter(ncols_desc=1, ncols_info=1, **d)) + 1
+            ncols
+            - len(
+                self.format_meter(  # type: ignore[call-arg]
+                    ncols_desc=1, ncols_info=1, **d
+                )
+            )
+            + 1
         )
         ncols_left = max(ncols_left, 0)
         if ncols_left:
             d["ncols_desc"] = d["ncols_info"] = ncols_left
         else:
             # work-around for zero-width description
             d["ncols_desc"] = d["ncols_info"] = 1
```

### Comparing `dvc-2.9.5/dvc/prompt.py` & `dvc-3.0.0a0/dvc/prompt.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 """Manages user prompts."""
 
 import logging
 import sys
 from getpass import getpass
+from typing import Collection, Optional
 
 logger = logging.getLogger(__name__)
 
 
-def ask(prompt, limited_to=None):
+def ask(prompt: str, limited_to: Optional[Collection[str]] = None):
     if not sys.stdout.isatty():
         return None
 
     while True:
         try:
             answer = input(prompt + " ").lower()
         except EOFError:
@@ -19,38 +20,35 @@
 
         if not limited_to:
             return answer
 
         if answer in limited_to:
             return answer
 
-        logger.info(
-            "Your response must be one of: {options}. "
-            "Please try again.".format(options=limited_to)
-        )
+        logger.info("Your response must be one of: %s. Please try again.", limited_to)
 
 
-def confirm(statement):
+def confirm(statement: str) -> bool:
     """Ask the user for confirmation about the specified statement.
 
     Args:
         statement (unicode): statement to ask the user confirmation about.
 
     Returns:
         bool: whether or not specified statement was confirmed.
     """
     prompt = f"{statement} [y/n]"
     answer = ask(prompt, limited_to=["yes", "no", "y", "n"])
     return answer and answer.startswith("y")
 
 
-def password(statement):
+def password(statement: str) -> str:
     """Ask the user for a password.
 
     Args:
         statement (str): string to prompt the user with.
 
     Returns:
         str: password entered by the user.
     """
-    logger.info(f"{statement}: ")
+    logger.info("%s: ", statement)
     return getpass("")
```

### Comparing `dvc-2.9.5/dvc/render/image.py` & `dvc-3.0.0a0/dvc/render/converter/image.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,110 +1,66 @@
-import json
+import base64
 import os
-from typing import TYPE_CHECKING
+from typing import TYPE_CHECKING, Any, Dict, List, Tuple
 
-from funcy import reraise
+from dvc.render import FILENAME_FIELD, REVISION_FIELD, SRC_FIELD
 
-from dvc.exceptions import DvcException
-from dvc.render.base import Renderer
-from dvc.render.utils import get_files
-from dvc.utils import relpath
+from . import Converter
 
 if TYPE_CHECKING:
     from dvc.types import StrPath
 
 
-class ImageRenderer(Renderer):
-    TYPE = "image"
-    DIV = """
-        <div
-            id="{id}"
-            style="border:1px solid black;text-align:center;
-            white-space: nowrap;overflow-y:hidden;">
-            {partial}
-        </div>"""
-
-    SCRIPTS = ""
-
-    @property
-    def needs_output_path(self):
-        return True
-
+class ImageConverter(Converter):
+    @staticmethod
     def _write_image(
-        self,
         path: "StrPath",
         revision: str,
         filename: str,
         image_data: bytes,
-    ):
+    ) -> "StrPath":
         img_path = os.path.join(
-            path, f"{revision}_{filename.replace(os.sep, '_')}"
+            path,
+            f"{revision}_{filename}".replace(os.sep, "_").replace("/", "_"),
         )
         with open(img_path, "wb") as fd:
             fd.write(image_data)
 
         return img_path
 
-    def _save_images(self, path: "StrPath"):
-
-        for rev, rev_data in self.data.items():
-            if "data" in rev_data:
-                for file, file_data in rev_data.get("data", {}).items():
-                    if "data" in file_data:
-                        if not os.path.isdir(path):
-                            os.makedirs(path, exist_ok=True)
-                        yield rev, file, self._write_image(
-                            os.path.abspath(path), rev, file, file_data["data"]
-                        )
-
-    def partial_html(self, **kwargs):
-        path = kwargs.get("path", None)
-        if not path:
-            raise DvcException("Can't save here")
-        static = os.path.join(path, "static")
-
-        div_content = []
-        for rev, _, img_path in self._save_images(static):
-            div_content.append(
-                """
-        <div
-            style="border:1px dotted black;margin:2px;display: inline-block;
-            overflow:hidden;margin-left:8px;">
-            <p>{title}</p>
-            <img src="{src}">
-        </div>""".format(
-                    title=rev, src=(relpath(img_path, path))
-                )
-            )
-        if div_content:
-            div_content.insert(0, f"<p>{self.filename}</p>")
-            return "\n".join(div_content)
-        return ""
-
-    def as_json(self, **kwargs):
-
-        with reraise(
-            KeyError,
-            DvcException(
-                f"{type(self).__name__} needs 'path' to store images."
-            ),
-        ):
-            path = kwargs["path"]
-
-        results = []
-
-        for revision, _, img_path in self._save_images(path):
-            results.append(
-                {
-                    self.TYPE_KEY: self.TYPE,
-                    self.REVISIONS_KEY: [revision],
-                    "url": img_path,
-                }
-            )
-
-        return json.dumps(results)
-
     @staticmethod
-    def matches(data):
-        files = get_files(data)
-        extensions = set(map(lambda f: os.path.splitext(f)[1], files))
-        return extensions.issubset({".jpg", ".jpeg", ".gif", ".png"})
+    def _encode_image(
+        image_data: bytes,
+    ) -> str:
+        base64_str = base64.b64encode(image_data).decode()
+        return f"data:image;base64,{base64_str}"
+
+    def convert(self) -> Tuple[List[Tuple[str, str, Any]], Dict]:
+        datas = []
+        for filename, image_data in self.data.items():
+            datas.append((filename, "", image_data))
+        return datas, self.properties
+
+    def flat_datapoints(self, revision: str) -> Tuple[List[Dict], Dict]:
+        """
+        Convert the DVC Plots content to DVC Render datapoints.
+        Return both generated datapoints and updated properties.
+        """
+        path = self.properties.get("out")
+        datapoints = []
+        datas, properties = self.convert()
+        for filename, _, image_data in datas:
+            if path:
+                if not os.path.isdir(path):
+                    os.makedirs(path, exist_ok=True)
+                src = self._write_image(
+                    os.path.abspath(path), revision, filename, image_data
+                )
+            else:
+                src = self._encode_image(image_data)
+            datapoint = {
+                REVISION_FIELD: revision,
+                FILENAME_FIELD: filename,
+                SRC_FIELD: src,
+            }
+            datapoints.append(datapoint)
+        return datapoints, properties
```

### Comparing `dvc-2.9.5/dvc/repo/__init__.py` & `dvc-3.0.0a0/dvc/repo/experiments/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,523 +1,516 @@
 import logging
 import os
-from collections import defaultdict
-from contextlib import contextmanager
-from functools import wraps
-from typing import TYPE_CHECKING, Callable, Optional, Set
-
-from funcy import cached_property
-
-from dvc.exceptions import FileMissingError
-from dvc.exceptions import IsADirectoryError as DvcIsADirectoryError
-from dvc.exceptions import NotDvcRepoError, OutputNotFoundError
-from dvc.ignore import DvcIgnoreFilter
-from dvc.utils import env2bool
-from dvc.utils.fs import path_isin
+import re
+from typing import TYPE_CHECKING, Dict, Iterable, List, Optional
+
+from funcy import chain, first
+
+from dvc.exceptions import DvcException
+from dvc.ui import ui
+from dvc.utils import relpath
+from dvc.utils.objects import cached_property
+
+from .cache import ExpCache
+from .exceptions import (
+    BaselineMismatchError,
+    ExperimentExistsError,
+    InvalidExpRefError,
+    MultipleBranchError,
+)
+from .refs import (
+    APPLY_STASH,
+    CELERY_FAILED_STASH,
+    CELERY_STASH,
+    EXEC_APPLY,
+    EXEC_CHECKPOINT,
+    EXEC_NAMESPACE,
+    EXPS_NAMESPACE,
+    WORKSPACE_STASH,
+    ExpRefInfo,
+)
+from .stash import ApplyStash
+from .utils import check_ref_format, exp_refs_by_rev, unlocked_repo
 
 if TYPE_CHECKING:
-    from dvc.fs.base import FileSystem
-    from dvc.objects.file import HashFile
-    from dvc.repo.scm_context import SCMContext
+    from .queue.base import BaseStashQueue, QueueEntry
+    from .queue.celery import LocalCeleryQueue
+    from .queue.tempdir import TempDirQueue
+    from .queue.workspace import WorkspaceQueue
+    from .stash import ExpStashEntry
 
 logger = logging.getLogger(__name__)
 
 
-@contextmanager
-def lock_repo(repo: "Repo"):
-    # pylint: disable=protected-access
-    depth = repo._lock_depth
-    repo._lock_depth += 1
-
-    try:
-        if depth > 0:
-            yield
-        else:
-            with repo.lock:
-                repo._reset()
-                yield
-                # Graph cache is no longer valid after we release the repo.lock
-                repo._reset()
-    finally:
-        repo._lock_depth = depth
-
-
-def locked(f):
-    @wraps(f)
-    def wrapper(repo, *args, **kwargs):
-        with lock_repo(repo):
-            return f(repo, *args, **kwargs)
-
-    return wrapper
-
-
-class Repo:
-    DVC_DIR = ".dvc"
-
-    from dvc.repo.add import add
-    from dvc.repo.checkout import checkout
-    from dvc.repo.commit import commit
-    from dvc.repo.destroy import destroy
-    from dvc.repo.diff import diff
-    from dvc.repo.fetch import fetch
-    from dvc.repo.freeze import freeze, unfreeze
-    from dvc.repo.gc import gc
-    from dvc.repo.get import get as _get
-    from dvc.repo.get_url import get_url as _get_url
-    from dvc.repo.imp import imp
-    from dvc.repo.imp_url import imp_url
-    from dvc.repo.install import install
-    from dvc.repo.ls import ls as _ls
-    from dvc.repo.move import move
-    from dvc.repo.pull import pull
-    from dvc.repo.push import push
-    from dvc.repo.remove import remove
-    from dvc.repo.reproduce import reproduce
-    from dvc.repo.run import run
-    from dvc.repo.status import status
-    from dvc.repo.update import update
-
-    ls = staticmethod(_ls)
-    get = staticmethod(_get)
-    get_url = staticmethod(_get_url)
-
-    def _get_repo_dirs(
-        self,
-        root_dir: str = None,
-        fs: "FileSystem" = None,
-        uninitialized: bool = False,
-    ):
-        from dvc.scm import SCM, Base, SCMError
-        from dvc.utils.fs import makedirs
+class Experiments:
+    """Class that manages experiments in a DVC repo.
 
-        dvc_dir = None
-        tmp_dir = None
-        try:
-            root_dir = self.find_root(root_dir, fs)
-            dvc_dir = os.path.join(root_dir, self.DVC_DIR)
-            tmp_dir = os.path.join(dvc_dir, "tmp")
-            makedirs(tmp_dir, exist_ok=True)
-        except NotDvcRepoError:
-            if not uninitialized:
-                raise
+    Args:
+        repo (dvc.repo.Repo): repo instance that these experiments belong to.
+    """
 
-            try:
-                scm = SCM(root_dir or os.curdir)
-            except SCMError:
-                scm = SCM(os.curdir, no_scm=True)
-
-            assert isinstance(scm, Base)
-            root_dir = scm.root_dir
-
-        return root_dir, dvc_dir, tmp_dir
-
-    def _get_database_dir(self, db_name):
-        # NOTE: by default, store SQLite-based remote indexes and state's
-        # `links` and `md5s` caches in the repository itself to avoid any
-        # possible state corruption in 'shared cache dir' scenario, but allow
-        # user to override this through config when, say, the repository is
-        # located on a mounted volume — see
-        # https://github.com/iterative/dvc/issues/4420
-        base_db_dir = self.config.get(db_name, {}).get("dir", None)
-        if not base_db_dir:
-            return self.tmp_dir
-
-        import hashlib
-
-        from dvc.utils.fs import makedirs
-
-        root_dir_hash = hashlib.sha224(
-            self.root_dir.encode("utf-8")
-        ).hexdigest()
-
-        db_dir = os.path.join(
-            base_db_dir,
-            self.DVC_DIR,
-            f"{os.path.basename(self.root_dir)}-{root_dir_hash[0:7]}",
-        )
+    BRANCH_RE = re.compile(
+        r"^(?P<baseline_rev>[a-f0-9]{7})-(?P<exp_sha>[a-f0-9]+)"
+        r"(?P<checkpoint>-checkpoint)?$"
+    )
 
-        makedirs(db_dir, exist_ok=True)
-        return db_dir
+    def __init__(self, repo):
+        from dvc.scm import NoSCMError
 
-    def __init__(
-        self,
-        root_dir=None,
-        fs=None,
-        rev=None,
-        subrepos=False,
-        uninitialized=False,
-        config=None,
-        url=None,
-        repo_factory=None,
-    ):
-        from dvc.config import Config
-        from dvc.data.db import ODBManager
-        from dvc.data_cloud import DataCloud
-        from dvc.fs.git import GitFileSystem
-        from dvc.fs.local import localfs
-        from dvc.lock import LockNoop, make_lock
-        from dvc.repo.live import Live
-        from dvc.repo.metrics import Metrics
-        from dvc.repo.params import Params
-        from dvc.repo.plots import Plots
-        from dvc.repo.stage import StageLoad
-        from dvc.scm import SCM
-        from dvc.stage.cache import StageCache
-        from dvc.state import State, StateNoop
-
-        self.url = url
-        self._fs_conf = {"repo_factory": repo_factory}
-        self._fs = fs or localfs
-        self._scm = None
-
-        if rev and not fs:
-            self._scm = SCM(root_dir or os.curdir)
-            self._fs = GitFileSystem(scm=self._scm, rev=rev)
+        if repo.config["core"].get("no_scm", False):
+            raise NoSCMError
 
-        self.root_dir, self.dvc_dir, self.tmp_dir = self._get_repo_dirs(
-            root_dir=root_dir, fs=self.fs, uninitialized=uninitialized
-        )
+        self.repo = repo
 
-        self.config = Config(self.dvc_dir, fs=self.fs, config=config)
-        self._uninitialized = uninitialized
+    @property
+    def scm(self):
+        from dvc.scm import SCMError
 
-        # used by RepoFileSystem to determine if it should traverse subrepos
-        self.subrepos = subrepos
+        if self.repo.scm.no_commits:
+            raise SCMError("Empty Git repo. Add a commit to use experiments.")
 
-        self.cloud = DataCloud(self)
-        self.stage = StageLoad(self)
+        return self.repo.scm
 
-        if isinstance(self.fs, GitFileSystem) or not self.dvc_dir:
-            self.lock = LockNoop()
-            self.state = StateNoop()
-            self.odb = ODBManager(self)
-        else:
-            self.lock = make_lock(
-                os.path.join(self.tmp_dir, "lock"),
-                tmp_dir=self.tmp_dir,
-                hardlink_lock=self.config["core"].get("hardlink_lock", False),
-                friendly=True,
-            )
+    @cached_property
+    def dvc_dir(self) -> str:
+        return relpath(self.repo.dvc_dir, self.repo.scm.root_dir)
 
-            state_db_dir = self._get_database_dir("state")
-            self.state = State(self.root_dir, state_db_dir, self.dvcignore)
-            self.odb = ODBManager(self)
-
-            self.stage_cache = StageCache(self)
-
-            self._ignore()
-
-        self.metrics = Metrics(self)
-        self.plots = Plots(self)
-        self.params = Params(self)
-        self.live = Live(self)
-
-        self.stage_collection_error_handler: Optional[
-            Callable[[str, Exception], None]
-        ] = None
-        self._lock_depth = 0
+    @cached_property
+    def args_file(self) -> str:
+        from .executor.base import BaseExecutor
 
-    def __str__(self):
-        return self.url or self.root_dir
+        return os.path.join(self.repo.tmp_dir, BaseExecutor.PACKED_ARGS_FILE)
 
     @cached_property
-    def index(self):
-        from dvc.repo.index import Index
+    def workspace_queue(self) -> "WorkspaceQueue":
+        from .queue.workspace import WorkspaceQueue
 
-        return Index(self)
+        return WorkspaceQueue(self.repo, WORKSPACE_STASH)
 
-    @staticmethod
-    def open(url, *args, **kwargs):
-        if url is None:
-            url = os.getcwd()
+    @cached_property
+    def tempdir_queue(self) -> "TempDirQueue":
+        from .queue.tempdir import TempDirQueue
 
-        if os.path.exists(url):
-            try:
-                return Repo(url, *args, **kwargs)
-            except NotDvcRepoError:
-                pass  # fallthrough to external_repo
+        # NOTE: tempdir and workspace stash is shared since both
+        # implementations immediately push -> pop (queue length is only 0 or 1)
+        return TempDirQueue(self.repo, WORKSPACE_STASH)
 
-        from dvc.external_repo import external_repo
+    @cached_property
+    def celery_queue(self) -> "LocalCeleryQueue":
+        from .queue.celery import LocalCeleryQueue
 
-        return external_repo(url, *args, **kwargs)
+        return LocalCeleryQueue(self.repo, CELERY_STASH, CELERY_FAILED_STASH)
 
     @cached_property
-    def scm(self):
-        from dvc.scm import SCM, SCMError
+    def apply_stash(self) -> ApplyStash:
+        return ApplyStash(self.scm, APPLY_STASH)
 
-        if self._scm:
-            return self._scm
+    @cached_property
+    def cache(self) -> ExpCache:
+        return ExpCache(self.repo)
 
-        no_scm = self.config["core"].get("no_scm", False)
-        try:
-            return SCM(self.root_dir, no_scm=no_scm)
-        except SCMError:
-            if self._uninitialized:
-                # might not be a git/dvc repo at all
-                # used in `params/metrics/plots/live` targets
-                return SCM(self.root_dir, no_scm=True)
-            raise
+    @property
+    def stash_revs(self) -> Dict[str, "ExpStashEntry"]:
+        revs = {}
+        for queue in (self.workspace_queue, self.celery_queue):
+            revs.update(queue.stash.stash_revs)
+        return revs
 
-    @cached_property
-    def scm_context(self) -> "SCMContext":
-        from dvc.repo.scm_context import SCMContext
+    def reproduce_one(
+        self,
+        tmp_dir: bool = False,
+        copy_paths: Optional[List[str]] = None,
+        message: Optional[str] = None,
+        **kwargs,
+    ):
+        """Reproduce and checkout a single (standalone) experiment."""
+        exp_queue: "BaseStashQueue" = (
+            self.tempdir_queue if tmp_dir else self.workspace_queue
+        )
+        self.queue_one(exp_queue, **kwargs)
+        results = self._reproduce_queue(
+            exp_queue, copy_paths=copy_paths, message=message
+        )
+        exp_rev = first(results)
+        if exp_rev is not None:
+            self._log_reproduced(results, tmp_dir=tmp_dir)
+        return results
 
-        return SCMContext(self.scm, self.config)
+    def queue_one(
+        self,
+        queue: "BaseStashQueue",
+        checkpoint_resume: Optional[str] = None,
+        reset: bool = False,
+        **kwargs,
+    ) -> "QueueEntry":
+        """Queue a single experiment."""
+        if reset:
+            self.reset_checkpoints()
+
+        if kwargs.pop("machine", None) is not None:
+            # TODO: decide how to handle queued remote execution
+            raise NotImplementedError
 
-    @cached_property
-    def dvcignore(self) -> DvcIgnoreFilter:
+        if checkpoint_resume:
+            from dvc.scm import resolve_rev
 
-        return DvcIgnoreFilter(self.fs, self.root_dir)
+            resume_rev = resolve_rev(self.scm, checkpoint_resume)
+            try:
+                self.check_baseline(resume_rev)
+                checkpoint_resume = resume_rev
+            except BaselineMismatchError as exc:
+                raise DvcException(
+                    f"Cannot resume from '{checkpoint_resume}' as it is not "
+                    "derived from your current workspace."
+                ) from exc
+        else:
+            checkpoint_resume = self._workspace_resume_rev()
 
-    def get_rev(self):
-        from dvc.fs.local import LocalFileSystem
+        return self.new(
+            queue,
+            checkpoint_resume=checkpoint_resume,
+            reset=reset,
+            **kwargs,
+        )
 
-        assert self.scm
-        if isinstance(self.fs, LocalFileSystem):
-            from dvc.scm import map_scm_exception
+    def _workspace_resume_rev(self) -> Optional[str]:
+        last_checkpoint = self._get_last_checkpoint()
+        last_applied = self._get_last_applied()
+        if last_checkpoint and last_applied:
+            return last_applied
+        return None
 
-            with map_scm_exception():
-                return self.scm.get_rev()
-        return self.fs.rev
+    def reproduce_celery(  # noqa: C901
+        self, entries: Optional[Iterable["QueueEntry"]] = None, **kwargs
+    ) -> Dict[str, str]:
+        results: Dict[str, str] = {}
+        if entries is None:
+            entries = list(
+                chain(
+                    self.celery_queue.iter_active(),
+                    self.celery_queue.iter_queued(),
+                )
+            )
 
-    @cached_property
-    def experiments(self):
-        from dvc.repo.experiments import Experiments
+        logger.debug(
+            "reproduce all these entries '%s'",
+            entries,
+        )
 
-        return Experiments(self)
+        if not entries:
+            return results
 
-    @cached_property
-    def machine(self):
-        from dvc.machine import MachineManager
+        self.celery_queue.start_workers(count=kwargs.get("jobs", 1))
+        failed = []
+        try:
+            ui.write(
+                "Following logs for all queued experiments. Use Ctrl+C to "
+                "stop following logs (experiment execution will continue).\n"
+            )
+            for entry in entries:
+                # wait for task execution to start
+                self.celery_queue.wait_for_start(entry, sleep_interval=1)
+                self.celery_queue.follow(entry)
+                # wait for task collection to complete
+                try:
+                    result = self.celery_queue.get_result(entry)
+                except FileNotFoundError:
+                    result = None
+                if result is None or result.exp_hash is None:
+                    name = entry.name or entry.stash_rev[:7]
+                    failed.append(name)
+                elif result.ref_info:
+                    exp_rev = self.scm.get_ref(str(result.ref_info))
+                    results[exp_rev] = result.exp_hash
+        except KeyboardInterrupt:
+            ui.write(
+                "Experiment(s) are still executing in the background. To "
+                "abort execution use 'dvc queue kill' or 'dvc queue stop'."
+            )
+        if failed:
+            names = ", ".join(name for name in failed)
+            ui.error(f"Failed to reproduce experiment(s) '{names}'")
+        if results:
+            self._log_reproduced((rev for rev in results), True)
+        return results
+
+    def _log_reproduced(self, revs: Iterable[str], tmp_dir: bool = False):
+        names = []
+        rev_names = self.get_exact_name(revs)
+        for rev in revs:
+            name = rev_names[rev]
+            names.append(name if name else rev[:7])
+        ui.write("\nRan experiment(s): {}".format(", ".join(names)))
+        if tmp_dir:
+            ui.write(
+                "To apply the results of an experiment to your workspace "
+                "run:\n\n"
+                "\tdvc exp apply <exp>"
+            )
+        else:
+            ui.write("Experiment results have been applied to your workspace.")
+        ui.write(
+            "\nTo promote an experiment to a Git branch run:\n\n"
+            "\tdvc exp branch <exp> <branch>\n"
+        )
 
-        if self.tmp_dir and (
-            self.config["feature"].get("machine", False)
-            or env2bool("DVC_TEST")
-        ):
-            return MachineManager(self)
-        return None
+    def new(
+        self,
+        queue: "BaseStashQueue",
+        *args,
+        checkpoint_resume: Optional[str] = None,
+        **kwargs,
+    ) -> "QueueEntry":
+        """Create and enqueue a new experiment.
 
-    @property
-    def fs(self) -> "FileSystem":
-        return self._fs
+        Experiment will be derived from the current workspace.
+        """
+        if checkpoint_resume is not None:
+            return self._resume_checkpoint(
+                queue, *args, resume_rev=checkpoint_resume, **kwargs
+            )
 
-    @fs.setter
-    def fs(self, fs: "FileSystem"):
-        self._fs = fs
-        # Our graph cache is no longer valid, as it was based on the previous
-        # fs.
-        self._reset()
-
-    def __repr__(self):
-        return f"{self.__class__.__name__}: '{self.root_dir}'"
-
-    @classmethod
-    def find_root(cls, root=None, fs=None) -> str:
-        from dvc.fs.local import LocalFileSystem, localfs
-
-        root = root or os.curdir
-        root_dir = os.path.realpath(root)
-        fs = fs or localfs
-
-        if not fs.isdir(root_dir):
-            raise NotDvcRepoError(f"directory '{root}' does not exist")
-
-        while True:
-            dvc_dir = fs.path.join(root_dir, cls.DVC_DIR)
-            if fs.isdir(dvc_dir):
-                return root_dir
-            if isinstance(fs, LocalFileSystem) and os.path.ismount(root_dir):
-                break
-            parent = fs.path.parent(root_dir)
-            if parent == root_dir:
-                break
-            root_dir = parent
-
-        msg = "you are not inside of a DVC repository"
-
-        if isinstance(fs, LocalFileSystem):
-            msg = f"{msg} (checked up to mount point '{root_dir}')"
-
-        raise NotDvcRepoError(msg)
-
-    @classmethod
-    def find_dvc_dir(cls, root=None):
-        root_dir = cls.find_root(root)
-        return os.path.join(root_dir, cls.DVC_DIR)
-
-    @staticmethod
-    def init(root_dir=os.curdir, no_scm=False, force=False, subdir=False):
-        from dvc.repo.init import init
+        name = kwargs.get("name", None)
+        baseline_sha = kwargs.get("baseline_rev") or self.repo.scm.get_rev()
 
-        return init(
-            root_dir=root_dir, no_scm=no_scm, force=force, subdir=subdir
-        )
+        if name:
+            exp_ref = ExpRefInfo(baseline_sha=baseline_sha, name=name)
+            check_ref_format(self.scm, exp_ref)
+            force = kwargs.get("force", False)
+            if self.scm.get_ref(str(exp_ref)) and not force:
+                raise ExperimentExistsError(exp_ref.name)
 
-    def unprotect(self, target):
-        return self.odb.local.unprotect(target)
+        return queue.put(*args, **kwargs)
 
-    def _ignore(self):
-        flist = [self.config.files["local"], self.tmp_dir]
+    def _resume_checkpoint(
+        self,
+        queue: "BaseStashQueue",
+        *args,
+        resume_rev: Optional[str] = None,
+        **kwargs,
+    ) -> "QueueEntry":
+        """Create and queue a resumed checkpoint experiment."""
+        assert resume_rev
+
+        branch: Optional[str] = None
+        try:
+            allow_multiple = bool(kwargs.get("params", None))
+            branch = self.get_branch_by_rev(resume_rev, allow_multiple=allow_multiple)
+            if not branch:
+                raise DvcException(
+                    f"Could not find checkpoint experiment '{resume_rev[:7]}'"
+                )
+            baseline_rev = self._get_baseline(branch)
+        except MultipleBranchError as exc:
+            baselines = {
+                info.baseline_sha for info in exc.ref_infos if info.baseline_sha
+            }
+            if len(baselines) == 1:
+                baseline_rev = baselines.pop()
+            else:
+                raise
 
-        if path_isin(self.odb.local.cache_dir, self.root_dir):
-            flist += [self.odb.local.cache_dir]
+        logger.debug(
+            "Checkpoint run from '%s' with baseline '%s'",
+            resume_rev[:7],
+            baseline_rev,
+        )
+        return queue.put(
+            *args,
+            resume_rev=resume_rev,
+            baseline_rev=baseline_rev,
+            branch=branch,
+            **kwargs,
+        )
 
-        for file in flist:
-            self.scm_context.ignore(file)
+    def _get_last_checkpoint(self) -> Optional[str]:
+        try:
+            last_checkpoint = self.scm.get_ref(EXEC_CHECKPOINT)
+            if last_checkpoint:
+                self.check_baseline(last_checkpoint)
+            return last_checkpoint
+        except BaselineMismatchError:
+            # If HEAD has moved since the the last checkpoint run,
+            # the specified checkpoint is no longer relevant
+            self.scm.remove_ref(EXEC_CHECKPOINT)
+        return None
 
-    def brancher(self, *args, **kwargs):
-        from dvc.repo.brancher import brancher
+    def _get_last_applied(self) -> Optional[str]:
+        try:
+            last_applied = self.scm.get_ref(EXEC_APPLY)
+            if last_applied:
+                self.check_baseline(last_applied)
+            return last_applied
+        except BaselineMismatchError:
+            # If HEAD has moved since the the last applied experiment,
+            # the applied experiment is no longer relevant
+            self.scm.remove_ref(EXEC_APPLY)
+        return None
 
-        return brancher(self, *args, **kwargs)
+    def reset_checkpoints(self):
+        self.scm.remove_ref(EXEC_CHECKPOINT)
+        self.scm.remove_ref(EXEC_APPLY)
 
-    def used_objs(
+    @unlocked_repo
+    def _reproduce_queue(
         self,
-        targets=None,
-        all_branches=False,
-        with_deps=False,
-        all_tags=False,
-        all_commits=False,
-        all_experiments=False,
-        remote=None,
-        force=False,
-        jobs=None,
-        recursive=False,
-        used_run_cache=None,
-        revs=None,
-    ):
-        """Get the stages related to the given target and collect
-        the `info` of its outputs.
-
-        This is useful to know what files from the cache are _in use_
-        (namely, a file described as an output on a stage).
+        queue: "BaseStashQueue",
+        copy_paths: Optional[List[str]] = None,
+        message: Optional[str] = None,
+        **kwargs,
+    ) -> Dict[str, str]:
+        """Reproduce queued experiments.
 
-        The scope is, by default, the working directory, but you can use
-        `all_branches`/`all_tags`/`all_commits`/`all_experiments` to expand
-        the scope.
+        Arguments:
+            queue: Experiment queue.
 
         Returns:
-            A dict mapping (remote) ODB instances to sets of objects that
-            belong to each ODB. If the ODB instance is None, the objects
-            are naive and do not belong to a specific remote ODB.
+            dict mapping successfully reproduced experiment revs to their
+            results.
         """
-        used = defaultdict(set)
+        exec_results = queue.reproduce(copy_paths=copy_paths, message=message)
+
+        results: Dict[str, str] = {}
+        for _, exp_result in exec_results.items():
+            results.update(exp_result)
+        return results
+
+    def check_baseline(self, exp_rev):
+        baseline_sha = self.repo.scm.get_rev()
+        if exp_rev == baseline_sha:
+            return exp_rev
+
+        exp_baseline = self._get_baseline(exp_rev)
+        if exp_baseline is None:
+            # if we can't tell from branch name, fall back to parent commit
+            exp_commit = self.scm.resolve_commit(exp_rev)
+            if exp_commit:
+                exp_baseline = first(exp_commit.parents)
+        if exp_baseline == baseline_sha:
+            return exp_baseline
+        raise BaselineMismatchError(exp_baseline, baseline_sha)
+
+    def get_baseline(self, rev):
+        """Return the baseline rev for an experiment rev."""
+        return self._get_baseline(rev)
+
+    def _get_baseline(self, rev):
+        from dvc.scm import resolve_rev
+
+        rev = resolve_rev(self.scm, rev)
+
+        if rev in self.stash_revs:
+            entry = self.stash_revs.get(rev)
+            if entry:
+                return entry.baseline_rev
+            return None
+
+        ref_info = first(exp_refs_by_rev(self.scm, rev))
+        if ref_info:
+            return ref_info.baseline_sha
+        return None
 
-        def _add_suffix(objs: Set["HashFile"], suffix: str) -> None:
-            from itertools import chain
+    def get_branch_by_rev(
+        self, rev: str, allow_multiple: bool = False
+    ) -> Optional[str]:
+        """Returns full refname for the experiment branch containing rev."""
+        ref_infos = list(exp_refs_by_rev(self.scm, rev))
+        if not ref_infos:
+            return None
+        if len(ref_infos) > 1 and not allow_multiple:
+            for ref_info in ref_infos:
+                if self.scm.get_ref(str(ref_info)) == rev:
+                    return str(ref_info)
+            raise MultipleBranchError(rev, ref_infos)
+        return str(ref_infos[0])
 
-            from dvc.data import iterobjs
+    def get_exact_name(self, revs: Iterable[str]) -> Dict[str, Optional[str]]:
+        """Returns preferred name for the specified revision.
 
-            for obj in chain.from_iterable(map(iterobjs, objs)):
-                if obj.name is not None:
-                    obj.name += suffix
-
-        for branch in self.brancher(
-            revs=revs,
-            all_branches=all_branches,
-            all_tags=all_tags,
-            all_commits=all_commits,
-            all_experiments=all_experiments,
+        Prefers tags, branches (heads), experiments in that order.
+        """
+        result: Dict[str, Optional[str]] = {}
+        exclude = f"{EXEC_NAMESPACE}/*"
+        ref_dict = self.scm.describe(revs, base=EXPS_NAMESPACE, exclude=exclude)
+        for rev in revs:
+            name: Optional[str] = None
+            ref = ref_dict[rev]
+            if ref:
+                try:
+                    name = ExpRefInfo.from_ref(ref).name
+                except InvalidExpRefError:
+                    pass
+            if not name:
+                if rev in self.stash_revs:
+                    name = self.stash_revs[rev].name
+                else:
+                    failed_stash = self.celery_queue.failed_stash
+                    if failed_stash and rev in failed_stash.stash_revs:
+                        name = failed_stash.stash_revs[rev].name
+            result[rev] = name
+        return result
+
+    def get_running_exps(self, fetch_refs: bool = True) -> Dict[str, Dict]:
+        """Return info for running experiments."""
+        result = {}
+        for queue in (
+            self.workspace_queue,
+            self.tempdir_queue,
+            self.celery_queue,
         ):
-            for odb, objs in self.index.used_objs(
-                targets,
-                remote=remote,
-                force=force,
-                jobs=jobs,
-                recursive=recursive,
-                with_deps=with_deps,
-            ).items():
-                if branch:
-                    _add_suffix(objs, f" ({branch})")
-                used[odb].update(objs)
-
-        if used_run_cache:
-            for odb, objs in self.stage_cache.get_used_objs(
-                used_run_cache, remote=remote, force=force, jobs=jobs
-            ).items():
-                used[odb].update(objs)
+            result.update(queue.get_running_exps(fetch_refs))
+        return result
 
-        return used
+    def apply(self, *args, **kwargs):
+        from dvc.repo.experiments.apply import apply
 
-    @property
-    def stages(self):  # obsolete, only for backward-compatibility
-        return self.index.stages
+        return apply(self.repo, *args, **kwargs)
 
-    def find_outs_by_path(self, path, outs=None, recursive=False, strict=True):
-        # using `outs_graph` to ensure graph checks are run
-        outs = outs or self.index.outs_graph
+    def branch(self, *args, **kwargs):
+        from dvc.repo.experiments.branch import branch
 
-        abs_path = os.path.abspath(path)
-        fs_path = abs_path
+        return branch(self.repo, *args, **kwargs)
 
-        def func(out):
-            def eq(one, two):
-                return one == two
+    def diff(self, *args, **kwargs):
+        from dvc.repo.experiments.diff import diff
 
-            match = eq if strict else out.fs.path.isin_or_eq
+        return diff(self.repo, *args, **kwargs)
 
-            if out.scheme == "local" and match(fs_path, out.fs_path):
-                return True
+    def show(self, *args, **kwargs):
+        from dvc.repo.experiments.show import show
 
-            if recursive and out.fs.path.isin(out.fs_path, fs_path):
-                return True
+        return show(self.repo, *args, **kwargs)
 
-            return False
+    def run(self, *args, **kwargs):
+        from dvc.repo.experiments.run import run
 
-        matched = list(filter(func, outs))
-        if not matched:
-            raise OutputNotFoundError(path, self)
+        return run(self.repo, *args, **kwargs)
 
-        return matched
+    def save(self, *args, **kwargs):
+        from dvc.repo.experiments.save import save
 
-    def is_dvc_internal(self, path):
-        path_parts = os.path.normpath(path).split(os.path.sep)
-        return self.DVC_DIR in path_parts
+        return save(self.repo, *args, **kwargs)
 
-    @cached_property
-    def dvcfs(self):
-        from dvc.fs.dvc import DvcFileSystem
+    def push(self, *args, **kwargs):
+        from dvc.repo.experiments.push import push
 
-        return DvcFileSystem(repo=self)
+        return push(self.repo, *args, **kwargs)
 
-    @cached_property
-    def repo_fs(self):
-        from dvc.fs.repo import RepoFileSystem
+    def pull(self, *args, **kwargs):
+        from dvc.repo.experiments.pull import pull
 
-        return RepoFileSystem(self, subrepos=self.subrepos, **self._fs_conf)
+        return pull(self.repo, *args, **kwargs)
 
-    @cached_property
-    def index_db_dir(self):
-        return self._get_database_dir("index")
+    def ls(self, *args, **kwargs):
+        from dvc.repo.experiments.ls import ls
 
-    @contextmanager
-    def open_by_relpath(self, path, remote=None, mode="r", encoding=None):
-        """Opens a specified resource as a file descriptor"""
-        from dvc.fs.repo import RepoFileSystem
+        return ls(self.repo, *args, **kwargs)
 
-        fs = RepoFileSystem(self, subrepos=True)
-        path = self.fs.path.join(self.root_dir, path)
-        try:
-            with fs.open(
-                path, mode=mode, encoding=encoding, remote=remote
-            ) as fobj:
-                yield fobj
-        except FileNotFoundError as exc:
-            raise FileMissingError(path) from exc
-        except IsADirectoryError as exc:
-            raise DvcIsADirectoryError(f"'{path}' is a directory") from exc
-
-    def close(self):
-        self.scm.close()
-        self.state.close()
-
-    def _reset(self):
-        self.state.close()
-        self.scm._reset()  # pylint: disable=protected-access
-        self.__dict__.pop("index", None)
-        self.__dict__.pop("dvcignore", None)
-
-    def __enter__(self):
-        return self
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        self._reset()
-        self.scm.close()
+    def remove(self, *args, **kwargs):
+        from dvc.repo.experiments.remove import remove
+
+        return remove(self.repo, *args, **kwargs)
+
+    def clean(self, *args, **kwargs):
+        from dvc.repo.experiments.clean import clean
+
+        return clean(self.repo, *args, **kwargs)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `dvc-2.9.5/dvc/repo/add.py` & `dvc-3.0.0a0/dvc/repo/add.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,45 +1,59 @@
 import logging
 import os
 from contextlib import contextmanager
 from itertools import tee
-from typing import TYPE_CHECKING, Any, Iterator, List
+from typing import TYPE_CHECKING, Any, Iterator, List, NamedTuple, Optional
 
 import colorama
 
-from dvc.ui import ui
-
-from ..exceptions import (
+from dvc.exceptions import (
     CacheLinkError,
+    DvcException,
     InvalidArgumentError,
     OutputDuplicationError,
+    OutputNotFoundError,
     OverlappingOutputPathsError,
     RecursiveAddingWhileUsingFilename,
 )
-from ..repo.scm_context import scm_context
-from ..utils import LARGE_DIR_SIZE, glob_targets, resolve_output, resolve_paths
-from ..utils.collections import ensure_list, validate
+from dvc.repo.scm_context import scm_context
+from dvc.ui import ui
+from dvc.utils import LARGE_DIR_SIZE, glob_targets, resolve_output, resolve_paths
+from dvc.utils.collections import ensure_list, validate
+
 from . import locked
 
 if TYPE_CHECKING:
     from dvc.repo import Repo
     from dvc.stage import Stage
     from dvc.types import TargetType
 
 Stages = List["Stage"]
 logger = logging.getLogger(__name__)
 
 
-OVERLAPPING_OUTPUT_FMT = (
+class StageInfo(NamedTuple):
+    stage: "Stage"
+    output_exists: bool
+
+
+OVERLAPPING_CHILD_FMT = (
     "Cannot add '{out}', because it is overlapping with other "
     "DVC tracked output: '{parent}'.\n"
     "To include '{out}' in '{parent}', run "
     "'dvc commit {parent_stage}'"
 )
 
+OVERLAPPING_PARENT_FMT = (
+    "Cannot add '{parent}', because it is overlapping with other "
+    "DVC tracked output: '{out}'.\n"
+    "To include '{out}' in '{parent}', run "
+    "'dvc remove {out_stage}' and then 'dvc add {parent}'"
+)
+
 
 def check_recursive_and_fname(args):
     if args.recursive and args.fname:
         raise RecursiveAddingWhileUsingFilename()
 
 
 def transform_targets(args):
@@ -82,40 +96,47 @@
 
 
 @contextmanager
 def translate_graph_error(stages: Stages) -> Iterator[None]:
     try:
         yield
     except OverlappingOutputPathsError as exc:
-        msg = OVERLAPPING_OUTPUT_FMT.format(
-            out=exc.overlapping_out.fs_path,
-            parent=exc.parent.fs_path,
-            parent_stage=exc.parent.stage.addressing,
+        if exc.parent in [o for s in stages for o in s.outs]:
+            msg = OVERLAPPING_PARENT_FMT.format(
+                out=exc.overlapping_out,
+                parent=exc.parent,
+                out_stage=exc.overlapping_out.stage.addressing,
+            )
+        else:
+            msg = OVERLAPPING_CHILD_FMT.format(
+                out=exc.overlapping_out,
+                parent=exc.parent,
+                parent_stage=exc.parent.stage.addressing,
+            )
+        raise OverlappingOutputPathsError(  # noqa: B904
+            exc.parent, exc.overlapping_out, msg
         )
-        raise OverlappingOutputPathsError(exc.parent, exc.overlapping_out, msg)
     except OutputDuplicationError as exc:
-        raise OutputDuplicationError(
+        raise OutputDuplicationError(  # noqa: B904
             exc.output, list(set(exc.stages) - set(stages))
         )
 
 
-def progress_iter(stages: Stages) -> Iterator["Stage"]:
+def progress_iter(stages: List[StageInfo]) -> Iterator["StageInfo"]:
     total = len(stages)
     desc = "Adding..."
-    with ui.progress(
-        stages, total=total, desc=desc, unit="file", leave=True
-    ) as pbar:
+    with ui.progress(stages, total=total, desc=desc, unit="file", leave=True) as pbar:
         if total == 1:
             pbar.bar_format = desc
             pbar.refresh()
 
-        for stage in pbar:
+        for item in pbar:
             if total > 1:
-                pbar.set_msg(f"{stage.outs[0]}")
-            yield stage
+                pbar.set_msg(f"{item.stage.outs[0]}")
+            yield item
             if total == 1:  # restore bar format for stats
                 # pylint: disable=no-member
                 pbar.bar_format = pbar.BAR_FMT_DEFAULT
 
 
 LINK_FAILURE_MESSAGE = (
     "\nSome targets could not be linked from cache to workspace.\n{}\n"
@@ -144,20 +165,20 @@
     check_arg_combinations,
 )
 
 
 @validate(*VALIDATORS)
 @locked
 @scm_context
-def add(  # noqa: C901
+def add(
     repo: "Repo",
     targets: "TargetType",
     recursive: bool = False,
     no_commit: bool = False,
-    fname: str = None,
+    fname: Optional[str] = None,
     to_remote: bool = False,
     **kwargs: Any,
 ):
     to_cache = bool(kwargs.get("out")) and not to_remote
     transfer = to_remote or to_cache
 
     glob = kwargs.get("glob", False)
@@ -165,39 +186,41 @@
     # pass one for creating stages, other one is used for iterating here
     add_targets, sources = tee(add_targets)
 
     # collect targets and build stages as we go
     desc = "Collecting targets"
     stages_it = create_stages(repo, add_targets, fname, transfer, **kwargs)
     stages = list(ui.progress(stages_it, desc=desc, unit="file"))
+
+    stages_list = [stage for stage, _ in stages]
     msg = "Collecting stages from the workspace"
-    with translate_graph_error(stages), ui.status(msg) as status:
+    with translate_graph_error(stages_list), ui.status(msg) as status:
         # remove existing stages that are to-be replaced with these
         # new stages for the graph checks.
-        new_index = repo.index.update(stages)
-        status.update("Checking graph")
-        new_index.check_graph()
+        repo.check_graph(
+            stages=stages_list, callback=lambda: status.update("Checking graph")
+        )
 
     odb = None
     if to_remote:
         odb = repo.cloud.get_remote_odb(kwargs.get("remote"), "add")
 
     with warn_link_failures() as link_failures:
-        for stage, source in zip(progress_iter(stages), sources):
+        for (stage, output_exists), source in zip(progress_iter(stages), sources):
+            out = stage.outs[0]
             if to_remote or to_cache:
                 stage.transfer(source, to_remote=to_remote, odb=odb, **kwargs)
             else:
                 try:
-                    stage.save()
-                    if not no_commit:
-                        stage.commit()
+                    path = out.fs.path.abspath(source) if output_exists else None
+                    stage.add_outs(path, no_commit=no_commit)
                 except CacheLinkError:
                     link_failures.append(str(stage.relpath))
             stage.dump()
-    return stages
+    return stages_list
 
 
 LARGE_DIR_RECURSIVE_ADD_WARNING = (
     "You are adding a large directory '{target}' recursively.\n"
     "Consider tracking it as a whole instead with "
     "`{cyan}dvc add {target}{nc}`."
 )
@@ -240,30 +263,38 @@
     else:
         yield target
 
 
 def create_stages(
     repo: "Repo",
     targets: Iterator[str],
-    fname: str = None,
+    fname: Optional[str] = None,
     transfer: bool = False,
     external: bool = False,
+    force: bool = False,
     **kwargs: Any,
-) -> Iterator["Stage"]:
+) -> Iterator[StageInfo]:
     for target in targets:
         if kwargs.get("out"):
-            target = resolve_output(target, kwargs["out"])
+            target = resolve_output(target, kwargs["out"], force=force)
         path, wdir, out = resolve_paths(
             repo, target, always_local=transfer and not kwargs.get("out")
         )
 
-        stage = repo.stage.create(
-            single_stage=True,
-            validate=False,
-            fname=fname or path,
-            wdir=wdir,
-            outs=[out],
-            external=external,
-        )
-        if kwargs.get("desc"):
-            stage.outs[0].desc = kwargs["desc"]
-        yield stage
+        try:
+            (out_obj,) = repo.find_outs_by_path(target, strict=False)
+            stage = out_obj.stage
+            if not stage.is_data_source:
+                raise DvcException(f"cannot update {out!r}: not a data source")
+            output_exists = True
+        except OutputNotFoundError:
+            stage = repo.stage.create(
+                single_stage=True,
+                validate=False,
+                fname=fname or path,
+                wdir=wdir,
+                outs=[out],
+                external=external,
+                force=force,
+            )
+            output_exists = False
+        yield StageInfo(stage, output_exists)
```

### Comparing `dvc-2.9.5/dvc/repo/checkout.py` & `dvc-3.0.0a0/dvc/repo/checkout.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,17 +1,12 @@
 import logging
 import os
-from typing import TYPE_CHECKING, Set
+from typing import TYPE_CHECKING, Dict, List, Set
 
-from dvc.exceptions import (
-    CheckoutError,
-    CheckoutErrorSuggestGit,
-    NoOutputOrStageError,
-)
-from dvc.progress import Tqdm
+from dvc.exceptions import CheckoutError, CheckoutErrorSuggestGit, NoOutputOrStageError
 from dvc.utils import relpath
 
 from . import locked
 
 if TYPE_CHECKING:
     from . import Repo
     from .stage import StageInfo
@@ -24,34 +19,29 @@
         return str(path)
 
     path = relpath(path)
     return os.path.join(path, "") if os.path.isdir(path) else path
 
 
 def _remove_unused_links(repo):
-    used = [out.fspath for out in repo.index.outs if out.scheme == "local"]
+    used = [out.fspath for out in repo.index.outs if out.protocol == "local"]
     unused = repo.state.get_unused_links(used, repo.fs)
     ret = [_fspath_dir(u) for u in unused]
     repo.state.remove_links(unused, repo.fs)
     return ret
 
 
 def get_all_files_numbers(pairs):
-    return sum(
-        stage.get_all_files_number(filter_info) for stage, filter_info in pairs
-    )
+    return sum(stage.get_all_files_number(filter_info) for stage, filter_info in pairs)
 
 
 def _collect_pairs(
     self: "Repo", targets, with_deps: bool, recursive: bool
 ) -> Set["StageInfo"]:
-    from dvc.stage.exceptions import (
-        StageFileBadNameError,
-        StageFileDoesNotExistError,
-    )
+    from dvc.stage.exceptions import StageFileBadNameError, StageFileDoesNotExistError
 
     pairs: Set["StageInfo"] = set()
     for target in targets:
         try:
             pairs.update(
                 self.stage.collect_granular(
                     target, with_deps=with_deps, recursive=recursive
@@ -76,32 +66,41 @@
     with_deps=False,
     force=False,
     relink=False,
     recursive=False,
     allow_missing=False,
     **kwargs,
 ):
+    from dvc.fs.callbacks import Callback
 
-    stats = {"added": [], "deleted": [], "modified": [], "failed": []}
+    stats: Dict[str, List[str]] = {
+        "added": [],
+        "deleted": [],
+        "modified": [],
+        "failed": [],
+    }
     if not targets:
         targets = [None]
         stats["deleted"] = _remove_unused_links(self)
 
     if isinstance(targets, str):
         targets = [targets]
 
     pairs = _collect_pairs(self, targets, with_deps, recursive)
     total = get_all_files_numbers(pairs)
-    with Tqdm(
-        total=total, unit="file", desc="Checkout", disable=total == 0
-    ) as pbar:
+    with Callback.as_tqdm_callback(
+        unit="file",
+        desc="Checkout",
+        disable=total == 0,
+    ) as cb:
+        cb.set_size(total)
         for stage, filter_info in pairs:
             result = stage.checkout(
                 force=force,
-                progress_callback=pbar.update_msg,
+                progress_callback=cb,
                 relink=relink,
                 filter_info=filter_info,
                 allow_missing=allow_missing,
                 **kwargs,
             )
             for key, items in result.items():
                 stats[key].extend(_fspath_dir(path) for path in items)
```

### Comparing `dvc-2.9.5/dvc/repo/collect.py` & `dvc-3.0.0a0/dvc/repo/collect.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,89 +1,78 @@
 import logging
-import os
-from typing import TYPE_CHECKING, Callable, Iterable, List, Tuple
-
-from dvc.types import AnyPath
+from typing import TYPE_CHECKING, Callable, Iterable, List, Optional, Tuple
 
 if TYPE_CHECKING:
     from dvc.output import Output
     from dvc.repo import Repo
 
 logger = logging.getLogger(__name__)
 
 
 FilterFn = Callable[["Output"], bool]
 Outputs = List["Output"]
-AnyPaths = List[AnyPath]
 StrPaths = List[str]
 
 
 def _collect_outs(
-    repo: "Repo", output_filter: FilterFn = None, deps: bool = False
+    repo: "Repo", output_filter: Optional[FilterFn] = None, deps: bool = False
 ) -> Outputs:
     index = repo.index
     index.check_graph()  # ensure graph is correct
     return list(filter(output_filter, index.deps if deps else index.outs))
 
 
 def _collect_paths(
     repo: "Repo",
     targets: Iterable[str],
     recursive: bool = False,
-    rev: str = None,
-):
-    from dvc.fs.repo import RepoFileSystem
-    from dvc.utils import relpath
+) -> StrPaths:
+    from dvc.fs.dvc import DVCFileSystem
 
-    fs_paths = [os.path.abspath(target) for target in targets]
-    fs = RepoFileSystem(repo)
+    fs = DVCFileSystem(repo=repo)
+    fs_paths = [fs.from_os_path(target) for target in targets]
 
-    target_paths = []
+    target_paths: StrPaths = []
     for fs_path in fs_paths:
-
         if recursive and fs.isdir(fs_path):
-            target_paths.extend(repo.dvcignore.find(fs, fs_path))
-
-        if not fs.exists(fs_path):
-            rel = relpath(fs_path)
-            if rev == "workspace" or rev == "":
-                logger.warning("'%s' was not found in current workspace.", rel)
-            else:
-                logger.warning("'%s' was not found at: '%s'.", rel, rev)
+            target_paths.extend(fs.find(fs_path))
         target_paths.append(fs_path)
+
     return target_paths
 
 
-def _filter_duplicates(
-    outs: Outputs, fs_paths: StrPaths
+def _filter_outs(
+    outs: Outputs, fs_paths: StrPaths, duplicates=False
 ) -> Tuple[Outputs, StrPaths]:
     res_outs: Outputs = []
     fs_res_paths = fs_paths
 
     for out in outs:
-        if out.fs_path in fs_paths:
+        fs_path = out.repo.dvcfs.from_os_path(out.fs_path)
+        if fs_path in fs_paths:
             res_outs.append(out)
-            # MUTATING THE SAME LIST!!
-            fs_res_paths.remove(out.fs_path)
+            if not duplicates:
+                # MUTATING THE SAME LIST!!
+                fs_res_paths.remove(fs_path)
 
     return res_outs, fs_res_paths
 
 
 def collect(
     repo: "Repo",
     deps: bool = False,
-    targets: Iterable[str] = None,
-    output_filter: FilterFn = None,
-    rev: str = None,
+    targets: Optional[Iterable[str]] = None,
+    output_filter: Optional[FilterFn] = None,
     recursive: bool = False,
+    duplicates: bool = False,
 ) -> Tuple[Outputs, StrPaths]:
     assert targets or output_filter
 
     outs: Outputs = _collect_outs(repo, output_filter=output_filter, deps=deps)
 
     if not targets:
         fs_paths: StrPaths = []
         return outs, fs_paths
 
-    target_paths = _collect_paths(repo, targets, recursive=recursive, rev=rev)
+    target_paths = _collect_paths(repo, targets, recursive=recursive)
 
-    return _filter_duplicates(outs, target_paths)
+    return _filter_outs(outs, target_paths, duplicates=duplicates)
```

### Comparing `dvc-2.9.5/dvc/repo/commit.py` & `dvc-3.0.0a0/dvc/repo/commit.py`

 * *Files 26% similar despite different names*

```diff
@@ -26,43 +26,45 @@
 
 
 def prompt_to_commit(stage, changes, force=False):
     from dvc.stage.exceptions import StageCommitError
 
     if not (force or prompt.confirm(_prepare_message(stage, changes))):
         raise StageCommitError(
-            "unable to commit changed {}. Use `-f|--force` to "
-            "force.".format(stage)
+            f"unable to commit changed {stage}. Use `-f|--force` to force."
         )
 
 
 @locked
 def commit(
     self,
     target,
     with_deps=False,
     recursive=False,
     force=False,
     allow_missing=False,
     data_only=False,
+    relink=True,
 ):
-    from dvc.dvcfile import Dvcfile
-
     stages_info = [
         info
         for info in self.stage.collect_granular(
             target, with_deps=with_deps, recursive=recursive
         )
         if not data_only or info.stage.is_data_source
     ]
     for stage_info in stages_info:
         stage = stage_info.stage
-        changes = stage.changed_entries()
-        if any(changes):
-            prompt_to_commit(stage, changes, force=force)
+        if force:
             stage.save(allow_missing=allow_missing)
+        else:
+            changes = stage.changed_entries()
+            if any(changes):
+                prompt_to_commit(stage, changes, force=force)
+                stage.save(allow_missing=allow_missing)
         stage.commit(
-            filter_info=stage_info.filter_info, allow_missing=allow_missing
+            filter_info=stage_info.filter_info,
+            allow_missing=allow_missing,
+            relink=relink,
         )
-
-        Dvcfile(self, stage.path).dump(stage, update_pipeline=False)
+        stage.dump(update_pipeline=False)
     return [s.stage for s in stages_info]
```

### Comparing `dvc-2.9.5/dvc/repo/destroy.py` & `dvc-3.0.0a0/dvc/repo/destroy.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/dvc/repo/experiments/__init__.py` & `dvc-3.0.0a0/dvc/repo/experiments/executor/base.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,850 +1,860 @@
 import logging
 import os
-import re
-from functools import wraps
-from typing import Dict, Iterable, List, Mapping, Optional, Type
+import pickle  # nosec B403
+import shutil
+from abc import ABC, abstractmethod
+from contextlib import contextmanager
+from dataclasses import asdict, dataclass
+from enum import IntEnum
+from functools import partial
+from itertools import chain
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Callable,
+    Dict,
+    Iterable,
+    Iterator,
+    List,
+    NamedTuple,
+    Optional,
+    Tuple,
+    Type,
+    TypeVar,
+    Union,
+)
 
-from funcy import cached_property, first
+from funcy import get_in
+from scmrepo.exceptions import SCMError
 
-from dvc.dependency.param import MissingParamsError
-from dvc.env import DVCLIVE_RESUME
+from dvc.env import DVC_EXP_AUTO_PUSH, DVC_EXP_GIT_REMOTE
 from dvc.exceptions import DvcException
-from dvc.utils import relpath
-
-from .base import (
-    EXEC_APPLY,
+from dvc.repo.experiments.exceptions import CheckpointExistsError, ExperimentExistsError
+from dvc.repo.experiments.refs import (
+    EXEC_BASELINE,
     EXEC_BRANCH,
     EXEC_CHECKPOINT,
-    EXEC_NAMESPACE,
-    EXPS_NAMESPACE,
-    EXPS_STASH,
-    BaselineMismatchError,
-    ExperimentExistsError,
     ExpRefInfo,
-    ExpStashEntry,
-    InvalidExpRefError,
-    MultipleBranchError,
-)
-from .executor.base import (
-    EXEC_PID_DIR,
-    EXEC_TMP_DIR,
-    BaseExecutor,
-    ExecutorInfo,
 )
-from .executor.manager.base import BaseExecutorManager
-from .executor.manager.local import (
-    TempDirExecutorManager,
-    WorkspaceExecutorManager,
-)
-from .executor.manager.ssh import SSHExecutorManager
-from .utils import exp_refs_by_rev
+from dvc.repo.experiments.utils import to_studio_params
+from dvc.repo.metrics.show import _collect_top_level_metrics
+from dvc.repo.params.show import _collect_top_level_params
+from dvc.stage.serialize import to_lockfile
+from dvc.ui import ui
+from dvc.utils import dict_sha256, env2bool, relpath
+from dvc.utils.fs import remove
+from dvc.utils.studio import env_to_config
+
+if TYPE_CHECKING:
+    from queue import Queue
+
+    from dvc.repo import Repo
+    from dvc.repo.experiments.stash import ExpStashEntry
+    from dvc.scm import Git
+    from dvc.stage import PipelineStage
 
 logger = logging.getLogger(__name__)
 
 
-def scm_locked(f):
-    # Lock the experiments workspace so that we don't try to perform two
-    # different sequences of git operations at once
-    @wraps(f)
-    def wrapper(exp, *args, **kwargs):
-        from dvc.scm import map_scm_exception
-
-        with map_scm_exception(), exp.scm_lock:
-            return f(exp, *args, **kwargs)
-
-    return wrapper
-
-
-def unlocked_repo(f):
-    @wraps(f)
-    def wrapper(exp, *args, **kwargs):
-        exp.repo.lock.unlock()
-        exp.repo._reset()  # pylint: disable=protected-access
-        try:
-            ret = f(exp, *args, **kwargs)
-        finally:
-            exp.repo.lock.lock()
-        return ret
+class ExecutorResult(NamedTuple):
+    exp_hash: Optional[str]
+    ref_info: Optional["ExpRefInfo"]
+    force: bool
+
+
+class TaskStatus(IntEnum):
+    PENDING = 0
+    PREPARING = 1
+    RUNNING = 2
+    SUCCESS = 3
+    FAILED = 4
+    CANCELED = 5
+    FINISHED = 6
+
+
+@dataclass
+class ExecutorInfo:
+    git_url: str
+    baseline_rev: str
+    location: str
+    root_dir: str
+    dvc_dir: str
+    name: Optional[str] = None
+    wdir: Optional[str] = None
+    result_hash: Optional[str] = None
+    result_ref: Optional[str] = None
+    result_force: bool = False
+    status: TaskStatus = TaskStatus.PENDING
+
+    @classmethod
+    def from_dict(cls, d):
+        if d.pop("collected", None):
+            d["status"] = TaskStatus.FINISHED
+        return cls(**d)
 
-    return wrapper
+    def asdict(self):
+        return asdict(self)
 
+    @property
+    def result(self) -> Optional["ExecutorResult"]:
+        if self.result_hash is None:
+            return None
+        return ExecutorResult(
+            self.result_hash,
+            ExpRefInfo.from_ref(self.result_ref) if self.result_ref else None,
+            self.result_force,
+        )
 
-class Experiments:
-    """Class that manages experiments in a DVC repo.
+    def dump_json(self, filename: str):
+        from dvc.utils.serialize import modify_json
 
-    Args:
-        repo (dvc.repo.Repo): repo instance that these experiments belong to.
-    """
+        os.makedirs(os.path.dirname(filename), exist_ok=True)
+        with modify_json(filename) as d:
+            d.update(self.asdict())
 
-    STASH_EXPERIMENT_FORMAT = "dvc-exp:{rev}:{baseline_rev}:{name}"
-    STASH_EXPERIMENT_RE = re.compile(
-        r"(?:commit: )"
-        r"dvc-exp:(?P<rev>[0-9a-f]+):(?P<baseline_rev>[0-9a-f]+)"
-        r":(?P<name>[^~^:\\?\[\]*]*)"
-        r"(:(?P<branch>.+))?$"
-    )
-    BRANCH_RE = re.compile(
-        r"^(?P<baseline_rev>[a-f0-9]{7})-(?P<exp_sha>[a-f0-9]+)"
-        r"(?P<checkpoint>-checkpoint)?$"
-    )
-
-    def __init__(self, repo):
-        from dvc.lock import make_lock
-        from dvc.scm import NoSCMError
-
-        if repo.config["core"].get("no_scm", False):
-            raise NoSCMError
-
-        self.repo = repo
-        self.scm_lock = make_lock(
-            os.path.join(self.repo.tmp_dir, "exp_scm_lock"),
-            tmp_dir=self.repo.tmp_dir,
-            hardlink_lock=repo.config["core"].get("hardlink_lock", False),
-        )
+    @classmethod
+    def load_json(cls, filename: str) -> "ExecutorInfo":
+        from dvc.utils.serialize import load_json
 
-    @property
-    def scm(self):
-        return self.repo.scm
+        return cls.from_dict(load_json(filename))
 
-    @cached_property
-    def dvc_dir(self):
-        return relpath(self.repo.dvc_dir, self.repo.scm.root_dir)
-
-    @cached_property
-    def args_file(self):
-        return os.path.join(self.repo.tmp_dir, BaseExecutor.PACKED_ARGS_FILE)
-
-    @cached_property
-    def stash(self):
-        from scmrepo.git import Stash
 
-        return Stash(self.scm, EXPS_STASH)
+_T = TypeVar("_T", bound="BaseExecutor")
 
-    @property
-    def stash_revs(self) -> Dict[str, ExpStashEntry]:
-        revs = {}
-        for i, entry in enumerate(self.stash):
-            msg = entry.message.decode("utf-8").strip()
-            m = self.STASH_EXPERIMENT_RE.match(msg)
-            if m:
-                revs[entry.new_sha.decode("utf-8")] = ExpStashEntry(
-                    i,
-                    m.group("rev"),
-                    m.group("baseline_rev"),
-                    m.group("branch"),
-                    m.group("name"),
-                )
-        return revs
 
-    def _stash_exp(
+class BaseExecutor(ABC):
+    """Base class for executing experiments in parallel.
+
+    Parameters:
+        root_dir: Path to SCM root.
+        dvc_dir: Path to .dvc dir relative to SCM root.
+        baseline_rev: Experiment baseline revision.
+        wdir: Path to exec working directory relative to SCM root.
+        name: Executor (experiment) name.
+        result: Completed executor result.
+    """
+
+    PACKED_ARGS_FILE = "repro.dat"
+    WARN_UNTRACKED = False
+    QUIET = False
+    INFOFILE_EXT = ".run"
+    DEFAULT_LOCATION: str = "workspace"
+
+    def __init__(
         self,
-        *args,
-        params: Optional[dict] = None,
-        resume_rev: Optional[str] = None,
-        baseline_rev: Optional[str] = None,
-        branch: Optional[str] = None,
+        root_dir: str,
+        dvc_dir: str,
+        baseline_rev: str,
+        status: TaskStatus,
+        wdir: Optional[str] = None,
         name: Optional[str] = None,
+        location: Optional[str] = None,
+        result: Optional["ExecutorResult"] = None,
         **kwargs,
     ):
-        """Stash changes from the workspace as an experiment.
-
-        Args:
-            params: Optional dictionary of parameter values to be used.
-                Values take priority over any parameters specified in the
-                user's workspace.
-            resume_rev: Optional checkpoint resume rev.
-            baseline_rev: Optional baseline rev for this experiment, defaults
-                to the current SCM rev.
-            branch: Optional experiment branch name. If specified, the
-                experiment will be added to `branch` instead of creating
-                a new branch.
-            name: Optional experiment name. If specified this will be used as
-                the human-readable name in the experiment branch ref. Has no
-                effect of branch is specified.
-        """
-        with self.scm.detach_head(client="dvc") as orig_head:
-            stash_head = orig_head
-            if baseline_rev is None:
-                baseline_rev = orig_head
-
-            with self.scm.stash_workspace() as workspace:
-                try:
-                    if workspace:
-                        self.stash.apply(workspace)
-
-                    if resume_rev:
-                        # move HEAD to the resume rev so that the stashed diff
-                        # only contains changes relative to resume rev
-                        stash_head = resume_rev
-                        self.scm.set_ref(
-                            "HEAD",
-                            resume_rev,
-                            message=f"dvc: resume from HEAD {resume_rev[:7]}",
-                        )
-                        self.scm.reset()
-
-                    # update experiment params from command line
-                    if params:
-                        self._update_params(params)
-
-                    # DVC commit data deps to preserve state across workspace
-                    # & tempdir runs
-                    self._stash_commit_deps(*args, **kwargs)
-
-                    if resume_rev:
-                        if branch:
-                            branch_name = ExpRefInfo.from_ref(branch).name
-                        else:
-                            branch_name = f"{resume_rev[:7]}"
-                        if self.scm.is_dirty():
-                            logger.info(
-                                "Modified checkpoint experiment based on "
-                                "'%s' will be created",
-                                branch_name,
-                            )
-                            branch = None
-                        elif (
-                            not branch
-                            or self.scm.get_ref(branch) != resume_rev
-                        ):
-                            msg = [
-                                (
-                                    "Nothing to do for unchanged checkpoint "
-                                    f"'{resume_rev[:7]}'. "
-                                )
-                            ]
-                            if branch:
-                                msg.append(
-                                    "To resume from the head of this "
-                                    "experiment, use "
-                                    f"'dvc exp apply {branch_name}'."
-                                )
-                            else:
-                                names = [
-                                    ref_info.name
-                                    for ref_info in exp_refs_by_rev(
-                                        self.scm, resume_rev
-                                    )
-                                ]
-                                if len(names) > 3:
-                                    names[3:] = [
-                                        f"... ({len(names) - 3} more)"
-                                    ]
-                                msg.append(
-                                    "To resume an experiment containing this "
-                                    "checkpoint, apply one of these heads:\n"
-                                    "\t{}".format(", ".join(names))
-                                )
-                            raise DvcException("".join(msg))
-                        else:
-                            logger.info(
-                                "Existing checkpoint experiment '%s' will be "
-                                "resumed",
-                                branch_name,
-                            )
-                        if name:
-                            logger.warning(
-                                "Ignoring option '--name %s' for resumed "
-                                "experiment. Existing experiment name will"
-                                "be preserved instead.",
-                                name,
-                            )
-
-                    # save additional repro command line arguments
-                    run_env = {DVCLIVE_RESUME: "1"} if resume_rev else {}
-                    self._pack_args(*args, run_env=run_env, **kwargs)
-
-                    # save experiment as a stash commit
-                    msg = self._stash_msg(
-                        stash_head,
-                        baseline_rev=baseline_rev,
-                        branch=branch,
-                        name=name,
-                    )
-                    stash_rev = self.stash.push(message=msg)
-                    logger.debug(
-                        (
-                            "Stashed experiment '%s' with baseline '%s' "
-                            "for future execution."
-                        ),
-                        stash_rev[:7],
-                        baseline_rev[:7],
-                    )
-                finally:
-                    if resume_rev:
-                        # NOTE: this set_ref + reset() is equivalent to
-                        # `git reset orig_head` (our SCM reset() only operates
-                        # on HEAD rather than any arbitrary commit)
-                        self.scm.set_ref(
-                            "HEAD", orig_head, message="dvc: restore HEAD"
-                        )
-                        self.scm.reset()
-                    # Revert any of our changes before prior unstashing
-                    self.scm.reset(hard=True)
-
-        return stash_rev
-
-    def _stash_commit_deps(self, *args, **kwargs):
-        if len(args):
-            targets = args[0]
-        else:
-            targets = kwargs.get("targets")
-        if isinstance(targets, str):
-            targets = [targets]
-        elif not targets:
-            targets = [None]
-        for target in targets:
-            self.repo.commit(
-                target,
-                with_deps=True,
-                recursive=kwargs.get("recursive", False),
-                force=True,
-                allow_missing=True,
-                data_only=True,
-            )
+        self.dvc_dir = dvc_dir
+        self.root_dir = root_dir
+        self.wdir = wdir
+        self.name = name
+        self.baseline_rev = baseline_rev
+        self.location: str = location or self.DEFAULT_LOCATION
+        self.result = result
+        self.status = status
 
-    def _stash_msg(
+    @abstractmethod
+    def init_git(
         self,
-        rev: str,
-        baseline_rev: str,
+        repo: "Repo",
+        scm: "Git",
+        stash_rev: str,
+        entry: "ExpStashEntry",
+        infofile: Optional[str],
         branch: Optional[str] = None,
-        name: Optional[str] = None,
     ):
-        if not baseline_rev:
-            baseline_rev = rev
-        msg = self.STASH_EXPERIMENT_FORMAT.format(
-            rev=rev, baseline_rev=baseline_rev, name=name if name else ""
-        )
-        if branch:
-            return f"{msg}:{branch}"
-        return msg
+        """Init git repo and populate it using exp refs from the specified
+        SCM instance.
+        """
 
-    def _pack_args(self, *args, **kwargs):
-        import pickle
+    @property
+    @abstractmethod
+    def git_url(self) -> str:
+        pass
+
+    @abstractmethod
+    def init_cache(self, repo: "Repo", rev: str, run_cache: bool = True):
+        """Initialize DVC cache."""
+
+    @abstractmethod
+    def collect_cache(
+        self, repo: "Repo", exp_ref: "ExpRefInfo", run_cache: bool = True
+    ):
+        """Collect DVC cache."""
 
-        if os.path.exists(self.args_file) and self.scm.is_tracked(
-            self.args_file
-        ):
-            logger.warning(
-                (
-                    "Temporary DVC file '.dvc/tmp/%s' exists and was "
-                    "likely committed to Git by mistake. It should be removed "
-                    "with:\n"
-                    "\tgit rm .dvc/tmp/%s"
+    @property
+    def info(self) -> "ExecutorInfo":
+        if self.result is not None:
+            result_dict: Dict[str, Any] = {
+                "result_hash": self.result.exp_hash,
+                "result_ref": (
+                    str(self.result.ref_info) if self.result.ref_info else None
                 ),
-                BaseExecutor.PACKED_ARGS_FILE,
-                BaseExecutor.PACKED_ARGS_FILE,
+                "result_force": self.result.force,
+            }
+        else:
+            result_dict = {}
+        return ExecutorInfo(
+            git_url=self.git_url,
+            baseline_rev=self.baseline_rev,
+            location=self.location,
+            root_dir=self.root_dir,
+            dvc_dir=self.dvc_dir,
+            name=self.name,
+            wdir=self.wdir,
+            status=self.status,
+            **result_dict,
+        )
+
+    @classmethod
+    def from_info(cls: Type[_T], info: "ExecutorInfo") -> _T:
+        if info.result_hash:
+            result: Optional["ExecutorResult"] = ExecutorResult(
+                info.result_hash,
+                (ExpRefInfo.from_ref(info.result_ref) if info.result_ref else None),
+                info.result_force,
             )
-            with open(self.args_file, "rb") as fobj:
-                try:
-                    data = pickle.load(fobj)
-                except Exception:  # pylint: disable=broad-except
-                    data = {}
-            extra = int(data.get("extra", 0)) + 1
         else:
-            extra = None
-        BaseExecutor.pack_repro_args(
-            self.args_file, *args, extra=extra, **kwargs
-        )
-        self.scm.add(self.args_file)
-
-    def _format_new_params_msg(self, new_params, config_path):
-        """Format an error message for when new parameters are identified"""
-        new_param_count = len(new_params)
-        pluralise = "s are" if new_param_count > 1 else " is"
-        param_list = ", ".join(new_params)
-        return (
-            f"{new_param_count} parameter{pluralise} missing "
-            f"from '{config_path}': {param_list}"
-        )
-
-    def _update_params(self, params: dict):
-        """Update experiment params files with the specified values."""
-        from dvc.utils.collections import NewParamsFound, merge_params
-        from dvc.utils.serialize import MODIFIERS
-
-        logger.debug("Using experiment params '%s'", params)
-
-        for path in params:
-            suffix = self.repo.fs.path.suffix(path).lower()
-            modify_data = MODIFIERS[suffix]
-            with modify_data(path, fs=self.repo.fs) as data:
-                try:
-                    merge_params(data, params[path], allow_new=False)
-                except NewParamsFound as e:
-                    msg = self._format_new_params_msg(e.new_params, path)
-                    raise MissingParamsError(msg)
-
-        # Force params file changes to be staged in git
-        # Otherwise in certain situations the changes to params file may be
-        # ignored when we `git stash` them since mtime is used to determine
-        # whether the file is dirty
-        self.scm.add(list(params.keys()))
-
-    def reproduce_one(
-        self,
-        queue: bool = False,
-        tmp_dir: bool = False,
-        checkpoint_resume: Optional[str] = None,
-        reset: bool = False,
-        machine: Optional[str] = None,
+            result = None
+        return cls(
+            root_dir=info.root_dir,
+            dvc_dir=info.dvc_dir,
+            baseline_rev=info.baseline_rev,
+            status=info.status,
+            name=info.name,
+            wdir=info.wdir,
+            result=result,
+        )
+
+    @classmethod
+    @abstractmethod
+    def from_stash_entry(
+        cls: Type[_T],
+        repo: "Repo",
+        entry: "ExpStashEntry",
         **kwargs,
-    ):
-        """Reproduce and checkout a single experiment."""
-        if queue and not checkpoint_resume:
-            reset = True
-
-        if reset:
-            self.reset_checkpoints()
-
-        if not (queue or tmp_dir or machine):
-            staged, _, _ = self.scm.status()
-            if staged:
-                logger.warning(
-                    "Your workspace contains staged Git changes which will be "
-                    "unstaged before running this experiment."
-                )
-                self.scm.reset()
-
-        if checkpoint_resume:
-            from dvc.scm import resolve_rev
-
-            resume_rev = resolve_rev(self.scm, checkpoint_resume)
-            try:
-                self.check_baseline(resume_rev)
-                checkpoint_resume = resume_rev
-            except BaselineMismatchError as exc:
-                raise DvcException(
-                    f"Cannot resume from '{checkpoint_resume}' as it is not "
-                    "derived from your current workspace."
-                ) from exc
-        else:
-            checkpoint_resume = self._workspace_resume_rev()
+    ) -> _T:
+        pass
 
-        stash_rev = self.new(
-            checkpoint_resume=checkpoint_resume, reset=reset, **kwargs
-        )
-        if queue:
-            logger.info(
-                "Queued experiment '%s' for future execution.", stash_rev[:7]
-            )
-            return [stash_rev]
-        if tmp_dir or queue:
-            manager_cls: Type = TempDirExecutorManager
-        elif machine:
-            manager_cls = SSHExecutorManager
-        else:
-            manager_cls = WorkspaceExecutorManager
-        results = self._reproduce_revs(
-            revs=[stash_rev],
-            keep_stash=False,
-            manager_cls=manager_cls,
-            machine=machine,
-        )
-        exp_rev = first(results)
-        if exp_rev is not None:
-            self._log_reproduced(results, tmp_dir=tmp_dir)
-        return results
-
-    def _workspace_resume_rev(self) -> Optional[str]:
-        last_checkpoint = self._get_last_checkpoint()
-        last_applied = self._get_last_applied()
-        if last_checkpoint and last_applied:
-            return last_applied
-        return None
-
-    def reproduce_queued(self, **kwargs):
-        results = self._reproduce_revs(**kwargs)
-        if results:
-            self._log_reproduced(results, tmp_dir=True)
-        return results
-
-    def _log_reproduced(self, revs: Iterable[str], tmp_dir: bool = False):
-        names = []
-        for rev in revs:
-            name = self.get_exact_name(rev)
-            names.append(name if name else rev[:7])
-        logger.info("\nReproduced experiment(s): %s", ", ".join(names))
-        if tmp_dir:
-            logger.info(
-                "To apply the results of an experiment to your workspace "
-                "run:\n\n"
-                "\tdvc exp apply <exp>"
-            )
-        else:
-            logger.info(
-                "Experiment results have been applied to your workspace."
-            )
-        logger.info(
-            "\nTo promote an experiment to a Git branch run:\n\n"
-            "\tdvc exp branch <exp> <branch>\n"
+    @classmethod
+    def _from_stash_entry(
+        cls: Type[_T],
+        repo: "Repo",
+        entry: "ExpStashEntry",
+        root_dir: str,
+        **kwargs,
+    ) -> _T:
+        return cls(
+            root_dir=root_dir,
+            dvc_dir=relpath(repo.dvc_dir, repo.scm.root_dir),
+            baseline_rev=entry.baseline_rev,
+            status=TaskStatus.PREPARING,
+            name=entry.name,
+            wdir=relpath(os.getcwd(), repo.scm.root_dir),
+            **kwargs,
         )
 
-    def _validate_new_ref(self, exp_ref: ExpRefInfo):
-        from .utils import check_ref_format
-
-        if not exp_ref.name:
-            return
-
-        check_ref_format(self.scm, exp_ref)
-
-        if self.scm.get_ref(str(exp_ref)):
-            raise ExperimentExistsError(exp_ref.name)
-
-    @scm_locked
-    def new(self, *args, checkpoint_resume: Optional[str] = None, **kwargs):
-        """Create a new experiment.
-
-        Experiment will be reproduced and checked out into the user's
-        workspace.
-        """
-        if checkpoint_resume is not None:
-            return self._resume_checkpoint(
-                *args, resume_rev=checkpoint_resume, **kwargs
+    @classmethod
+    def _get_top_level_paths(cls, repo: "Repo") -> List["str"]:
+        return list(
+            chain(
+                _collect_top_level_metrics(repo),
+                _collect_top_level_params(repo),
+                repo.index._plot_sources,  # pylint: disable=protected-access
             )
+        )
 
-        name = kwargs.get("name", None)
-        baseline_sha = kwargs.get("baseline_rev") or self.repo.scm.get_rev()
-        exp_ref = ExpRefInfo(baseline_sha=baseline_sha, name=name)
-
-        try:
-            self._validate_new_ref(exp_ref)
-        except ExperimentExistsError as err:
-            if not (kwargs.get("force", False) or kwargs.get("reset", False)):
-                raise err
-
-        return self._stash_exp(*args, **kwargs)
-
-    def _resume_checkpoint(
-        self, *args, resume_rev: Optional[str] = None, **kwargs
-    ):
-        """Resume an existing (checkpoint) experiment.
+    @classmethod
+    def save(
+        cls,
+        info: "ExecutorInfo",
+        force: bool = False,
+        include_untracked: Optional[List[str]] = None,
+        message: Optional[str] = None,
+    ) -> ExecutorResult:
+        from dvc.dvcfile import LOCK_FILE
+        from dvc.repo import Repo
+
+        exp_hash: Optional[str] = None
+        exp_ref: Optional[ExpRefInfo] = None
+
+        dvc = Repo(os.path.join(info.root_dir, info.dvc_dir))
+        old_cwd = os.getcwd()
+        if info.wdir:
+            os.chdir(os.path.join(dvc.scm.root_dir, info.wdir))
+        else:
+            os.chdir(dvc.root_dir)
 
-        Experiment will be reproduced and checked out into the user's
-        workspace.
-        """
-        assert resume_rev
+        include_untracked = include_untracked or []
+        include_untracked.extend(cls._get_top_level_paths(dvc))
+        # dvc repro automatically stages dvc.lock. Running redundant `git add`
+        # on it causes an error when exiting the detached head context.
+        if LOCK_FILE in dvc.scm.untracked_files():
+            include_untracked.append(LOCK_FILE)
 
-        branch: Optional[str] = None
         try:
-            allow_multiple = bool(kwargs.get("params", None))
-            branch = self.get_branch_by_rev(
-                resume_rev, allow_multiple=allow_multiple
+            stages = dvc.commit([], force=True, relink=False)
+            exp_hash = cls.hash_exp(stages)
+            if include_untracked:
+                dvc.scm.add(include_untracked)
+            cls.commit(
+                dvc.scm,  # type: ignore[arg-type]
+                exp_hash,
+                exp_name=info.name,
+                force=force,
+                message=message,
             )
-            if not branch:
-                raise DvcException(
-                    "Could not find checkpoint experiment "
-                    f"'{resume_rev[:7]}'"
+            ref: Optional[str] = dvc.scm.get_ref(EXEC_BRANCH, follow=False)
+            exp_ref = ExpRefInfo.from_ref(ref) if ref else None
+            untracked = dvc.scm.untracked_files()
+            if untracked:
+                logger.warning(
+                    "The following untracked files were present in "
+                    "the workspace before saving but "
+                    "will not be included in the experiment commit:\n"
+                    "\t%s",
+                    ", ".join(untracked),
                 )
-            baseline_rev = self._get_baseline(branch)
-        except MultipleBranchError as exc:
-            baselines = {
-                info.baseline_sha
-                for info in exc.ref_infos
-                if info.baseline_sha
-            }
-            if len(baselines) == 1:
-                baseline_rev = baselines.pop()
-            else:
-                raise
+            info.result_hash = exp_hash
+            info.result_ref = ref
+            info.result_force = False
+            info.status = TaskStatus.SUCCESS
+        except DvcException:
+            info.status = TaskStatus.FAILED
+            raise
+        finally:
+            dvc.close()
+            os.chdir(old_cwd)
 
-        logger.debug(
-            "Checkpoint run from '%s' with baseline '%s'",
-            resume_rev[:7],
-            baseline_rev,
-        )
-        return self._stash_exp(
-            *args,
-            resume_rev=resume_rev,
-            baseline_rev=baseline_rev,
-            branch=branch,
-            **kwargs,
-        )
+        return ExecutorResult(ref, exp_ref, info.result_force)
 
-    def _get_last_checkpoint(self) -> Optional[str]:
-        try:
-            last_checkpoint = self.scm.get_ref(EXEC_CHECKPOINT)
-            if last_checkpoint:
-                self.check_baseline(last_checkpoint)
-            return last_checkpoint
-        except BaselineMismatchError:
-            # If HEAD has moved since the the last checkpoint run,
-            # the specified checkpoint is no longer relevant
-            self.scm.remove_ref(EXEC_CHECKPOINT)
-        return None
+    @staticmethod
+    def hash_exp(stages: Iterable["PipelineStage"]) -> str:
+        from dvc.stage import PipelineStage
+
+        exp_data = {}
+        for stage in stages:
+            if isinstance(stage, PipelineStage):
+                exp_data.update(to_lockfile(stage))
+        return dict_sha256(exp_data)
+
+    def cleanup(self, infofile: Optional[str] = None):
+        if infofile is not None:
+            info = ExecutorInfo.load_json(infofile)
+            if info.status < TaskStatus.FAILED:
+                info.status = TaskStatus.FINISHED
+            info.dump_json(infofile)
+
+    # TODO: come up with better way to stash repro arguments
+    @staticmethod
+    def pack_repro_args(path, *args, fs=None, extra=None, **kwargs):
+        dpath = os.path.dirname(path)
+        if fs:
+            open_func = fs.open
+            fs.makedirs(dpath)
+        else:
+            open_func = open
+            os.makedirs(dpath, exist_ok=True)
 
-    def _get_last_applied(self) -> Optional[str]:
-        try:
-            last_applied = self.scm.get_ref(EXEC_APPLY)
-            if last_applied:
-                self.check_baseline(last_applied)
-            return last_applied
-        except BaselineMismatchError:
-            # If HEAD has moved since the the last applied experiment,
-            # the applied experiment is no longer relevant
-            self.scm.remove_ref(EXEC_APPLY)
-        return None
-
-    def reset_checkpoints(self):
-        self.scm.remove_ref(EXEC_CHECKPOINT)
-        self.scm.remove_ref(EXEC_APPLY)
+        data = {"args": args, "kwargs": kwargs}
+        if extra is not None:
+            data["extra"] = extra
+        with open_func(path, "wb") as fobj:
+            pickle.dump(data, fobj)
+
+    @staticmethod
+    def unpack_repro_args(path):
+        with open(path, "rb") as fobj:
+            data = pickle.load(fobj)  # noqa: S301 # nosec B301
+        return data["args"], data["kwargs"]
 
-    @scm_locked
-    def _reproduce_revs(
+    def fetch_exps(
         self,
-        revs: Optional[Iterable] = None,
-        keep_stash: Optional[bool] = True,
-        manager_cls: Type = TempDirExecutorManager,
-        machine: Optional[str] = None,
+        dest_scm: "Git",
+        refs: List[str],
+        force: bool = False,
+        on_diverged: Optional[Callable[[str, bool], None]] = None,
         **kwargs,
-    ) -> Mapping[str, str]:
-        """Reproduce the specified experiments.
+    ) -> Iterable[str]:
+        """Fetch reproduced experiment refs into the specified SCM.
 
         Args:
-            revs: If revs is not specified, all stashed experiments will be
-                reproduced.
-            keep_stash: If True, stashed experiments will be preserved if they
-                fail to reproduce successfully.
-
-        Returns:
-            dict mapping successfully reproduced experiment revs to their
-            hashes.
+            dest_scm: Destination Git instance.
+            refs: reference names to be fetched from the remotes.
+            force: If True, diverged refs will be overwritten
+            on_diverged: Callback in the form on_diverged(ref, is_checkpoint)
+                to be called when an experiment ref has diverged.
+
+        Extra kwargs will be passed into the remote git client.
         """
-        stash_revs = self.stash_revs
 
-        # to_run contains mapping of:
-        #   input_rev: (stash_index, rev, baseline_rev)
-        # where input_rev contains the changes to execute (usually a stash
-        # commit), rev is the original SCM commit to be checked out, and
-        # baseline_rev is the experiment baseline.
-        if revs is None:
-            to_run = dict(stash_revs)
+        if EXEC_CHECKPOINT in refs:
+            refs.remove(EXEC_CHECKPOINT)
+            has_checkpoint = True
         else:
-            to_run = {
-                rev: stash_revs[rev]
-                if rev in stash_revs
-                else ExpStashEntry(None, rev, rev, None, None)
-                for rev in revs
-            }
-
-        logger.debug(
-            "Reproducing experiment revs '%s'",
-            ", ".join(rev[:7] for rev in to_run),
-        )
-
-        manager = manager_cls.from_stash_entries(
-            self.scm,
-            os.path.join(self.repo.tmp_dir, EXEC_TMP_DIR),
-            self.repo,
-            to_run,
-            machine_name=machine,
-        )
-        try:
-            exec_results = {}
-            exec_results.update(self._executors_repro(manager, **kwargs))
-        finally:
-            # only drop successfully run stashed experiments
-            to_drop: List[int] = [
-                entry.stash_index
-                for rev, entry in to_run.items()
-                if (
-                    entry.stash_index is not None
-                    and (not keep_stash or rev in exec_results)
-                )
-            ]
-            for index in sorted(to_drop, reverse=True):
-                self.stash.drop(index)
-
-        result: Dict[str, str] = {}
-        for _, exp_result in exec_results.items():
-            result.update(exp_result)
-        return result
+            has_checkpoint = False
 
-    @unlocked_repo
-    def _executors_repro(
-        self,
-        manager: "BaseExecutorManager",
-        **kwargs,
-    ) -> Dict[str, Dict[str, str]]:
-        """Run dvc repro for the specified BaseExecutors in parallel.
+        def on_diverged_ref(orig_ref: str, new_rev: str):
+            if force:
+                logger.debug("Replacing existing experiment '%s'", orig_ref)
+                return True
 
-        Returns:
-            dict mapping stash revs to the successfully executed experiments
-            for each stash rev.
-        """
-        return manager.exec_queue(self.repo, **kwargs)
+            if on_diverged:
+                return on_diverged(orig_ref, has_checkpoint)
 
-    def check_baseline(self, exp_rev):
-        baseline_sha = self.repo.scm.get_rev()
-        if exp_rev == baseline_sha:
-            return exp_rev
-
-        exp_baseline = self._get_baseline(exp_rev)
-        if exp_baseline is None:
-            # if we can't tell from branch name, fall back to parent commit
-            exp_commit = self.scm.resolve_commit(exp_rev)
-            if exp_commit:
-                exp_baseline = first(exp_commit.parents)
-        if exp_baseline == baseline_sha:
-            return exp_baseline
-        raise BaselineMismatchError(exp_baseline, baseline_sha)
-
-    @scm_locked
-    def get_baseline(self, rev):
-        """Return the baseline rev for an experiment rev."""
-        return self._get_baseline(rev)
-
-    def _get_baseline(self, rev):
-        from dvc.scm import resolve_rev
-
-        rev = resolve_rev(self.scm, rev)
-
-        if rev in self.stash_revs:
-            entry = self.stash_revs.get(rev)
-            if entry:
-                return entry.baseline_rev
-            return None
+            self._raise_ref_conflict(dest_scm, orig_ref, new_rev, has_checkpoint)
+            logger.debug("Reproduced existing experiment '%s'", orig_ref)
+            return False
 
-        ref_info = first(exp_refs_by_rev(self.scm, rev))
-        if ref_info:
-            return ref_info.baseline_sha
-        return None
-
-    def get_branch_by_rev(
-        self, rev: str, allow_multiple: bool = False
-    ) -> Optional[str]:
-        """Returns full refname for the experiment branch containing rev."""
-        ref_infos = list(exp_refs_by_rev(self.scm, rev))
-        if not ref_infos:
-            return None
-        if len(ref_infos) > 1 and not allow_multiple:
-            for ref_info in ref_infos:
-                if self.scm.get_ref(str(ref_info)) == rev:
-                    return str(ref_info)
-            raise MultipleBranchError(rev, ref_infos)
-        return str(ref_infos[0])
+        # fetch experiments
+        try:
+            refspecs = [f"{ref}:{ref}" for ref in refs]
+            # update last run checkpoint (if it exists)
+            if has_checkpoint:
+                refspecs.append(f"{EXEC_CHECKPOINT}:{EXEC_CHECKPOINT}")
+            dest_scm.fetch_refspecs(
+                self.git_url,
+                refspecs,
+                on_diverged=on_diverged_ref,
+                force=force or has_checkpoint,
+                **kwargs,
+            )
+        except SCMError:
+            pass
 
-    def get_exact_name(self, rev: str):
-        """Returns preferred name for the specified revision.
+        return refs
 
-        Prefers tags, branches (heads), experiments in that orer.
-        """
-        exclude = f"{EXEC_NAMESPACE}/*"
-        ref = self.scm.describe(rev, base=EXPS_NAMESPACE, exclude=exclude)
-        if ref:
-            try:
-                name = ExpRefInfo.from_ref(ref).name
-                if name:
-                    return name
-            except InvalidExpRefError:
-                pass
-        if rev in self.stash_revs:
-            return self.stash_revs[rev].name
-        return None
+    @classmethod
+    def _validate_remotes(cls, dvc: "Repo", git_remote: Optional[str]):
+        from scmrepo.exceptions import InvalidRemote
 
-    def get_running_exps(self) -> Dict[str, int]:
-        """Return info for running experiments."""
         from dvc.scm import InvalidRemoteSCMRepo
-        from dvc.utils.serialize import load_json
-
-        from .executor.local import TempDirExecutor
-
-        result = {}
-        pid_dir = os.path.join(
-            self.repo.tmp_dir,
-            EXEC_TMP_DIR,
-            EXEC_PID_DIR,
-        )
-        for fname in self.repo.fs.find(pid_dir):
-            rev, ext = os.path.splitext(os.path.basename(fname))
-            if ext != BaseExecutor.INFOFILE_EXT:
-                continue
-
-            try:
-                info = ExecutorInfo.from_dict(load_json(fname))
-                if info.result is not None:
-                    continue
-                if rev == "workspace":
-                    # If we are appending to a checkpoint branch in a workspace
-                    # run, show the latest checkpoint as running.
-                    last_rev = self.scm.get_ref(EXEC_BRANCH)
-                    if last_rev:
-                        result[last_rev] = info.asdict()
-                    else:
-                        result[rev] = info.asdict()
-                else:
-                    result[rev] = info.asdict()
-                    if info.git_url:
-
-                        def on_diverged(_ref: str, _checkpoint: bool):
-                            return False
 
-                        executor = TempDirExecutor.from_info(info)
-                        try:
-                            for ref in executor.fetch_exps(
-                                self.scm,
-                                on_diverged=on_diverged,
-                            ):
-                                logger.debug(
-                                    "Updated running experiment '%s'.", ref
-                                )
-                                last_rev = self.scm.get_ref(ref)
-                                result[rev]["last"] = last_rev
-                                if last_rev:
-                                    result[last_rev] = info.asdict()
-                        except InvalidRemoteSCMRepo:
-                            # ignore stale info files
-                            del result[rev]
-            except OSError:
-                pass
-        return result
+        if git_remote == dvc.root_dir:
+            logger.warning(
+                (
+                    "'%s' points to the current Git repo, experiment "
+                    "Git refs will not be pushed. But DVC cache and run cache "
+                    "will automatically be pushed to the default DVC remote "
+                    "(if any) on each experiment commit."
+                ),
+                git_remote,
+            )
+        try:
+            dvc.scm.validate_git_remote(git_remote)
+        except InvalidRemote as exc:
+            raise InvalidRemoteSCMRepo(str(exc))  # noqa: B904
+        dvc.cloud.get_remote_odb()
+
+    @classmethod
+    def reproduce(
+        cls,
+        info: "ExecutorInfo",
+        rev: str,
+        queue: Optional["Queue"] = None,
+        infofile: Optional[str] = None,
+        log_errors: bool = True,
+        log_level: Optional[int] = None,
+        copy_paths: Optional[List[str]] = None,
+        message: Optional[str] = None,
+        **kwargs,
+    ) -> "ExecutorResult":
+        """Run dvc repro and return the result.
 
-    def apply(self, *args, **kwargs):
-        from dvc.repo.experiments.apply import apply
+        Returns tuple of (exp_hash, exp_ref, force) where exp_hash is the
+            experiment hash (or None on error), exp_ref is the experiment ref,
+            and force is a bool specifying whether or not this experiment
+            should force overwrite any existing duplicates.
+        """
+        from dvc.repo.checkout import checkout as dvc_checkout
+        from dvc.repo.reproduce import reproduce as dvc_reproduce
+        from dvc.stage import PipelineStage
+
+        auto_push = env2bool(DVC_EXP_AUTO_PUSH)
+        git_remote = os.getenv(DVC_EXP_GIT_REMOTE, None)
+
+        unchanged = []
+
+        if queue is not None:
+            queue.put((rev, os.getpid()))
+        if log_errors and log_level is not None:
+            cls._set_log_level(log_level)
+
+        def filter_pipeline(stages):
+            unchanged.extend(
+                [stage for stage in stages if isinstance(stage, PipelineStage)]
+            )
 
-        return apply(self.repo, *args, **kwargs)
+        exp_hash: Optional[str] = None
+        exp_ref: Optional["ExpRefInfo"] = None
+        repro_force: bool = False
+
+        with cls._repro_dvc(
+            info,
+            infofile,
+            log_errors=log_errors,
+            copy_paths=copy_paths,
+            message=message,
+            **kwargs,
+        ) as dvc:
+            if auto_push:
+                cls._validate_remotes(dvc, git_remote)
+
+            args, kwargs = cls._repro_args(dvc)
+            if args:
+                targets: Optional[Union[list, str]] = args[0]
+            else:
+                targets = kwargs.get("targets")
 
-    def branch(self, *args, **kwargs):
-        from dvc.repo.experiments.branch import branch
+            repro_force = kwargs.get("force", False)
+            logger.trace(  # type: ignore[attr-defined]
+                "Executor repro with force = '%s'", str(repro_force)
+            )
 
-        return branch(self.repo, *args, **kwargs)
+            repro_dry = kwargs.get("dry")
 
-    def diff(self, *args, **kwargs):
-        from dvc.repo.experiments.diff import diff
+            # NOTE: checkpoint outs are handled as a special type of persist
+            # out:
+            #
+            # - checkpoint out may not yet exist if this is the first time this
+            #   experiment has been run, this is not an error condition for
+            #   experiments
+            # - if experiment was run with --reset, the checkpoint out will be
+            #   removed at the start of the experiment (regardless of any
+            #   dvc.lock entry for the checkpoint out)
+            # - if run without --reset, the checkpoint out will be checked out
+            #   using any hash present in dvc.lock (or removed if no entry
+            #   exists in dvc.lock)
+            checkpoint_reset: bool = kwargs.pop("reset", False)
+            if not repro_dry:
+                dvc_checkout(
+                    dvc,
+                    targets=targets,
+                    with_deps=targets is not None,
+                    force=True,
+                    quiet=True,
+                    allow_missing=True,
+                    checkpoint_reset=checkpoint_reset,
+                    recursive=kwargs.get("recursive", False),
+                )
 
-        return diff(self.repo, *args, **kwargs)
+            checkpoint_func = partial(
+                cls.checkpoint_callback,
+                dvc,
+                dvc.scm,
+                info.name,
+                repro_force or checkpoint_reset,
+            )
 
-    def show(self, *args, **kwargs):
-        from dvc.repo.experiments.show import show
+            stages = dvc_reproduce(
+                dvc,
+                *args,
+                on_unchanged=filter_pipeline,
+                checkpoint_func=checkpoint_func,
+                **kwargs,
+            )
+            if paths := cls._get_top_level_paths(dvc):
+                logger.debug("Staging top-level files: %s", paths)
+                dvc.scm_context.add(paths)
+
+            exp_hash = cls.hash_exp(stages)
+            if not repro_dry:
+                ref, exp_ref, repro_force = cls._repro_commit(
+                    dvc,
+                    info,
+                    stages,
+                    exp_hash,
+                    auto_push,
+                    git_remote,
+                    repro_force,
+                    message=message,
+                )
+                info.result_hash = exp_hash
+                info.result_ref = ref
+                info.result_force = repro_force
+
+        # ideally we would return stages here like a normal repro() call, but
+        # stages is not currently picklable and cannot be returned across
+        # multiprocessing calls
+        return ExecutorResult(exp_hash, exp_ref, repro_force)
+
+    @classmethod
+    def _repro_commit(
+        cls,
+        dvc,
+        info,
+        stages,
+        exp_hash,
+        auto_push,
+        git_remote,
+        repro_force,
+        message: Optional[str] = None,
+    ) -> Tuple[Optional[str], Optional["ExpRefInfo"], bool]:
+        is_checkpoint = any(stage.is_checkpoint for stage in stages)
+        cls.commit(
+            dvc.scm,
+            exp_hash,
+            exp_name=info.name,
+            force=repro_force,
+            checkpoint=is_checkpoint,
+            message=message,
+        )
+        if auto_push:
+            cls._auto_push(dvc, dvc.scm, git_remote)
+        ref: Optional[str] = dvc.scm.get_ref(EXEC_BRANCH, follow=False)
+        exp_ref: Optional["ExpRefInfo"] = ExpRefInfo.from_ref(ref) if ref else None
+        if cls.WARN_UNTRACKED:
+            untracked = dvc.scm.untracked_files()
+            if untracked:
+                logger.warning(
+                    (
+                        "The following untracked files were present in "
+                        "the experiment directory after reproduction but "
+                        "will not be included in experiment commits:\n"
+                        "\t%s"
+                    ),
+                    ", ".join(untracked),
+                )
+        return ref, exp_ref, repro_force
 
-        return show(self.repo, *args, **kwargs)
+    @classmethod
+    @contextmanager
+    def _repro_dvc(  # noqa: C901
+        cls,
+        info: "ExecutorInfo",
+        infofile: Optional[str] = None,
+        log_errors: bool = True,
+        copy_paths: Optional[List[str]] = None,
+        message: Optional[str] = None,
+        **kwargs,
+    ) -> Iterator["Repo"]:
+        from dvc_studio_client.post_live_metrics import post_live_metrics
 
-    def run(self, *args, **kwargs):
-        from dvc.repo.experiments.run import run
+        from dvc.repo import Repo
+        from dvc.stage.monitor import CheckpointKilledError
 
-        return run(self.repo, *args, **kwargs)
+        with Repo(os.path.join(info.root_dir, info.dvc_dir)) as dvc:
+            info.status = TaskStatus.RUNNING
+            if infofile is not None:
+                info.dump_json(infofile)
+            if cls.QUIET:
+                dvc.scm_context.quiet = cls.QUIET
+            old_cwd = os.getcwd()
 
-    def gc(self, *args, **kwargs):
-        from dvc.repo.experiments.gc import gc
+            for path in copy_paths or []:
+                cls._copy_path(os.path.realpath(path), os.path.join(dvc.root_dir, path))
 
-        return gc(self.repo, *args, **kwargs)
+            if info.wdir:
+                os.chdir(os.path.join(dvc.scm.root_dir, info.wdir))
+            else:
+                os.chdir(dvc.root_dir)
 
-    def push(self, *args, **kwargs):
-        from dvc.repo.experiments.push import push
+            args_path = os.path.join(dvc.tmp_dir, cls.PACKED_ARGS_FILE)
+            if os.path.exists(args_path):
+                _, kwargs = cls.unpack_repro_args(args_path)
+            dvc_studio_config = dvc.config.get("studio")
+            # set missing config options using saved config
+            # inferring repo url will fail if not set here
+            run_env_config = env_to_config(kwargs.get("run_env", {}))
+            dvc_studio_config = {**run_env_config, **dvc_studio_config}
+            try:
+                post_live_metrics(
+                    "start",
+                    info.baseline_rev,
+                    info.name,  # type: ignore[arg-type]
+                    "dvc",
+                    params=to_studio_params(dvc.params.show()),
+                    dvc_studio_config=dvc_studio_config,
+                    message=message,
+                )
+                logger.debug("Running repro in '%s'", os.getcwd())
+                yield dvc
+                info.status = TaskStatus.SUCCESS
+            except CheckpointKilledError:
+                info.status = TaskStatus.FAILED
+                raise
+            except DvcException:
+                if log_errors:
+                    logger.exception("")
+                info.status = TaskStatus.FAILED
+                raise
+            except Exception:
+                if log_errors:
+                    logger.exception("unexpected error")
+                info.status = TaskStatus.FAILED
+                raise
+            finally:
+                post_live_metrics(
+                    "done",
+                    info.baseline_rev,
+                    info.name,  # type: ignore[arg-type]
+                    "dvc",
+                    experiment_rev=dvc.experiments.scm.get_ref(EXEC_BRANCH),
+                    metrics=get_in(dvc.metrics.show(), ["", "data"]),
+                    dvc_studio_config=dvc_studio_config,
+                )
 
-        return push(self.repo, *args, **kwargs)
+                if infofile is not None:
+                    info.dump_json(infofile)
+                os.chdir(old_cwd)
+
+    @classmethod
+    def _repro_args(cls, dvc):
+        args_path = os.path.join(dvc.tmp_dir, cls.PACKED_ARGS_FILE)
+        if os.path.exists(args_path):
+            args, kwargs = cls.unpack_repro_args(args_path)
+            remove(args_path)
+            # explicitly git rm/unstage the args file
+            dvc.scm.add([args_path], force=True)
+        else:
+            args = []
+            kwargs = {}
+        return args, kwargs
+
+    @staticmethod
+    def _auto_push(
+        dvc: "Repo",
+        scm: "Git",
+        git_remote: Optional[str],
+        push_cache=True,
+        run_cache=True,
+    ):
+        branch = scm.get_ref(EXEC_BRANCH, follow=False)
+        try:
+            dvc.experiments.push(
+                git_remote,
+                branch,
+                push_cache=push_cache,
+                run_cache=run_cache,
+            )
+        except BaseException as exc:  # noqa: BLE001, pylint: disable=W0703
+            logger.warning(
+                (
+                    "Something went wrong while auto pushing experiment "
+                    "to the remote '%s': %s"
+                ),
+                git_remote,
+                exc,
+            )
 
-    def pull(self, *args, **kwargs):
-        from dvc.repo.experiments.pull import pull
+    @classmethod
+    def checkpoint_callback(
+        cls,
+        dvc: "Repo",
+        scm: "Git",
+        name: Optional[str],
+        force: bool,
+        unchanged: Iterable["PipelineStage"],
+        stages: Iterable["PipelineStage"],
+    ):
+        exp_hash = cls.hash_exp(list(stages) + list(unchanged))
+        exp_rev = cls.commit(scm, exp_hash, exp_name=name, force=force, checkpoint=True)
 
-        return pull(self.repo, *args, **kwargs)
+        if env2bool(DVC_EXP_AUTO_PUSH):
+            git_remote = os.getenv(DVC_EXP_GIT_REMOTE)
+            cls._auto_push(dvc, scm, git_remote)
+        ui.write(f"Checkpoint experiment iteration '{exp_rev[:7]}'.")
+
+    @classmethod
+    def commit(
+        cls,
+        scm: "Git",
+        exp_hash: str,
+        exp_name: Optional[str] = None,
+        force: bool = False,
+        checkpoint: bool = False,
+        message: Optional[str] = None,
+    ):
+        """Commit stages as an experiment and return the commit SHA."""
+        rev = scm.get_rev()
+        if not scm.is_dirty(untracked_files=False):
+            logger.debug("No changes to commit")
 
-    def ls(self, *args, **kwargs):
-        from dvc.repo.experiments.ls import ls
+        check_conflict = False
+        branch = scm.get_ref(EXEC_BRANCH, follow=False)
+        if branch:
+            old_ref = rev
+            logger.debug("Commit to current experiment branch '%s'", branch)
+        else:
+            baseline_rev = scm.get_ref(EXEC_BASELINE)
+            name = exp_name if exp_name else f"exp-{exp_hash[:5]}"
+            ref_info = ExpRefInfo(baseline_rev, name)
+            branch = str(ref_info)
+            old_ref = None
+            if scm.get_ref(branch):
+                if not force:
+                    check_conflict = True
+                logger.debug(
+                    "%s existing experiment branch '%s'",
+                    "Replace" if force else "Reuse",
+                    branch,
+                )
+            else:
+                logger.debug("Commit to new experiment branch '%s'", branch)
 
-        return ls(self.repo, *args, **kwargs)
+        scm.add([], update=True)
+        message = message or f"dvc: commit experiment {exp_hash}"
+        scm.commit(message, no_verify=True)
+        new_rev = scm.get_rev()
+        if check_conflict:
+            new_rev = cls._raise_ref_conflict(scm, branch, new_rev, checkpoint)
+        else:
+            scm.set_ref(branch, new_rev, old_ref=old_ref)
+        scm.set_ref(EXEC_BRANCH, branch, symbolic=True)
+        if checkpoint:
+            scm.set_ref(EXEC_CHECKPOINT, new_rev)
+        return new_rev
+
+    @staticmethod
+    def _raise_ref_conflict(scm, ref, new_rev, checkpoint):
+        # If this commit is a duplicate of the existing commit at 'ref', return
+        # the existing commit. Otherwise, error out and require user to re-run
+        # with --force as needed
+        orig_rev = scm.get_ref(ref)
+        if scm.diff(orig_rev, new_rev):
+            if checkpoint:
+                raise CheckpointExistsError(ref)
+            raise ExperimentExistsError(ref)
+        return orig_rev
+
+    @staticmethod
+    def _set_log_level(level):
+        # When executor.reproduce is run in a multiprocessing child process,
+        # dvc.cli.main will not be called for that child process so we need to
+        # setup logging ourselves
+        dvc_logger = logging.getLogger("dvc")
+        if level is not None:
+            dvc_logger.setLevel(level)
 
-    def remove(self, *args, **kwargs):
-        from dvc.repo.experiments.remove import remove
+    @staticmethod
+    def _copy_path(src, dst):
+        try:
+            if os.path.isfile(src):
+                shutil.copy(src, dst)
+            elif os.path.isdir(src):
+                shutil.copytree(src, dst)
+            else:
+                raise DvcException(
+                    f"Unable to copy '{src}'. It is not a file or directory."
+                )
+        except OSError as exc:
+            raise DvcException(f"Unable to copy '{src}' to '{dst}'.") from exc
 
-        return remove(self.repo, *args, **kwargs)
+    @contextmanager
+    def set_temp_refs(self, scm: "Git", temp_dict: Dict[str, str]):
+        try:
+            for ref, rev in temp_dict.items():
+                scm.set_ref(ref, rev)
+            yield
+        finally:
+            for ref in temp_dict:
+                if scm.get_ref(ref):
+                    scm.remove_ref(ref)
```

### Comparing `dvc-2.9.5/dvc/repo/experiments/apply.py` & `dvc-3.0.0a0/dvc/repo/experiments/apply.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,86 +1,70 @@
 import logging
 import os
+from typing import TYPE_CHECKING, Optional
 
 from dvc.repo import locked
 from dvc.repo.scm_context import scm_context
+from dvc.scm import Git
+from dvc.ui import ui
 from dvc.utils.fs import remove
 
-from .base import (
-    EXEC_APPLY,
-    ApplyConflictError,
-    BaselineMismatchError,
-    InvalidExpRevError,
-)
+from .exceptions import BaselineMismatchError, InvalidExpRevError
 from .executor.base import BaseExecutor
+from .refs import EXEC_APPLY
+
+if TYPE_CHECKING:
+    from dvc.repo import Repo
+    from dvc.repo.experiments import Experiments
 
 logger = logging.getLogger(__name__)
 
 
 @locked
 @scm_context
-def apply(repo, rev, force=True, **kwargs):
-    from scmrepo.exceptions import MergeConflictError
-
+def apply(repo: "Repo", rev: str, **kwargs):  # noqa: C901
     from dvc.repo.checkout import checkout as dvc_checkout
-    from dvc.scm import RevError, SCMError, resolve_rev
+    from dvc.scm import RevError, resolve_rev
+
+    exps: "Experiments" = repo.experiments
 
-    exps = repo.experiments
+    is_stash: bool = False
 
+    assert isinstance(repo.scm, Git)
     try:
         exp_rev = resolve_rev(repo.scm, rev)
-        exps.check_baseline(exp_rev)
-    except (RevError, BaselineMismatchError) as exc:
+    except RevError as exc:
+        (
+            exp_ref_info,
+            queue_entry,
+        ) = exps.celery_queue.get_ref_and_entry_by_names(
+            rev
+        )[rev]
+        if exp_ref_info:
+            exp_rev = repo.scm.get_ref(str(exp_ref_info))
+        elif queue_entry:
+            exp_rev = queue_entry.stash_rev
+            is_stash = True
+        else:
+            raise InvalidExpRevError(rev) from exc
+    except BaselineMismatchError as exc:
         raise InvalidExpRevError(rev) from exc
 
-    stash_rev = exp_rev in exps.stash_revs
-    if not stash_rev and not exps.get_branch_by_rev(
-        exp_rev, allow_multiple=True
-    ):
-        raise InvalidExpRevError(exp_rev)
-
-    # Note that we don't use stash_workspace() here since we need finer control
-    # over the merge behavior when we unstash everything
-    if repo.scm.is_dirty(untracked_files=True):
-        logger.debug("Stashing workspace")
-        workspace = repo.scm.stash.push(include_untracked=True)
-    else:
-        workspace = None
-
-    from scmrepo.exceptions import SCMError as _SCMError
-
-    try:
-        repo.scm.merge(exp_rev, commit=False)
-    except _SCMError as exc:
-        raise SCMError(str(exc))
-
-    if workspace:
-        try:
-            repo.scm.stash.apply(workspace)
-        except MergeConflictError as exc:
-            # Applied experiment conflicts with user's workspace changes
-            if force:
-                # prefer applied experiment changes over prior stashed changes
-                repo.scm.checkout_index(ours=True)
-            else:
-                # revert applied changes and restore user's workspace
-                repo.scm.reset(hard=True)
-                repo.scm.stash.pop()
-                raise ApplyConflictError(rev) from exc
-        except _SCMError as exc:
-            raise ApplyConflictError(rev) from exc
-        repo.scm.stash.drop()
-    repo.scm.reset()
-
-    if stash_rev:
-        args_path = os.path.join(repo.tmp_dir, BaseExecutor.PACKED_ARGS_FILE)
-        if os.path.exists(args_path):
-            remove(args_path)
-
+    _apply(repo, exp_rev, name=rev, is_stash=is_stash)
     dvc_checkout(repo, **kwargs)
 
     repo.scm.set_ref(EXEC_APPLY, exp_rev)
-    logger.info(
-        "Changes for experiment '%s' have been applied to your current "
-        "workspace.",
-        rev,
+    ui.write(
+        f"Changes for experiment '{rev}' have been applied to your current workspace.",
     )
+
+
+def _apply(repo: "Repo", rev: str, name: Optional[str] = None, is_stash: bool = False):
+    exps: "Experiments" = repo.experiments
+
+    with exps.apply_stash.preserve_workspace(rev, name=name):
+        with repo.scm.detach_head(rev, force=True):
+            if is_stash:
+                assert repo.tmp_dir is not None
+                args_path = os.path.join(repo.tmp_dir, BaseExecutor.PACKED_ARGS_FILE)
+                if os.path.exists(args_path):
+                    remove(args_path)
```

### Comparing `dvc-2.9.5/dvc/repo/experiments/branch.py` & `dvc-3.0.0a0/dvc/repo/experiments/branch.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,58 +1,60 @@
 import logging
 
 from dvc.exceptions import InvalidArgumentError
 from dvc.repo import locked
 from dvc.repo.scm_context import scm_context
 from dvc.scm import RevError
 
-from .base import InvalidExpRevError
+from .exceptions import InvalidExpRevError
 from .utils import exp_refs_by_rev
 
 logger = logging.getLogger(__name__)
 
 
 @locked
 @scm_context
-def branch(repo, exp_rev, branch_name, *args, **kwargs):
+def branch(repo, exp_rev, branch_name=None, **kwargs):
     from dvc.scm import resolve_rev
 
     try:
         rev = resolve_rev(repo.scm, exp_rev)
     except RevError:
-        raise InvalidArgumentError(exp_rev)
+        raise InvalidArgumentError(exp_rev)  # noqa: B904
     ref_info = None
 
     ref_infos = list(exp_refs_by_rev(repo.scm, rev))
     if len(ref_infos) == 1:
         ref_info = ref_infos[0]
     elif len(ref_infos) > 1:
         current_rev = repo.scm.get_rev()
         for info in ref_infos:
             if info.baseline_sha == current_rev:
                 ref_info = info
                 break
         if not ref_info:
             msg = [
-                f"Ambiguous experiment name '{exp_rev}' can refer to "
-                "multiple experiments. To create a branch use a full "
-                "experiment ref:",
+                (
+                    f"Ambiguous experiment name '{exp_rev}' can refer to "
+                    "multiple experiments. To create a branch use a full "
+                    "experiment ref:"
+                ),
                 "",
             ]
             msg.extend([str(info) for info in ref_infos])
             raise InvalidArgumentError("\n".join(msg))
 
     if not ref_info:
         raise InvalidExpRevError(exp_rev)
 
+    branch_name = branch_name or f"{ref_info.name}-branch"
+
     branch_ref = f"refs/heads/{branch_name}"
     if repo.scm.get_ref(branch_ref):
-        raise InvalidArgumentError(
-            f"Git branch '{branch_name}' already exists."
-        )
+        raise InvalidArgumentError(f"Git branch '{branch_name}' already exists.")
 
     target = repo.scm.get_ref(str(ref_info))
     repo.scm.set_ref(
         branch_ref,
         target,
         message=f"dvc: Created from experiment '{ref_info.name}'",
     )
```

### Comparing `dvc-2.9.5/dvc/repo/experiments/diff.py` & `dvc-3.0.0a0/dvc/repo/experiments/diff.py`

 * *Files 19% similar despite different names*

```diff
@@ -3,37 +3,34 @@
 from dvc.utils.diff import diff as _diff
 from dvc.utils.diff import format_dict
 
 logger = logging.getLogger(__name__)
 
 
 def diff(repo, *args, a_rev=None, b_rev=None, param_deps=False, **kwargs):
-    from dvc.repo.experiments.show import _collect_experiment_commit
+    from dvc.repo.experiments.collect import collect_rev
     from dvc.scm import resolve_rev
 
     if repo.scm.no_commits:
         return {}
 
     if a_rev:
         rev = resolve_rev(repo.scm, a_rev)
-        old = _collect_experiment_commit(repo, rev, param_deps=param_deps)
     else:
-        old = _collect_experiment_commit(repo, "HEAD", param_deps=param_deps)
+        rev = resolve_rev(repo.scm, "HEAD")
+    old = collect_rev(repo, rev, param_deps=param_deps)
 
     if b_rev:
         rev = resolve_rev(repo.scm, b_rev)
-        new = _collect_experiment_commit(repo, rev, param_deps=param_deps)
     else:
-        new = _collect_experiment_commit(
-            repo, "workspace", param_deps=param_deps
-        )
+        rev = "workspace"
+    new = collect_rev(repo, rev, param_deps=param_deps)
 
     with_unchanged = kwargs.pop("all", False)
-
     return {
         key: _diff(
-            format_dict(old.get("data", {}).get(key, {})),
-            format_dict(new.get("data", {}).get(key, {})),
+            format_dict(getattr(old.data, key, {})),
+            format_dict(getattr(new.data, key, {})),
             with_unchanged=with_unchanged,
         )
         for key in ["metrics", "params"]
     }
```

### Comparing `dvc-2.9.5/dvc/repo/experiments/executor/base.py` & `dvc-3.0.0a0/dvc/repo/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,703 +1,596 @@
 import logging
 import os
-import pickle
-from abc import ABC, abstractmethod
+from collections import defaultdict
 from contextlib import contextmanager
-from dataclasses import asdict, dataclass
-from functools import partial
+from functools import wraps
 from typing import (
     TYPE_CHECKING,
-    Any,
     Callable,
-    Dict,
+    ContextManager,
     Iterable,
-    NamedTuple,
     Optional,
     Tuple,
-    Type,
-    TypeVar,
     Union,
 )
 
-from dvc.env import DVC_EXP_AUTO_PUSH, DVC_EXP_GIT_REMOTE
-from dvc.exceptions import DvcException
-from dvc.stage.serialize import to_lockfile
-from dvc.utils import dict_sha256, env2bool, relpath
-from dvc.utils.fs import remove
-
-from ..base import (
-    EXEC_BASELINE,
-    EXEC_BRANCH,
-    EXEC_CHECKPOINT,
-    EXEC_NAMESPACE,
-    EXPS_NAMESPACE,
-    EXPS_STASH,
-    CheckpointExistsError,
-    ExperimentExistsError,
-    ExpRefInfo,
-    UnchangedExperimentError,
-)
+from dvc.exceptions import NotDvcRepoError, OutputNotFoundError
+from dvc.ignore import DvcIgnoreFilter
+from dvc.utils import env2bool
+from dvc.utils.fs import path_isin
+from dvc.utils.objects import cached_property
 
 if TYPE_CHECKING:
-    from multiprocessing import Queue
+    from dvc.fs import FileSystem
+    from dvc.fs.data import DataFileSystem
+    from dvc.fs.dvc import DVCFileSystem
+    from dvc.lock import LockBase
+    from dvc.machine import MachineManager
+    from dvc.scm import Git, NoSCM
+    from dvc.stage import Stage
+    from dvc.types import DictStrAny
+    from dvc_data.hashfile.state import StateBase
+    from dvc_data.index import DataIndex
+
+    from .experiments import Experiments
+    from .index import Index
+    from .scm_context import SCMContext
 
-    from scmrepo.git import Git
+logger = logging.getLogger(__name__)
 
-    from dvc.repo import Repo
-    from dvc.stage import PipelineStage
 
-    from ..base import ExpStashEntry
+@contextmanager
+def lock_repo(repo: "Repo"):
+    # pylint: disable=protected-access
+    depth: int = repo._lock_depth
+    repo._lock_depth += 1
+
+    try:
+        if depth > 0:
+            yield
+        else:
+            with repo.lock:
+                repo._reset()
+                yield
+                # Graph cache is no longer valid after we release the repo.lock
+                repo._reset()
+    finally:
+        repo._lock_depth = depth
+
+
+def locked(f):
+    @wraps(f)
+    def wrapper(repo, *args, **kwargs):
+        with lock_repo(repo):
+            return f(repo, *args, **kwargs)
+
+    return wrapper
+
+
+class Repo:
+    DVC_DIR = ".dvc"
+
+    from dvc.repo.add import add  # type: ignore[misc]
+    from dvc.repo.checkout import checkout  # type: ignore[misc]
+    from dvc.repo.commit import commit  # type: ignore[misc]
+    from dvc.repo.destroy import destroy  # type: ignore[misc]
+    from dvc.repo.diff import diff  # type: ignore[misc]
+    from dvc.repo.fetch import fetch  # type: ignore[misc]
+    from dvc.repo.freeze import freeze, unfreeze  # type: ignore[misc]
+    from dvc.repo.gc import gc  # type: ignore[misc]
+    from dvc.repo.get import get as _get  # type: ignore[misc]
+    from dvc.repo.get_url import get_url as _get_url  # type: ignore[misc]
+    from dvc.repo.imp import imp  # type: ignore[misc]
+    from dvc.repo.imp_url import imp_url  # type: ignore[misc]
+    from dvc.repo.install import install  # type: ignore[misc]
+    from dvc.repo.ls import ls as _ls  # type: ignore[misc]
+    from dvc.repo.ls_url import ls_url as _ls_url  # type: ignore[misc]
+    from dvc.repo.move import move  # type: ignore[misc]
+    from dvc.repo.pull import pull  # type: ignore[misc]
+    from dvc.repo.push import push  # type: ignore[misc]
+    from dvc.repo.remove import remove  # type: ignore[misc]
+    from dvc.repo.reproduce import reproduce  # type: ignore[misc]
+    from dvc.repo.run import run  # type: ignore[misc]
+    from dvc.repo.status import status  # type: ignore[misc]
+    from dvc.repo.update import update  # type: ignore[misc]
+
+    from .data import status as data_status  # type: ignore[misc]
+
+    ls = staticmethod(_ls)
+    ls_url = staticmethod(_ls_url)
+    get = staticmethod(_get)
+    get_url = staticmethod(_get_url)
 
-logger = logging.getLogger(__name__)
+    def _get_repo_dirs(
+        self,
+        root_dir: Optional[str] = None,
+        fs: Optional["FileSystem"] = None,
+        uninitialized: bool = False,
+        scm: Optional[Union["Git", "NoSCM"]] = None,
+    ) -> Tuple[str, Optional[str]]:
+        from dvc.fs import localfs
+        from dvc.scm import SCM, SCMError
 
+        dvc_dir: Optional[str] = None
+        try:
+            root_dir = self.find_root(root_dir, fs)
+            fs = fs or localfs
+            dvc_dir = fs.path.join(root_dir, self.DVC_DIR)
+        except NotDvcRepoError:
+            if not uninitialized:
+                raise
+
+            if not scm:
+                try:
+                    scm = SCM(root_dir or os.curdir)
+                    if scm.dulwich.repo.bare:
+                        raise NotDvcRepoError(f"{scm.root_dir} is a bare git repo")
+                except SCMError:
+                    scm = SCM(os.curdir, no_scm=True)
 
-EXEC_TMP_DIR = "exps"
-EXEC_PID_DIR = "run"
+            if not fs or not root_dir:
+                root_dir = scm.root_dir
 
+        assert root_dir
+        return root_dir, dvc_dir
 
-class ExecutorResult(NamedTuple):
-    exp_hash: Optional[str]
-    ref_info: Optional["ExpRefInfo"]
-    force: bool
-
-
-@dataclass
-class ExecutorInfo:
-    git_url: str
-    baseline_rev: str
-    location: str
-    root_dir: str
-    dvc_dir: str
-    name: Optional[str] = None
-    wdir: Optional[str] = None
-    result_hash: Optional[str] = None
-    result_ref: Optional[str] = None
-    result_force: bool = False
+    def __init__(  # noqa: PLR0915
+        self,
+        root_dir: Optional[str] = None,
+        fs: Optional["FileSystem"] = None,
+        rev: Optional[str] = None,
+        subrepos: bool = False,
+        uninitialized: bool = False,
+        config: Optional["DictStrAny"] = None,
+        url: Optional[str] = None,
+        repo_factory: Optional[Callable] = None,
+        scm: Optional[Union["Git", "NoSCM"]] = None,
+    ):
+        from dvc.cachemgr import CacheManager
+        from dvc.config import Config
+        from dvc.data_cloud import DataCloud
+        from dvc.fs import GitFileSystem, LocalFileSystem, localfs
+        from dvc.lock import LockNoop, make_lock
+        from dvc.repo.artifacts import Artifacts
+        from dvc.repo.metrics import Metrics
+        from dvc.repo.params import Params
+        from dvc.repo.plots import Plots
+        from dvc.repo.stage import StageLoad
+        from dvc.scm import SCM
+        from dvc.stage.cache import StageCache
+        from dvc_data.hashfile.state import State, StateNoop
+
+        self.url = url
+        self._fs_conf = {"repo_factory": repo_factory}
+        self._fs = fs or localfs
+        self._scm = scm
+        self._data_index = None
+
+        if rev and not fs:
+            self._scm = scm = SCM(root_dir or os.curdir)
+            root_dir = "/"
+            self._fs = GitFileSystem(scm=self._scm, rev=rev)
+
+        self.root_dir: str
+        self.dvc_dir: Optional[str]
+        (
+            self.root_dir,
+            self.dvc_dir,
+        ) = self._get_repo_dirs(
+            root_dir=root_dir,
+            fs=self.fs,
+            uninitialized=uninitialized,
+            scm=scm,
+        )
 
-    @classmethod
-    def from_dict(cls, d):
-        return cls(**d)
+        self.config: Config = Config(self.dvc_dir, fs=self.fs, config=config)
+        self._uninitialized = uninitialized
 
-    def asdict(self):
-        return asdict(self)
+        # used by DVCFileSystem to determine if it should traverse subrepos
+        self.subrepos = subrepos
 
-    @property
-    def result(self) -> Optional["ExecutorResult"]:
-        if self.result_hash is None:
+        self.cloud: "DataCloud" = DataCloud(self)
+        self.stage: "StageLoad" = StageLoad(self)
+
+        self.lock: "LockBase"
+        self.cache: CacheManager
+        self.state: "StateBase"
+        if isinstance(self.fs, GitFileSystem) or not self.dvc_dir:
+            self.lock = LockNoop()
+            self.state = StateNoop()
+            self.cache = CacheManager(self)
+        else:
+            if isinstance(self.fs, LocalFileSystem):
+                assert self.tmp_dir
+                self.fs.makedirs(self.tmp_dir, exist_ok=True)
+
+                self.lock = make_lock(
+                    self.fs.path.join(self.tmp_dir, "lock"),
+                    tmp_dir=self.tmp_dir,
+                    hardlink_lock=self.config["core"].get("hardlink_lock", False),
+                    friendly=True,
+                )
+                os.makedirs(self.site_cache_dir, exist_ok=True)
+                self.state = State(self.root_dir, self.site_cache_dir, self.dvcignore)
+            else:
+                self.lock = LockNoop()
+                self.state = StateNoop()
+
+            self.cache = CacheManager(self)
+
+            self.stage_cache = StageCache(self)
+
+            self._ignore()
+
+        self.metrics: Metrics = Metrics(self)
+        self.plots: Plots = Plots(self)
+        self.params: Params = Params(self)
+        self.artifacts: Artifacts = Artifacts(self)
+
+        self.stage_collection_error_handler: Optional[
+            Callable[[str, Exception], None]
+        ] = None
+        self._lock_depth: int = 0
+
+    def __str__(self):
+        return self.url or self.root_dir
+
+    @cached_property
+    def local_dvc_dir(self):
+        from dvc.fs import GitFileSystem, LocalFileSystem
+
+        if not self.dvc_dir:
             return None
-        return ExecutorResult(
-            self.result_hash,
-            ExpRefInfo.from_ref(self.result_ref) if self.result_ref else None,
-            self.result_force,
+
+        if isinstance(self.fs, LocalFileSystem):
+            return self.dvc_dir
+
+        if not isinstance(self.fs, GitFileSystem):
+            return None
+
+        relparts = ()
+        if self.root_dir != "/":
+            # subrepo
+            relparts = self.fs.path.relparts(self.root_dir, "/")
+
+        dvc_dir = os.path.join(
+            self.scm.root_dir,
+            *relparts,
+            self.DVC_DIR,
         )
+        if os.path.exists(dvc_dir):
+            return dvc_dir
 
-    def dump_json(self, filename: str):
-        from dvc.utils.fs import makedirs
-        from dvc.utils.serialize import modify_json
+        return None
 
-        makedirs(os.path.dirname(filename), exist_ok=True)
-        with modify_json(filename) as d:
-            d.update(self.asdict())
+    @cached_property
+    def tmp_dir(self):
+        if self.local_dvc_dir is None:
+            return None
 
+        return os.path.join(self.local_dvc_dir, "tmp")
 
-_T = TypeVar("_T", bound="BaseExecutor")
+    @cached_property
+    def index(self) -> "Index":
+        from dvc.repo.index import Index
+
+        return Index.from_repo(self)
+
+    def check_graph(
+        self, stages: Iterable["Stage"], callback: Optional[Callable] = None
+    ) -> None:
+        if not getattr(self, "_skip_graph_checks", False):
+            new = self.index.update(stages)
+            if callable(callback):
+                callback()
+            new.check_graph()
 
+    @staticmethod
+    def open(url, *args, **kwargs):  # noqa: A003
+        from .open_repo import open_repo
 
-class BaseExecutor(ABC):
-    """Base class for executing experiments in parallel.
+        return open_repo(url, *args, **kwargs)
 
-    Parameters:
-        root_dir: Path to SCM root.
-        dvc_dir: Path to .dvc dir relative to SCM root.
-        baseline_rev: Experiment baseline revision.
-        wdir: Path to exec working directory relative to SCM root.
-        name: Executor (experiment) name.
-        result: Completed executor result.
-    """
+    @cached_property
+    def scm(self) -> Union["Git", "NoSCM"]:
+        from dvc.scm import SCM, SCMError
 
-    PACKED_ARGS_FILE = "repro.dat"
-    WARN_UNTRACKED = False
-    QUIET = False
-    INFOFILE_EXT = ".run"
-    DEFAULT_LOCATION: str = "workspace"
+        if self._scm:
+            return self._scm
 
-    def __init__(
-        self,
-        root_dir: str,
-        dvc_dir: str,
-        baseline_rev: str,
-        wdir: Optional[str] = None,
-        name: Optional[str] = None,
-        location: Optional[str] = None,
-        result: Optional["ExecutorResult"] = None,
-        **kwargs,
-    ):
-        self.dvc_dir = dvc_dir
-        self.root_dir = root_dir
-        self.wdir = wdir
-        self.name = name
-        self.baseline_rev = baseline_rev
-        self.location: str = location or self.DEFAULT_LOCATION
-        self.result = result
-
-    @abstractmethod
-    def init_git(self, scm: "Git", branch: Optional[str] = None):
-        """Init git repo and populate it using exp refs from the specified
-        SCM instance.
-        """
+        no_scm = self.config["core"].get("no_scm", False)
+        try:
+            return SCM(self.root_dir, no_scm=no_scm)
+        except SCMError:
+            if self._uninitialized:
+                # might not be a git/dvc repo at all
+                # used in `params/metrics/plots` targets
+                return SCM(self.root_dir, no_scm=True)
+            raise
+
+    @cached_property
+    def scm_context(self) -> "SCMContext":
+        from dvc.repo.scm_context import SCMContext
+
+        return SCMContext(self.scm, self.config)
+
+    @cached_property
+    def dvcignore(self) -> DvcIgnoreFilter:
+        return DvcIgnoreFilter(self.fs, self.root_dir)
+
+    def get_rev(self):
+        from dvc.fs import GitFileSystem, LocalFileSystem
+
+        assert self.scm
+        if isinstance(self.fs, LocalFileSystem):
+            from dvc.scm import map_scm_exception
+
+            with map_scm_exception():
+                return self.scm.get_rev()
+        assert isinstance(self.fs, GitFileSystem)
+        return self.fs.rev
+
+    @cached_property
+    def experiments(self) -> "Experiments":
+        from dvc.repo.experiments import Experiments
+
+        return Experiments(self)
+
+    @cached_property
+    def machine(self) -> Optional["MachineManager"]:
+        from dvc.machine import MachineManager
+
+        if self.tmp_dir and (
+            self.config["feature"].get("machine", False) or env2bool("DVC_TEST")
+        ):
+            return MachineManager(self)
+        return None
 
     @property
-    @abstractmethod
-    def git_url(self) -> str:
-        pass
-
-    @abstractmethod
-    def init_cache(self, repo: "Repo", rev: str, run_cache: bool = True):
-        """Initialize DVC cache."""
-
-    @abstractmethod
-    def collect_cache(
-        self, repo: "Repo", exp_ref: "ExpRefInfo", run_cache: bool = True
-    ):
-        """Collect DVC cache."""
+    def fs(self) -> "FileSystem":
+        return self._fs
+
+    @fs.setter
+    def fs(self, fs: "FileSystem"):
+        self._fs = fs
+        # Our graph cache is no longer valid, as it was based on the previous
+        # fs.
+        self._reset()
 
     @property
-    def info(self) -> "ExecutorInfo":
-        if self.result is not None:
-            result_dict: Dict[str, Any] = {
-                "result_hash": self.result.exp_hash,
-                "result_ref": (
-                    str(self.result.ref_info) if self.result.ref_info else None
-                ),
-                "result_force": self.result.force,
-            }
-        else:
-            result_dict = {}
-        return ExecutorInfo(
-            git_url=self.git_url,
-            baseline_rev=self.baseline_rev,
-            location=self.location,
-            root_dir=self.root_dir,
-            dvc_dir=self.dvc_dir,
-            name=self.name,
-            wdir=self.wdir,
-            **result_dict,
-        )
+    def data_index(self) -> "DataIndex":
+        from dvc_data.index import DataIndex
 
-    @classmethod
-    def from_info(cls: Type[_T], info: "ExecutorInfo") -> _T:
-        if info.result_hash:
-            result: Optional["ExecutorResult"] = ExecutorResult(
-                info.result_hash,
-                (
-                    ExpRefInfo.from_ref(info.result_ref)
-                    if info.result_ref
-                    else None
-                ),
-                info.result_force,
-            )
-        else:
-            result = None
-        return cls(
-            root_dir=info.root_dir,
-            dvc_dir=info.dvc_dir,
-            baseline_rev=info.baseline_rev,
-            name=info.name,
-            wdir=info.wdir,
-            result=result,
-        )
+        if self._data_index is None:
+            index_dir = os.path.join(self.site_cache_dir, "index", "data")
+            os.makedirs(index_dir, exist_ok=True)
+            self._data_index = DataIndex.open(os.path.join(index_dir, "db.db"))
+
+        return self._data_index
+
+    def __repr__(self):
+        return f"{self.__class__.__name__}: '{self.root_dir}'"
 
     @classmethod
-    @abstractmethod
-    def from_stash_entry(
-        cls: Type[_T],
-        repo: "Repo",
-        stash_rev: str,
-        entry: "ExpStashEntry",
-        **kwargs,
-    ) -> _T:
-        pass
+    def find_root(cls, root=None, fs=None) -> str:
+        from dvc.fs import LocalFileSystem, localfs
+
+        fs = fs or localfs
+        root = root or os.curdir
+        root_dir = fs.path.realpath(root)
+
+        if not fs.isdir(root_dir):
+            raise NotDvcRepoError(f"directory '{root}' does not exist")
+
+        while True:
+            dvc_dir = fs.path.join(root_dir, cls.DVC_DIR)
+            if fs.isdir(dvc_dir):
+                return root_dir
+            if isinstance(fs, LocalFileSystem) and os.path.ismount(root_dir):
+                break
+            parent = fs.path.parent(root_dir)
+            if parent == root_dir:
+                break
+            root_dir = parent
+
+        msg = "you are not inside of a DVC repository"
+
+        if isinstance(fs, LocalFileSystem):
+            msg = f"{msg} (checked up to mount point '{root_dir}')"
+
+        raise NotDvcRepoError(msg)
 
     @classmethod
-    def _from_stash_entry(
-        cls: Type[_T],
-        repo: "Repo",
-        stash_rev: str,
-        entry: "ExpStashEntry",
-        root_dir: str,
-        **kwargs,
-    ) -> _T:
-        executor = cls(
-            root_dir=root_dir,
-            dvc_dir=relpath(repo.dvc_dir, repo.scm.root_dir),
-            baseline_rev=entry.baseline_rev,
-            name=entry.name,
-            wdir=relpath(os.getcwd(), repo.scm.root_dir),
-            **kwargs,
-        )
-        executor.init_git(repo.scm, branch=entry.branch)
-        executor.init_cache(repo, stash_rev)
-        return executor
+    def find_dvc_dir(cls, root=None, fs=None) -> str:
+        from dvc.fs import localfs
+
+        fs = fs or localfs
+        root_dir = cls.find_root(root, fs=fs)
+        return fs.path.join(root_dir, cls.DVC_DIR)
 
     @staticmethod
-    def hash_exp(stages: Iterable["PipelineStage"]) -> str:
-        from dvc.stage import PipelineStage
+    def init(root_dir=os.curdir, no_scm=False, force=False, subdir=False) -> "Repo":
+        from dvc.repo.init import init
 
-        exp_data = {}
-        for stage in stages:
-            if isinstance(stage, PipelineStage):
-                exp_data.update(to_lockfile(stage))
-        return dict_sha256(exp_data)
+        return init(root_dir=root_dir, no_scm=no_scm, force=force, subdir=subdir)
 
-    def cleanup(self):
-        pass
+    def unprotect(self, target):
+        return self.cache.repo.unprotect(target)
 
-    # TODO: come up with better way to stash repro arguments
-    @staticmethod
-    def pack_repro_args(path, *args, fs=None, extra=None, **kwargs):
-        dpath = os.path.dirname(path)
-        if fs:
-            open_func = fs.open
-            fs.makedirs(dpath)
-        else:
-            from dvc.utils.fs import makedirs
+    def _ignore(self):
+        flist = [self.config.files["local"]]
+        if tmp_dir := self.tmp_dir:
+            flist.append(tmp_dir)
+        if path_isin(self.cache.repo.path, self.root_dir):
+            flist.append(self.cache.repo.path)
 
-            open_func = open
-            makedirs(dpath, exist_ok=True)
+        for file in flist:
+            self.scm_context.ignore(file)
 
-        data = {"args": args, "kwargs": kwargs}
-        if extra is not None:
-            data["extra"] = extra
-        with open_func(path, "wb") as fobj:
-            pickle.dump(data, fobj)
+    def brancher(self, *args, **kwargs):
+        from dvc.repo.brancher import brancher
 
-    @staticmethod
-    def unpack_repro_args(path):
-        with open(path, "rb") as fobj:
-            data = pickle.load(fobj)
-        return data["args"], data["kwargs"]
+        return brancher(self, *args, **kwargs)
+
+    def switch(self, rev: str) -> ContextManager[str]:
+        from dvc.repo.brancher import switch
 
-    def fetch_exps(
+        return switch(self, rev)
+
+    def used_objs(  # noqa: PLR0913
         self,
-        dest_scm: "Git",
-        force: bool = False,
-        on_diverged: Callable[[str, bool], None] = None,
-        **kwargs,
-    ) -> Iterable[str]:
-        """Fetch reproduced experiment refs into the specified SCM.
-
-        Args:
-            dest_scm: Destination Git instance.
-            force: If True, diverged refs will be overwritten
-            on_diverged: Callback in the form on_diverged(ref, is_checkpoint)
-                to be called when an experiment ref has diverged.
+        targets=None,
+        all_branches=False,
+        with_deps=False,
+        all_tags=False,
+        all_commits=False,
+        all_experiments=False,
+        commit_date: Optional[str] = None,
+        remote=None,
+        force=False,
+        jobs=None,
+        recursive=False,
+        used_run_cache=None,
+        revs=None,
+        num=1,
+        push: bool = False,
+    ):
+        """Get the stages related to the given target and collect
+        the `info` of its outputs.
+
+        This is useful to know what files from the cache are _in use_
+        (namely, a file described as an output on a stage).
 
-        Extra kwargs will be passed into the remote git client.
+        The scope is, by default, the working directory, but you can use
+        `all_branches`/`all_tags`/`all_commits`/`all_experiments` to expand
+        the scope.
+
+        Returns:
+            A dict mapping (remote) ODB instances to sets of objects that
+            belong to each ODB. If the ODB instance is None, the objects
+            are naive and do not belong to a specific remote ODB.
         """
-        from ..utils import iter_remote_refs
+        used = defaultdict(set)
 
-        refs = []
-        has_checkpoint = False
-        for ref in iter_remote_refs(
-            dest_scm,
-            self.git_url,
-            base=EXPS_NAMESPACE,
-            **kwargs,
+        for _ in self.brancher(
+            revs=revs,
+            all_branches=all_branches,
+            all_tags=all_tags,
+            all_commits=all_commits,
+            all_experiments=all_experiments,
+            commit_date=commit_date,
+            num=num,
         ):
-            if ref == EXEC_CHECKPOINT:
-                has_checkpoint = True
-            elif not ref.startswith(EXEC_NAMESPACE) and ref != EXPS_STASH:
-                refs.append(ref)
-
-        def on_diverged_ref(orig_ref: str, new_rev: str):
-            if force:
-                logger.debug("Replacing existing experiment '%s'", orig_ref)
+            for odb, objs in self.index.used_objs(
+                targets,
+                remote=remote,
+                force=force,
+                jobs=jobs,
+                recursive=recursive,
+                with_deps=with_deps,
+                push=push,
+            ).items():
+                used[odb].update(objs)
+
+        if used_run_cache:
+            for odb, objs in self.stage_cache.get_used_objs(
+                used_run_cache, remote=remote, force=force, jobs=jobs
+            ).items():
+                used[odb].update(objs)
+
+        return used
+
+    def find_outs_by_path(self, path, outs=None, recursive=False, strict=True):
+        # using `outs_graph` to ensure graph checks are run
+        outs = outs or self.index.outs_graph
+
+        abs_path = self.fs.path.abspath(path)
+        fs_path = abs_path
+
+        def func(out):
+            def eq(one, two):
+                return one == two
+
+            match = eq if strict else out.fs.path.isin_or_eq
+
+            if out.protocol == "local" and match(fs_path, out.fs_path):
                 return True
 
-            self._raise_ref_conflict(
-                dest_scm, orig_ref, new_rev, has_checkpoint
-            )
-            if on_diverged:
-                on_diverged(orig_ref, has_checkpoint)
-            logger.debug("Reproduced existing experiment '%s'", orig_ref)
-            return False
+            if recursive and out.fs.path.isin(out.fs_path, fs_path):
+                return True
 
-        # fetch experiments
-        dest_scm.fetch_refspecs(
-            self.git_url,
-            [f"{ref}:{ref}" for ref in refs],
-            on_diverged=on_diverged_ref,
-            force=force,
-            **kwargs,
-        )
-        # update last run checkpoint (if it exists)
-        if has_checkpoint:
-            dest_scm.fetch_refspecs(
-                self.git_url,
-                [f"{EXEC_CHECKPOINT}:{EXEC_CHECKPOINT}"],
-                force=True,
-                **kwargs,
-            )
-        return refs
+            return False
 
-    @classmethod
-    def _validate_remotes(cls, dvc: "Repo", git_remote: Optional[str]):
-        from scmrepo.exceptions import InvalidRemote
+        matched = list(filter(func, outs))
+        if not matched:
+            raise OutputNotFoundError(path, self)
 
-        from dvc.scm import InvalidRemoteSCMRepo
+        return matched
 
-        if git_remote == dvc.root_dir:
-            logger.warning(
-                f"'{git_remote}' points to the current Git repo, experiment "
-                "Git refs will not be pushed. But DVC cache and run cache "
-                "will automatically be pushed to the default DVC remote "
-                "(if any) on each experiment commit."
-            )
-        try:
-            dvc.scm.validate_git_remote(git_remote)
-        except InvalidRemote as exc:
-            raise InvalidRemoteSCMRepo(str(exc))
-        dvc.cloud.get_remote_odb()
+    def is_dvc_internal(self, path):
+        path_parts = self.fs.path.normpath(path).split(self.fs.sep)
+        return self.DVC_DIR in path_parts
 
-    @classmethod
-    def reproduce(
-        cls,
-        info: "ExecutorInfo",
-        rev: str,
-        queue: Optional["Queue"] = None,
-        infofile: Optional[str] = None,
-        log_errors: bool = True,
-        log_level: Optional[int] = None,
-        **kwargs,
-    ) -> "ExecutorResult":
-        """Run dvc repro and return the result.
-
-        Returns tuple of (exp_hash, exp_ref, force) where exp_hash is the
-            experiment hash (or None on error), exp_ref is the experiment ref,
-            and force is a bool specifying whether or not this experiment
-            should force overwrite any existing duplicates.
-        """
-        from dvc.repo.checkout import checkout as dvc_checkout
-        from dvc.repo.reproduce import reproduce as dvc_reproduce
-        from dvc.stage import PipelineStage
-
-        auto_push = env2bool(DVC_EXP_AUTO_PUSH)
-        git_remote = os.getenv(DVC_EXP_GIT_REMOTE, None)
-
-        unchanged = []
-
-        if queue is not None:
-            queue.put((rev, os.getpid()))
-        if log_errors and log_level is not None:
-            cls._set_log_level(log_level)
-
-        def filter_pipeline(stages):
-            unchanged.extend(
-                [stage for stage in stages if isinstance(stage, PipelineStage)]
-            )
-
-        exp_hash: Optional[str] = None
-        exp_ref: Optional["ExpRefInfo"] = None
-        repro_force: bool = False
-
-        if infofile is not None:
-            info.dump_json(infofile)
-
-        with cls._repro_dvc(
-            info,
-            log_errors=log_errors,
-            **kwargs,
-        ) as dvc:
-            if auto_push:
-                cls._validate_remotes(dvc, git_remote)
-
-            args, kwargs = cls._repro_args(dvc)
-            if args:
-                targets: Optional[Union[list, str]] = args[0]
-            else:
-                targets = kwargs.get("targets")
+    @cached_property
+    def datafs(self) -> "DataFileSystem":
+        from dvc.fs.data import DataFileSystem
 
-            repro_force = kwargs.get("force", False)
-            logger.trace(  # type: ignore[attr-defined]
-                "Executor repro with force = '%s'", str(repro_force)
-            )
-
-            repro_dry = kwargs.get("dry")
-
-            # NOTE: checkpoint outs are handled as a special type of persist
-            # out:
-            #
-            # - checkpoint out may not yet exist if this is the first time this
-            #   experiment has been run, this is not an error condition for
-            #   experiments
-            # - if experiment was run with --reset, the checkpoint out will be
-            #   removed at the start of the experiment (regardless of any
-            #   dvc.lock entry for the checkpoint out)
-            # - if run without --reset, the checkpoint out will be checked out
-            #   using any hash present in dvc.lock (or removed if no entry
-            #   exists in dvc.lock)
-            checkpoint_reset: bool = kwargs.pop("reset", False)
-            if not repro_dry:
-                dvc_checkout(
-                    dvc,
-                    targets=targets,
-                    with_deps=targets is not None,
-                    force=True,
-                    quiet=True,
-                    allow_missing=True,
-                    checkpoint_reset=checkpoint_reset,
-                    recursive=kwargs.get("recursive", False),
-                )
+        return DataFileSystem(index=self.index.data["repo"])
 
-            checkpoint_func = partial(
-                cls.checkpoint_callback,
-                dvc,
-                dvc.scm,
-                info.name,
-                repro_force or checkpoint_reset,
-            )
-            stages = dvc_reproduce(
-                dvc,
-                *args,
-                on_unchanged=filter_pipeline,
-                checkpoint_func=checkpoint_func,
-                **kwargs,
-            )
-
-            exp_hash = cls.hash_exp(stages)
-            if not repro_dry:
-                ref, exp_ref, repro_force = cls._repro_commit(
-                    dvc,
-                    info,
-                    stages,
-                    exp_hash,
-                    checkpoint_reset,
-                    auto_push,
-                    git_remote,
-                    repro_force,
-                )
-            info.result_hash = exp_hash
-            info.result_ref = ref
-            info.result_force = repro_force
-
-        if infofile is not None:
-            info.dump_json(infofile)
-
-        # ideally we would return stages here like a normal repro() call, but
-        # stages is not currently picklable and cannot be returned across
-        # multiprocessing calls
-        return ExecutorResult(exp_hash, exp_ref, repro_force)
+    @cached_property
+    def dvcfs(self) -> "DVCFileSystem":
+        from dvc.fs.dvc import DVCFileSystem
 
-    @classmethod
-    def _repro_commit(
-        cls,
-        dvc,
-        info,
-        stages,
-        exp_hash,
-        checkpoint_reset,
-        auto_push,
-        git_remote,
-        repro_force,
-    ) -> Tuple[Optional[str], Optional["ExpRefInfo"], bool]:
-        try:
-            is_checkpoint = any(stage.is_checkpoint for stage in stages)
-            if is_checkpoint and checkpoint_reset:
-                # For reset checkpoint stages, we need to force
-                # overwriting existing checkpoint refs even though
-                # repro may not have actually been run with --force
-                repro_force = True
-            cls.commit(
-                dvc.scm,
-                exp_hash,
-                exp_name=info.name,
-                force=repro_force,
-                checkpoint=is_checkpoint,
-            )
-            if auto_push:
-                cls._auto_push(dvc, dvc.scm, git_remote)
-        except UnchangedExperimentError:
-            pass
-        ref: Optional[str] = dvc.scm.get_ref(EXEC_BRANCH, follow=False)
-        exp_ref: Optional["ExpRefInfo"] = (
-            ExpRefInfo.from_ref(ref) if ref else None
-        )
-        if cls.WARN_UNTRACKED:
-            untracked = dvc.scm.untracked_files()
-            if untracked:
-                logger.warning(
-                    "The following untracked files were present in "
-                    "the experiment directory after reproduction but "
-                    "will not be included in experiment commits:\n"
-                    "\t%s",
-                    ", ".join(untracked),
-                )
-        return ref, exp_ref, repro_force
+        return DVCFileSystem(repo=self, subrepos=self.subrepos, **self._fs_conf)
 
-    @classmethod
-    @contextmanager
-    def _repro_dvc(
-        cls,
-        info: "ExecutorInfo",
-        log_errors: bool = True,
-        **kwargs,
-    ):
-        from dvc.repo import Repo
-        from dvc.stage.monitor import CheckpointKilledError
+    @cached_property
+    def site_cache_dir(self) -> str:
+        import getpass
+        import hashlib
 
-        dvc = Repo(os.path.join(info.root_dir, info.dvc_dir))
-        if cls.QUIET:
-            dvc.scm_context.quiet = cls.QUIET
-        old_cwd = os.getcwd()
-        if info.wdir:
-            os.chdir(os.path.join(dvc.scm.root_dir, info.wdir))
-        else:
-            os.chdir(dvc.root_dir)
+        from dvc.dirs import site_cache_dir
+        from dvc.fs import GitFileSystem
 
-        try:
-            logger.debug("Running repro in '%s'", os.getcwd())
-            yield dvc
-        except CheckpointKilledError:
-            raise
-        except DvcException:
-            if log_errors:
-                logger.exception("")
-            raise
-        except Exception:
-            if log_errors:
-                logger.exception("unexpected error")
-            raise
-        finally:
-            dvc.close()
-            os.chdir(old_cwd)
+        cache_dir = self.config["core"].get("site_cache_dir") or site_cache_dir()
 
-    @classmethod
-    def _repro_args(cls, dvc):
-        args_path = os.path.join(dvc.tmp_dir, cls.PACKED_ARGS_FILE)
-        if os.path.exists(args_path):
-            args, kwargs = cls.unpack_repro_args(args_path)
-            remove(args_path)
-            # explicitly git rm/unstage the args file
-            dvc.scm.add([args_path])
+        if isinstance(self.fs, GitFileSystem):
+            relparts = ()
+            if self.root_dir != "/":
+                # subrepo
+                relparts = self.fs.path.relparts(self.root_dir, "/")
+            root_dir = os.path.join(self.scm.root_dir, *relparts)
         else:
-            args = []
-            kwargs = {}
-        return args, kwargs
+            root_dir = self.root_dir
 
-    @staticmethod
-    def _auto_push(
-        dvc: "Repo",
-        scm: "Git",
-        git_remote: Optional[str],
-        push_cache=True,
-        run_cache=True,
-    ):
-        branch = scm.get_ref(EXEC_BRANCH, follow=False)
-        try:
-            dvc.experiments.push(
-                git_remote,
-                branch,
-                push_cache=push_cache,
-                run_cache=run_cache,
-            )
-        except BaseException as exc:  # pylint: disable=broad-except
-            logger.warning(
-                "Something went wrong while auto pushing experiment "
-                f"to the remote '{git_remote}': {exc}"
-            )
+        repos_dir = os.path.join(cache_dir, "repo")
 
-    @classmethod
-    def checkpoint_callback(
-        cls,
-        dvc: "Repo",
-        scm: "Git",
-        name: Optional[str],
-        force: bool,
-        unchanged: Iterable["PipelineStage"],
-        stages: Iterable["PipelineStage"],
-    ):
+        umask = os.umask(0)
         try:
-            exp_hash = cls.hash_exp(list(stages) + list(unchanged))
-            exp_rev = cls.commit(
-                scm, exp_hash, exp_name=name, force=force, checkpoint=True
-            )
-
-            if env2bool(DVC_EXP_AUTO_PUSH):
-                git_remote = os.getenv(DVC_EXP_GIT_REMOTE)
-                cls._auto_push(dvc, scm, git_remote)
-            logger.info("Checkpoint experiment iteration '%s'.", exp_rev[:7])
-        except UnchangedExperimentError:
-            pass
-
-    @classmethod
-    def commit(
-        cls,
-        scm: "Git",
-        exp_hash: str,
-        exp_name: Optional[str] = None,
-        force: bool = False,
-        checkpoint: bool = False,
-    ):
-        """Commit stages as an experiment and return the commit SHA."""
-        rev = scm.get_rev()
-        if not scm.is_dirty():
-            logger.debug("No changes to commit")
-            raise UnchangedExperimentError(rev)
-
-        check_conflict = False
-        branch = scm.get_ref(EXEC_BRANCH, follow=False)
-        if branch:
-            old_ref = rev
-            logger.debug("Commit to current experiment branch '%s'", branch)
-        else:
-            baseline_rev = scm.get_ref(EXEC_BASELINE)
-            name = exp_name if exp_name else f"exp-{exp_hash[:5]}"
-            ref_info = ExpRefInfo(baseline_rev, name)
-            branch = str(ref_info)
-            old_ref = None
-            if scm.get_ref(branch):
-                if not force:
-                    check_conflict = True
-                logger.debug(
-                    "%s existing experiment branch '%s'",
-                    "Replace" if force else "Reuse",
-                    branch,
-                )
-            else:
-                logger.debug("Commit to new experiment branch '%s'", branch)
+            os.makedirs(repos_dir, mode=0o777, exist_ok=True)
+        finally:
+            os.umask(umask)
 
-        scm.add([], update=True)
-        scm.commit(f"dvc: commit experiment {exp_hash}", no_verify=True)
-        new_rev = scm.get_rev()
-        if check_conflict:
-            new_rev = cls._raise_ref_conflict(scm, branch, new_rev, checkpoint)
-        else:
-            scm.set_ref(branch, new_rev, old_ref=old_ref)
-        scm.set_ref(EXEC_BRANCH, branch, symbolic=True)
-        if checkpoint:
-            scm.set_ref(EXEC_CHECKPOINT, new_rev)
-        return new_rev
+        md5 = hashlib.md5(  # noqa: S324  # nosec B324, B303
+            str((root_dir, getpass.getuser())).encode()
+        )
+        repo_token = md5.hexdigest()
+        return os.path.join(repos_dir, repo_token)
 
-    @staticmethod
-    def _raise_ref_conflict(scm, ref, new_rev, checkpoint):
-        # If this commit is a duplicate of the existing commit at 'ref', return
-        # the existing commit. Otherwise, error out and require user to re-run
-        # with --force as needed
-        orig_rev = scm.get_ref(ref)
-        if scm.diff(orig_rev, new_rev):
-            if checkpoint:
-                raise CheckpointExistsError(ref)
-            raise ExperimentExistsError(ref)
-        return orig_rev
+    def close(self):
+        self.scm.close()
+        self.state.close()
+        if self._data_index is not None:
+            self._data_index.close()
+
+    def _reset(self):
+        self.scm._reset()  # pylint: disable=protected-access
+        self.state.close()
+        self.__dict__.pop("index", None)
+        self.__dict__.pop("dvcignore", None)
+        self.__dict__.pop("dvcfs", None)
+        self.__dict__.pop("datafs", None)
 
-    @staticmethod
-    def _set_log_level(level):
-        from dvc.logger import disable_other_loggers
+    def __enter__(self):
+        return self
 
-        # When executor.reproduce is run in a multiprocessing child process,
-        # dvc.cli.main will not be called for that child process so we need to
-        # setup logging ourselves
-        dvc_logger = logging.getLogger("dvc")
-        disable_other_loggers()
-        if level is not None:
-            dvc_logger.setLevel(level)
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        self.close()
```

### Comparing `dvc-2.9.5/dvc/repo/experiments/executor/local.py` & `dvc-3.0.0a0/dvc/repo/experiments/executor/local.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,35 +1,40 @@
 import logging
 import os
 from contextlib import ExitStack
 from tempfile import mkdtemp
-from typing import TYPE_CHECKING, Optional
+from typing import TYPE_CHECKING, Optional, Union
 
-from funcy import cached_property
+from configobj import ConfigObj
+from funcy import retry
+from shortuuid import uuid
 
-from dvc.scm import SCM
-from dvc.utils.fs import remove
-
-from ..base import (
+from dvc.lock import LockError
+from dvc.repo.experiments.refs import (
     EXEC_APPLY,
     EXEC_BASELINE,
     EXEC_BRANCH,
     EXEC_CHECKPOINT,
     EXEC_HEAD,
     EXEC_MERGE,
     EXEC_NAMESPACE,
+    TEMP_NAMESPACE,
 )
-from .base import EXEC_TMP_DIR, BaseExecutor
+from dvc.repo.experiments.utils import EXEC_TMP_DIR, get_exp_rwlock
+from dvc.scm import SCM, Git
+from dvc.utils.fs import remove
+from dvc.utils.objects import cached_property
 
-if TYPE_CHECKING:
-    from scmrepo.git import Git
+from .base import BaseExecutor, TaskStatus
 
+if TYPE_CHECKING:
     from dvc.repo import Repo
-
-    from ..base import ExpRefInfo, ExpStashEntry
+    from dvc.repo.experiments.refs import ExpRefInfo
+    from dvc.repo.experiments.stash import ExpStashEntry
+    from dvc.scm import NoSCM
 
 logger = logging.getLogger(__name__)
 
 
 class BaseLocalExecutor(BaseExecutor):
     """Base local machine executor."""
 
@@ -37,21 +42,21 @@
     def git_url(self) -> str:
         root_dir = os.path.abspath(self.root_dir)
         if os.name == "nt":
             root_dir = root_dir.replace(os.sep, "/")
         return f"file://{root_dir}"
 
     @cached_property
-    def scm(self):
+    def scm(self) -> Union["Git", "NoSCM"]:
         return SCM(self.root_dir)
 
-    def cleanup(self):
-        super().cleanup()
+    def cleanup(self, infofile: Optional[str] = None):
         self.scm.close()
         del self.scm
+        super().cleanup(infofile)
 
     def collect_cache(
         self, repo: "Repo", exp_ref: "ExpRefInfo", run_cache: bool = True
     ):
         """Collect DVC cache."""
 
 
@@ -59,71 +64,124 @@
     """Temp directory experiment executor."""
 
     # Temp dir executors should warn if untracked files exist (to help with
     # debugging user code), and suppress other DVC hints (like `git add`
     # suggestions) that are not applicable outside of workspace runs
     WARN_UNTRACKED = True
     QUIET = True
-    DEFAULT_LOCATION = "temp"
+    DEFAULT_LOCATION = "tempdir"
 
-    def init_git(self, scm: "Git", branch: Optional[str] = None):
+    @retry(180, errors=LockError, timeout=1)
+    def init_git(
+        self,
+        repo: "Repo",
+        scm: "Git",
+        stash_rev: str,
+        entry: "ExpStashEntry",
+        infofile: Optional[str],
+        branch: Optional[str] = None,
+    ):
         from dulwich.repo import Repo as DulwichRepo
 
-        from ..utils import push_refspec
+        from dvc.repo.experiments.utils import push_refspec
 
         DulwichRepo.init(os.fspath(self.root_dir))
 
-        refspec = f"{EXEC_NAMESPACE}/"
-        push_refspec(scm, self.git_url, refspec, refspec)
-        if branch:
-            push_refspec(scm, self.git_url, branch, branch)
-            self.scm.set_ref(EXEC_BRANCH, branch, symbolic=True)
-        elif self.scm.get_ref(EXEC_BRANCH):
-            self.scm.remove_ref(EXEC_BRANCH)
+        self.status = TaskStatus.PREPARING
+        if infofile:
+            self.info.dump_json(infofile)
+
+        temp_head = f"{TEMP_NAMESPACE}/head-{uuid()}"
+        temp_merge = f"{TEMP_NAMESPACE}/merge-{uuid()}"
+        temp_baseline = f"{TEMP_NAMESPACE}/baseline-{uuid()}"
+
+        temp_ref_dict = {
+            temp_head: entry.head_rev,
+            temp_merge: stash_rev,
+            temp_baseline: entry.baseline_rev,
+        }
+        with get_exp_rwlock(
+            repo, writes=[temp_head, temp_merge, temp_baseline]
+        ), self.set_temp_refs(scm, temp_ref_dict):
+            # Executor will be initialized with an empty git repo that
+            # we populate by pushing:
+            #   EXEC_HEAD - the base commit for this experiment
+            #   EXEC_MERGE - the unmerged changes (from our stash)
+            #       to be reproduced
+            #   EXEC_BASELINE - the baseline commit for this experiment
+            refspec = [
+                (temp_head, EXEC_HEAD),
+                (temp_merge, EXEC_MERGE),
+                (temp_baseline, EXEC_BASELINE),
+            ]
+
+            if branch:
+                refspec.append((branch, branch))
+                with get_exp_rwlock(repo, reads=[branch]):
+                    push_refspec(scm, self.git_url, refspec)
+                self.scm.set_ref(EXEC_BRANCH, branch, symbolic=True)
+            else:
+                push_refspec(scm, self.git_url, refspec)
+                if self.scm.get_ref(EXEC_BRANCH):
+                    self.scm.remove_ref(EXEC_BRANCH)
 
-        if self.scm.get_ref(EXEC_CHECKPOINT):
-            self.scm.remove_ref(EXEC_CHECKPOINT)
+            if self.scm.get_ref(EXEC_CHECKPOINT):
+                self.scm.remove_ref(EXEC_CHECKPOINT)
 
         # checkout EXEC_HEAD and apply EXEC_MERGE on top of it without
         # committing
+        assert isinstance(self.scm, Git)
         head = EXEC_BRANCH if branch else EXEC_HEAD
         self.scm.checkout(head, detach=True)
         merge_rev = self.scm.get_ref(EXEC_MERGE)
-        self.scm.merge(merge_rev, squash=True, commit=False)
 
-    def _config(self, cache_dir):
+        self.scm.stash.apply(merge_rev)
+        self._update_config(repo.config.read("local"))
+
+    def _update_config(self, update):
         local_config = os.path.join(
             self.root_dir,
             self.dvc_dir,
             "config.local",
         )
         logger.debug("Writing experiments local config '%s'", local_config)
-        with open(local_config, "w", encoding="utf-8") as fobj:
-            fobj.write(f"[cache]\n    dir = {cache_dir}")
+        if os.path.exists(local_config):
+            conf_obj = ConfigObj(local_config)
+            conf_obj.merge(update)
+        else:
+            conf_obj = ConfigObj(update)
+        if conf_obj:
+            with open(local_config, "wb") as fobj:
+                conf_obj.write(fobj)
 
-    def init_cache(self, repo: "Repo", rev: str, run_cache: bool = True):
+    def init_cache(
+        self, repo: "Repo", rev: str, run_cache: bool = True  # noqa: ARG002
+    ):
         """Initialize DVC cache."""
-        self._config(repo.odb.local.cache_dir)
+        self._update_config({"cache": {"dir": repo.cache.repo.path}})
 
-    def cleanup(self):
-        super().cleanup()
+    def cleanup(self, infofile: Optional[str] = None):
+        super().cleanup(infofile)
         logger.debug("Removing tmpdir '%s'", self.root_dir)
         remove(self.root_dir)
 
     @classmethod
     def from_stash_entry(
         cls,
         repo: "Repo",
-        stash_rev: str,
         entry: "ExpStashEntry",
+        wdir: Optional[str] = None,
         **kwargs,
     ):
-        tmp_dir = mkdtemp(dir=os.path.join(repo.tmp_dir, EXEC_TMP_DIR))
+        assert repo.tmp_dir
+        parent_dir: str = wdir or os.path.join(repo.tmp_dir, EXEC_TMP_DIR)
+        os.makedirs(parent_dir, exist_ok=True)
+        tmp_dir = mkdtemp(dir=parent_dir)
         try:
-            executor = cls._from_stash_entry(repo, stash_rev, entry, tmp_dir)
+            executor = cls._from_stash_entry(repo, entry, tmp_dir, **kwargs)
             logger.debug("Init temp dir executor in '%s'", tmp_dir)
             return executor
         except Exception:
             remove(tmp_dir)
             raise
 
 
@@ -133,43 +191,66 @@
         self._detach_stack = ExitStack()
         self._orig_checkpoint = self.scm.get_ref(EXEC_CHECKPOINT)
 
     @classmethod
     def from_stash_entry(
         cls,
         repo: "Repo",
-        stash_rev: str,
         entry: "ExpStashEntry",
         **kwargs,
     ):
         root_dir = repo.scm.root_dir
-        executor = cls._from_stash_entry(repo, stash_rev, entry, root_dir)
+        executor: "WorkspaceExecutor" = cls._from_stash_entry(
+            repo, entry, root_dir, **kwargs
+        )
         logger.debug("Init workspace executor in '%s'", root_dir)
         return executor
 
-    def init_git(self, scm: "Git", branch: Optional[str] = None):
-        self._detach_stack.enter_context(
-            self.scm.detach_head(
-                self.scm.get_ref(EXEC_HEAD),
-                force=True,
-                client="dvc",
+    @retry(180, errors=LockError, timeout=1)
+    def init_git(
+        self,
+        repo: "Repo",
+        scm: "Git",
+        stash_rev: str,
+        entry: "ExpStashEntry",
+        infofile: Optional[str],
+        branch: Optional[str] = None,
+    ):
+        self.status = TaskStatus.PREPARING
+        if infofile:
+            self.info.dump_json(infofile)
+
+        assert isinstance(self.scm, Git)
+
+        with get_exp_rwlock(repo, writes=[EXEC_NAMESPACE]):
+            scm.set_ref(EXEC_HEAD, entry.head_rev)
+            scm.set_ref(EXEC_MERGE, stash_rev)
+            scm.set_ref(EXEC_BASELINE, entry.baseline_rev)
+            self._detach_stack.enter_context(
+                self.scm.detach_head(
+                    self.scm.get_ref(EXEC_HEAD),
+                    force=True,
+                    client="dvc",
+                )
             )
-        )
-        merge_rev = self.scm.get_ref(EXEC_MERGE)
-        self.scm.merge(merge_rev, squash=True, commit=False)
-        if branch:
-            self.scm.set_ref(EXEC_BRANCH, branch, symbolic=True)
-        elif scm.get_ref(EXEC_BRANCH):
-            self.scm.remove_ref(EXEC_BRANCH)
+            merge_rev = self.scm.get_ref(EXEC_MERGE)
+            self.scm.stash.apply(merge_rev)
+            if branch:
+                self.scm.set_ref(EXEC_BRANCH, branch, symbolic=True)
+            elif scm.get_ref(EXEC_BRANCH):
+                self.scm.remove_ref(EXEC_BRANCH)
 
     def init_cache(self, repo: "Repo", rev: str, run_cache: bool = True):
         pass
 
-    def cleanup(self):
+    def cleanup(self, infofile: Optional[str] = None):
+        super().cleanup(infofile)
+        if infofile:
+            remove(os.path.dirname(infofile))
         with self._detach_stack:
             self.scm.remove_ref(EXEC_BASELINE)
+            self.scm.remove_ref(EXEC_MERGE)
             if self.scm.get_ref(EXEC_BRANCH):
                 self.scm.remove_ref(EXEC_BRANCH)
             checkpoint = self.scm.get_ref(EXEC_CHECKPOINT)
             if checkpoint and checkpoint != self._orig_checkpoint:
                 self.scm.set_ref(EXEC_APPLY, checkpoint)
-        super().cleanup()
```

### Comparing `dvc-2.9.5/dvc/repo/experiments/executor/manager/base.py` & `dvc-3.0.0a0/dvc/repo/experiments/queue/workspace.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,275 +1,276 @@
+import json
 import logging
 import os
-from abc import ABC
-from collections import defaultdict, deque
-from collections.abc import Mapping
-from typing import TYPE_CHECKING, Deque, Dict, Generator, Optional, Tuple, Type
-
-from dvc.proc.manager import ProcessManager
-
-from ...base import (
-    EXEC_BASELINE,
-    EXEC_HEAD,
-    EXEC_MERGE,
-    CheckpointExistsError,
-    ExperimentExistsError,
-    ExpRefInfo,
-    ExpStashEntry,
-)
-from ..base import EXEC_PID_DIR, BaseExecutor
-from ..local import TempDirExecutor, WorkspaceExecutor
+from collections import defaultdict
+from typing import TYPE_CHECKING, Collection, Dict, Generator, List, Optional
 
-if TYPE_CHECKING:
-    from scmrepo.git import Git
+import psutil
+from funcy import first
 
-    from dvc.repo import Repo
+from dvc.exceptions import DvcException
+from dvc.repo.experiments.exceptions import ExpQueueEmptyError
+from dvc.repo.experiments.executor.base import ExecutorInfo, TaskStatus
+from dvc.repo.experiments.executor.local import WorkspaceExecutor
+from dvc.repo.experiments.refs import EXEC_BRANCH, WORKSPACE_STASH
+from dvc.repo.experiments.utils import get_exp_rwlock
+from dvc.utils.fs import remove
+from dvc.utils.serialize import load_json
 
-logger = logging.getLogger(__name__)
+from .base import BaseStashQueue, QueueEntry, QueueGetResult
 
+if TYPE_CHECKING:
+    from dvc.repo.experiments import Experiments
+    from dvc.repo.experiments.executor.base import BaseExecutor, ExecutorResult
+    from dvc.repo.experiments.serialize import ExpRange
 
-class BaseExecutorManager(ABC, Mapping):
-    """Manages executors for a collection of experiments to be run."""
+    from .base import QueueDoneResult
 
-    EXECUTOR_CLS: Type = BaseExecutor
+logger = logging.getLogger(__name__)
 
-    def __init__(
-        self,
-        scm: "Git",
-        wdir: str,
-        **kwargs,
-    ):
-        from dvc.utils.fs import makedirs
 
-        self.scm = scm
-        makedirs(wdir, exist_ok=True)
-        self.wdir = wdir
-        self.proc = ProcessManager(self.pid_dir)
-        self._attached: Dict[str, "BaseExecutor"] = {}
-        self._detached: Dict[str, "BaseExecutor"] = dict(self._load_infos())
-        self._queue: Deque[Tuple[str, "BaseExecutor"]] = deque()
+class WorkspaceQueue(BaseStashQueue):
+    _EXEC_NAME: Optional[str] = "workspace"
 
-    def __getitem__(self, key: str) -> "BaseExecutor":
+    def put(self, *args, **kwargs) -> QueueEntry:
+        kwargs.pop("copy_paths", None)
+        with get_exp_rwlock(self.repo, writes=["workspace", WORKSPACE_STASH]):
+            return self._stash_exp(*args, **kwargs)
+
+    def get(self) -> QueueGetResult:
+        revs = self.stash.stash_revs
+        if not revs:
+            raise ExpQueueEmptyError("No experiments in the queue.")
+        stash_rev, stash_entry = first(revs.items())
+        entry = QueueEntry(
+            self.repo.root_dir,
+            self.scm.root_dir,
+            self.ref,
+            stash_rev,
+            stash_entry.baseline_rev,
+            stash_entry.branch,
+            stash_entry.name,
+            stash_entry.head_rev,
+        )
+        executor = self.init_executor(self.repo.experiments, entry)
+        return QueueGetResult(entry, executor)
+
+    def iter_queued(self) -> Generator[QueueEntry, None, None]:
+        for rev, entry in self.stash.stash_revs.items():
+            yield QueueEntry(
+                self.repo.root_dir,
+                self.scm.root_dir,
+                self.ref,
+                rev,
+                entry.baseline_rev,
+                entry.branch,
+                entry.name,
+                entry.head_rev,
+            )
+
+    def iter_active(self) -> Generator[QueueEntry, None, None]:
+        # Workspace run state is reflected in the workspace itself and does not
+        # need to be handled via the queue
+        raise NotImplementedError
+
+    def iter_done(self) -> Generator["QueueDoneResult", None, None]:
+        raise NotImplementedError
+
+    def iter_failed(self) -> Generator["QueueDoneResult", None, None]:
+        raise NotImplementedError
+
+    def iter_success(self) -> Generator["QueueDoneResult", None, None]:
+        raise NotImplementedError
+
+    def reproduce(
+        self, copy_paths: Optional[List[str]] = None, message: Optional[str] = None
+    ) -> Dict[str, Dict[str, str]]:
+        results: Dict[str, Dict[str, str]] = defaultdict(dict)
         try:
-            return self._attached[key]
-        except KeyError:
+            while True:
+                entry, executor = self.get()
+                results.update(
+                    self._reproduce_entry(
+                        entry, executor, copy_paths=copy_paths, message=message
+                    )
+                )
+        except ExpQueueEmptyError:
             pass
-        return self._detached[key]
+        return results
 
-    def __iter__(self):
-        yield from self._attached
-        yield from self._detached
+    def _reproduce_entry(
+        self, entry: QueueEntry, executor: "BaseExecutor", **kwargs
+    ) -> Dict[str, Dict[str, str]]:
+        kwargs.pop("copy_paths", None)
+        from dvc.stage.monitor import CheckpointKilledError
+        from dvc_task.proc.process import ProcessInfo
 
-    def __len__(self):
-        return len(self._attached) + len(self._detached)
+        results: Dict[str, Dict[str, str]] = defaultdict(dict)
+        exec_name = self._EXEC_NAME or entry.stash_rev
+        proc_info = ProcessInfo(os.getpid(), None, None, None, None)
+        proc_info_path = self._proc_info_path(exec_name)
+        os.makedirs(os.path.dirname(proc_info_path), exist_ok=True)
+        proc_info.dump(proc_info_path)
+        infofile = self.get_infofile_path(exec_name)
+        try:
+            rev = entry.stash_rev
+            exec_result = executor.reproduce(
+                info=executor.info,
+                rev=rev,
+                infofile=infofile,
+                log_level=logger.getEffectiveLevel(),
+                log_errors=not isinstance(executor, WorkspaceExecutor),
+                message=kwargs.get("message"),
+            )
+            if not exec_result.exp_hash:
+                raise DvcException(f"Failed to reproduce experiment '{rev[:7]}'")
+            if exec_result.ref_info:
+                results[rev].update(
+                    self.collect_executor(self.repo.experiments, executor, exec_result)
+                )
+        except CheckpointKilledError:
+            # Checkpoint errors have already been logged
+            return {}
+        except DvcException:
+            raise
+        except Exception as exc:  # noqa: BLE001
+            raise DvcException(f"Failed to reproduce experiment '{rev[:7]}'") from exc
+        finally:
+            executor.cleanup(infofile)
+            remove(self._proc_info_path(exec_name))
+        return results
 
-    @property
-    def pid_dir(self) -> str:
-        return os.path.join(self.wdir, EXEC_PID_DIR)
+    def _proc_info_path(self, name: str) -> str:
+        return os.path.join(self.pid_dir, name, f"{name}.json")
 
-    def enqueue(self, rev: str, executor: "BaseExecutor"):
-        assert rev not in self
-        self._queue.append((rev, executor))
-
-    def _load_infos(self) -> Generator[Tuple[str, "BaseExecutor"], None, None]:
-        import json
-        from urllib.parse import urlparse
-
-        from ..base import ExecutorInfo
-        from ..ssh import SSHExecutor
-
-        def make_executor(info: "ExecutorInfo"):
-            if info.git_url:
-                scheme = urlparse(info.git_url).scheme
-                if scheme == "file":
-                    cls: Type = TempDirExecutor
-                elif scheme == "ssh":
-                    cls = SSHExecutor
-                else:
-                    raise NotImplementedError
-            else:
-                cls = WorkspaceExecutor
-            return cls.from_info(info)
+    @property
+    def _active_pid(self) -> Optional[int]:
+        from dvc_task.proc.process import ProcessInfo
 
-        for name in self.proc:
-            infofile = self.get_infofile_path(name)
-            try:
-                with open(infofile, encoding="utf-8") as fobj:
-                    info = ExecutorInfo.from_dict(json.load(fobj))
-                yield name, make_executor(info)
-            except OSError:
-                continue
-
-    def get_infofile_path(self, name: str) -> str:
-        return os.path.join(
-            self.pid_dir,
-            name,
-            f"{name}{BaseExecutor.INFOFILE_EXT}",
-        )
+        assert self._EXEC_NAME
+        name = self._EXEC_NAME
+        try:
+            proc_info = ProcessInfo.load(self._proc_info_path(name))
+            pid = proc_info.pid
+            if psutil.pid_exists(pid):
+                return pid
+            logger.debug("Workspace exec PID '%d' no longer exists, removing.", pid)
+            remove(self._proc_info_path(name))
+        except (FileNotFoundError, json.JSONDecodeError):
+            pass
+        return None
 
-    @classmethod
-    def from_stash_entries(
-        cls,
-        scm: "Git",
-        wdir: str,
-        repo: "Repo",
-        to_run: Dict[str, ExpStashEntry],
-        **kwargs,
-    ):
-        manager = cls(scm, wdir)
-        manager._enqueue_stash_entries(scm, repo, to_run, **kwargs)
-        return manager
+    @staticmethod
+    def collect_executor(  # pylint: disable=unused-argument
+        exp: "Experiments",
+        executor: "BaseExecutor",  # noqa: ARG004
+        exec_result: "ExecutorResult",
+    ) -> Dict[str, str]:
+        results: Dict[str, str] = {}
+        exp_rev = exp.scm.get_ref(EXEC_BRANCH)
+        if exp_rev:
+            assert exec_result.exp_hash
+            logger.debug("Collected experiment '%s'.", exp_rev[:7])
+            results[exp_rev] = exec_result.exp_hash
 
-    def _enqueue_stash_entries(
-        self,
-        scm: "Git",
-        repo: "Repo",
-        to_run: Dict[str, ExpStashEntry],
-        **kwargs,
-    ):
-        try:
-            for stash_rev, entry in to_run.items():
-                scm.set_ref(EXEC_HEAD, entry.head_rev)
-                scm.set_ref(EXEC_MERGE, stash_rev)
-                scm.set_ref(EXEC_BASELINE, entry.baseline_rev)
-
-                # Executor will be initialized with an empty git repo that
-                # we populate by pushing:
-                #   EXEC_HEAD - the base commit for this experiment
-                #   EXEC_MERGE - the unmerged changes (from our stash)
-                #       to be reproduced
-                #   EXEC_BASELINE - the baseline commit for this experiment
-                executor = self.EXECUTOR_CLS.from_stash_entry(
-                    repo,
-                    stash_rev,
-                    entry,
-                    **kwargs,
-                )
-                self.enqueue(stash_rev, executor)
-        finally:
-            for ref in (EXEC_HEAD, EXEC_MERGE, EXEC_BASELINE):
-                scm.remove_ref(ref)
+        return results
 
-    def exec_queue(
-        self, repo: "Repo", jobs: Optional[int] = 1, detach: bool = False
-    ):
-        """Run dvc repro for queued executors in parallel."""
-        if detach:
-            raise NotImplementedError
-            # TODO use ProcessManager.spawn() to support detached runs
-        return self._exec_attached(repo, jobs=jobs)
-
-    def _exec_attached(self, repo: "Repo", jobs: Optional[int] = 1):
-        import signal
-        from concurrent.futures import (
-            CancelledError,
-            ProcessPoolExecutor,
-            wait,
-        )
-        from multiprocessing import Manager
+    def get_result(self, entry: QueueEntry) -> Optional["ExecutorResult"]:
+        raise NotImplementedError
 
-        from dvc.stage.monitor import CheckpointKilledError
+    def kill(self, revs: Collection[str]) -> None:
+        raise NotImplementedError
 
-        result: Dict[str, Dict[str, str]] = defaultdict(dict)
+    def shutdown(self, kill: bool = False):
+        raise NotImplementedError
 
-        manager = Manager()
-        pid_q = manager.Queue()
+    def logs(
+        self,
+        rev: str,
+        encoding: Optional[str] = None,
+        follow: bool = False,
+    ):
+        raise NotImplementedError
 
-        with ProcessPoolExecutor(max_workers=jobs) as workers:
-            futures = {}
-            while self._queue:
-                rev, executor = self._queue.popleft()
-                infofile = self.get_infofile_path(rev)
-                future = workers.submit(
-                    executor.reproduce,
-                    info=executor.info,
-                    rev=rev,
-                    queue=pid_q,
-                    infofile=infofile,
-                    log_level=logger.getEffectiveLevel(),
-                )
-                futures[future] = (rev, executor)
-                self._attached[rev] = executor
+    def get_running_exps(
+        self,
+        fetch_refs: bool = True,  # noqa: ARG002
+    ) -> Dict[str, Dict]:
+        assert self._EXEC_NAME
+        result: Dict[str, Dict] = {}
+        if self._active_pid is None:
+            return result
 
-            try:
-                wait(futures)
-            except KeyboardInterrupt:
-                # forward SIGINT to any running executor processes and
-                # cancel any remaining futures
-                workers.shutdown(wait=False)
-                pids = {}
-                for future, (rev, _) in futures.items():
-                    if future.running():
-                        # if future has already been started by the scheduler
-                        # we still have to wait until it tells us its PID
-                        while rev not in pids:
-                            rev, pid = pid_q.get()
-                            pids[rev] = pid
-                        os.kill(pids[rev], signal.SIGINT)
-                    elif not future.done():
-                        future.cancel()
-
-            for future, (rev, executor) in futures.items():
-                rev, executor = futures[future]
-
-                try:
-                    exc = future.exception()
-                    if exc is None:
-                        exec_result = future.result()
-                        result[rev].update(
-                            self._collect_executor(repo, executor, exec_result)
-                        )
-                    elif not isinstance(exc, CheckpointKilledError):
-                        logger.error(
-                            "Failed to reproduce experiment '%s'", rev[:7]
-                        )
-                except CancelledError:
-                    logger.error(
-                        "Cancelled before attempting to reproduce experiment "
-                        "'%s'",
-                        rev[:7],
-                    )
-                finally:
-                    self.cleanup_executor(rev, executor)
+        infofile = self.get_infofile_path(self._EXEC_NAME)
 
+        try:
+            info = ExecutorInfo.from_dict(load_json(infofile))
+        except OSError:
+            return result
+
+        if info.status < TaskStatus.FAILED:
+            # If we are appending to a checkpoint branch in a workspace
+            # run, show the latest checkpoint as running.
+            if info.status == TaskStatus.SUCCESS:
+                return result
+            last_rev = self.scm.get_ref(EXEC_BRANCH)
+            if last_rev:
+                result[last_rev] = info.asdict()
+            else:
+                result[self._EXEC_NAME] = info.asdict()
         return result
 
-    def _collect_executor(self, repo, executor, exec_result) -> Dict[str, str]:
-        # NOTE: GitPython Repo instances cannot be re-used
-        # after process has received SIGINT or SIGTERM, so we
-        # need this hack to re-instantiate git instances after
-        # checkpoint runs. See:
-        # https://github.com/gitpython-developers/GitPython/issues/427
-        # del self.repo.scm
-
-        results = {}
-
-        def on_diverged(ref: str, checkpoint: bool):
-            ref_info = ExpRefInfo.from_ref(ref)
-            if checkpoint:
-                raise CheckpointExistsError(ref_info.name)
-            raise ExperimentExistsError(ref_info.name)
-
-        for ref in executor.fetch_exps(
-            self.scm,
-            force=exec_result.force,
-            on_diverged=on_diverged,
-        ):
-            exp_rev = self.scm.get_ref(ref)
-            if exp_rev:
-                logger.debug("Collected experiment '%s'.", exp_rev[:7])
-                results[exp_rev] = exec_result.exp_hash
+    def collect_active_data(
+        self,
+        baseline_revs: Optional[Collection[str]],
+        fetch_refs: bool = False,  # noqa: ARG002
+        **kwargs,
+    ) -> Dict[str, List["ExpRange"]]:
+        from dvc.repo.experiments.collect import collect_exec_branch
+        from dvc.repo.experiments.serialize import (
+            ExpExecutor,
+            ExpRange,
+            LocalExpExecutor,
+        )
 
-        if exec_result.ref_info is not None:
-            executor.collect_cache(repo, exec_result.ref_info)
+        result: Dict[str, List[ExpRange]] = defaultdict(list)
+        pid = self._active_pid
+        if pid is None:
+            return result
 
-        return results
+        assert self._EXEC_NAME
+        infofile = self.get_infofile_path(self._EXEC_NAME)
+        try:
+            info = ExecutorInfo.from_dict(load_json(infofile))
+        except OSError:
+            return result
+
+        if (
+            (not baseline_revs or info.baseline_rev in baseline_revs)
+            and info.status < TaskStatus.FAILED
+            and info.status != TaskStatus.SUCCESS
+        ):
+            local_exec = LocalExpExecutor(root=info.root_dir, pid=pid)
+            exps = list(collect_exec_branch(self.repo, info.baseline_rev, **kwargs))
+            exps[0].name = info.name
+            result[info.baseline_rev] = [
+                ExpRange(
+                    exps,
+                    executor=ExpExecutor("running", name="workspace", local=local_exec),
+                    name=info.name,
+                )
+            ]
+        return result
 
-    def cleanup_executor(self, rev: str, executor: "BaseExecutor"):
-        from dvc.utils.fs import remove
+    def collect_queued_data(
+        self,
+        baseline_revs: Optional[Collection[str]],
+        **kwargs,
+    ) -> Dict[str, List["ExpRange"]]:
+        raise NotImplementedError
 
-        executor.cleanup()
-        try:
-            self.proc.remove(rev)
-        except KeyError:
-            pass
-        remove(os.path.join(self.pid_dir, rev))
+    def collect_failed_data(
+        self,
+        baseline_revs: Optional[Collection[str]],
+        **kwargs,
+    ) -> Dict[str, List["ExpRange"]]:
+        raise NotImplementedError
```

### Comparing `dvc-2.9.5/dvc/repo/experiments/executor/manager/local.py` & `dvc-3.0.0a0/dvc/repo/experiments/queue/utils.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,112 +1,90 @@
 import logging
-from collections import defaultdict
-from typing import TYPE_CHECKING, Dict, Optional
+from typing import TYPE_CHECKING, Dict, List
 
-from ...base import (
-    EXEC_BASELINE,
-    EXEC_BRANCH,
-    EXEC_HEAD,
-    EXEC_MERGE,
-    ExpStashEntry,
-)
-from ..local import TempDirExecutor, WorkspaceExecutor
-from .base import BaseExecutorManager
-
-if TYPE_CHECKING:
-    from scmrepo.git import Git
+from scmrepo.exceptions import SCMError
 
-    from dvc.repo import Repo
+from dvc.repo.experiments.executor.base import ExecutorInfo, TaskStatus
+from dvc.repo.experiments.refs import (
+    EXEC_CHECKPOINT,
+    EXEC_NAMESPACE,
+    EXPS_NAMESPACE,
+    EXPS_STASH,
+)
+from dvc.repo.experiments.utils import get_exp_rwlock, iter_remote_refs
 
 logger = logging.getLogger(__name__)
 
 
-class TempDirExecutorManager(BaseExecutorManager):
-    EXECUTOR_CLS = TempDirExecutor
+if TYPE_CHECKING:
+    from dvc.scm import Git
 
+    from .base import BaseStashQueue
 
-class WorkspaceExecutorManager(BaseExecutorManager):
-    EXECUTOR_CLS = WorkspaceExecutor
 
-    @classmethod
-    def from_stash_entries(
-        cls,
-        scm: "Git",
-        wdir: str,
-        repo: "Repo",
-        to_run: Dict[str, ExpStashEntry],
-        **kwargs,
-    ):
-        manager = cls(scm, wdir)
-        try:
-            assert len(to_run) == 1
-            for stash_rev, entry in to_run.items():
-                scm.set_ref(EXEC_HEAD, entry.head_rev)
-                scm.set_ref(EXEC_MERGE, stash_rev)
-                scm.set_ref(EXEC_BASELINE, entry.baseline_rev)
-
-                executor = cls.EXECUTOR_CLS.from_stash_entry(
-                    repo,
-                    stash_rev,
-                    entry,
-                    **kwargs,
-                )
-                manager.enqueue(stash_rev, executor)
-        finally:
-            for ref in (EXEC_MERGE,):
-                scm.remove_ref(ref)
-        return manager
-
-    def _collect_executor(self, repo, executor, exec_result) -> Dict[str, str]:
-        results = {}
-        exp_rev = self.scm.get_ref(EXEC_BRANCH)
-        if exp_rev:
-            logger.debug("Collected experiment '%s'.", exp_rev[:7])
-            results[exp_rev] = exec_result.exp_hash
-        return results
+def get_remote_executor_refs(scm: "Git", remote_url: str) -> List[str]:
+    """Get result list refs from a remote repository
 
-    def exec_queue(
-        self, repo: "Repo", jobs: Optional[int] = 1, detach: bool = False
+    Args:
+        remote_url : remote executor's url
+    """
+    refs = []
+    for ref in iter_remote_refs(
+        scm,
+        remote_url,
+        base=EXPS_NAMESPACE,
     ):
-        """Run a single WorkspaceExecutor.
-
-        Workspace execution is done within the main DVC process
-        (rather than in multiprocessing context)
-        """
-        from dvc.exceptions import DvcException
-        from dvc.stage.monitor import CheckpointKilledError
-
-        assert len(self._queue) == 1
-        assert not detach
-        result: Dict[str, Dict[str, str]] = defaultdict(dict)
-        rev, executor = self._queue.popleft()
-
-        exec_name = "workspace"
-        infofile = self.get_infofile_path(exec_name)
-        try:
-            exec_result = executor.reproduce(
-                info=executor.info,
-                rev=rev,
-                infofile=infofile,
-                log_level=logger.getEffectiveLevel(),
-                log_errors=not isinstance(executor, WorkspaceExecutor),
-            )
-            if not exec_result.exp_hash:
-                raise DvcException(
-                    f"Failed to reproduce experiment '{rev[:7]}'"
-                )
-            if exec_result.ref_info:
-                result[rev].update(
-                    self._collect_executor(repo, executor, exec_result)
-                )
-        except CheckpointKilledError:
-            # Checkpoint errors have already been logged
-            return {}
-        except DvcException:
-            raise
-        except Exception as exc:
-            raise DvcException(
-                f"Failed to reproduce experiment '{rev[:7]}'"
-            ) from exc
-        finally:
-            self.cleanup_executor(exec_name, executor)
+        if ref == EXEC_CHECKPOINT or (
+            not ref.startswith(EXEC_NAMESPACE) and ref != EXPS_STASH
+        ):
+            refs.append(ref)
+    return refs
+
+
+def fetch_running_exp_from_temp_dir(
+    queue: "BaseStashQueue", rev: str, fetch_refs: bool
+) -> Dict[str, Dict]:
+    """Fetch status of running exps out of current working directory
+
+    Args:
+        queue (BaseStashQueue):
+        rev (str): stash revision of the experiment
+        fetch_refs (bool): fetch running checkpoint results to local or not.
+
+    Returns:
+        Dict[str, Dict]: _description_
+    """
+    from dvc.repo.experiments.executor.local import TempDirExecutor
+    from dvc.scm import InvalidRemoteSCMRepo
+    from dvc.utils.serialize import load_json
+
+    result: Dict[str, Dict] = {}
+    infofile = queue.get_infofile_path(rev)
+    try:
+        info = ExecutorInfo.from_dict(load_json(infofile))
+    except OSError:
         return result
+    if info.status <= TaskStatus.RUNNING:
+        result[rev] = info.asdict()
+        if info.git_url and fetch_refs and info.status > TaskStatus.PREPARING:
+
+            def on_diverged(_ref: str, _checkpoint: bool):
+                return True
+
+            executor = TempDirExecutor.from_info(info)
+            try:
+                refs = get_remote_executor_refs(queue.scm, executor.git_url)
+                with get_exp_rwlock(queue.repo, writes=refs):
+                    for ref in executor.fetch_exps(
+                        queue.scm,
+                        refs,
+                        on_diverged=on_diverged,
+                    ):
+                        logger.debug("Updated running experiment '%s'.", ref)
+                        last_rev = queue.scm.get_ref(ref)
+                        result[rev]["last"] = last_rev
+                        if last_rev:
+                            result[last_rev] = info.asdict()
+            except (InvalidRemoteSCMRepo, SCMError):
+                # ignore stale info files
+                del result[rev]
+    return result
```

### Comparing `dvc-2.9.5/dvc/repo/experiments/executor/ssh.py` & `dvc-3.0.0a0/dvc/repo/experiments/executor/ssh.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,35 +1,35 @@
 import logging
 import os
 import posixpath
 import sys
 from contextlib import contextmanager
-from typing import TYPE_CHECKING, Callable, Iterable, Optional
+from typing import TYPE_CHECKING, Callable, Iterable, List, Optional
 
+from dvc_ssh import SSHFileSystem
 from funcy import first
 
-from dvc.fs.ssh import SSHFileSystem
-from dvc.repo.experiments.base import (
+from dvc.repo.experiments.refs import (
+    EXEC_BASELINE,
     EXEC_BRANCH,
     EXEC_CHECKPOINT,
     EXEC_HEAD,
     EXEC_MERGE,
     EXEC_NAMESPACE,
 )
 
-from .base import BaseExecutor, ExecutorInfo, ExecutorResult
+from .base import BaseExecutor, ExecutorInfo, ExecutorResult, TaskStatus
 
 if TYPE_CHECKING:
-    from multiprocessing import Queue
-
-    from scmrepo.git import Git
+    from queue import Queue
 
     from dvc.repo import Repo
-
-    from ..base import ExpRefInfo, ExpStashEntry
+    from dvc.repo.experiments.refs import ExpRefInfo
+    from dvc.repo.experiments.stash import ExpStashEntry
+    from dvc.scm import Git
 
 logger = logging.getLogger(__name__)
 
 
 @contextmanager
 def _sshfs(fs_factory, **kwargs):
     if fs_factory:
@@ -72,22 +72,21 @@
 
         return "-".join([name or "dvc-exp", "executor", uuid()])
 
     @classmethod
     def from_stash_entry(
         cls,
         repo: "Repo",
-        stash_rev: str,
         entry: "ExpStashEntry",
         **kwargs,
     ):
         machine_name: Optional[str] = kwargs.pop("machine_name", None)
+        assert repo.machine
         executor = cls._from_stash_entry(
             repo,
-            stash_rev,
             entry,
             cls.gen_dirname(entry.name),
             location=machine_name,
             **repo.machine.get_executor_kwargs(machine_name),
             setup_script=repo.machine.get_setup_script(machine_name),
         )
         logger.debug("Init SSH executor for host '%s'", executor.host)
@@ -110,57 +109,71 @@
         assert self._repo_abspath
         user = f"{self.username}@" if self.username else ""
         port = f":{self.port}" if self.port is not None else ""
         return f"ssh://{user}{self.host}{port}{self._repo_abspath}"
 
     @staticmethod
     def _git_client_args(fs):
-        kwargs = {
+        return {
             "password": fs.fs_args.get("password"),
             "key_filename": first(fs.fs_args.get("client_keys", [])),
         }
-        return kwargs
 
-    def init_git(self, scm: "Git", branch: Optional[str] = None):
-        from ..utils import push_refspec
+    def init_git(
+        self,
+        repo: "Repo",  # noqa: ARG002
+        scm: "Git",
+        stash_rev: str,
+        entry: "ExpStashEntry",
+        infofile: Optional[str],
+        branch: Optional[str] = None,
+    ):
+        from dvc.repo.experiments.utils import push_refspec
+
+        self.status = TaskStatus.PREPARING
+        if infofile:
+            self.info.dump_json(infofile)
 
         with self.sshfs() as fs:
             fs.makedirs(self.root_dir)
             self._ssh_cmd(fs, "git init .")
             self._ssh_cmd(fs, "git config user.name dvc-exp")
-            self._ssh_cmd(
-                fs, "git config user.email dvc-exp@noreply.localhost"
-            )
+            self._ssh_cmd(fs, "git config user.email dvc-exp@noreply.localhost")
 
             result = self._ssh_cmd(fs, "pwd")
             path = result.stdout.strip()
             self._repo_abspath = path
 
             # TODO: support multiple client key retries in git backends
             # (see https://github.com/iterative/dvc/issues/6508)
             kwargs = self._git_client_args(fs)
-            refspec = f"{EXEC_NAMESPACE}/"
-            push_refspec(scm, self.git_url, refspec, refspec, **kwargs)
+
+            ref_dict = {
+                EXEC_HEAD: entry.head_rev,
+                EXEC_MERGE: stash_rev,
+                EXEC_BASELINE: entry.baseline_rev,
+            }
+            with self.set_temp_refs(scm, ref_dict):
+                exec_namespace = f"{EXEC_NAMESPACE}/"
+                refspec = [(exec_namespace, exec_namespace)]
+                push_refspec(scm, self.git_url, refspec, **kwargs)
+
             if branch:
-                push_refspec(scm, self.git_url, branch, branch, **kwargs)
+                push_refspec(scm, self.git_url, [(branch, branch)], **kwargs)
                 self._ssh_cmd(fs, f"git symbolic-ref {EXEC_BRANCH} {branch}")
             else:
-                self._ssh_cmd(
-                    fs, f"git symbolic-ref -d {EXEC_BRANCH}", check=False
-                )
-            self._ssh_cmd(
-                fs, f"git update-ref -d {EXEC_CHECKPOINT}", check=False
-            )
+                self._ssh_cmd(fs, f"git symbolic-ref -d {EXEC_BRANCH}", check=False)
+            self._ssh_cmd(fs, f"git update-ref -d {EXEC_CHECKPOINT}", check=False)
 
             # checkout EXEC_HEAD and apply EXEC_MERGE on top of it without
             # committing
             head = EXEC_BRANCH if branch else EXEC_HEAD
             self._ssh_cmd(fs, f"git checkout {head}")
             merge_rev = scm.get_ref(EXEC_MERGE)
-            self._ssh_cmd(fs, f"git merge --squash --no-commit {merge_rev}")
+            self._ssh_cmd(fs, f"git stash apply {merge_rev}")
 
             if self._setup_script:
                 self._init_setup_script(fs)
 
     @classmethod
     def _setup_script_path(cls, dvc_dir: str):
         return posixpath.join(
@@ -170,15 +183,16 @@
         )
 
     def _init_setup_script(self, fs: "SSHFileSystem"):
         assert self._repo_abspath
         script_path = self._setup_script_path(
             posixpath.join(self._repo_abspath, self.dvc_dir)
         )
-        fs.upload(self._setup_script, script_path)
+        assert self._setup_script
+        fs.put_file(self._setup_script, script_path)
 
     def _ssh_cmd(self, sshfs, cmd, chdir=None, **kwargs):
         working_dir = chdir or self.root_dir
         return sshfs.fs.execute(f"cd {working_dir};{cmd}", **kwargs)
 
     def init_cache(self, repo: "Repo", rev: str, run_cache: bool = True):
         from dvc.repo.push import push
@@ -199,39 +213,41 @@
         from dvc.repo.experiments.pull import _pull_cache
 
         with self.get_odb() as odb:
             _pull_cache(repo, exp_ref, run_cache=run_cache, odb=odb)
 
     @contextmanager
     def get_odb(self):
-        from dvc.data.db import ODBManager, get_odb
+        from dvc.cachemgr import CacheManager, get_odb
 
         cache_path = posixpath.join(
             self._repo_abspath,
             self.dvc_dir,
-            ODBManager.CACHE_DIR,
+            CacheManager.CACHE_DIR,
         )
 
         with self.sshfs() as fs:
             yield get_odb(fs, cache_path, **fs.config)
 
     def fetch_exps(self, *args, **kwargs) -> Iterable[str]:
         with self.sshfs() as fs:
             kwargs.update(self._git_client_args(fs))
             return super().fetch_exps(*args, **kwargs)
 
     @classmethod
     def reproduce(
         cls,
         info: "ExecutorInfo",
-        rev: str,
-        queue: Optional["Queue"] = None,
+        rev: str,  # noqa: ARG003
+        queue: Optional["Queue"] = None,  # noqa: ARG003
         infofile: Optional[str] = None,
         log_errors: bool = True,
         log_level: Optional[int] = None,
+        copy_paths: Optional[List[str]] = None,  # noqa: ARG003
+        message: Optional[str] = None,  # noqa: ARG003
         **kwargs,
     ) -> "ExecutorResult":
         """Reproduce an experiment on a remote machine over SSH.
 
         Internally uses 'dvc exp exec-run' over SSH.
         """
         import json
@@ -242,31 +258,25 @@
 
         fs_factory: Optional[Callable] = kwargs.pop("fs_factory", None)
         if log_errors and log_level is not None:
             cls._set_log_level(log_level)
 
         with _sshfs(fs_factory) as fs:
             while not fs.exists("/var/log/dvc-machine-init.log"):
-                logger.info(
-                    "Waiting for dvc-machine startup script to complete..."
-                )
+                logger.info("Waiting for dvc-machine startup script to complete...")
                 time.sleep(5)
-            logger.info(
-                "Reproducing experiment on '%s'", fs.fs_args.get("host")
-            )
+            logger.info("Reproducing experiment on '%s'", fs.fs_args.get("host"))
             with TemporaryFile(mode="w+", encoding="utf-8") as fobj:
                 json.dump(info.asdict(), fobj)
                 fobj.seek(0)
-                fs.upload_fobj(fobj, infofile)
+                fs.put_file(fobj, infofile)
             cmd = ["source ~/.profile"]
             script_path = cls._setup_script_path(info.dvc_dir)
             if fs.exists(posixpath.join(info.root_dir, script_path)):
-                cmd.extend(
-                    [f"pushd {info.root_dir}", f"source {script_path}", "popd"]
-                )
+                cmd.extend([f"pushd {info.root_dir}", f"source {script_path}", "popd"])
             exec_cmd = f"dvc exp exec-run --infofile {infofile}"
             if log_level is not None:
                 if log_level <= logging.TRACE:  # type: ignore[attr-defined]
                     exec_cmd += " -vv"
                 elif log_level <= logging.DEBUG:
                     exec_cmd += " -v"
             cmd.append(exec_cmd)
```

### Comparing `dvc-2.9.5/dvc/repo/experiments/init.py` & `dvc-3.0.0a0/dvc/repo/experiments/collect.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,344 +1,380 @@
+import itertools
 import logging
 import os
-from contextlib import contextmanager
-from functools import partial
+from datetime import datetime
 from typing import (
     TYPE_CHECKING,
-    Any,
-    Callable,
+    Collection,
     Dict,
     Iterable,
+    Iterator,
+    List,
     Optional,
-    TextIO,
-    Tuple,
     Union,
-    cast,
 )
 
-from funcy import compact, lremove, lsplit
-from rich.rule import Rule
-from rich.syntax import Syntax
-
-from dvc.exceptions import DvcException
-from dvc.stage import PipelineStage
-from dvc.stage.serialize import to_pipeline_file
-from dvc.types import OptStr
-from dvc.utils import humanize
-from dvc.utils.serialize import dumps_yaml
+from funcy import first
+from scmrepo.exceptions import SCMError as InnerSCMError
 
-if TYPE_CHECKING:
-    from dvc.repo import Repo
-    from dvc.dvcfile import DVCFile
-    from rich.tree import Tree
+from dvc.scm import Git, SCMError, iter_revs
 
-from dvc.ui import ui
+from .exceptions import InvalidExpRefError
+from .refs import EXEC_BRANCH, ExpRefInfo
+from .serialize import ExpRange, ExpState, SerializableError, SerializableExp
 
-PROMPTS = {
-    "cmd": "[b]Command[/b] to execute",
-    "code": "Path to a [b]code[/b] file/directory",
-    "data": "Path to a [b]data[/b] file/directory",
-    "models": "Path to a [b]model[/b] file/directory",
-    "params": "Path to a [b]parameters[/b] file",
-    "metrics": "Path to a [b]metrics[/b] file",
-    "plots": "Path to a [b]plots[/b] file/directory",
-    "live": "Path to log [b]dvclive[/b] outputs",
-}
-
-
-def _prompts(
-    keys: Iterable[str],
-    defaults: Dict[str, str] = None,
-    validator: Callable[[str, str], Union[str, Tuple[str, str]]] = None,
-    allow_omission: bool = True,
-    stream: Optional[TextIO] = None,
-) -> Dict[str, OptStr]:
-    from dvc.ui.prompt import Prompt
-
-    defaults = defaults or {}
-    return {
-        key: Prompt.prompt_(
-            PROMPTS[key],
-            console=ui.error_console,
-            default=defaults.get(key),
-            validator=partial(validator, key) if validator else None,
-            allow_omission=allow_omission,
-            stream=stream,
-        )
-        for key in keys
-    }
+if TYPE_CHECKING:
+    from dvc.repo import Repo
 
+    from .cache import ExpCache
 
-@contextmanager
-def _disable_logging(highest_level=logging.CRITICAL):
-    previous_level = logging.root.manager.disable
+logger = logging.getLogger(__name__)
 
-    logging.disable(highest_level)
 
+def collect_rev(
+    repo: "Repo",
+    rev: str,
+    param_deps: bool = False,
+    force: bool = False,
+    cache: Optional["ExpCache"] = None,
+    **kwargs,
+) -> ExpState:
+    """Collect experiment state for the given revision.
+
+    Exp will be loaded from cache when available unless rev is 'workspace' or
+    force is set.
+    """
+    from dvc.fs import LocalFileSystem
+
+    cache = cache or repo.experiments.cache
+    assert cache
+    # TODO: support filtering serialized exp when param_deps is set
+    if rev != "workspace" and not (force or param_deps):
+        cached_exp = cache.get(rev)
+        if cached_exp:
+            if isinstance(cached_exp, SerializableError):
+                return ExpState(rev=rev, error=cached_exp)
+            return ExpState(rev=rev, data=cached_exp)
+    if rev == "workspace" and isinstance(repo.fs, LocalFileSystem):
+        orig_cwd: Optional[str] = os.getcwd()
+        os.chdir(repo.root_dir)
+    else:
+        orig_cwd = None
     try:
-        yield
+        data = _collect_rev(
+            repo,
+            rev,
+            param_deps=param_deps,
+            force=force,
+            **kwargs,
+        )
+        if not (rev == "workspace" or param_deps or data.contains_error):
+            cache.put(data, force=True)
+        return ExpState(rev=rev, data=data)
+    except Exception as exc:  # noqa: BLE001, pylint: disable=broad-except
+        logger.debug("", exc_info=True)
+        error = SerializableError(str(exc), type(exc).__name__)
+        return ExpState(rev=rev, error=error)
     finally:
-        logging.disable(previous_level)
-
-
-def build_workspace_tree(workspace: Dict[str, str], label: str) -> "Tree":
-    from rich.tree import Tree
+        if orig_cwd:
+            os.chdir(orig_cwd)
 
-    tree = Tree(label, highlight=True)
-    for value in sorted(workspace.values()):
-        tree.add(f"[green]{value}[/green]")
-    return tree
 
+def _collect_rev(
+    repo: "Repo",
+    rev: str,
+    param_deps: bool = False,
+    **kwargs,
+) -> SerializableExp:
+    with repo.switch(rev) as rev:
+        if rev == "workspace":
+            timestamp: Optional[datetime] = None
+        else:
+            commit = repo.scm.resolve_commit(rev)
+            timestamp = datetime.fromtimestamp(commit.commit_time)
 
-def display_workspace_tree(
-    workspace: Dict[str, str], label: str, stderr: bool = False
-) -> None:
-    d = workspace.copy()
-    d.pop("cmd", None)
+        return SerializableExp.from_repo(
+            repo,
+            rev=rev,
+            param_deps=param_deps,
+            timestamp=timestamp,
+        )
 
-    if d:
-        ui.write(build_workspace_tree(d, label), styled=True, stderr=stderr)
-    ui.write(styled=True, stderr=stderr)
 
+def collect_branch(
+    repo: "Repo",
+    rev: str,
+    end_rev: Optional[str] = None,
+    **kwargs,
+) -> Iterator["ExpState"]:
+    """Iterate over exp states in a Git branch.
+
+    Git branch will be traversed in reverse, starting from rev.
+
+    Args:
+        rev: Branch tip (head).
+        end_rev: If specified, traversal will stop when end_rev is reached
+            (exclusive, end_rev will not be collected).
+    """
+    try:
+        for branch_rev in repo.scm.branch_revs(rev, end_rev):
+            yield collect_rev(repo, branch_rev, **kwargs)
+    except (SCMError, InnerSCMError):
+        pass
 
-PIPELINE_FILE_LINK = "https://s.dvc.org/g/pipeline-files"
 
+def collect_exec_branch(
+    repo: "Repo",
+    baseline_rev: str,
+    **kwargs,
+) -> Iterator["ExpState"]:
+    """Iterate over active experiment branch for the current executor."""
+    last_rev = repo.scm.get_ref(EXEC_BRANCH) or repo.scm.get_rev()
+    last_rev = repo.scm.get_rev()
+    yield collect_rev(repo, "workspace", **kwargs)
+    if last_rev != baseline_rev:
+        yield from collect_branch(repo, last_rev, baseline_rev, **kwargs)
 
-def init_interactive(
-    name: str,
-    defaults: Dict[str, str],
-    provided: Dict[str, str],
-    validator: Callable[[str, str], Union[str, Tuple[str, str]]] = None,
-    live: bool = False,
-    stream: Optional[TextIO] = None,
-) -> Dict[str, str]:
-    command = provided.pop("cmd", None)
-    primary = lremove(provided.keys(), ["code", "data", "models", "params"])
-    secondary = lremove(
-        provided.keys(), ["live"] if live else ["metrics", "plots"]
-    )
-    prompts = primary + secondary
 
-    workspace = {**defaults, **provided}
-    if not live and "live" not in provided:
-        workspace.pop("live", None)
-    for key in ("plots", "metrics"):
-        if live and key not in provided:
-            workspace.pop(key, None)
-
-    ret: Dict[str, str] = {}
-    if command:
-        ret["cmd"] = command
-
-    if not prompts and command:
-        return ret
-
-    ui.error_write(
-        f"This command will guide you to set up a [bright_blue]{name}[/]",
-        "stage in [green]dvc.yaml[/].",
-        f"\nSee [repr.url]{PIPELINE_FILE_LINK}[/].\n",
-        styled=True,
-    )
+def collect_queued(
+    repo: "Repo",
+    baseline_revs: Collection[str],
+    **kwargs,
+) -> Dict[str, List["ExpRange"]]:
+    """Collect queued experiments derived from the specified revisions.
+
+    Args:
+        repo: Repo.
+        baseline_revs: Resolved baseline Git SHAs.
+
+    Returns:
+        Dict mapping baseline revision to list of queued experiments.
+    """
+    if not baseline_revs:
+        return {}
+    return repo.experiments.celery_queue.collect_queued_data(baseline_revs, **kwargs)
 
-    if not command:
-        ret.update(
-            compact(_prompts(["cmd"], allow_omission=False, stream=stream))
-        )
-        if prompts:
-            ui.error_write(styled=True)
 
-    if not prompts:
-        return ret
+def collect_active(
+    repo: "Repo",
+    baseline_revs: Collection[str],
+    **kwargs,
+) -> Dict[str, List["ExpRange"]]:
+    """Collect active (running) experiments derived from the specified revisions.
+
+    Args:
+        repo: Repo.
+        baseline_revs: Resolved baseline Git SHAs.
+
+    Returns:
+        Dict mapping baseline revision to list of active experiments.
+    """
+    if not baseline_revs:
+        return {}
+    result: Dict[str, List["ExpRange"]] = {}
+    exps = repo.experiments
+    for queue in (exps.workspace_queue, exps.tempdir_queue, exps.celery_queue):
+        for baseline, active_exps in queue.collect_active_data(
+            baseline_revs, **kwargs
+        ).items():
+            if baseline in result:
+                result[baseline].extend(active_exps)
+            else:
+                result[baseline] = list(active_exps)
+    return result
 
-    ui.error_write(
-        "Enter the paths for dependencies and outputs of the command.",
-        styled=True,
-    )
 
-    tree_label = "Using experiment project structure:"
-    display_workspace_tree(workspace, tree_label, stderr=True)
-    ret.update(
-        compact(
-            _prompts(prompts, defaults, validator=validator, stream=stream)
-        )
-    )
-    return ret
+def collect_failed(
+    repo: "Repo",
+    baseline_revs: Collection[str],
+    **kwargs,
+) -> Dict[str, List["ExpRange"]]:
+    """Collect failed experiments derived from the specified revisions.
+
+    Args:
+        repo: Repo.
+        baseline_revs: Resolved baseline Git SHAs.
+
+    Returns:
+        Dict mapping baseline revision to list of active experiments.
+    """
+    if not baseline_revs:
+        return {}
+    return repo.experiments.celery_queue.collect_failed_data(baseline_revs, **kwargs)
 
 
-def _check_stage_exists(
-    dvcfile: "DVCFile", name: str, force: bool = False
-) -> None:
-    if not force and dvcfile.exists() and name in dvcfile.stages:
-        from dvc.stage.exceptions import DuplicateStageName
-
-        hint = "Use '--force' to overwrite."
-        raise DuplicateStageName(
-            f"Stage '{name}' already exists in 'dvc.yaml'. {hint}"
-        )
+def collect_successful(
+    repo: "Repo",
+    baseline_revs: Collection[str],
+    **kwargs,
+) -> Dict[str, List["ExpRange"]]:
+    """Collect successful experiments derived from the specified revisions.
+
+    Args:
+        repo: Repo.
+        baseline_revs: Resolved baseline Git SHAs.
+
+    Returns:
+        Dict mapping baseline revision to successful experiments.
+    """
+    result: Dict[str, List["ExpRange"]] = {}
+    for baseline_rev in baseline_revs:
+        result[baseline_rev] = list(_collect_baseline(repo, baseline_rev, **kwargs))
+    return result
 
 
-def validate_prompts(
-    repo: "Repo", key: str, value: str
-) -> Union[Any, Tuple[Any, str]]:
-    from dvc.ui.prompt import InvalidResponse
-
-    if key == "params":
-        import errno
-
-        from dvc.dependency.param import ParamsDependency
-
-        assert isinstance(value, str)
-        msg_format = (
-            "[prompt.invalid]'{0}' {1}. "
-            "Please retry with an existing parameters file."
-        )
+def _collect_baseline(
+    repo: "Repo",
+    baseline_rev: str,
+    **kwargs,
+) -> Iterator["ExpRange"]:
+    """Iterate over experiments derived from a baseline revision.
+
+    Args:
+        repo: Repo.
+        baseline_revs: Resolved baseline Git SHAs.
+
+    Yields:
+        Tuple of (timestamp, exp_range).
+    """
+    ref_info = ExpRefInfo(baseline_sha=baseline_rev)
+    refs: Optional[Iterable[str]] = kwargs.get("refs")
+    if refs:
+        ref_it = (ref for ref in iter(refs) if ref.startswith(str(ref_info)))
+    else:
+        ref_it = repo.scm.iter_refs(base=str(ref_info))
+    for ref in ref_it:
         try:
-            ParamsDependency(None, value, repo=repo).validate_filepath()
-        except (IsADirectoryError, FileNotFoundError) as e:
-            suffices = {
-                errno.EISDIR: "is a directory",
-                errno.ENOENT: "does not exist",
-            }
-            raise InvalidResponse(msg_format.format(value, suffices[e.errno]))
-    elif key in ("code", "data"):
-        if not os.path.exists(value):
-            typ = "file" if is_file(value) else "directory"
-            return (
-                value,
-                f"[yellow]'{value}' does not exist, "
-                f"the {typ} will be created. ",
-            )
-    return value
-
-
-def is_file(path: str) -> bool:
-    _, ext = os.path.splitext(path)
-    return bool(ext)
+            ref_info = ExpRefInfo.from_ref(ref)
+            exp_rev = repo.scm.get_ref(ref)
+            if not exp_rev:
+                continue
+        except (InvalidExpRefError, SCMError, InnerSCMError):
+            continue
+        exps = list(collect_branch(repo, exp_rev, baseline_rev, **kwargs))
+        if exps:
+            exps[0].name = ref_info.name
+            yield ExpRange(exps, name=ref_info.name)
 
 
-def init_deps(stage: PipelineStage) -> None:
-    from funcy import rpartial
-
-    from dvc.dependency import ParamsDependency
-    from dvc.fs.local import localfs
-
-    new_deps = [dep for dep in stage.deps if not dep.exists]
-    params, deps = lsplit(rpartial(isinstance, ParamsDependency), new_deps)
-
-    if new_deps:
-        paths = map("[green]{0}[/]".format, new_deps)
-        ui.write(f"Created {humanize.join(paths)}.", styled=True)
+def collect(
+    repo: "Repo",
+    revs: Union[List[str], str, None] = None,
+    all_branches: bool = False,
+    all_tags: bool = False,
+    all_commits: bool = False,
+    num: int = 1,
+    hide_queued: bool = False,
+    hide_failed: bool = False,
+    sha_only: bool = False,
+    **kwargs,
+) -> List["ExpState"]:
+    """Collect baseline revisions and derived experiments."""
+    assert isinstance(repo.scm, Git)
+    if repo.scm.no_commits:
+        return []
+    if not any([revs, all_branches, all_tags, all_commits]):
+        revs = ["HEAD"]
+    if isinstance(revs, str):
+        revs = [revs]
+    cached_refs = list(repo.scm.iter_refs())
+    baseline_revs = list(
+        iter_revs(
+            repo.scm,
+            revs=revs,
+            num=num,
+            all_branches=all_branches,
+            all_tags=all_tags,
+            all_commits=all_commits,
+        )
+    )
+    if sha_only:
+        baseline_names: Dict[str, Optional[str]] = {}
+    else:
+        baseline_names = _describe(repo.scm, baseline_revs, refs=cached_refs)
 
-    # always create a file for params, detect file/folder based on extension
-    # for other dependencies
-    dirs = [dep.fs_path for dep in deps if not is_file(dep.fs_path)]
-    files = [dep.fs_path for dep in deps + params if is_file(dep.fs_path)]
-    for path in dirs:
-        localfs.makedirs(path)
-    for path in files:
-        localfs.makedirs(localfs.path.parent(path), exist_ok=True)
-        with localfs.open(path, "w", encoding="utf-8"):
-            pass
+    workspace_data = collect_rev(repo, "workspace", **kwargs)
+    result: List["ExpState"] = [workspace_data]
+    queued = collect_queued(repo, baseline_revs, **kwargs) if not hide_queued else {}
+    active = collect_active(repo, baseline_revs, **kwargs)
+    failed = collect_failed(repo, baseline_revs, **kwargs) if not hide_failed else {}
+    successful = collect_successful(repo, baseline_revs, **kwargs)
+
+    for baseline_rev in baseline_revs:
+        baseline_data = collect_rev(repo, baseline_rev)
+        experiments = list(
+            itertools.chain.from_iterable(
+                _sorted_ranges(collected.get(baseline_rev, []))
+                for collected in (active, successful, queued, failed)
+            )
+        )
+        result.append(
+            ExpState(
+                rev=baseline_rev,
+                name=baseline_names.get(baseline_rev),
+                data=baseline_data.data,
+                error=baseline_data.error,
+                experiments=experiments if experiments else None,
+            )
+        )
+    return result
 
 
-def init(
-    repo: "Repo",
-    name: str = "train",
-    type: str = "default",  # pylint: disable=redefined-builtin
-    defaults: Dict[str, str] = None,
-    overrides: Dict[str, str] = None,
-    interactive: bool = False,
-    force: bool = False,
-    stream: Optional[TextIO] = None,
-) -> PipelineStage:
-    from dvc.dvcfile import make_dvcfile
-
-    dvcfile = make_dvcfile(repo, "dvc.yaml")
-    _check_stage_exists(dvcfile, name, force=force)
-
-    defaults = defaults.copy() if defaults else {}
-    overrides = overrides.copy() if overrides else {}
-
-    with_live = type == "dl"
-
-    if interactive:
-        defaults = init_interactive(
-            name,
-            validator=partial(validate_prompts, repo),
-            defaults=defaults,
-            live=with_live,
-            provided=overrides,
-            stream=stream,
-        )
+def _describe(
+    scm: "Git",
+    revs: Iterable[str],
+    refs: Optional[Iterable[str]] = None,
+) -> Dict[str, Optional[str]]:
+    """Describe revisions using a tag, branch.
+
+    The first matching name will be returned for each rev. Names are preferred in this
+    order:
+        - current branch (if rev matches HEAD and HEAD is a branch)
+        - tags
+        - branches
+
+    Returns:
+        Dict mapping revisions from revs to a name.
+    """
+
+    head_rev = scm.get_rev()
+    head_ref = scm.get_ref("HEAD", follow=False)
+    if head_ref and head_ref.startswith("refs/heads/"):
+        head_branch = head_ref[len("refs/heads/") :]
     else:
-        if with_live:
-            # suppress `metrics`/`plots` if live is selected, unless
-            # it is also provided via overrides/cli.
-            # This makes output to be a checkpoint as well.
-            defaults.pop("metrics", None)
-            defaults.pop("plots", None)
-        else:
-            defaults.pop("live", None)  # suppress live otherwise
+        head_branch = None
 
-    context: Dict[str, str] = {**defaults, **overrides}
-    assert "cmd" in context
+    tags = {}
+    branches = {}
+    ref_it = iter(refs) if refs else scm.iter_refs()
+    for ref in ref_it:
+        is_tag = ref.startswith("refs/tags/")
+        is_branch = ref.startswith("refs/heads/")
+        if not (is_tag or is_branch):
+            continue
+        rev = scm.get_ref(ref)
+        if not rev:
+            logger.debug("unresolved ref %s", ref)
+            continue
+        if is_tag and rev not in tags:
+            tags[rev] = ref[len("refs/tags/") :]
+        if is_branch and rev not in branches:
+            branches[rev] = ref[len("refs/heads/") :]
+    names: Dict[str, Optional[str]] = {}
+    for rev in revs:
+        if rev == head_rev and head_branch:
+            names[rev] = head_branch
+        else:
+            names[rev] = tags.get(rev) or branches.get(rev)
+    return names
 
-    params_kv = []
-    params = context.get("params")
-    if params:
-        from dvc.dependency.param import (
-            MissingParamsFile,
-            ParamsDependency,
-            ParamsIsADirectoryError,
-        )
 
-        try:
-            params_d = ParamsDependency(None, params, repo=repo).read_file()
-        except (MissingParamsFile, ParamsIsADirectoryError) as exc:
-            raise DvcException(f"{exc}.")  # swallow cause for display
-        params_kv.append({params: list(params_d.keys())})
-
-    checkpoint_out = bool(context.get("live"))
-    models = context.get("models")
-    stage = repo.stage.create(
-        name=name,
-        cmd=context["cmd"],
-        deps=compact([context.get("code"), context.get("data")]),
-        params=params_kv,
-        metrics_no_cache=compact([context.get("metrics")]),
-        plots_no_cache=compact([context.get("plots")]),
-        live=context.get("live"),
-        force=force,
-        **{"checkpoints" if checkpoint_out else "outs": compact([models])},
-    )
+def _sorted_ranges(exp_ranges: Iterable["ExpRange"]) -> List["ExpRange"]:
+    """Return list of ExpRange sorted by timestamp."""
 
-    if interactive:
-        ui.error_write(Rule(style="green"), styled=True)
-        _yaml = dumps_yaml(to_pipeline_file(cast(PipelineStage, stage)))
-        syn = Syntax(_yaml, "yaml", theme="ansi_dark")
-        ui.error_write(syn, styled=True)
-
-    from dvc.ui.prompt import Confirm
-
-    if interactive and not Confirm.ask(
-        "Do you want to add the above contents to dvc.yaml?",
-        console=ui.error_console,
-        default=True,
-        stream=stream,
-    ):
-        raise DvcException("Aborting ...")
-
-    if interactive:
-        ui.error_write()  # add a newline after the prompts
-
-    with _disable_logging(), repo.scm_context(autostage=True, quiet=True):
-        stage.dump(update_lock=False)
-        stage.ignore_outs()
-        if not interactive:
-            label = "Using experiment project structure:"
-            display_workspace_tree(context, label)
-        init_deps(stage)
-        if params:
-            repo.scm_context.track_file(params)
+    def _head_timestamp(exp_range: "ExpRange") -> datetime:
+        head_exp = first(exp_range.revs)
+        if head_exp and head_exp.data and head_exp.data.timestamp:
+            return head_exp.data.timestamp
+        return datetime.fromtimestamp(0)
 
-    assert isinstance(stage, PipelineStage)
-    return stage
+    return sorted(exp_ranges, key=_head_timestamp, reverse=True)
```

### Comparing `dvc-2.9.5/dvc/repo/experiments/pull.py` & `dvc-3.0.0a0/dvc/repo/experiments/pull.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,87 +1,116 @@
 import logging
-from typing import Iterable, Union
+from typing import Iterable, List, Mapping, Optional, Set, Union
+
+from funcy import group_by
+from scmrepo.git.backend.base import SyncStatus
 
-from dvc.exceptions import DvcException
 from dvc.repo import locked
 from dvc.repo.scm_context import scm_context
-from dvc.scm import TqdmGit
+from dvc.scm import TqdmGit, iter_revs
+from dvc.ui import ui
 
 from .exceptions import UnresolvedExpNamesError
-from .utils import exp_commits, resolve_name
+from .refs import ExpRefInfo
+from .utils import exp_commits, exp_refs, exp_refs_by_baseline, resolve_name
 
 logger = logging.getLogger(__name__)
 
 
 @locked
 @scm_context
-def pull(
+def pull(  # noqa: C901
     repo,
     git_remote: str,
     exp_names: Union[Iterable[str], str],
-    *args,
+    all_commits=False,
+    rev: Optional[Union[List[str], str]] = None,
+    num=1,
     force: bool = False,
     pull_cache: bool = False,
     **kwargs,
-):
-    if isinstance(exp_names, str):
-        exp_names = [exp_names]
-    exp_ref_dict = resolve_name(repo.scm, exp_names, git_remote)
-    unresolved_exp_names = [
-        exp_name
-        for exp_name, exp_ref in exp_ref_dict.items()
-        if exp_ref is None
-    ]
-    if unresolved_exp_names:
-        raise UnresolvedExpNamesError(unresolved_exp_names)
+) -> Iterable[str]:
+    exp_ref_set: Set["ExpRefInfo"] = set()
+    if all_commits:
+        exp_ref_set.update(exp_refs(repo.scm, git_remote))
+    else:
+        if exp_names:
+            if isinstance(exp_names, str):
+                exp_names = [exp_names]
+            exp_ref_dict = resolve_name(repo.scm, exp_names, git_remote)
+
+            unresolved_exp_names = []
+            for exp_name, exp_ref in exp_ref_dict.items():
+                if exp_ref is None:
+                    unresolved_exp_names.append(exp_name)
+                else:
+                    exp_ref_set.add(exp_ref)
+
+            if unresolved_exp_names:
+                raise UnresolvedExpNamesError(unresolved_exp_names)
+
+        if rev:
+            if isinstance(rev, str):
+                rev = [rev]
+            rev_dict = iter_revs(repo.scm, rev, num)
+            rev_set = set(rev_dict.keys())
+            ref_info_dict = exp_refs_by_baseline(repo.scm, rev_set, git_remote)
+            for _, ref_info_list in ref_info_dict.items():
+                exp_ref_set.update(ref_info_list)
+
+    pull_result = _pull(repo, git_remote, exp_ref_set, force)
+
+    if pull_result[SyncStatus.DIVERGED]:
+        diverged_refs = [ref.name for ref in pull_result[SyncStatus.DIVERGED]]
+        ui.warn(
+            f"Local experiment '{diverged_refs}' has diverged from remote "
+            "experiment with the same name. To override the local experiment "
+            "re-run with '--force'."
+        )
 
-    exp_ref_set = exp_ref_dict.values()
-    _pull(repo, git_remote, exp_ref_set, force, pull_cache, **kwargs)
+    if pull_cache:
+        pull_cache_ref = (
+            pull_result[SyncStatus.UP_TO_DATE] + pull_result[SyncStatus.SUCCESS]
+        )
+        _pull_cache(repo, pull_cache_ref, **kwargs)
+
+    return [ref.name for ref in pull_result[SyncStatus.SUCCESS]]
 
 
 def _pull(
     repo,
     git_remote: str,
-    exp_refs,
+    refs: Iterable["ExpRefInfo"],
     force: bool,
-    pull_cache: bool,
-    **kwargs,
-):
-    def on_diverged(refname: str, rev: str) -> bool:
-        if repo.scm.get_ref(refname) == rev:
-            return True
-        exp_name = refname.split("/")[-1]
-        raise DvcException(
-            f"Local experiment '{exp_name}' has diverged from remote "
-            "experiment with the same name. To override the local experiment "
-            "re-run with '--force'."
-        )
-
-    refspec_list = [f"{exp_ref}:{exp_ref}" for exp_ref in exp_refs]
-    logger.debug(f"git pull experiment '{git_remote}' -> '{refspec_list}'")
+) -> Mapping[SyncStatus, List["ExpRefInfo"]]:
+    refspec_list = [f"{exp_ref}:{exp_ref}" for exp_ref in refs]
+    logger.debug("git pull experiment '%s' -> '%s'", git_remote, refspec_list)
 
     with TqdmGit(desc="Fetching git refs") as pbar:
-        repo.scm.fetch_refspecs(
+        results: Mapping[str, SyncStatus] = repo.scm.fetch_refspecs(
             git_remote,
             refspec_list,
             force=force,
-            on_diverged=on_diverged,
             progress=pbar.update_git,
         )
 
-    if pull_cache:
-        _pull_cache(repo, exp_refs, **kwargs)
+    def group_result(refspec):
+        return results[str(refspec)]
+
+    pull_result: Mapping[SyncStatus, List["ExpRefInfo"]] = group_by(group_result, refs)
+
+    return pull_result
 
 
 def _pull_cache(
     repo,
-    exp_ref,
+    refs: Union[ExpRefInfo, Iterable["ExpRefInfo"]],
     dvc_remote=None,
     jobs=None,
     run_cache=False,
     odb=None,
 ):
-    revs = list(exp_commits(repo.scm, exp_ref))
-    logger.debug(f"dvc fetch experiment '{exp_ref}'")
-    repo.fetch(
-        jobs=jobs, remote=dvc_remote, run_cache=run_cache, revs=revs, odb=odb
-    )
+    if isinstance(refs, ExpRefInfo):
+        refs = [refs]
+    revs = list(exp_commits(repo.scm, refs))
+    logger.debug("dvc fetch experiment '%s'", refs)
+    repo.fetch(jobs=jobs, remote=dvc_remote, run_cache=run_cache, revs=revs, odb=odb)
```

### Comparing `dvc-2.9.5/dvc/repo/experiments/push.py` & `dvc-3.0.0a0/tests/unit/repo/experiments/test_utils.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,81 +1,76 @@
-import logging
-from typing import Iterable, Union
+import pytest
 
-from dvc.exceptions import DvcException
-from dvc.repo import locked
-from dvc.repo.scm_context import scm_context
-from dvc.scm import TqdmGit
-
-from .exceptions import UnresolvedExpNamesError
-from .utils import exp_commits, push_refspec, resolve_name
-
-logger = logging.getLogger(__name__)
-
-
-@locked
-@scm_context
-def push(
-    repo,
-    git_remote: str,
-    exp_names: Union[Iterable[str], str],
-    *args,
-    force: bool = False,
-    push_cache: bool = False,
-    **kwargs,
-):
-    if isinstance(exp_names, str):
-        exp_names = [exp_names]
-
-    exp_ref_dict = resolve_name(repo.scm, exp_names)
-    unresolved_exp_names = [
-        exp_name
-        for exp_name, exp_ref in exp_ref_dict.items()
-        if exp_ref is None
-    ]
-    if unresolved_exp_names:
-        raise UnresolvedExpNamesError(unresolved_exp_names)
-
-    exp_ref_set = exp_ref_dict.values()
-    _push(repo, git_remote, exp_ref_set, force, push_cache, **kwargs)
-
-
-def _push(
-    repo,
-    git_remote: str,
-    exp_refs,
-    force: bool,
-    push_cache: bool,
-    **kwargs,
-):
-    def on_diverged(refname: str, rev: str) -> bool:
-        if repo.scm.get_ref(refname) == rev:
-            return True
-        exp_name = refname.split("/")[-1]
-        raise DvcException(
-            f"Local experiment '{exp_name}' has diverged from remote "
-            "experiment with the same name. To override the remote experiment "
-            "re-run with '--force'."
-        )
-
-    logger.debug(f"git push experiment '{exp_refs}' -> '{git_remote}'")
-
-    for exp_ref in exp_refs:
-        with TqdmGit(desc="Pushing git refs") as pbar:
-            push_refspec(
-                repo.scm,
-                git_remote,
-                str(exp_ref),
-                str(exp_ref),
-                force=force,
-                on_diverged=on_diverged,
-                progress=pbar.update_git,
-            )
-
-    if push_cache:
-        _push_cache(repo, exp_refs, **kwargs)
-
-
-def _push_cache(repo, exp_refs, dvc_remote=None, jobs=None, run_cache=False):
-    revs = list(exp_commits(repo.scm, exp_refs))
-    logger.debug(f"dvc push experiment '{exp_refs}'")
-    repo.push(jobs=jobs, remote=dvc_remote, run_cache=run_cache, revs=revs)
+from dvc.exceptions import InvalidArgumentError
+from dvc.repo.experiments.refs import EXPS_NAMESPACE, ExpRefInfo
+from dvc.repo.experiments.utils import check_ref_format, resolve_name, to_studio_params
+
+
+def commit_exp_ref(tmp_dir, scm, file="foo", contents="foo", name="foo"):
+    tmp_dir.scm_gen(file, contents, commit="init")
+    rev = scm.get_rev()
+    ref = "/".join([EXPS_NAMESPACE, "ab", "c123", name])
+    scm.gitpython.set_ref(ref, rev)
+    return ref, rev
+
+
+@pytest.mark.parametrize("use_url", [True, False])
+@pytest.mark.parametrize("name_only", [True, False])
+def test_resolve_exp_ref(tmp_dir, scm, git_upstream, name_only, use_url):
+    ref, _ = commit_exp_ref(tmp_dir, scm)
+    name = "foo" if name_only else ref
+    result = resolve_name(scm, [name, "notexist"])
+    assert isinstance(result[name], ExpRefInfo)
+    assert str(result[name]) == ref
+    assert result["notexist"] is None
+
+    scm.push_refspecs(git_upstream.url, f"{ref}:{ref}")
+    remote = git_upstream.url if use_url else git_upstream.remote
+    name = "foo" if name_only else ref
+    remote_ref_info = resolve_name(scm, [name], remote)[name]
+    assert isinstance(remote_ref_info, ExpRefInfo)
+    assert str(remote_ref_info) == ref
+
+
+@pytest.mark.parametrize(
+    "name,result",
+    [
+        ("name", True),
+        ("group/name", False),
+        ("na me", False),
+        ("invalid/.name", False),
+        ("@", pytest.param(False, marks=pytest.mark.xfail)),
+        (":", False),
+        ("^", False),
+        ("*", False),
+        ("~", False),
+        ("?", False),
+    ],
+)
+def test_run_check_ref_format(scm, name, result):
+    ref = ExpRefInfo("abc123", name)
+    if result:
+        check_ref_format(scm, ref)
+    else:
+        with pytest.raises(InvalidArgumentError):
+            check_ref_format(scm, ref)
+
+
+@pytest.mark.parametrize(
+    "params,expected",
+    [
+        (
+            {"workspace": {"data": {"params.yaml": {"data": {"foo": 1}}}}},
+            {"params.yaml": {"foo": 1}},
+        ),
+        (
+            {"workspace": {"data": {"params.yaml": {"error": "FileNotFound"}}}},
+            {"params.yaml": {}},
+        ),
+        (
+            {"workspace": {"error": "something went wrong"}},
+            {},
+        ),
+    ],
+)
+def test_to_studio_params(params, expected):
+    assert to_studio_params(params) == expected
```

### Comparing `dvc-2.9.5/dvc/repo/experiments/show.py` & `dvc-3.0.0a0/dvc/repo/experiments/queue/tempdir.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,203 +1,204 @@
 import logging
-from collections import OrderedDict, defaultdict
-from datetime import datetime
-from typing import Any, Callable, Dict, List, Optional, Union
-
-from dvc.repo import Repo, locked  # pylint: disable=unused-import
-from dvc.repo.experiments.base import ExpRefInfo
-from dvc.repo.metrics.show import _gather_metrics
-from dvc.repo.params.show import _gather_params
-from dvc.scm import iter_revs
-from dvc.utils import error_handler, onerror_collect
+import os
+from collections import defaultdict
+from typing import TYPE_CHECKING, Collection, Dict, Generator, List, Optional
+
+from funcy import first
+
+from dvc.exceptions import DvcException
+from dvc.repo.experiments.exceptions import ExpQueueEmptyError
+from dvc.repo.experiments.executor.base import ExecutorInfo, TaskStatus
+from dvc.repo.experiments.executor.local import TempDirExecutor
+from dvc.repo.experiments.utils import EXEC_PID_DIR, EXEC_TMP_DIR
+from dvc.utils.objects import cached_property
+
+from .base import BaseStashQueue, QueueEntry, QueueGetResult
+from .utils import fetch_running_exp_from_temp_dir
+from .workspace import WorkspaceQueue
+
+if TYPE_CHECKING:
+    from dvc.repo.experiments import Experiments
+    from dvc.repo.experiments.executor.base import BaseExecutor, ExecutorResult
+    from dvc.repo.experiments.serialize import ExpRange
+    from dvc_task.proc.manager import ProcessManager
 
 logger = logging.getLogger(__name__)
 
 
-@error_handler
-def _collect_experiment_commit(
-    repo,
-    exp_rev,
-    stash=False,
-    sha_only=True,
-    param_deps=False,
-    running=None,
-    onerror: Optional[Callable] = None,
-):
-    from dvc.dependency import ParamsDependency, RepoDependency
-
-    res: Dict[str, Optional[Any]] = defaultdict(dict)
-    for rev in repo.brancher(revs=[exp_rev]):
-        if rev == "workspace":
-            if exp_rev != "workspace":
-                continue
-            res["timestamp"] = None
-        else:
-            commit = repo.scm.resolve_commit(rev)
-            res["timestamp"] = datetime.fromtimestamp(commit.commit_time)
+_STANDALONE_TMP_DIR = os.path.join(EXEC_TMP_DIR, "standalone")
 
-        params = _gather_params(
-            repo, rev=rev, targets=None, deps=param_deps, onerror=onerror
-        )
-        if params:
-            res["params"] = params
 
-        res["deps"] = {
-            dep.def_path: {
-                "hash": dep.hash_info.value,
-                "size": dep.meta.size,
-                "nfiles": dep.meta.nfiles,
-            }
-            for dep in repo.index.deps
-            if not isinstance(dep, (ParamsDependency, RepoDependency))
-        }
-
-        res["outs"] = {
-            out.def_path: {
-                "hash": out.hash_info.value,
-                "size": out.meta.size,
-                "nfiles": out.meta.nfiles,
-            }
-            for out in repo.index.outs
-            if not (out.is_metric or out.is_plot)
-        }
-
-        res["queued"] = stash
-        if running is not None and exp_rev in running:
-            res["running"] = True
-            res["executor"] = running[exp_rev].get("location")
-        else:
-            res["running"] = False
-            res["executor"] = None
-        if not stash:
-            vals = _gather_metrics(
-                repo, targets=None, rev=rev, recursive=False, onerror=onerror
-            )
-            res["metrics"] = vals
+class TempDirQueue(WorkspaceQueue):
+    """Standalone/tempdir exp queue implementation."""
+
+    _EXEC_NAME: Optional[str] = None
 
-        if not sha_only and rev != "workspace":
-            for refspec in ["refs/tags", "refs/heads"]:
-                name = repo.scm.describe(rev, base=refspec)
-                if name:
-                    break
-            if not name:
-                name = repo.experiments.get_exact_name(rev)
-            if name:
-                name = name.rsplit("/")[-1]
-                res["name"] = name
-
-    return res
-
-
-def _collect_experiment_branch(
-    res, repo, branch, baseline, onerror: Optional[Callable] = None, **kwargs
-):
-    from dvc.scm import resolve_rev
-
-    exp_rev = resolve_rev(repo.scm, branch)
-    prev = None
-    revs = list(repo.scm.branch_revs(exp_rev, baseline))
-    for rev in revs:
-        collected_exp = _collect_experiment_commit(
-            repo, rev, onerror=onerror, **kwargs
+    @cached_property
+    def _standalone_tmp_dir(self) -> str:
+        assert self.repo.tmp_dir is not None
+        return os.path.join(self.repo.tmp_dir, _STANDALONE_TMP_DIR)
+
+    @cached_property
+    def pid_dir(self) -> str:
+        return os.path.join(self._standalone_tmp_dir, EXEC_PID_DIR)
+
+    @cached_property
+    def proc(self) -> "ProcessManager":
+        from dvc_task.proc.manager import ProcessManager
+
+        return ProcessManager(self.pid_dir)
+
+    def get(self) -> QueueGetResult:
+        revs = self.stash.stash_revs
+        if not revs:
+            raise ExpQueueEmptyError("No stashed standalone experiments.")
+        stash_rev, stash_entry = first(revs.items())
+        entry = QueueEntry(
+            self.repo.root_dir,
+            self.scm.root_dir,
+            self.ref,
+            stash_rev,
+            stash_entry.baseline_rev,
+            stash_entry.branch,
+            stash_entry.name,
+            stash_entry.head_rev,
         )
-        if len(revs) > 1:
-            exp = {"checkpoint_tip": exp_rev}
-            if prev:
-                res[prev]["data"][  # type: ignore[unreachable]
-                    "checkpoint_parent"
-                ] = rev
-            if rev in res:
-                res[rev]["data"].update(exp)
-                res.move_to_end(rev)
-            else:
-                exp.update(collected_exp["data"])
-        else:
-            exp = collected_exp["data"]
-        if rev not in res:
-            res[rev] = {"data": exp}
-        prev = rev
-    if len(revs) > 1:
-        res[prev]["data"]["checkpoint_parent"] = baseline
-    return res
-
-
-@locked
-def show(
-    repo: "Repo",
-    all_branches=False,
-    all_tags=False,
-    revs: Union[List[str], str, None] = None,
-    all_commits=False,
-    sha_only=False,
-    num=1,
-    param_deps=False,
-    onerror: Optional[Callable] = None,
-):
-
-    if onerror is None:
-        onerror = onerror_collect
-
-    res: Dict[str, Dict] = defaultdict(OrderedDict)
-
-    if not any([revs, all_branches, all_tags, all_commits]):
-        revs = ["HEAD"]
-    if isinstance(revs, str):
-        revs = [revs]
-
-    found_revs: Dict[str, List[str]] = {"workspace": []}
-    found_revs.update(
-        iter_revs(repo.scm, revs, num, all_branches, all_tags, all_commits)
-    )
-
-    running = repo.experiments.get_running_exps()
-
-    for rev in found_revs:
-        res[rev]["baseline"] = _collect_experiment_commit(
-            repo,
-            rev,
-            sha_only=sha_only,
-            param_deps=param_deps,
-            running=running,
-            onerror=onerror,
+        executor = self.init_executor(
+            self.repo.experiments,
+            entry,
+            TempDirExecutor,
+            wdir=self._standalone_tmp_dir,
         )
+        return QueueGetResult(entry, executor)
 
-        if rev == "workspace":
-            continue
+    def iter_active(self) -> Generator[QueueEntry, None, None]:
+        # NOTE: Yielded queue entries are not complete for performance reasons.
+        # Retrieving exec ref information is unavailable without doing a
+        # git-fetch, and is unneeded in the common use cases for iter_active.
+        for stash_rev in self.proc:
+            infofile = self.get_infofile_path(stash_rev)
+            executor_info = ExecutorInfo.load_json(infofile)
+            if executor_info.status <= TaskStatus.SUCCESS and os.path.exists(
+                executor_info.root_dir
+            ):
+                yield QueueEntry(
+                    self.repo.root_dir,
+                    self.scm.root_dir,
+                    self.ref,
+                    stash_rev,
+                    executor_info.baseline_rev,
+                    None,  # branch unavailable without doing a git-fetch
+                    executor_info.name,
+                    None,
+                )
+
+    def _reproduce_entry(
+        self,
+        entry: QueueEntry,
+        executor: "BaseExecutor",
+        copy_paths: Optional[List[str]] = None,
+        message: Optional[str] = None,
+        **kwargs,
+    ) -> Dict[str, Dict[str, str]]:
+        from dvc.stage.monitor import CheckpointKilledError
+
+        results: Dict[str, Dict[str, str]] = defaultdict(dict)
+        exec_name = self._EXEC_NAME or entry.stash_rev
+        infofile = self.get_infofile_path(exec_name)
+        try:
+            rev = entry.stash_rev
+            exec_result = executor.reproduce(
+                info=executor.info,
+                rev=rev,
+                infofile=infofile,
+                log_level=logger.getEffectiveLevel(),
+                log_errors=True,
+                copy_paths=copy_paths,
+                message=message,
+            )
+            if not exec_result.exp_hash:
+                raise DvcException(f"Failed to reproduce experiment '{rev[:7]}'")
+            if exec_result.ref_info:
+                results[rev].update(
+                    self.collect_executor(self.repo.experiments, executor, exec_result)
+                )
+        except CheckpointKilledError:
+            results[rev].update(
+                self.collect_executor(self.repo.experiments, executor, exec_result)
+            )
 
-        ref_info = ExpRefInfo(baseline_sha=rev)
-        commits = [
-            (ref, repo.scm.resolve_commit(ref))
-            for ref in repo.scm.iter_refs(base=str(ref_info))
-        ]
-        for exp_ref, _ in sorted(
-            commits, key=lambda x: x[1].commit_time, reverse=True
-        ):
-            ref_info = ExpRefInfo.from_ref(exp_ref)
-            assert ref_info.baseline_sha == rev
-            _collect_experiment_branch(
-                res[rev],
-                repo,
-                exp_ref,
-                rev,
-                sha_only=sha_only,
-                param_deps=param_deps,
-                running=running,
-                onerror=onerror,
+            return results
+        except DvcException:
+            raise
+        except Exception as exc:  # noqa: BLE001
+            raise DvcException(f"Failed to reproduce experiment '{rev[:7]}'") from exc
+        finally:
+            executor.cleanup(infofile)
+        return results
+
+    @staticmethod
+    def collect_executor(
+        exp: "Experiments",
+        executor: "BaseExecutor",
+        exec_result: "ExecutorResult",
+    ) -> Dict[str, str]:
+        return BaseStashQueue.collect_executor(exp, executor, exec_result)
+
+    def get_running_exps(self, fetch_refs: bool = True) -> Dict[str, Dict]:
+        result: Dict[str, Dict] = {}
+        for entry in self.iter_active():
+            result.update(
+                fetch_running_exp_from_temp_dir(self, entry.stash_rev, fetch_refs)
+            )
+        return result
+
+    def collect_active_data(
+        self,
+        baseline_revs: Optional[Collection[str]],
+        fetch_refs: bool = False,
+        **kwargs,
+    ) -> Dict[str, List["ExpRange"]]:
+        from dvc.repo import Repo
+        from dvc.repo.experiments.collect import collect_exec_branch
+        from dvc.repo.experiments.serialize import (
+            ExpExecutor,
+            ExpRange,
+            LocalExpExecutor,
+        )
+
+        result: Dict[str, List[ExpRange]] = defaultdict(list)
+        for entry in self.iter_active():
+            if baseline_revs and entry.baseline_rev not in baseline_revs:
+                continue
+            if fetch_refs:
+                fetch_running_exp_from_temp_dir(self, entry.stash_rev, fetch_refs)
+            proc_info = self.proc.get(entry.stash_rev)
+            infofile = self.get_infofile_path(entry.stash_rev)
+            executor_info = ExecutorInfo.load_json(infofile)
+            if proc_info:
+                local_exec: Optional[LocalExpExecutor] = LocalExpExecutor(
+                    root=executor_info.root_dir,
+                    log=proc_info.stdout,
+                    pid=proc_info.pid,
+                )
+            else:
+                local_exec = None
+            dvc_root = os.path.join(executor_info.root_dir, executor_info.dvc_dir)
+            with Repo(dvc_root) as repo:
+                exps = list(
+                    collect_exec_branch(repo, executor_info.baseline_rev, **kwargs)
+                )
+            exps[0].rev = entry.stash_rev
+            exps[0].name = entry.name
+            result[entry.baseline_rev].append(
+                ExpRange(
+                    exps,
+                    executor=ExpExecutor(
+                        "running",
+                        name=executor_info.location,
+                        local=local_exec,
+                    ),
+                    name=entry.name,
+                )
             )
-        # collect queued (not yet reproduced) experiments
-        for stash_rev, entry in repo.experiments.stash_revs.items():
-            if entry.baseline_rev in found_revs:
-                if stash_rev not in running or not running[stash_rev].get(
-                    "last"
-                ):
-                    experiment = _collect_experiment_commit(
-                        repo,
-                        stash_rev,
-                        sha_only=sha_only,
-                        stash=stash_rev not in running,
-                        param_deps=param_deps,
-                        running=running,
-                        onerror=onerror,
-                    )
-                    res[entry.baseline_rev][stash_rev] = experiment
-    return res
+        return result
```

### Comparing `dvc-2.9.5/dvc/repo/graph.py` & `dvc-3.0.0a0/dvc/repo/graph.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from typing import TYPE_CHECKING, Iterator, List, Set
 
-from dvc.fs.local import localfs
+from dvc.fs import localfs
 from dvc.utils.fs import path_isin
 
 if TYPE_CHECKING:
     from networkx import DiGraph
 
     from dvc.stage import Stage
 
@@ -29,18 +29,18 @@
 
 def get_pipeline(pipelines, node):
     found = [i for i in pipelines if i.has_node(node)]
     assert len(found) == 1
     return found[0]
 
 
-def get_pipelines(G):
+def get_pipelines(graph: "DiGraph"):
     import networkx as nx
 
-    return [G.subgraph(c).copy() for c in nx.weakly_connected_components(G)]
+    return [graph.subgraph(c).copy() for c in nx.weakly_connected_components(graph)]
 
 
 def collect_pipeline(stage: "Stage", graph: "DiGraph") -> Iterator["Stage"]:
     import networkx as nx
 
     pipeline = get_pipeline(get_pipelines(graph), stage)
     return nx.dfs_postorder_nodes(pipeline, stage)
@@ -93,57 +93,57 @@
     """
     import networkx as nx
 
     from dvc.exceptions import StagePathAsOutputError
 
     from .trie import build_outs_trie
 
-    G = nx.DiGraph()
+    graph = nx.DiGraph()
 
     # Use trie to efficiently find overlapping outs and deps
     outs_trie = outs_trie or build_outs_trie(stages)
 
     for stage in stages:
         out = outs_trie.shortest_prefix(localfs.path.parts(stage.path)).value
         if out:
             raise StagePathAsOutputError(stage, str(out))
 
     # Building graph
-    G.add_nodes_from(stages)
+    graph.add_nodes_from(stages)
     for stage in stages:
-        for dep in stage.deps:
-            if dep.fs_path is None:
-                continue
+        if stage.is_repo_import:
+            continue
 
+        for dep in stage.deps:
             dep_key = dep.fs.path.parts(dep.fs_path)
             overlapping = [n.value for n in outs_trie.prefixes(dep_key)]
             if outs_trie.has_subtrie(dep_key):
                 overlapping.extend(outs_trie.values(prefix=dep_key))
 
-            G.add_edges_from((stage, out.stage) for out in overlapping)
-    check_acyclic(G)
+            graph.add_edges_from((stage, out.stage) for out in overlapping)
+    check_acyclic(graph)
 
-    return G
+    return graph
 
 
 # NOTE: using stage graph instead of just list of stages to make sure that it
 # has already passed all the sanity checks like cycles/overlapping outputs and
 # so on.
 def build_outs_graph(graph, outs_trie):
     import networkx as nx
 
-    G = nx.DiGraph()
+    outs_graph = nx.DiGraph()
 
-    G.add_nodes_from(outs_trie.values())
+    outs_graph.add_nodes_from(outs_trie.values())
     for stage in graph.nodes():
         for dep in stage.deps:
             if dep.fs_path is None:
                 # RepoDependency don't have a path
                 continue
             dep_key = dep.fs.path.parts(dep.fs_path)
             overlapping = [n.value for n in outs_trie.prefixes(dep_key)]
             if outs_trie.has_subtrie(dep_key):
                 overlapping.extend(outs_trie.values(prefix=dep_key))
 
             for from_out in stage.outs:
-                G.add_edges_from((from_out, out) for out in overlapping)
-    return G
+                outs_graph.add_edges_from((from_out, out) for out in overlapping)
+    return outs_graph
```

### Comparing `dvc-2.9.5/dvc/repo/init.py` & `dvc-3.0.0a0/dvc/repo/init.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 from dvc.scm import SCM, SCMError
 from dvc.utils import relpath
 from dvc.utils.fs import remove
 
 logger = logging.getLogger(__name__)
 
 
-def init(root_dir=os.curdir, no_scm=False, force=False, subdir=False):
+def init(root_dir=os.curdir, no_scm=False, force=False, subdir=False):  # noqa: C901
     """
     Creates an empty repo on the given directory -- basically a
     `.dvc` directory with subdirectories for configuration and cache.
 
     It should be tracked by a SCM or use the `--no-scm` flag.
 
     If the given directory is not empty, you must use the `--force`
@@ -39,15 +39,15 @@
 
     root_dir = os.path.realpath(root_dir)
     dvc_dir = os.path.join(root_dir, Repo.DVC_DIR)
 
     try:
         scm = SCM(root_dir, search_parent_directories=subdir, no_scm=no_scm)
     except SCMError:
-        raise InitError(
+        raise InitError(  # noqa: B904
             f"{root_dir} is not tracked by any supported SCM tool (e.g. Git). "
             "Use `--no-scm` if you don't want to use any SCM or "
             "`--subdir` if initializing inside a subdirectory of a parent SCM "
             "repository."
         )
 
     if scm.is_ignored(dvc_dir):
@@ -71,14 +71,22 @@
         with config.edit() as conf:
             conf["core"]["no_scm"] = True
 
     dvcignore = init_dvcignore(root_dir)
 
     proj = Repo(root_dir)
 
+    if os.path.isdir(proj.site_cache_dir):
+        proj.close()
+        try:
+            remove(proj.site_cache_dir)
+        except OSError:
+            logger.debug("failed to remove %s", dvc_dir, exc_info=True)
+        proj = Repo(root_dir)
+
     with proj.scm_context(autostage=True) as context:
         files = [
             config.files["repo"],
             dvcignore,
         ]
         ignore_file = context.scm.ignore_file
         if ignore_file:
```

### Comparing `dvc-2.9.5/dvc/repo/install.py` & `dvc-3.0.0a0/dvc/repo/install.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,40 +1,42 @@
 from typing import TYPE_CHECKING
 
 from dvc.exceptions import DvcException
 
 if TYPE_CHECKING:
-    from scmrepo.git import Git
-
     from dvc.repo import Repo
+    from dvc.scm import Git
 
 
 def pre_commit_install(scm: "Git") -> None:
     import os
 
     from dvc.utils.serialize import modify_yaml
 
     config_path = os.path.join(scm.root_dir, ".pre-commit-config.yaml")
     with modify_yaml(config_path) as config:
         entry = {
             "repo": "https://github.com/iterative/dvc",
-            "rev": "master",
+            "rev": "main",
             "hooks": [
                 {
                     "id": "dvc-pre-commit",
+                    "additional_dependencies": [".[all]"],
                     "language_version": "python3",
                     "stages": ["commit"],
                 },
                 {
                     "id": "dvc-pre-push",
+                    "additional_dependencies": [".[all]"],
                     "language_version": "python3",
                     "stages": ["push"],
                 },
                 {
                     "id": "dvc-post-checkout",
+                    "additional_dependencies": [".[all]"],
                     "language_version": "python3",
                     "stages": ["post-checkout"],
                     "always_run": True,
                 },
             ],
         }
 
@@ -50,27 +52,29 @@
 
     hooks = ["post-checkout", "pre-commit", "pre-push"]
     for hook in hooks:
         try:
             scm.verify_hook(hook)
         except GitHookAlreadyExists as exc:
             link = format_link("https://man.dvc.org/install")
-            raise DvcException(f"{exc}. Please refer to {link} for more info.")
+            raise DvcException(  # noqa: B904
+                f"{exc}. Please refer to {link} for more info."
+            )
 
     for hook in hooks:
         scm.install_hook(hook, f"exec dvc git-hook {hook} $@")
 
 
 def install(self: "Repo", use_pre_commit_tool: bool = False) -> None:
     """Adds dvc commands to SCM hooks for the repo.
 
     If use_pre_commit_tool is set and pre-commit is installed it will be used
     to install the hooks.
     """
-    from scmrepo.git import Git
+    from dvc.scm import Git
 
     scm = self.scm
     if not isinstance(scm, Git):
         return
 
     driver = "dvc git-hook merge-driver --ancestor %O --our %A --their %B "
     scm.install_merge_driver("dvc", "DVC merge driver", driver)
```

### Comparing `dvc-2.9.5/dvc/repo/ls.py` & `dvc-3.0.0a0/dvc/repo/ls.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,14 +1,23 @@
 import os
-from itertools import chain
+from typing import TYPE_CHECKING, Optional
 
-from dvc.exceptions import PathMissingError
+if TYPE_CHECKING:
+    from dvc.fs.dvc import DVCFileSystem
+
+    from . import Repo
 
 
-def ls(url, path=None, rev=None, recursive=None, dvc_only=False):
+def ls(
+    url: str,
+    path: Optional[str] = None,
+    rev: Optional[str] = None,
+    recursive: Optional[bool] = None,
+    dvc_only: bool = False,
+):
     """Methods for getting files and outputs for the repo.
 
     Args:
         url (str): the repo url
         path (str, optional): relative path into the repo
         rev (str, optional): SHA commit, branch or tag name
         recursive (bool, optional): recursively walk the repo
@@ -25,56 +34,60 @@
             "isdir": bool,
             "isexec": bool,
         }
     """
     from . import Repo
 
     with Repo.open(url, rev=rev, subrepos=True, uninitialized=True) as repo:
-        fs_path = repo.root_dir
-        if path:
-            fs_path = os.path.abspath(repo.fs.path.join(fs_path, path))
-
-        ret = _ls(repo, fs_path, recursive, dvc_only)
+        path = path or ""
 
-        if path and not ret:
-            raise PathMissingError(path, repo, dvc_only=dvc_only)
+        ret = _ls(repo, path, recursive, dvc_only)
 
         ret_list = []
         for path, info in ret.items():
             info["path"] = path
             ret_list.append(info)
         ret_list.sort(key=lambda f: f["path"])
         return ret_list
 
 
-def _ls(repo, fs_path, recursive=None, dvc_only=False):
-    fs = repo.repo_fs
-    infos = []
-    for root, dirs, files in fs.walk(fs_path, dvcfiles=True):
-        entries = chain(files, dirs) if not recursive else files
-        infos.extend(fs.path.join(root, entry) for entry in entries)
+def _ls(
+    repo: "Repo",
+    path: str,
+    recursive: Optional[bool] = None,
+    dvc_only: bool = False,
+):
+    fs: "DVCFileSystem" = repo.dvcfs
+    fs_path = fs.from_os_path(path)
+
+    fs_path = fs.info(fs_path)["name"]
+
+    infos = {}
+    for root, dirs, files in fs.walk(
+        fs_path, dvcfiles=True, dvc_only=dvc_only, detail=True
+    ):
+        if not recursive:
+            files.update(dirs)
+
+        parts = fs.path.relparts(root, fs_path)
+        if parts == (".",):
+            parts = ()
+
+        for name, entry in files.items():
+            infos[os.path.join(*parts, name)] = entry
+
         if not recursive:
             break
 
     if not infos and fs.isfile(fs_path):
-        infos.append(fs_path)
+        infos[os.path.basename(path)] = fs.info(fs_path)
 
     ret = {}
-    for info in infos:
-        try:
-            _info = fs.info(info)
-        except FileNotFoundError:
-            # broken symlink
-            _info = {"type": "file", "isexec": False}
-
-        if _info.get("outs") or not dvc_only:
-            path = (
-                fs.path.name(fs_path)
-                if fs_path == info
-                else fs.path.relpath(info, fs_path)
-            )
-            ret[path] = {
-                "isout": _info.get("isout", False),
-                "isdir": _info["type"] == "directory",
-                "isexec": _info["isexec"],
-            }
+    for name, info in infos.items():
+        dvc_info = info.get("dvc_info", {})
+        ret[name] = {
+            "isout": dvc_info.get("isout", False),
+            "isdir": info["type"] == "directory",
+            "isexec": info.get("isexec", False),
+        }
+
     return ret
```

### Comparing `dvc-2.9.5/dvc/repo/metrics/diff.py` & `dvc-3.0.0a0/dvc/repo/metrics/diff.py`

 * *Files 24% similar despite different names*

```diff
@@ -7,14 +7,15 @@
         return {}
 
     with_unchanged = kwargs.pop("all", False)
 
     a_rev = a_rev or "HEAD"
     b_rev = b_rev or "workspace"
 
-    metrics = repo.metrics.show(*args, **kwargs, revs=[a_rev, b_rev])
+    metrics = repo.metrics.show(
+        *args, **kwargs, revs=[a_rev, b_rev], hide_workspace=False
+    )
+
     old = metrics.get(a_rev, {}).get("data", {})
     new = metrics.get(b_rev, {}).get("data", {})
 
-    return _diff(
-        format_dict(old), format_dict(new), with_unchanged=with_unchanged
-    )
+    return _diff(format_dict(old), format_dict(new), with_unchanged=with_unchanged)
```

### Comparing `dvc-2.9.5/dvc/repo/metrics/show.py` & `dvc-3.0.0a0/dvc/repo/metrics/show.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,137 +1,156 @@
 import logging
 import os
-from typing import List
+from typing import TYPE_CHECKING, List
 
-from dvc.fs.repo import RepoFileSystem
-from dvc.output import Output
+from scmrepo.exceptions import SCMError
+
+from dvc.fs.dvc import DVCFileSystem
 from dvc.repo import locked
-from dvc.repo.collect import StrPaths, collect
-from dvc.repo.live import summary_fs_path
+from dvc.repo.collect import collect
 from dvc.scm import NoSCMError
-from dvc.utils import error_handler, errored_revisions, onerror_collect
-from dvc.utils.serialize import load_yaml
+from dvc.utils import as_posix, error_handler, errored_revisions, onerror_collect
+from dvc.utils.collections import ensure_list
+from dvc.utils.serialize import load_path
+
+if TYPE_CHECKING:
+    from dvc.output import Output
 
 logger = logging.getLogger(__name__)
 
 
-def _is_metric(out: Output) -> bool:
-    return bool(out.metric) or bool(out.live)
+def _is_metric(out: "Output") -> bool:
+    return bool(out.metric)
 
 
-def _to_fs_paths(metrics: List[Output]) -> StrPaths:
+def _to_fs_paths(metrics: List["Output"]) -> List["str"]:
     result = []
     for out in metrics:
         if out.metric:
-            result.append(out.fs_path)
-        elif out.live:
-            fs_path = summary_fs_path(out)
-            if fs_path:
-                result.append(fs_path)
+            result.append(out.repo.dvcfs.from_os_path(out.fs_path))
     return result
 
 
-def _collect_metrics(repo, targets, revision, recursive):
+def _collect_top_level_metrics(repo):
+    top_metrics = repo.index._metrics  # pylint: disable=protected-access
+    for dvcfile, metrics in top_metrics.items():
+        wdir = repo.fs.path.relpath(repo.fs.path.parent(dvcfile), repo.root_dir)
+        for file in metrics:
+            path = repo.fs.path.join(wdir, as_posix(file))
+            yield repo.fs.path.normpath(path)
+
+
+def _collect_metrics(repo, targets, recursive):
     metrics, fs_paths = collect(
-        repo,
-        targets=targets,
-        output_filter=_is_metric,
-        recursive=recursive,
-        rev=revision,
+        repo, targets=targets, output_filter=_is_metric, recursive=recursive
     )
     return _to_fs_paths(metrics) + list(fs_paths)
 
 
 def _extract_metrics(metrics, path, rev):
-    if isinstance(metrics, (int, float)):
+    if isinstance(metrics, (int, float, str)):
         return metrics
 
     if not isinstance(metrics, dict):
         return None
 
     ret = {}
     for key, val in metrics.items():
         m = _extract_metrics(val, path, rev)
         if m not in (None, {}):
             ret[key] = m
         else:
             logger.debug(
-                "Could not parse '%s' metric from '%s' at '%s' "
-                "due to its unsupported type: '%s'",
+                (
+                    "Could not parse '%s' metric from '%s' at '%s' "
+                    "due to its unsupported type: '%s'"
+                ),
                 key,
                 path,
                 rev,
                 type(val).__name__,
             )
 
     return ret
 
 
 @error_handler
 def _read_metric(path, fs, rev, **kwargs):
-    val = load_yaml(path, fs=fs)
+    val = load_path(path, fs)
     val = _extract_metrics(val, path, rev)
     return val or {}
 
 
 def _read_metrics(repo, metrics, rev, onerror=None):
-    fs = RepoFileSystem(repo)
+    fs = DVCFileSystem(repo=repo)
+
+    relpath = ""
+    if repo.root_dir != repo.fs.path.getcwd():
+        relpath = repo.fs.path.relpath(repo.root_dir, repo.fs.path.getcwd())
 
     res = {}
     for metric in metrics:
+        rel_metric_path = os.path.join(relpath, *fs.path.parts(metric))
         if not fs.isfile(metric):
-            continue
+            if fs.isfile(rel_metric_path):
+                metric = rel_metric_path
+            else:
+                continue
 
-        res[fs.path.relpath(metric, os.getcwd())] = _read_metric(
-            metric, fs, rev, onerror=onerror
-        )
+        res[rel_metric_path] = _read_metric(metric, fs, rev, onerror=onerror)
 
     return res
 
 
 def _gather_metrics(repo, targets, rev, recursive, onerror=None):
-    metrics = _collect_metrics(repo, targets, rev, recursive)
+    metrics = _collect_metrics(repo, targets, recursive)
+    metrics.extend(_collect_top_level_metrics(repo))
     return _read_metrics(repo, metrics, rev, onerror=onerror)
 
 
 @locked
 def show(
     repo,
     targets=None,
     all_branches=False,
     all_tags=False,
     recursive=False,
     revs=None,
     all_commits=False,
     onerror=None,
+    hide_workspace=True,
 ):
     if onerror is None:
         onerror = onerror_collect
 
+    targets = ensure_list(targets)
+    targets = [repo.dvcfs.from_os_path(target) for target in targets]
+
     res = {}
     for rev in repo.brancher(
         revs=revs,
         all_branches=all_branches,
         all_tags=all_tags,
         all_commits=all_commits,
     ):
         res[rev] = error_handler(_gather_metrics)(
             repo, targets, rev, recursive, onerror=onerror
         )
 
-    # Hide workspace metrics if they are the same as in the active branch
-    try:
-        active_branch = repo.scm.active_branch()
-    except (TypeError, NoSCMError):
-        # TypeError - detached head
-        # NoSCMError - no repo case
-        pass
-    else:
-        if res.get("workspace") == res.get(active_branch):
-            res.pop("workspace", None)
+    if hide_workspace:
+        # Hide workspace metrics if they are the same as in the active branch
+        try:
+            active_branch = repo.scm.active_branch()
+        except (SCMError, NoSCMError):
+            # SCMError - detached head
+            # NoSCMError - no repo case
+            pass
+        else:
+            if res.get("workspace") == res.get(active_branch):
+                res.pop("workspace", None)
 
     errored = errored_revisions(res)
     if errored:
         from dvc.ui import ui
 
         ui.error_write(
             "DVC failed to load some metrics for following revisions:"
```

### Comparing `dvc-2.9.5/dvc/repo/move.py` & `dvc-3.0.0a0/dvc/repo/move.py`

 * *Files 4% similar despite different names*

```diff
@@ -28,21 +28,20 @@
           $ dvc move hello greetings
 
           Result: (greeting, greeting.dvc)
 
     It only works with outputs generated by `add` or `import`,
     also known as data sources.
     """
-    import dvc.output as Output
+    from dvc import output
+    from dvc.dvcfile import DVC_FILE_SUFFIX
     from dvc.stage import Stage
 
-    from ..dvcfile import DVC_FILE_SUFFIX, Dvcfile
-
-    from_out = Output.loads_from(Stage(self), [from_path])[0]
-    assert from_out.scheme == "local"
+    from_out = output.loads_from(Stage(self), [from_path])[0]
+    assert from_out.protocol == "local"
 
     to_path = _expand_target_path(from_path, to_path)
 
     outs = self.find_outs_by_path(from_out.fspath)
     assert len(outs) == 1
     out = outs[0]
     stage = out.stage
@@ -53,30 +52,27 @@
     stage_name = os.path.splitext(os.path.basename(stage.path))[0]
     from_name = os.path.basename(from_out.fspath)
     if stage_name == from_name:
         new_fname = os.path.join(
             os.path.dirname(to_path),
             os.path.basename(to_path) + DVC_FILE_SUFFIX,
         )
-        new_wdir = os.path.abspath(
-            os.path.join(os.curdir, os.path.dirname(to_path))
-        )
+        new_wdir = os.path.abspath(os.path.join(os.curdir, os.path.dirname(to_path)))
         to_path = os.path.relpath(to_path, new_wdir)
         new_stage = self.stage.create(
             single_stage=True,
             fname=new_fname,
             wdir=new_wdir,
             outs=[to_path],
             meta=stage.meta,
         )
 
         os.unlink(stage.path)
         stage = new_stage
     else:
         to_path = os.path.relpath(to_path, stage.wdir)
 
-    to_out = Output.loads_from(stage, [to_path], out.use_cache, out.metric)[0]
+    to_out = output.loads_from(stage, [to_path], out.use_cache, out.metric)[0]
 
     out.move(to_out)
     stage.save()
-
-    Dvcfile(self, stage.path).dump(stage)
+    stage.dump()
```

### Comparing `dvc-2.9.5/dvc/repo/params/diff.py` & `dvc-3.0.0a0/dvc/repo/params/diff.py`

 * *Files 6% similar despite different names*

```diff
@@ -7,15 +7,15 @@
         return {}
 
     with_unchanged = kwargs.pop("all", False)
 
     a_rev = a_rev or "HEAD"
     b_rev = b_rev or "workspace"
 
-    params = repo.params.show(*args, **kwargs, revs=[a_rev, b_rev])
+    params = repo.params.show(
+        *args, **kwargs, revs=[a_rev, b_rev], hide_workspace=False
+    )
 
     old = params.get(a_rev, {}).get("data", {})
     new = params.get(b_rev, {}).get("data", {})
 
-    return _diff(
-        format_dict(old), format_dict(new), with_unchanged=with_unchanged
-    )
+    return _diff(format_dict(old), format_dict(new), with_unchanged=with_unchanged)
```

### Comparing `dvc-2.9.5/dvc/repo/params/show.py` & `dvc-3.0.0a0/dvc/repo/params/show.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,149 +1,190 @@
 import logging
 import os
 from collections import defaultdict
 from copy import copy
-from typing import TYPE_CHECKING, Callable, Dict, List, Optional, Tuple
+from typing import TYPE_CHECKING, Callable, Dict, Iterable, List, Optional, Tuple
+
+from scmrepo.exceptions import SCMError
 
 from dvc.dependency.param import ParamsDependency
 from dvc.repo import locked
 from dvc.repo.collect import collect
 from dvc.scm import NoSCMError
 from dvc.stage import PipelineStage
 from dvc.ui import ui
-from dvc.utils import error_handler, errored_revisions, onerror_collect
-from dvc.utils.serialize import LOADERS
+from dvc.utils import as_posix, error_handler, errored_revisions, onerror_collect
+from dvc.utils.collections import ensure_list
+from dvc.utils.serialize import load_path
 
 if TYPE_CHECKING:
     from dvc.output import Output
     from dvc.repo import Repo
 
 logger = logging.getLogger(__name__)
 
 
 def _is_params(dep: "Output"):
     return isinstance(dep, ParamsDependency)
 
 
+def _collect_top_level_params(repo):
+    top_params = repo.index._params  # pylint: disable=protected-access
+    for dvcfile, params in top_params.items():
+        wdir = repo.fs.path.relpath(repo.fs.path.parent(dvcfile), repo.root_dir)
+        for file in params:
+            path = repo.fs.path.join(wdir, as_posix(file))
+            yield repo.fs.path.normpath(path)
+
+
 def _collect_configs(
-    repo: "Repo", rev, targets=None
+    repo: "Repo", targets=None, deps=False, stages=None
 ) -> Tuple[List["Output"], List[str]]:
-
     params, fs_paths = collect(
         repo,
         targets=targets or [],
         deps=True,
         output_filter=_is_params,
-        rev=rev,
+        duplicates=deps or stages is not None,
     )
     all_fs_paths = fs_paths + [p.fs_path for p in params]
-    if not targets:
-        default_params = os.path.join(
+    if not any([deps, targets, stages]):
+        default_params = repo.fs.path.join(
             repo.root_dir, ParamsDependency.DEFAULT_PARAMS_FILE
         )
-        if default_params not in all_fs_paths and repo.fs.exists(
-            default_params
-        ):
+        if default_params not in all_fs_paths and repo.fs.exists(default_params):
             fs_paths.append(default_params)
+    if targets and (deps or stages) and not params:
+        # A target has been provided but it is not used in the stages
+        fs_paths = []
     return params, fs_paths
 
 
 @error_handler
 def _read_fs_path(fs, fs_path, **kwargs):
-    suffix = fs.path.suffix(fs_path).lower()
-    loader = LOADERS[suffix]
-    return loader(fs_path, fs=fs)
+    return load_path(fs_path, fs)
 
 
 def _read_params(
     repo,
     params,
     params_fs_paths,
     deps=False,
     onerror: Optional[Callable] = None,
+    stages: Optional[Iterable[str]] = None,
 ):
-    res: Dict[str, Dict] = defaultdict(dict)
+    res: Dict[str, Dict] = defaultdict(lambda: defaultdict(dict))
     fs_paths = copy(params_fs_paths)
 
-    if deps:
+    if deps or stages:
         for param in params:
-            params_dict = error_handler(param.read_params_d)(onerror=onerror)
+            if stages and param.stage.addressing not in stages:
+                continue
+            params_dict = error_handler(param.read_params)(
+                onerror=onerror, flatten=False
+            )
             if params_dict:
-                res[
-                    repo.fs.path.relpath(param.fs_path, os.getcwd())
-                ] = params_dict
+                name = os.sep.join(repo.fs.path.relparts(param.fs_path))
+                res[name]["data"].update(params_dict["data"])
+                if name in fs_paths:
+                    fs_paths.remove(name)
     else:
         fs_paths += [param.fs_path for param in params]
 
     for fs_path in fs_paths:
         from_path = _read_fs_path(repo.fs, fs_path, onerror=onerror)
         if from_path:
-            res[repo.fs.path.relpath(fs_path, os.getcwd())] = from_path
+            name = os.sep.join(repo.fs.path.relparts(fs_path))
+            res[name] = from_path
 
     return res
 
 
-def _collect_vars(repo, params) -> Dict:
+def _collect_vars(repo, params, stages=None) -> Dict:
     vars_params: Dict[str, Dict] = defaultdict(dict)
+
     for stage in repo.index.stages:
         if isinstance(stage, PipelineStage) and stage.tracked_vars:
+            if stages and stage.addressing not in stages:
+                continue
             for file, vars_ in stage.tracked_vars.items():
                 # `params` file are shown regardless of `tracked` or not
                 # to reduce noise and duplication, they are skipped
                 if file in params:
                     continue
 
-                vars_params[file].update(vars_)
+                name = os.sep.join(repo.fs.path.parts(file))
+                vars_params[name].update(vars_)
     return vars_params
 
 
 @locked
-def show(repo, revs=None, targets=None, deps=False, onerror: Callable = None):
+def show(
+    repo,
+    revs=None,
+    targets=None,
+    deps=False,
+    onerror: Optional[Callable] = None,
+    stages=None,
+    hide_workspace=True,
+):
     if onerror is None:
         onerror = onerror_collect
     res = {}
 
+    targets = ensure_list(targets)
+    targets = [repo.dvcfs.from_os_path(target) for target in targets]
+
     for branch in repo.brancher(revs=revs):
         params = error_handler(_gather_params)(
-            repo=repo, rev=branch, targets=targets, deps=deps, onerror=onerror
+            repo=repo,
+            targets=targets,
+            deps=deps,
+            onerror=onerror,
+            stages=stages,
         )
 
         if params:
             res[branch] = params
 
-    # Hide workspace params if they are the same as in the active branch
-    try:
-        active_branch = repo.scm.active_branch()
-    except (TypeError, NoSCMError):
-        # TypeError - detached head
-        # NoSCMError - no repo case
-        pass
-    else:
-        if res.get("workspace") == res.get(active_branch):
-            res.pop("workspace", None)
+    if hide_workspace:
+        # Hide workspace params if they are the same as in the active branch
+        try:
+            active_branch = repo.scm.active_branch()
+        except (SCMError, NoSCMError):
+            # SCMError - detached head
+            # NoSCMError - no repo case
+            pass
+        else:
+            if res.get("workspace") == res.get(active_branch):
+                res.pop("workspace", None)
 
     errored = errored_revisions(res)
     if errored:
         ui.error_write(
             "DVC failed to load some parameters for following revisions:"
             f" '{', '.join(errored)}'."
         )
 
     return res
 
 
-def _gather_params(repo, rev, targets=None, deps=False, onerror=None):
-    param_outs, params_fs_paths = _collect_configs(repo, rev, targets=targets)
+def _gather_params(repo, targets=None, deps=False, onerror=None, stages=None):
+    param_outs, params_fs_paths = _collect_configs(
+        repo, targets=targets, deps=deps, stages=stages
+    )
+    params_fs_paths.extend(_collect_top_level_params(repo=repo))
     params = _read_params(
         repo,
         params=param_outs,
         params_fs_paths=params_fs_paths,
         deps=deps,
         onerror=onerror,
+        stages=stages,
     )
-    vars_params = _collect_vars(repo, params)
+    vars_params = _collect_vars(repo, params, stages=stages)
 
     # NOTE: only those that are not added as a ParamDependency are
     # included so we don't need to recursively merge them yet.
     for key, vals in vars_params.items():
         params[key]["data"] = vals
     return params
```

### Comparing `dvc-2.9.5/dvc/repo/plots/diff.py` & `dvc-3.0.0a0/dvc/repo/plots/diff.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 def _revisions(repo, revs, experiment):
     revisions = revs or []
     if experiment and len(revisions) == 1:
         baseline = repo.experiments.get_baseline(revisions[0])
         if baseline:
             revisions.append(baseline[:7])
     if len(revisions) <= 1:
-        if len(revisions) == 0 and repo.scm.is_dirty():
+        if len(revisions) == 0 and repo.scm.is_dirty(untracked_files=False):
             revisions.append("HEAD")
         revisions.append("workspace")
     return revisions
 
 
 def diff(repo, *args, revs=None, experiment=False, **kwargs):
-    return repo.plots.show(
-        *args, revs=_revisions(repo, revs, experiment), **kwargs
-    )
+    if repo.scm.no_commits:
+        return {}
+    return repo.plots.show(*args, revs=_revisions(repo, revs, experiment), **kwargs)
```

### Comparing `dvc-2.9.5/dvc/repo/pull.py` & `dvc-3.0.0a0/dvc/repo/pull.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,34 +1,35 @@
 import logging
 from typing import TYPE_CHECKING, Optional
 
 from dvc.repo import locked
 from dvc.utils import glob_targets
 
 if TYPE_CHECKING:
-    from dvc.objects.db.base import ObjectDB
+    from dvc_objects.db import ObjectDB
 
 logger = logging.getLogger(__name__)
 
 
 @locked
-def pull(
+def pull(  # noqa: PLR0913
     self,
     targets=None,
     jobs=None,
     remote=None,
     all_branches=False,
     with_deps=False,
     all_tags=False,
     force=False,
     recursive=False,
     all_commits=False,
     run_cache=False,
     glob=False,
     odb: Optional["ObjectDB"] = None,
+    allow_missing=False,
 ):
     if isinstance(targets, str):
         targets = [targets]
 
     expanded_targets = glob_targets(targets, glob=glob)
 
     processed_files_count = self.fetch(
@@ -44,11 +45,12 @@
         odb=odb,
     )
     stats = self.checkout(
         targets=expanded_targets,
         with_deps=with_deps,
         force=force,
         recursive=recursive,
+        allow_missing=allow_missing,
     )
 
     stats["fetched"] = processed_files_count
     return stats
```

### Comparing `dvc-2.9.5/dvc/repo/push.py` & `dvc-3.0.0a0/dvc/repo/push.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,20 +1,25 @@
-from typing import TYPE_CHECKING, Optional
+from contextlib import suppress
+from typing import TYPE_CHECKING, Optional, Sequence
 
-from dvc.exceptions import FileTransferError, UploadError
+from dvc.config import NoRemoteError
+from dvc.exceptions import InvalidArgumentError, UploadError
+from dvc.utils import glob_targets
 
-from ..utils import glob_targets
 from . import locked
 
 if TYPE_CHECKING:
-    from dvc.objects.db.base import ObjectDB
+    from dvc.data_cloud import Remote
+    from dvc.repo import Repo
+    from dvc.types import TargetType
+    from dvc_objects.db import ObjectDB
 
 
 @locked
-def push(
+def push(  # noqa: C901, PLR0913
     self,
     targets=None,
     jobs=None,
     remote=None,
     all_branches=False,
     with_deps=False,
     all_tags=False,
@@ -22,52 +27,93 @@
     all_commits=False,
     run_cache=False,
     revs=None,
     glob=False,
     odb: Optional["ObjectDB"] = None,
     include_imports=False,
 ):
-    used_run_cache = (
-        self.stage_cache.push(remote, odb=odb) if run_cache else []
-    )
+    worktree_remote: Optional["Remote"] = None
+    with suppress(NoRemoteError):
+        _remote = self.cloud.get_remote(name=remote)
+        if _remote and (_remote.worktree or _remote.fs.version_aware):
+            worktree_remote = _remote
+
+    pushed = 0
+    used_run_cache = self.stage_cache.push(remote, odb=odb) if run_cache else []
+    pushed += len(used_run_cache)
 
     if isinstance(targets, str):
         targets = [targets]
 
     expanded_targets = glob_targets(targets, glob=glob)
 
-    used = self.used_objs(
-        expanded_targets,
-        all_branches=all_branches,
-        all_tags=all_tags,
-        all_commits=all_commits,
-        with_deps=with_deps,
-        force=True,
-        remote=remote,
-        jobs=jobs,
-        recursive=recursive,
-        used_run_cache=used_run_cache,
-        revs=revs,
-    )
-
-    pushed = len(used_run_cache)
-    if odb:
-        all_ids = set()
-        for dest_odb, obj_ids in used.items():
-            if not include_imports and dest_odb and dest_odb.read_only:
-                continue
-            all_ids.update(obj_ids)
-        try:
-            pushed += self.cloud.push(all_ids, jobs, remote=remote, odb=odb)
-        except FileTransferError as exc:
-            raise UploadError(exc.amount)
+    if worktree_remote is not None:
+        pushed += _push_worktree(
+            self,
+            worktree_remote,
+            revs=revs,
+            all_branches=all_branches,
+            all_tags=all_tags,
+            all_commits=all_commits,
+            targets=expanded_targets,
+            jobs=jobs,
+            with_deps=with_deps,
+            recursive=recursive,
+        )
     else:
-        for dest_odb, obj_ids in used.items():
-            if dest_odb and dest_odb.read_only:
-                continue
-            try:
-                pushed += self.cloud.push(
+        used = self.used_objs(
+            expanded_targets,
+            all_branches=all_branches,
+            all_tags=all_tags,
+            all_commits=all_commits,
+            with_deps=with_deps,
+            force=True,
+            remote=remote,
+            jobs=jobs,
+            recursive=recursive,
+            used_run_cache=used_run_cache,
+            revs=revs,
+            push=True,
+        )
+
+        if odb:
+            all_ids = set()
+            for dest_odb, obj_ids in used.items():
+                if not include_imports and dest_odb and dest_odb.read_only:
+                    continue
+                all_ids.update(obj_ids)
+            result = self.cloud.push(all_ids, jobs, remote=remote, odb=odb)
+            if result.failed:
+                raise UploadError(len(result.failed))
+            pushed += len(result.transferred)
+        else:
+            for dest_odb, obj_ids in used.items():
+                if dest_odb and dest_odb.read_only:
+                    continue
+                result = self.cloud.push(
                     obj_ids, jobs, remote=remote, odb=odb or dest_odb
                 )
-            except FileTransferError as exc:
-                raise UploadError(exc.amount)
+                if result.failed:
+                    raise UploadError(len(result.failed))
+                pushed += len(result.transferred)
     return pushed
+
+
+def _push_worktree(
+    repo: "Repo",
+    remote: "Remote",
+    revs: Optional[Sequence[str]] = None,
+    all_branches: bool = False,
+    all_tags: bool = False,
+    all_commits: bool = False,
+    targets: Optional["TargetType"] = None,
+    jobs: Optional[int] = None,
+    **kwargs,
+) -> int:
+    from dvc.repo.worktree import push_worktree
+
+    if revs or all_branches or all_tags or all_commits:
+        raise InvalidArgumentError(
+            "Multiple rev push is unsupported for cloud versioned remotes"
+        )
+
+    return push_worktree(repo, remote, targets=targets, jobs=jobs, **kwargs)
```

### Comparing `dvc-2.9.5/dvc/repo/reproduce.py` & `dvc-3.0.0a0/dvc/repo/reproduce.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,118 +1,104 @@
 import logging
-import typing
 from functools import partial
+from typing import TYPE_CHECKING, Iterator, List
 
 from dvc.exceptions import DvcException, ReproductionError
 from dvc.repo.scm_context import scm_context
+from dvc.stage.exceptions import CheckpointKilledError
 
 from . import locked
 
-if typing.TYPE_CHECKING:
+if TYPE_CHECKING:
     from dvc.stage import Stage
 
     from . import Repo
 
 logger = logging.getLogger(__name__)
 
 
-def _reproduce_stage(stage: "Stage", **kwargs):
+def _reproduce_stage(stage: "Stage", **kwargs) -> List["Stage"]:
     def _run_callback(repro_callback):
-        _dump_stage(stage)
+        stage.dump(update_pipeline=False)
         _track_stage(stage)
         repro_callback([stage])
 
     checkpoint_func = kwargs.pop("checkpoint_func", None)
     if stage.is_checkpoint:
         if checkpoint_func:
             kwargs["checkpoint_func"] = partial(_run_callback, checkpoint_func)
         else:
             raise DvcException(
                 "Checkpoint stages are not supported in 'dvc repro'. "
-                "Checkpoint stages must be reproduced with 'dvc exp run' "
-                "or 'dvc exp resume'."
+                "Checkpoint stages must be reproduced with 'dvc exp run'. "
             )
 
     if stage.frozen and not stage.is_import:
         logger.warning(
-            "{} is frozen. Its dependencies are"
-            " not going to be reproduced.".format(stage)
+            "%s is frozen. Its dependencies are not going to be reproduced.",
+            stage,
         )
 
     stage = stage.reproduce(**kwargs)
     if not stage:
         return []
 
     if not kwargs.get("dry", False):
         track = checkpoint_func is not None
-        _dump_stage(stage)
+        stage.dump(update_pipeline=False)
         if track:
             _track_stage(stage)
 
     return [stage]
 
 
-def _dump_stage(stage):
-    from ..dvcfile import Dvcfile
-
-    dvcfile = Dvcfile(stage.repo, stage.path)
-    dvcfile.dump(stage, update_pipeline=False)
-
-
-def _get_stage_files(stage: "Stage") -> typing.Iterator[str]:
+def _get_stage_files(stage: "Stage") -> Iterator[str]:
     yield stage.dvcfile.relpath
     for dep in stage.deps:
         if (
             not dep.use_scm_ignore
             and dep.is_in_repo
-            and not stage.repo.repo_fs.isdvc(dep.fs_path)
+            and not stage.repo.dvcfs.isdvc(stage.repo.dvcfs.from_os_path(str(dep)))
         ):
             yield dep.fs_path
     for out in stage.outs:
         if not out.use_scm_ignore and out.is_in_repo:
             yield out.fs_path
-        if out.live:
-            from dvc.repo.live import summary_fs_path
-
-            summary = summary_fs_path(out)
-            if summary:
-                yield summary
 
 
 def _track_stage(stage: "Stage") -> None:
     from dvc.utils import relpath
 
     context = stage.repo.scm_context
     for path in _get_stage_files(stage):
         context.track_file(relpath(path))
     return context.track_changed_files()
 
 
 @locked
 @scm_context
-def reproduce(
+def reproduce(  # noqa: C901, PLR0912
     self: "Repo",
     targets=None,
     recursive=False,
     pipeline=False,
     all_pipelines=False,
     **kwargs,
 ):
     from .graph import get_pipeline, get_pipelines
 
     glob = kwargs.pop("glob", False)
-    accept_group = not glob
 
     if isinstance(targets, str):
         targets = [targets]
 
     if not all_pipelines and not targets:
-        from dvc.dvcfile import PIPELINE_FILE
+        from dvc.dvcfile import PROJECT_FILE
 
-        targets = [PIPELINE_FILE]
+        targets = [PROJECT_FILE]
 
     interactive = kwargs.get("interactive", False)
     if not interactive:
         kwargs["interactive"] = self.config["core"].get("interactive", False)
 
     stages = set()
     if pipeline or all_pipelines:
@@ -131,24 +117,32 @@
                     stages.add(stage)
     else:
         for target in targets:
             stages.update(
                 self.stage.collect(
                     target,
                     recursive=recursive,
-                    accept_group=accept_group,
                     glob=glob,
                 )
             )
 
+    if kwargs.get("pull", False):
+        logger.debug("Pulling run cache")
+        self.stage_cache.pull(None)
+
     return _reproduce_stages(self.index.graph, list(stages), **kwargs)
 
 
-def _reproduce_stages(
-    G, stages, downstream=False, single_item=False, on_unchanged=None, **kwargs
+def _reproduce_stages(  # noqa: C901
+    graph,
+    stages,
+    downstream=False,
+    single_item=False,
+    on_unchanged=None,
+    **kwargs,
 ):
     r"""Derive the evaluation of the given node for the given graph.
 
     When you _reproduce a stage_, you want to _evaluate the descendants_
     to know if it make sense to _recompute_ it. A post-ordered search
     will give us an order list of the nodes we want.
 
@@ -177,33 +171,32 @@
                 / \   \        --- reverse -->        \ /   /
                B   C   G                               D   F
                 \ /                                     \ /
                  A                                       E
 
     The derived evaluation of _downstream_ B would be: [B, D, E]
     """
-    steps = _get_steps(G, stages, downstream, single_item)
+    steps = _get_steps(graph, stages, downstream, single_item)
 
     force_downstream = kwargs.pop("force_downstream", False)
     result = []
-    unchanged = []
+    unchanged: List["Stage"] = []
     # `ret` is used to add a cosmetic newline.
-    ret = []
+    ret: List["Stage"] = []
     checkpoint_func = kwargs.pop("checkpoint_func", None)
-    for stage in steps:
+
+    for i, stage in enumerate(steps):
         if ret:
             logger.info("")
 
         if checkpoint_func:
             kwargs["checkpoint_func"] = partial(
                 _repro_callback, checkpoint_func, unchanged
             )
 
-        from dvc.stage.monitor import CheckpointKilledError
-
         try:
             ret = _reproduce_stage(stage, **kwargs)
 
             if len(ret) == 0:
                 unchanged.extend([stage])
             elif force_downstream:
                 # NOTE: we are walking our pipeline from the top to the
@@ -212,35 +205,48 @@
                 # the other stages down below, even if their direct
                 # dependencies didn't change.
                 kwargs["force"] = True
 
             if ret:
                 result.extend(ret)
         except CheckpointKilledError:
-            raise
-        except Exception as exc:
+            result.append(stage)
+            logger.warning(
+                (
+                    "Checkpoint stage '%s' was interrupted remaining stages in"
+                    " pipeline will not be reproduced."
+                ),
+                stage.addressing,
+            )
+            logger.warning(
+                "skipped stages '%s'",
+                ", ".join(s.addressing for s in steps[i + 1 :]),
+            )
+
+            break
+        except Exception as exc:  # noqa: BLE001
             raise ReproductionError(stage.addressing) from exc
 
     if on_unchanged is not None:
         on_unchanged(unchanged)
     return result
 
 
-def _get_steps(G, stages, downstream, single_item):
+def _get_steps(graph, stages, downstream, single_item):
     import networkx as nx
 
-    active = G.copy()
+    active = graph.copy()
     if not single_item:
         # NOTE: frozen stages don't matter for single_item
-        for stage in G:
+        for stage in graph:
             if stage.frozen:
                 # NOTE: disconnect frozen stage from its dependencies
-                active.remove_edges_from(G.out_edges(stage))
+                active.remove_edges_from(graph.out_edges(stage))
 
-    all_pipelines = []
+    all_pipelines: List["Stage"] = []
     for stage in stages:
         if downstream:
             # NOTE (py3 only):
             # Python's `deepcopy` defaults to pickle/unpickle the object.
             # Stages are complex objects (with references to `repo`,
             # `outs`, and `deps`) that cause struggles when you try
             # to serialize them. We need to create a copy of the graph
```

### Comparing `dvc-2.9.5/dvc/repo/run.py` & `dvc-3.0.0a0/dvc/repo/run.py`

 * *Files 14% similar despite different names*

```diff
@@ -15,20 +15,17 @@
 @scm_context
 def run(
     self: "Repo",
     no_exec: bool = False,
     no_commit: bool = False,
     run_cache: bool = True,
     force: bool = True,
-    **kwargs
+    **kwargs,
 ) -> Union["Stage", "PipelineStage"]:
-
-    kwargs.update(
-        {"force": force, "params": parse_params(kwargs.get("params", []))}
-    )
+    kwargs.update({"force": force, "params": parse_params(kwargs.get("params", []))})
     stage = self.stage.create(**kwargs)
 
     if no_exec:
         stage.ignore_outs()
     else:
         stage.run(no_commit=no_commit, run_cache=run_cache)
```

### Comparing `dvc-2.9.5/dvc/repo/scm_context.py` & `dvc-3.0.0a0/dvc/repo/scm_context.py`

 * *Files 10% similar despite different names*

```diff
@@ -5,52 +5,50 @@
 from typing import (
     TYPE_CHECKING,
     Any,
     Dict,
     Iterable,
     Iterator,
     List,
+    Optional,
     Set,
     Union,
 )
 
 from dvc.utils import relpath
 from dvc.utils.collections import ensure_list
 
 if TYPE_CHECKING:
-    from scmrepo.base import Base
-
     from dvc.repo import Repo
+    from dvc.scm import Base
 
 
 logger = logging.getLogger(__name__)
 
 
 class SCMContext:
-    def __init__(self, scm: "Base", config: Dict[str, Any] = None) -> None:
+    def __init__(self, scm: "Base", config: Optional[Dict[str, Any]] = None) -> None:
         from funcy import get_in
 
         self.scm: "Base" = scm
         self.autostage: bool = get_in(
             config or {}, ["core", "autostage"], default=False
         )
         self.ignored_paths: List[str] = []
         self.files_to_track: Set[str] = set()
         self.quiet: bool = False
 
-    def track_file(
-        self, paths: Union[str, Iterable[str], None] = None
-    ) -> None:
+    def track_file(self, paths: Union[str, Iterable[str], None] = None) -> None:
         """Track file to remind user to track new files or autostage later."""
         return self.files_to_track.update(ensure_list(paths))
 
     @staticmethod
     def _make_git_add_cmd(paths: Union[str, Iterable[str]]) -> str:
         files = " ".join(map(shlex.quote, ensure_list(paths)))
-        return f"\tgit add {files}".expandtabs(4)
+        return f"\tgit add {files}"
 
     def add(self, paths: Union[str, Iterable[str]]) -> None:
         from scmrepo.exceptions import UnsupportedIndexFormat
 
         try:
             return self.scm.add(paths)
         except UnsupportedIndexFormat:
@@ -74,15 +72,15 @@
         from scmrepo.exceptions import FileNotInRepoError
 
         from dvc.scm import SCMError
 
         try:
             gitignore_file = self.scm.ignore(path)
         except FileNotInRepoError as exc:
-            raise SCMError(str(exc))
+            raise SCMError(str(exc))  # noqa: B904
 
         if gitignore_file:
             logger.debug("Added '%s' to gitignore file.", path)
             self.track_file(relpath(gitignore_file))
             return self.ignored_paths.append(path)
 
     def ignore_remove(self, path: str) -> None:
@@ -90,22 +88,22 @@
 
         from dvc.scm import SCMError
 
         logger.debug("Removing '%s' from gitignore file.", path)
         try:
             gitignore_file = self.scm.ignore_remove(path)
         except FileNotInRepoError as exc:
-            raise SCMError(str(exc))
+            raise SCMError(str(exc))  # noqa: B904
 
         if gitignore_file:
             return self.track_file(relpath(gitignore_file))
 
     @contextmanager
     def __call__(
-        self, autostage: bool = None, quiet: bool = None
+        self, autostage: Optional[bool] = None, quiet: Optional[bool] = None
     ) -> Iterator["SCMContext"]:
         try:
             yield self
         except Exception:
             for path in self.ignored_paths:
                 self.ignore_remove(path)
             raise
@@ -120,35 +118,36 @@
         if quiet is None:
             quiet = self.quiet
 
         from dvc.scm import NoSCM
 
         if autostage:
             self.track_changed_files()
-        elif not quiet and not isinstance(self.scm, NoSCM):
+        elif (
+            not quiet
+            and not isinstance(self.scm, NoSCM)
+            and logger.isEnabledFor(logging.INFO)
+        ):
             add_cmd = self._make_git_add_cmd(self.files_to_track)
+            logger.info("\nTo track the changes with git, run:\n\n%s", add_cmd)
             logger.info(
-                f"\nTo track the changes with git, run:\n" f"\n{add_cmd}"
-            )
-            logger.info(
-                "\nTo enable auto staging, run:\n\n"
-                "\tdvc config core.autostage true"
+                "\nTo enable auto staging, run:\n\n\tdvc config core.autostage true"
             )
 
         self.files_to_track = set()
 
     def __enter__(self) -> "SCMContext":
         self._cm = self()  # pylint: disable=attribute-defined-outside-init
         return self._cm.__enter__()  # pylint: disable=no-member
 
     def __exit__(self, *exc_args) -> None:
         assert self._cm
         self._cm.__exit__(*exc_args)  # pylint: disable=no-member
 
 
-def scm_context(method, autostage: bool = None, quiet: bool = None):
+def scm_context(method, autostage: Optional[bool] = None, quiet: Optional[bool] = None):
     @wraps(method)
     def run(repo: "Repo", *args, **kw):
         with repo.scm_context(autostage=autostage, quiet=quiet):
             return method(repo, *args, **kw)
 
     return run
```

### Comparing `dvc-2.9.5/dvc/repo/stage.py` & `dvc-3.0.0a0/dvc/repo/stage.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,43 +1,33 @@
 import fnmatch
 import logging
-import os
 import typing
 from contextlib import suppress
-from functools import partial, wraps
-from typing import (
-    Callable,
-    Iterable,
-    List,
-    NamedTuple,
-    Optional,
-    Set,
-    Tuple,
-    Union,
-)
+from functools import wraps
+from typing import Iterable, List, NamedTuple, Optional, Set, Tuple, Union
 
 from dvc.exceptions import (
-    DvcException,
     NoOutputOrStageError,
+    OutputDuplicationError,
     OutputNotFoundError,
 )
 from dvc.repo import lock_repo
-from dvc.utils import parse_target, relpath
+from dvc.ui import ui
+from dvc.utils import as_posix, parse_target
 
 logger = logging.getLogger(__name__)
 
 if typing.TYPE_CHECKING:
     from networkx import DiGraph
 
     from dvc.repo import Repo
     from dvc.stage import PipelineStage, Stage
     from dvc.stage.loader import StageLoader
-    from dvc.types import OptStr
 
-PIPELINE_FILE = "dvc.yaml"
+PROJECT_FILE = "dvc.yaml"
 
 
 class StageInfo(NamedTuple):
     stage: "Stage"
     filter_info: Optional[str] = None
 
 
@@ -57,94 +47,102 @@
 
 def _maybe_collect_from_dvc_yaml(
     loader: "StageLoad", target, with_deps: bool, **load_kwargs
 ) -> StageIter:
     from dvc.stage.exceptions import StageNotFound
 
     stages: StageList = []
-    if loader.fs.exists(PIPELINE_FILE):
+    if loader.fs.exists(PROJECT_FILE):
         with suppress(StageNotFound):
-            stages = loader.load_all(PIPELINE_FILE, target, **load_kwargs)
-    return _collect_with_deps(stages, loader.graph) if with_deps else stages
+            stages = loader.load_all(PROJECT_FILE, target, **load_kwargs)
+    if with_deps:
+        return _collect_with_deps(stages, loader.repo.index.graph)
+    return stages
 
 
 def _collect_specific_target(
     loader: "StageLoad",
     target: str,
     with_deps: bool,
     recursive: bool,
-    accept_group: bool,
-) -> Tuple[StageIter, "OptStr", "OptStr"]:
+) -> Tuple[StageIter, Optional[str], Optional[str]]:
     from dvc.dvcfile import is_valid_filename
 
     # Optimization: do not collect the graph for a specific target
     file, name = parse_target(target)
 
     # if the target has a file, we can load directly from it.
     if not file:
         # but, if there's no file, parsing is ambiguous as it can be a
         # stage name in `dvc.yaml` file or an output. We prioritize
         # `dvc.yaml` stage name here. If it exists, then we move on.
         # else, we assume it's a output name in the `collect_granular()` below
         msg = "Checking if stage '%s' is in '%s'"
-        logger.debug(msg, target, PIPELINE_FILE)
+        logger.debug(msg, target, PROJECT_FILE)
         if not (recursive and loader.fs.isdir(target)):
-            stages = _maybe_collect_from_dvc_yaml(
-                loader, target, with_deps, accept_group=accept_group
-            )
+            stages = _maybe_collect_from_dvc_yaml(loader, target, with_deps)
             if stages:
                 return stages, file, name
     elif not with_deps and is_valid_filename(file):
-        stages = loader.load_all(file, name, accept_group=accept_group)
+        stages = loader.load_all(file, name)
         return stages, file, name
     return [], file, name
 
 
 def locked(f):
     @wraps(f)
     def wrapper(loader: "StageLoad", *args, **kwargs):
         with lock_repo(loader.repo):
             return f(loader, *args, **kwargs)
 
     return wrapper
 
 
 class StageLoad:
-    def __init__(self, repo: "Repo", fs=None) -> None:
+    def __init__(self, repo: "Repo") -> None:
         self.repo: "Repo" = repo
-        self._fs = fs
+
+    @property
+    def fs(self):
+        return self.repo.fs
 
     @locked
     def add(
         self,
         single_stage: bool = False,
-        fname: str = None,
+        fname: Optional[str] = None,
         validate: bool = True,
         force: bool = False,
         update_lock: bool = False,
         **stage_data,
     ):
         stage = self.create(
             single_stage=single_stage,
             fname=fname,
             validate=validate,
             force=force,
             **stage_data,
         )
-        with self.repo.scm_context:
-            stage.dump(update_lock=update_lock)
+        stage.dump(update_lock=update_lock)
+        try:
             stage.ignore_outs()
+        except FileNotFoundError as exc:
+            ui.warn(
+                f"Could not create .gitignore entry in {exc.filename}."
+                " DVC will attempt to create .gitignore entry again when"
+                " the stage is run."
+            )
 
         return stage
 
     def create(
         self,
         single_stage: bool = False,
         validate: bool = True,
-        fname: str = None,
+        fname: Optional[str] = None,
         force: bool = False,
         **stage_data,
     ) -> Union["Stage", "PipelineStage"]:
         """Creates a stage.
 
         Args:
             single_stage: if true, the .dvc file based stage is created,
@@ -153,76 +151,73 @@
             validate: if true, the new created stage is checked against the
                 stages in the repo. Eg: graph correctness,
                 potential overwrites in dvc.yaml file (unless `force=True`).
             force: ignores overwrites in dvc.yaml file
             stage_data: Stage data to create from
                 (see create_stage and loads_from for more information)
         """
-        from dvc.stage import PipelineStage, Stage, create_stage, restore_meta
+        from dvc.stage import PipelineStage, Stage, create_stage, restore_fields
         from dvc.stage.exceptions import InvalidStageName
-        from dvc.stage.utils import (
-            is_valid_name,
-            prepare_file_path,
-            validate_kwargs,
-        )
+        from dvc.stage.utils import is_valid_name, prepare_file_path, validate_kwargs
 
         stage_data = validate_kwargs(
             single_stage=single_stage, fname=fname, **stage_data
         )
         if single_stage:
             stage_cls = Stage
             path = fname or prepare_file_path(stage_data)
         else:
-            path = PIPELINE_FILE
+            path = PROJECT_FILE
             stage_cls = PipelineStage
             stage_name = stage_data["name"]
             if not (stage_name and is_valid_name(stage_name)):
                 raise InvalidStageName
 
-        stage = create_stage(
-            stage_cls, repo=self.repo, path=path, **stage_data
-        )
+        stage = create_stage(stage_cls, repo=self.repo, path=path, **stage_data)
         if validate:
             if not force:
                 from dvc.stage.utils import check_stage_exists
 
                 check_stage_exists(self.repo, stage, stage.path)
 
-            new_index = self.repo.index.add(stage)
-            new_index.check_graph()
+            try:
+                self.repo.check_graph(stages={stage})
+            except OutputDuplicationError as exc:
+                # Don't include the stage currently being added.
+                exc.stages.remove(stage)
+                raise OutputDuplicationError(exc.output, exc.stages) from None
 
-        restore_meta(stage)
+        restore_fields(stage)
         return stage
 
     def from_target(
-        self, target: str, accept_group: bool = False, glob: bool = False
+        self, target: str, accept_group: bool = True, glob: bool = False
     ) -> StageList:
         """
         Returns a list of stage from the provided target.
         (see load method below for further details)
         """
         path, name = parse_target(target, isa_glob=glob)
-        return self.load_all(
-            path=path, name=name, accept_group=accept_group, glob=glob
-        )
+        return self.load_all(path=path, name=name, accept_group=accept_group, glob=glob)
 
     def get_target(self, target: str) -> "Stage":
         """
         Returns a stage from the provided target.
         (see load_one method for further details)
         """
         path, name = parse_target(target)
         return self.load_one(path=path, name=name)
 
-    @staticmethod
-    def _get_filepath(path: str = None, name: str = None) -> str:
+    def _get_filepath(
+        self, path: Optional[str] = None, name: Optional[str] = None
+    ) -> str:
         if path:
-            return path
+            return self.repo.fs.path.realpath(path)
 
-        path = PIPELINE_FILE
+        path = PROJECT_FILE
         logger.debug("Assuming '%s' to be a stage inside '%s'", name, path)
         return path
 
     @staticmethod
     def _get_group_keys(stages: "StageLoader", group: str) -> Iterable[str]:
         from dvc.parsing import JOIN
 
@@ -230,268 +225,188 @@
             assert isinstance(key, str)
             if key.startswith(f"{group}{JOIN}"):
                 yield key
 
     def _get_keys(
         self,
         stages: "StageLoader",
-        name: str = None,
-        accept_group: bool = False,
+        name: Optional[str] = None,
+        accept_group: bool = True,
         glob: bool = False,
     ) -> Iterable[str]:
-
-        assert not (accept_group and glob)
-
         if not name:
             return stages.keys()
-
         if accept_group and stages.is_foreach_generated(name):
             return self._get_group_keys(stages, name)
         if glob:
             return fnmatch.filter(stages.keys(), name)
         return [name]
 
     def load_all(
         self,
-        path: str = None,
-        name: str = None,
-        accept_group: bool = False,
+        path: Optional[str] = None,
+        name: Optional[str] = None,
+        accept_group: bool = True,
         glob: bool = False,
     ) -> StageList:
         """Load a list of stages from a file.
 
         Args:
             path: if not provided, default `dvc.yaml` is assumed.
             name: required for `dvc.yaml` files, ignored for `.dvc` files.
             accept_group: if true, all of the the stages generated from `name`
                 foreach are returned.
             glob: if true, `name` is considered as a glob, which is
                 used to filter list of stages from the given `path`.
         """
-        from dvc.dvcfile import Dvcfile
+        from dvc.dvcfile import load_file
         from dvc.stage.loader import SingleStageLoader, StageLoader
 
         path = self._get_filepath(path, name)
-        dvcfile = Dvcfile(self.repo, path)
+        dvcfile = load_file(self.repo, path)
         # `dvcfile.stages` is not cached
-        stages = dvcfile.stages  # type: ignore
+        stages = dvcfile.stages  # type: ignore[attr-defined]
 
         if isinstance(stages, SingleStageLoader):
             stage = stages[name]
             return [stage]
 
         assert isinstance(stages, StageLoader)
         keys = self._get_keys(stages, name, accept_group, glob)
         return [stages[key] for key in keys]
 
-    def load_one(self, path: str = None, name: str = None) -> "Stage":
+    def load_one(
+        self, path: Optional[str] = None, name: Optional[str] = None
+    ) -> "Stage":
         """Load a single stage from a file.
 
         Args:
             path: if not provided, default `dvc.yaml` is assumed.
             name: required for `dvc.yaml` files, ignored for `.dvc` files.
         """
-        from dvc.dvcfile import Dvcfile
+        from dvc.dvcfile import load_file
 
         path = self._get_filepath(path, name)
-        dvcfile = Dvcfile(self.repo, path)
-
-        stages = dvcfile.stages  # type: ignore
+        dvcfile = load_file(self.repo, path)
+        stages = dvcfile.stages  # type: ignore[attr-defined]
 
         return stages[name]
 
-    def load_file(self, path: str = None) -> StageList:
+    def load_file(self, path: Optional[str] = None) -> StageList:
         """Load all of the stages from a file."""
         return self.load_all(path)
 
-    def load_glob(self, path: str, expr: str = None):
+    def load_glob(self, path: str, expr: Optional[str] = None):
         """Load stages from `path`, filtered with `expr` provided."""
         return self.load_all(path, expr, glob=True)
 
-    @property
-    def fs(self):
-        if self._fs:
-            return self._fs
-        return self.repo.fs
-
-    @property
-    def graph(self) -> "DiGraph":
-        return self.repo.index.graph
-
     def collect(
         self,
-        target: str = None,
+        target: Optional[str] = None,
         with_deps: bool = False,
         recursive: bool = False,
-        graph: "DiGraph" = None,
-        accept_group: bool = False,
+        graph: Optional["DiGraph"] = None,
         glob: bool = False,
     ) -> StageIter:
         """Collect list of stages from the provided target.
 
         Args:
             target: if not provided, all of the stages in the graph are
                 returned.
                 Target can be:
-                - a stage name in the `dvc.yaml` file.
+                - a foreach group name or a stage name in the `dvc.yaml` file.
+                - a generated stage name from a foreach group.
                 - a path to `dvc.yaml` or `.dvc` file.
                 - in case of a stage to a dvc.yaml file in a different
                   directory than current working directory, it can be a path
                   to dvc.yaml file, followed by a colon `:`, followed by stage
                   name (eg: `../dvc.yaml:build`).
                 - in case of `recursive`, it can be a path to a directory.
-                - in case of `accept_group`, it can be a group name of
-                    `foreach` generated stage.
                 - in case of `glob`, it can be a wildcard pattern to match
                   stages. Example: `build*` for stages in `dvc.yaml` file, or
                   `../dvc.yaml:build*` for stages in dvc.yaml in a different
                   directory.
                   Note that, glob only applies for the stage name, not to the
                   file, so `**/dvc.yaml:build*` is not possible.
             with_deps: if true, the stages including their dependencies are
                 returned.
             recursive: if true and if `target` is a directory, all of the
                 stages inside that directory is returned.
             graph: graph to use. Defaults to `repo.graph`.
-            accept_group: if true, all of the `foreach` generated stages of
-                the specified target is returned.
             glob: Use `target` as a pattern to match stages in a file.
         """
         if not target:
-            return list(graph) if graph else list(self.repo.index)
+            return list(graph) if graph else self.repo.index.stages
 
         if recursive and self.fs.isdir(target):
             from dvc.repo.graph import collect_inside_path
 
-            path = os.path.abspath(target)
-            return collect_inside_path(path, graph or self.graph)
+            path = self.fs.path.abspath(target)
+            return collect_inside_path(path, graph or self.repo.index.graph)
 
-        stages = self.from_target(target, accept_group=accept_group, glob=glob)
+        stages = self.from_target(target, glob=glob)
         if not with_deps:
             return stages
 
-        return _collect_with_deps(stages, graph or self.graph)
+        return _collect_with_deps(stages, graph or self.repo.index.graph)
 
     def collect_granular(
         self,
-        target: str = None,
+        target: Optional[str] = None,
         with_deps: bool = False,
         recursive: bool = False,
-        graph: "DiGraph" = None,
-        accept_group: bool = False,
+        graph: Optional["DiGraph"] = None,
     ) -> List[StageInfo]:
         """Collects a list of (stage, filter_info) from the given target.
 
         Priority is in the order of following in case of ambiguity:
         - .dvc file or .yaml file
         - dir if recursive and directory exists
-        - stage_name
+        - foreach_group_name or stage_name
+        - generated stage name from a foreach group
         - output file
 
         Args:
             target: if not provided, all of the stages without any filters are
                 returned.
                 If `target` is a path to a dvc-tracked output,
                 a (stage, output_path) is returned.
                 Otherwise, the details above for `target` in `collect()`
                 applies.
 
             (see `collect()` for other arguments)
         """
         if not target:
-            return [StageInfo(stage) for stage in self.repo.index]
+            return [StageInfo(stage) for stage in self.repo.index.stages]
 
-        stages, file, _ = _collect_specific_target(
-            self, target, with_deps, recursive, accept_group
-        )
+        target = as_posix(target)
+
+        stages, file, _ = _collect_specific_target(self, target, with_deps, recursive)
         if not stages:
             if not (recursive and self.fs.isdir(target)):
                 try:
                     (out,) = self.repo.find_outs_by_path(target, strict=False)
-                    return [StageInfo(out.stage, os.path.abspath(target))]
+                    return [StageInfo(out.stage, self.fs.path.abspath(target))]
                 except OutputNotFoundError:
                     pass
 
             from dvc.dvcfile import is_valid_filename
-            from dvc.stage.exceptions import (
-                StageFileDoesNotExistError,
-                StageNotFound,
-            )
+            from dvc.stage.exceptions import StageFileDoesNotExistError, StageNotFound
 
             try:
                 stages = self.collect(
                     target,
                     with_deps,
                     recursive,
                     graph,
-                    accept_group=accept_group,
                 )
             except StageFileDoesNotExistError as exc:
                 # collect() might try to use `target` as a stage name
                 # and throw error that dvc.yaml does not exist, whereas it
                 # should say that both stage name and file does not exist.
                 if file and is_valid_filename(file):
                     raise
                 raise NoOutputOrStageError(target, exc.file) from exc
             except StageNotFound as exc:
                 raise NoOutputOrStageError(target, exc.file) from exc
 
         return [StageInfo(stage) for stage in stages]
-
-    def _collect_repo(self, onerror: Callable[[str, Exception], None] = None):
-        """Collects all of the stages present in the DVC repo.
-
-        Args:
-            onerror (optional): callable that will be called with two args:
-                the filepath whose collection failed and the exc instance.
-                It can report the error to continue with the collection
-                (and, skip failed ones), or raise the exception to abort
-                the collection.
-        """
-        from dvc.dvcfile import is_valid_filename
-        from dvc.fs.local import LocalFileSystem
-
-        scm = self.repo.scm
-        sep = os.sep
-        outs: Set[str] = set()
-
-        is_local_fs = isinstance(self.fs, LocalFileSystem)
-
-        def is_ignored(path):
-            # apply only for the local fs
-            return is_local_fs and scm.is_ignored(path)
-
-        def is_dvcfile_and_not_ignored(root, file):
-            return is_valid_filename(file) and not is_ignored(
-                f"{root}{sep}{file}"
-            )
-
-        def is_out_or_ignored(root, directory):
-            dir_path = f"{root}{sep}{directory}"
-            # trailing slash needed to check if a directory is gitignored
-            return dir_path in outs or is_ignored(f"{dir_path}{sep}")
-
-        for root, dirs, files in self.repo.dvcignore.walk(
-            self.fs, self.repo.root_dir
-        ):
-            dvcfile_filter = partial(is_dvcfile_and_not_ignored, root)
-            for file in filter(dvcfile_filter, files):
-                file_path = os.path.join(root, file)
-                try:
-                    new_stages = self.load_file(file_path)
-                except DvcException as exc:
-                    if onerror:
-                        onerror(relpath(file_path), exc)
-                        continue
-                    raise
-
-                yield from new_stages
-                outs.update(
-                    out.fspath
-                    for stage in new_stages
-                    for out in stage.outs
-                    if out.scheme == "local"
-                )
-            dirs[:] = [d for d in dirs if not is_out_or_ignored(root, d)]
-
-    def collect_repo(self, onerror: Callable[[str, Exception], None] = None):
-        return list(self._collect_repo(onerror))
```

### Comparing `dvc-2.9.5/dvc/repo/status.py` & `dvc-3.0.0a0/dvc/repo/status.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,41 +1,38 @@
 import logging
-from itertools import compress
-
-from funcy.py3 import cat
+from itertools import chain, compress
 
 from dvc.exceptions import InvalidArgumentError
 
 from . import locked
 
 logger = logging.getLogger(__name__)
 
 
 def _joint_status(pairs):
     status_info = {}
 
     for stage, filter_info in pairs:
-        if stage.frozen and not stage.is_repo_import:
+        if stage.frozen and not (stage.is_repo_import or stage.is_versioned_import):
             logger.warning(
-                "{} is frozen. Its dependencies are"
-                " not going to be shown in the status output.".format(stage)
+                (
+                    "%s is frozen. Its dependencies are"
+                    " not going to be shown in the status output."
+                ),
+                stage,
             )
-        status_info.update(
-            stage.status(check_updates=True, filter_info=filter_info)
-        )
+        status_info.update(stage.status(check_updates=True, filter_info=filter_info))
 
     return status_info
 
 
 def _local_status(self, targets=None, with_deps=False, recursive=False):
     targets = targets or [None]
-    pairs = cat(
-        self.stage.collect_granular(
-            t, with_deps=with_deps, recursive=recursive
-        )
+    pairs = chain.from_iterable(
+        self.stage.collect_granular(t, with_deps=with_deps, recursive=recursive)
         for t in targets
     )
 
     return _joint_status(pairs)
 
 
 def _cloud_status(
@@ -82,24 +79,23 @@
         all_tags=all_tags,
         all_commits=all_commits,
         with_deps=with_deps,
         force=True,
         remote=remote,
         jobs=jobs,
         recursive=recursive,
+        push=True,
     )
 
     ret = {}
     for odb, obj_ids in used.items():
         if odb is not None:
             # ignore imported objects
             continue
-        status_info = self.cloud.status(
-            obj_ids, jobs, remote=remote, log_missing=False
-        )
+        status_info = self.cloud.status(obj_ids, jobs, remote=remote)
         for status_ in ("deleted", "new", "missing"):
             for hash_info in getattr(status_info, status_, []):
                 ret[hash_info.obj_name] = status_
 
     return ret
 
 
@@ -138,10 +134,8 @@
             [all_branches, all_tags, all_commits, jobs],
         )
     )
     if ignored:
         msg = "The following options are meaningless for local status: {}"
         raise InvalidArgumentError(msg.format(", ".join(ignored)))
 
-    return _local_status(
-        self, targets, with_deps=with_deps, recursive=recursive
-    )
+    return _local_status(self, targets, with_deps=with_deps, recursive=recursive)
```

### Comparing `dvc-2.9.5/dvc/repo/trie.py` & `dvc-3.0.0a0/dvc/repo/trie.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/dvc/rwlock.py` & `dvc-3.0.0a0/dvc/rwlock.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,109 +1,162 @@
 import json
+import logging
 import os
 from collections import defaultdict
 from contextlib import contextmanager
 
+import psutil
 from voluptuous import Invalid, Optional, Required, Schema
 
 from .exceptions import DvcException
-from .fs.local import localfs
+from .fs import localfs
+from .lock import make_lock
 from .utils import relpath
 
+logger = logging.getLogger(__name__)
+
+
 INFO_SCHEMA = {Required("pid"): int, Required("cmd"): str}
 
 SCHEMA = Schema(
     {
         Optional("write", default={}): {str: INFO_SCHEMA},
         Optional("read", default={}): {str: [INFO_SCHEMA]},
     }
 )
 
+RWLOCK_FILE = "rwlock"
+RWLOCK_LOCK = "rwlock.lock"
+
 
 class RWLockFileCorruptedError(DvcException):
     def __init__(self, path):
         super().__init__(
-            "Unable to read RWLock-file '{}'. JSON structure is "
-            "corrupted".format(relpath(path))
+            "Unable to read RWLock-file '{}'. JSON structure is corrupted".format(
+                relpath(path)
+            )
         )
 
 
 class RWLockFileFormatError(DvcException):
     def __init__(self, path):
         super().__init__(f"RWLock-file '{relpath(path)}' format error.")
 
 
 @contextmanager
-def _edit_rwlock(lock_dir):
-    path = os.path.join(lock_dir, "rwlock")
-    try:
-        with open(path, encoding="utf-8") as fobj:
-            lock = SCHEMA(json.load(fobj))
-    except FileNotFoundError:
-        lock = SCHEMA({})
-    except json.JSONDecodeError as exc:
-        raise RWLockFileCorruptedError(path) from exc
-    except Invalid as exc:
-        raise RWLockFileFormatError(path) from exc
-    lock["read"] = defaultdict(list, lock["read"])
-    lock["write"] = defaultdict(dict, lock["write"])
-    yield lock
-    with open(path, "w+", encoding="utf-8") as fobj:
-        json.dump(lock, fobj)
+def _edit_rwlock(lock_dir, fs, hardlink):
+    path = fs.path.join(lock_dir, RWLOCK_FILE)
+
+    rwlock_guard = make_lock(
+        fs.path.join(lock_dir, RWLOCK_LOCK),
+        tmp_dir=lock_dir,
+        hardlink_lock=hardlink,
+    )
+    with rwlock_guard:
+        try:
+            with fs.open(path, encoding="utf-8") as fobj:
+                lock = SCHEMA(json.load(fobj))
+        except FileNotFoundError:
+            lock = SCHEMA({})
+        except json.JSONDecodeError as exc:
+            raise RWLockFileCorruptedError(path) from exc
+        except Invalid as exc:
+            raise RWLockFileFormatError(path) from exc
+        lock["read"] = defaultdict(list, lock["read"])
+        lock["write"] = defaultdict(dict, lock["write"])
+        yield lock
+        with fs.open(path, "w", encoding="utf-8") as fobj:
+            json.dump(lock, fobj)
 
 
 def _infos_to_str(infos):
     return "\n".join(
         "  (PID {}): {}".format(info["pid"], info["cmd"]) for info in infos
     )
 
 
-def _check_blockers(lock, info, *, mode, waiters):
+def _check_blockers(tmp_dir, lock, info, *, mode, waiters):  # noqa: C901, PLR0912
     from .lock import LockError
 
-    for waiter_path in waiters:
-        blockers = [
-            blocker
-            for path, infos in lock[mode].items()
-            if localfs.path.overlaps(waiter_path, path)
-            for blocker in (infos if isinstance(infos, list) else [infos])
-            if blocker != info
-        ]
+    non_existing_pid = set()
 
-        if not blockers:
+    blockers = []
+    to_release = defaultdict(list)
+    for path, infos in lock[mode].items():
+        for waiter_path in waiters:
+            if localfs.path.overlaps(waiter_path, path):
+                break
+        else:
             continue
 
+        infos = infos if isinstance(infos, list) else [infos]
+        for blocker in infos:
+            if blocker == info:
+                continue
+
+            pid = int(blocker["pid"])
+
+            if pid in non_existing_pid:
+                pass
+            elif psutil.pid_exists(pid):
+                blockers.append(blocker)
+                continue
+            else:
+                non_existing_pid.add(pid)
+                cmd = blocker["cmd"]
+                logger.warning(
+                    (
+                        "Process '%s' with (Pid %s), in RWLock-file '%s'"
+                        " had been killed. Auto remove it from the lock file."
+                    ),
+                    cmd,
+                    pid,
+                    relpath(path),
+                )
+            to_release[json.dumps(blocker, sort_keys=True)].append(path)
+
+    if to_release:
+        for info_json, path_list in to_release.items():
+            info = json.loads(info_json)
+            if mode == "read":
+                _release_read(lock, info, path_list)
+            elif mode == "write":
+                _release_write(lock, info, path_list)
+
+    if blockers:
         raise LockError(
-            "'{path}' is busy, it is being blocked by:\n"
-            "{blockers}\n"
+            f"'{waiter_path}' is busy, it is being blocked by:\n"
+            f"{_infos_to_str(blockers)}\n"
             "\n"
             "If there are no processes with such PIDs, you can manually "
-            "remove '.dvc/tmp/rwlock' and try again.".format(
-                path=waiter_path, blockers=_infos_to_str(blockers)
-            )
+            f"remove '{tmp_dir}/rwlock' and try again."
         )
 
 
 def _acquire_read(lock, info, paths):
     changes = []
 
+    lock["read"] = lock.get("read", defaultdict(list))
+
     for path in paths:
         readers = lock["read"][path]
         if info in readers:
             continue
 
         changes.append(path)
         readers.append(info)
 
     return changes
 
 
 def _acquire_write(lock, info, paths):
     changes = []
 
+    lock["write"] = lock.get("write", defaultdict(dict))
+
     for path in paths:
         if lock["write"][path] == info:
             continue
 
         changes.append(path)
         lock["write"][path] = info
 
@@ -129,40 +182,41 @@
         if not lock["read"][url]:
             del lock["read"][url]
         if not lock["read"]:
             del lock["read"]
 
 
 @contextmanager
-def rwlock(tmp_dir, cmd, read, write):
+def rwlock(tmp_dir, fs, cmd, read, write, hardlink):
     """Create non-thread-safe RWLock for file paths.
 
     Args:
         tmp_dir (str): existing directory where to create the rwlock file.
+        fs (FileSystem): fs instance that tmp_dir belongs to.
         cmd (str): command that will be working on these file path.
         read ([str]): file paths that are going to be read.
         write ([str]): file paths that are going to be written.
+        hardlink (bool): use hardlink lock to guard rwlock file when on edit.
 
     Raises:
         LockError: raised if file paths we want to read is being written to by
             another command or if file paths we want to write is being written
             to or read from by another command.
         RWLockFileCorruptedError: raised if rwlock file is not a valid JSON.
         RWLockFileFormatError: raised if rwlock file is a valid JSON, but
             has internal format that doesn't pass our schema validation.
     """
     info = {"pid": os.getpid(), "cmd": cmd}
 
-    with _edit_rwlock(tmp_dir) as lock:
-
-        _check_blockers(lock, info, mode="write", waiters=read + write)
-        _check_blockers(lock, info, mode="read", waiters=write)
+    with _edit_rwlock(tmp_dir, fs, hardlink) as lock:
+        _check_blockers(tmp_dir, lock, info, mode="write", waiters=read + write)
+        _check_blockers(tmp_dir, lock, info, mode="read", waiters=write)
 
         rchanges = _acquire_read(lock, info, read)
         wchanges = _acquire_write(lock, info, write)
 
     try:
         yield
     finally:
-        with _edit_rwlock(tmp_dir) as lock:
+        with _edit_rwlock(tmp_dir, fs, hardlink) as lock:
             _release_write(lock, info, wchanges)
             _release_read(lock, info, rchanges)
```

### Comparing `dvc-2.9.5/dvc/schema.py` & `dvc-3.0.0a0/dvc/schema.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,16 +1,21 @@
 from collections.abc import Mapping
 
-from voluptuous import Any, Optional, Required, Schema
+from voluptuous import Any, Equal, Optional, Required, Schema
 
 from dvc import dependency, output
-from dvc.data.meta import Meta
-from dvc.output import CHECKSUMS_SCHEMA, Output
+from dvc.annotations import ANNOTATION_SCHEMA, ARTIFACT_SCHEMA
+from dvc.output import (
+    CHECKSUMS_SCHEMA,
+    CLOUD_SCHEMA,
+    DIR_FILES_SCHEMA,
+    META_SCHEMA,
+    Output,
+)
 from dvc.parsing import DO_KWD, FOREACH_KWD, VARS_KWD
-from dvc.parsing.versions import SCHEMA_KWD, lockfile_version_schema
 from dvc.stage.params import StageParams
 
 STAGES = "stages"
 SINGLE_STAGE_SCHEMA = {
     StageParams.PARAM_MD5: output.CHECKSUM_SCHEMA,
     StageParams.PARAM_CMD: Any(str, list, None),
     StageParams.PARAM_WDIR: Any(str, None),
@@ -21,63 +26,56 @@
     StageParams.PARAM_META: object,
     StageParams.PARAM_ALWAYS_CHANGED: bool,
     StageParams.PARAM_DESC: str,
 }
 
 DATA_SCHEMA = {
     **CHECKSUMS_SCHEMA,
+    **META_SCHEMA,
     Required("path"): str,
-    Meta.PARAM_SIZE: int,
-    Meta.PARAM_NFILES: int,
-    Meta.PARAM_ISEXEC: bool,
+    Output.PARAM_CLOUD: CLOUD_SCHEMA,
+    Output.PARAM_FILES: [DIR_FILES_SCHEMA],
 }
 LOCK_FILE_STAGE_SCHEMA = {
     Required(StageParams.PARAM_CMD): Any(str, list),
     StageParams.PARAM_DEPS: [DATA_SCHEMA],
     StageParams.PARAM_PARAMS: {str: {str: object}},
     StageParams.PARAM_OUTS: [DATA_SCHEMA],
 }
 
 LOCKFILE_STAGES_SCHEMA = {str: LOCK_FILE_STAGE_SCHEMA}
-
-LOCKFILE_V1_SCHEMA = LOCKFILE_STAGES_SCHEMA
-LOCKFILE_V2_SCHEMA = {
+LOCKFILE_SCHEMA = {
+    Required("schema"): Equal("2.0", "invalid schema version"),
     STAGES: LOCKFILE_STAGES_SCHEMA,
-    Required(SCHEMA_KWD): lockfile_version_schema,
 }
 
 OUT_PSTAGE_DETAILED_SCHEMA = {
     str: {
+        **ANNOTATION_SCHEMA,  # type: ignore[arg-type]
         Output.PARAM_CACHE: bool,
         Output.PARAM_PERSIST: bool,
         Output.PARAM_CHECKPOINT: bool,
-        Output.PARAM_DESC: str,
         Output.PARAM_REMOTE: str,
+        Output.PARAM_PUSH: bool,
     }
 }
 
+PLOTS = "plots"
 PLOT_PROPS = {
     Output.PARAM_PLOT_TEMPLATE: str,
     Output.PARAM_PLOT_X: str,
     Output.PARAM_PLOT_Y: str,
     Output.PARAM_PLOT_X_LABEL: str,
     Output.PARAM_PLOT_Y_LABEL: str,
     Output.PARAM_PLOT_TITLE: str,
     Output.PARAM_PLOT_HEADER: bool,
 }
 PLOT_PROPS_SCHEMA = {**OUT_PSTAGE_DETAILED_SCHEMA[str], **PLOT_PROPS}
 PLOT_PSTAGE_SCHEMA = {str: Any(PLOT_PROPS_SCHEMA, [PLOT_PROPS_SCHEMA])}
 
-LIVE_PROPS = {
-    Output.PARAM_LIVE_SUMMARY: bool,
-    Output.PARAM_LIVE_HTML: bool,
-}
-LIVE_PROPS_SCHEMA = {**PLOT_PROPS_SCHEMA, **LIVE_PROPS}
-LIVE_PSTAGE_SCHEMA = {str: LIVE_PROPS_SCHEMA}
-
 PARAM_PSTAGE_NON_DEFAULT_SCHEMA = {str: [str]}
 
 VARS_SCHEMA = [str, dict]
 
 STAGE_DEFINITION = {
     Required(StageParams.PARAM_CMD): Any(str, list),
     Optional(StageParams.PARAM_WDIR): str,
@@ -85,45 +83,53 @@
     Optional(StageParams.PARAM_PARAMS): [Any(str, dict)],
     Optional(VARS_KWD): VARS_SCHEMA,
     Optional(StageParams.PARAM_FROZEN): bool,
     Optional(StageParams.PARAM_META): object,
     Optional(StageParams.PARAM_DESC): str,
     Optional(StageParams.PARAM_ALWAYS_CHANGED): bool,
     Optional(StageParams.PARAM_OUTS): [Any(str, OUT_PSTAGE_DETAILED_SCHEMA)],
-    Optional(StageParams.PARAM_METRICS): [
-        Any(str, OUT_PSTAGE_DETAILED_SCHEMA)
-    ],
+    Optional(StageParams.PARAM_METRICS): [Any(str, OUT_PSTAGE_DETAILED_SCHEMA)],
     Optional(StageParams.PARAM_PLOTS): [Any(str, PLOT_PSTAGE_SCHEMA)],
-    Optional(StageParams.PARAM_LIVE): Any(str, LIVE_PSTAGE_SCHEMA),
 }
 
 
 def either_or(primary, fallback, fallback_includes=None):
     def validator(data):
         schema = primary
-        if (
-            isinstance(data, Mapping)
-            and set(fallback_includes or []) & data.keys()
-        ):
+        if isinstance(data, Mapping) and set(fallback_includes or []) & data.keys():
             schema = fallback
         return Schema(schema)(data)
 
     return validator
 
 
+PLOT_DEFINITION = {
+    Output.PARAM_PLOT_X: Any(str, {str: str}),
+    Output.PARAM_PLOT_Y: Any(str, [str], {str: Any(str, [str])}),
+    Output.PARAM_PLOT_X_LABEL: str,
+    Output.PARAM_PLOT_Y_LABEL: str,
+    Output.PARAM_PLOT_TITLE: str,
+    Output.PARAM_PLOT_TEMPLATE: str,
+}
+SINGLE_PLOT_SCHEMA = {str: Any(PLOT_DEFINITION, None)}
+ARTIFACTS = "artifacts"
+SINGLE_ARTIFACT_SCHEMA = Schema({str: ARTIFACT_SCHEMA})
 FOREACH_IN = {
     Required(FOREACH_KWD): Any(dict, list, str),
     Required(DO_KWD): STAGE_DEFINITION,
 }
 SINGLE_PIPELINE_STAGE_SCHEMA = {
     str: either_or(STAGE_DEFINITION, FOREACH_IN, [FOREACH_KWD, DO_KWD])
 }
 MULTI_STAGE_SCHEMA = {
+    PLOTS: [Any(str, SINGLE_PLOT_SCHEMA)],
     STAGES: SINGLE_PIPELINE_STAGE_SCHEMA,
     VARS_KWD: VARS_SCHEMA,
+    StageParams.PARAM_PARAMS: [str],
+    StageParams.PARAM_METRICS: [str],
+    ARTIFACTS: SINGLE_ARTIFACT_SCHEMA,
 }
 
 COMPILED_SINGLE_STAGE_SCHEMA = Schema(SINGLE_STAGE_SCHEMA)
 COMPILED_MULTI_STAGE_SCHEMA = Schema(MULTI_STAGE_SCHEMA)
 COMPILED_LOCK_FILE_STAGE_SCHEMA = Schema(LOCK_FILE_STAGE_SCHEMA)
-COMPILED_LOCKFILE_V1_SCHEMA = Schema(LOCKFILE_V1_SCHEMA)
-COMPILED_LOCKFILE_V2_SCHEMA = Schema(LOCKFILE_V2_SCHEMA)
+COMPILED_LOCKFILE_SCHEMA = Schema(LOCKFILE_SCHEMA)
```

### Comparing `dvc-2.9.5/dvc/scm.py` & `dvc-3.0.0a0/dvc/scm.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,18 +1,28 @@
 """Manages source control systems (e.g. Git)."""
+import os
 from contextlib import contextmanager
 from functools import partial
-from typing import TYPE_CHECKING, Iterator, List, Mapping, Optional
+from typing import (
+    TYPE_CHECKING,
+    Iterator,
+    List,
+    Literal,
+    Mapping,
+    Optional,
+    Union,
+    overload,
+)
 
 from funcy import group_by
 from scmrepo.base import Base  # noqa: F401, pylint: disable=unused-import
 from scmrepo.git import Git
 from scmrepo.noscm import NoSCM
 
-from dvc.exceptions import DvcException, InvalidArgumentError
+from dvc.exceptions import DvcException
 from dvc.progress import Tqdm
 
 if TYPE_CHECKING:
     from scmrepo.progress import GitProgressEvent
 
 
 class SCMError(DvcException):
@@ -56,16 +66,46 @@
     except InternalSCMError as exc:
         into = SCMError(str(exc))
         if with_cause:
             raise into from exc
         raise into
 
 
+@overload
 def SCM(
-    root_dir, search_parent_directories=True, no_scm=False
+    root_dir: str,
+    *,
+    search_parent_directories: bool = ...,
+    no_scm: Literal[False] = ...,
+) -> "Git":
+    ...
+
+
+@overload
+def SCM(
+    root_dir: str,
+    *,
+    search_parent_directories: bool = ...,
+    no_scm: Literal[True],
+) -> "NoSCM":
+    ...
+
+
+@overload
+def SCM(
+    root_dir: str,
+    *,
+    search_parent_directories: bool = ...,
+    no_scm: bool = ...,
+) -> Union["Git", "NoSCM"]:
+    ...
+
+
+def SCM(
+    root_dir, *, search_parent_directories=True, no_scm=False
 ):  # pylint: disable=invalid-name
     """Returns SCM instance that corresponds to a repo at the specified
     path.
 
     Args:
         root_dir (str): path to a root directory of the repo.
         search_parent_directories (bool): whether to look for repo root in
@@ -74,112 +114,153 @@
 
     Returns:
         dvc.scm.base.Base: SCM instance.
     """
     with map_scm_exception():
         if no_scm:
             return NoSCM(root_dir, _raise_not_implemented_as=NoSCMError)
-        return Git(
-            root_dir, search_parent_directories=search_parent_directories
-        )
+        return Git(root_dir, search_parent_directories=search_parent_directories)
 
 
 class TqdmGit(Tqdm):
+    BAR_FMT = (
+        "{desc}|{bar}|{postfix[info]}{n_fmt}/{total_fmt} [{elapsed}, {rate_fmt:>11}]"
+    )
+
     def __init__(self, *args, **kwargs):
         kwargs.setdefault("unit", "obj")
+        kwargs.setdefault("bar_format", self.BAR_FMT)
         super().__init__(*args, **kwargs)
+        self._last_phase = None
 
     def update_git(self, event: "GitProgressEvent") -> None:
         phase, completed, total, message, *_ = event
         if phase:
             message = (phase + " | " + message) if message else phase
         if message:
-            self.postfix["info"] = f" {message} | "
-        if completed:
+            self.set_msg(message)
+        force_refresh = (  # force-refresh progress bar when:
+            (total and completed and completed >= total)  # the task completes
+            or total != self.total  # the total changes
+            or phase != self._last_phase  # or, the phase changes
+        )
+        if completed is not None:
             self.update_to(completed, total)
+        if force_refresh:
+            self.refresh()
+        self._last_phase = phase
 
 
 def clone(url: str, to_path: str, **kwargs):
     from scmrepo.exceptions import CloneError as InternalCloneError
 
-    with TqdmGit(desc="Cloning") as pbar:
+    from dvc.repo.experiments.utils import fetch_all_exps
+
+    with TqdmGit(desc=f"Cloning {os.path.basename(url)}") as pbar:
         try:
-            return Git.clone(url, to_path, progress=pbar.update_git, **kwargs)
+            git = Git.clone(url, to_path, progress=pbar.update_git, **kwargs)
+            if "shallow_branch" not in kwargs:
+                fetch_all_exps(git, url, progress=pbar.update_git)
+            return git
         except InternalCloneError as exc:
-            raise CloneError(str(exc))
+            raise CloneError("SCM error") from exc
 
 
-def resolve_rev(scm: "Git", rev: str) -> str:
+def resolve_rev(scm: Union["Git", "NoSCM"], rev: str) -> str:
     from scmrepo.exceptions import RevError as InternalRevError
 
     from dvc.repo.experiments.utils import fix_exp_head
 
     try:
         return scm.resolve_rev(fix_exp_head(scm, rev))
     except InternalRevError as exc:
+        assert isinstance(scm, Git)
         # `scm` will only resolve git branch and tag names,
         # if rev is not a sha it may be an abbreviated experiment name
-        if not rev.startswith("refs/"):
-            from dvc.repo.experiments.utils import (
-                AmbiguousExpRefInfo,
-                resolve_name,
-            )
+        if not (rev == "HEAD" or rev.startswith("refs/")):
+            from dvc.repo.experiments.utils import AmbiguousExpRefInfo, resolve_name
 
             try:
                 ref_infos = resolve_name(scm, rev).get(rev)
             except AmbiguousExpRefInfo:
-                raise RevError(f"ambiguous Git revision '{rev}'")
+                raise RevError(f"ambiguous Git revision '{rev}'")  # noqa: B904
             if ref_infos:
                 return scm.get_ref(str(ref_infos))
 
-        raise RevError(str(exc))
+        raise RevError(str(exc))  # noqa: B904
+
+
+def _get_n_commits(scm: "Git", revs: List[str], num: int) -> List[str]:
+    results = []
+    for rev in revs:
+        if num == 0:
+            continue
+        results.append(rev)
+        n = 1
+        while True:
+            if num == n:
+                break
+            try:
+                head = f"{rev}~{n}"
+                results.append(resolve_rev(scm, head))
+            except RevError:
+                break
+            n += 1
+    return results
 
 
 def iter_revs(
     scm: "Git",
-    head_revs: Optional[List[str]] = None,
+    revs: Optional[List[str]] = None,
     num: int = 1,
     all_branches: bool = False,
     all_tags: bool = False,
     all_commits: bool = False,
     all_experiments: bool = False,
+    commit_date: Optional[str] = None,
 ) -> Mapping[str, List[str]]:
+    from scmrepo.exceptions import SCMError as _SCMError
 
-    if num < 1 and num != -1:
-        raise InvalidArgumentError(f"Invalid number of commits '{num}'")
+    from dvc.repo.experiments.utils import exp_commits
 
     if not any(
-        [head_revs, all_branches, all_tags, all_commits, all_experiments]
+        [
+            revs,
+            all_branches,
+            all_tags,
+            all_commits,
+            all_experiments,
+            commit_date,
+        ]
     ):
         return {}
 
-    head_revs = head_revs or []
-    revs = []
-    for rev in head_revs:
-        revs.append(rev)
-        n = 1
-        while True:
-            if num == n:
-                break
-            try:
-                head = f"{rev}~{n}"
-                revs.append(resolve_rev(scm, head))
-            except RevError:
-                break
-            n += 1
+    revs = revs or []
+    results: List[str] = _get_n_commits(scm, revs, num)
 
     if all_commits:
-        revs.extend(scm.list_all_commits())
+        results.extend(scm.list_all_commits())
     else:
         if all_branches:
-            revs.extend(scm.list_branches())
+            results.extend(scm.list_branches())
 
         if all_tags:
-            revs.extend(scm.list_tags())
+            results.extend(scm.list_tags())
 
-    if all_experiments:
-        from dvc.repo.experiments.utils import exp_commits
+        if commit_date:
+            from datetime import datetime
 
-        revs.extend(exp_commits(scm))
+            commit_datestamp = datetime.strptime(commit_date, "%Y-%m-%d").timestamp()
+
+            def _time_filter(rev):
+                try:
+                    return scm.resolve_commit(rev).commit_time >= commit_datestamp
+                except _SCMError:
+                    return True
+
+            results.extend(filter(_time_filter, scm.list_all_commits()))
+
+    if all_experiments:
+        results.extend(exp_commits(scm))
 
     rev_resolver = partial(resolve_rev, scm)
-    return group_by(rev_resolver, revs)
+    return group_by(rev_resolver, results)
```

### Comparing `dvc-2.9.5/dvc/stage/__init__.py` & `dvc-3.0.0a0/dvc/stage/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,33 @@
 import logging
 import os
 import string
 from collections import defaultdict
 from contextlib import suppress
 from dataclasses import dataclass
-from typing import TYPE_CHECKING, Any, Dict, Optional, Set
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Dict,
+    Iterable,
+    List,
+    Optional,
+    Set,
+    Tuple,
+    Type,
+    TypeVar,
+    Union,
+)
 
-from funcy import cached_property, project
+from funcy import project
 
 from dvc import prompt
-from dvc.exceptions import (
-    CacheLinkError,
-    CheckoutError,
-    DvcException,
-    MergeError,
-)
+from dvc.exceptions import CacheLinkError, CheckoutError, DvcException, MergeError
 from dvc.utils import relpath
+from dvc.utils.objects import cached_property
 
 from . import params
 from .decorators import rwlocked
 from .exceptions import StageUpdateError
 from .imports import sync_import, update_import
 from .run import run_stage
 from .utils import (
@@ -31,25 +39,33 @@
     compute_md5,
     fill_stage_dependencies,
     fill_stage_outputs,
     get_dump,
 )
 
 if TYPE_CHECKING:
-    from dvc.dvcfile import DVCFile
-    from dvc.hash_info import HashInfo
-    from dvc.objects.db.base import ObjectDB
+    from dvc.dvcfile import ProjectFile, SingleStageFile
+    from dvc.output import Output
+    from dvc.repo import Repo
+    from dvc.types import StrPath
+    from dvc_data.hashfile.hash_info import HashInfo
+    from dvc_objects.db import ObjectDB
 
 logger = logging.getLogger(__name__)
 # Disallow all punctuation characters except hyphen and underscore
 INVALID_STAGENAME_CHARS = set(string.punctuation) - {"_", "-"}
 Env = Dict[str, str]
+ChangedEntries = Tuple[List[str], List[str], Optional[str]]
 
+_T = TypeVar("_T")
 
-def loads_from(cls, repo, path, wdir, data):
+
+def loads_from(
+    cls: Type[_T], repo: "Repo", path: str, wdir: str, data: Dict[str, Any]
+) -> _T:
     kw = {
         "repo": repo,
         "path": path,
         "wdir": wdir,
         **project(
             data,
             [
@@ -69,65 +85,66 @@
 
 @dataclass
 class RawData:
     parametrized: bool = False
     generated_from: Optional[str] = None
 
 
-def create_stage(cls, repo, path, external=False, **kwargs):
+def create_stage(cls: Type[_T], repo, path, external=False, **kwargs) -> _T:
     from dvc.dvcfile import check_dvcfile_path
 
     wdir = os.path.abspath(kwargs.get("wdir", None) or os.curdir)
     path = os.path.abspath(path)
+
     check_dvcfile_path(repo, path)
     check_stage_path(repo, wdir, is_wdir=kwargs.get("wdir"))
     check_stage_path(repo, os.path.dirname(path))
 
     stage = loads_from(cls, repo, path, wdir, kwargs)
     fill_stage_outputs(stage, **kwargs)
     if not external:
         check_no_externals(stage)
     fill_stage_dependencies(
-        stage, **project(kwargs, ["deps", "erepo", "params"])
+        stage, **project(kwargs, ["deps", "erepo", "params", "fs_config"])
     )
     check_circular_dependency(stage)
     check_duplicated_arguments(stage)
 
     return stage
 
 
-def restore_meta(stage):
+def restore_fields(stage: "Stage") -> None:
     from .exceptions import StageNotFound
 
     if not stage.dvcfile.exists():
         return
 
     try:
         old = stage.reload()
     except StageNotFound:
         return
 
     # will be used to restore comments later
-    # noqa, pylint: disable=protected-access
+    # pylint: disable=protected-access
     stage._stage_text = old._stage_text
-
     stage.meta = old.meta
     stage.desc = old.desc
 
-    old_desc = {out.def_path: out.desc for out in old.outs}
-
+    old_outs = {out.def_path: out for out in old.outs}
     for out in stage.outs:
-        out.desc = old_desc.get(out.def_path, None)
+        old_out = old_outs.get(out.def_path, None)
+        if old_out is not None:
+            out.restore_fields(old_out)
 
 
 class Stage(params.StageParams):
     # pylint:disable=no-value-for-parameter
     # rwlocked() confuses pylint
 
-    def __init__(
+    def __init__(  # noqa: PLR0913
         self,
         repo,
         path=None,
         cmd=None,
         wdir=os.curdir,
         deps=None,
         outs=None,
@@ -167,31 +184,30 @@
     @path.setter
     def path(self, path: str):
         self._path = path
         self.__dict__.pop("path_in_repo", None)
         self.__dict__.pop("relpath", None)
 
     @property
-    def dvcfile(self) -> "DVCFile":
+    def dvcfile(self) -> Union["ProjectFile", "SingleStageFile"]:
         if self.path and self._dvcfile and self.path == self._dvcfile.path:
             return self._dvcfile
 
         if not self.path:
             raise DvcException(
-                "Stage does not have any path set "
-                "and is detached from dvcfile."
+                "Stage does not have any path set and is detached from dvcfile."
             )
 
-        from dvc.dvcfile import make_dvcfile
+        from dvc.dvcfile import load_file
 
-        self._dvcfile = make_dvcfile(self.repo, self.path)
+        self._dvcfile = load_file(self.repo, self.path)
         return self._dvcfile
 
     @dvcfile.setter
-    def dvcfile(self, dvcfile: "DVCFile") -> None:
+    def dvcfile(self, dvcfile: Union["ProjectFile", "SingleStageFile"]) -> None:
         self._dvcfile = dvcfile
 
     def __repr__(self):
         return f"Stage: '{self.addressing}'"
 
     def __str__(self):
         return f"stage: '{self.addressing}'"
@@ -211,83 +227,80 @@
         return (
             self.__class__ == other.__class__
             and self.repo is other.repo
             and self.path_in_repo == other.path_in_repo
         )
 
     @cached_property
-    def path_in_repo(self):
+    def path_in_repo(self) -> str:
         return relpath(self.path, self.repo.root_dir)
 
     @cached_property
-    def relpath(self):
+    def relpath(self) -> str:
         return relpath(self.path)
 
     @property
-    def is_data_source(self):
+    def is_data_source(self) -> bool:
         """Whether the DVC file was created with `dvc add` or `dvc import`"""
         return self.cmd is None
 
     @property
-    def is_callback(self):
+    def is_callback(self) -> bool:
         """
         A callback stage is always considered as changed,
         so it runs on every `dvc repro` call.
         """
         return self.cmd and not any((self.deps, self.outs))
 
     @property
-    def is_import(self):
+    def is_import(self) -> bool:
         """Whether the DVC file was created with `dvc import`."""
         return not self.cmd and len(self.deps) == 1 and len(self.outs) == 1
 
     @property
-    def is_repo_import(self):
+    def is_partial_import(self) -> bool:
+        """
+        Whether the DVC file was created using `dvc import --no-download`
+        or `dvc import-url --no-download`.
+        """
+        return self.is_import and (not self.outs[0].hash_info)
+
+    @property
+    def is_repo_import(self) -> bool:
         if not self.is_import:
             return False
 
         from dvc.dependency import RepoDependency
 
         return isinstance(self.deps[0], RepoDependency)
 
     @property
-    def is_checkpoint(self):
+    def is_versioned_import(self) -> bool:
+        return self.is_import and self.deps[0].fs.version_aware
+
+    @property
+    def is_checkpoint(self) -> bool:
         """
         A stage containing checkpoint outs is always considered as changed
         since the checkpoint out is a circular dependency.
         """
         return any(out.checkpoint for out in self.outs)
 
     def short_description(self) -> Optional["str"]:
         desc: Optional["str"] = None
         if self.desc:
             with suppress(ValueError):
                 # try to use first non-empty line as a description
                 line = next(filter(None, self.desc.splitlines()))
-                desc = line.strip()
+                return line.strip()
         return desc
 
     def _read_env(self, out, checkpoint_func=None) -> Env:
         env: Env = {}
-        if out.live:
-            from dvc.env import DVCLIVE_HTML, DVCLIVE_PATH, DVCLIVE_SUMMARY
-            from dvc.output import Output
-            from dvc.schema import LIVE_PROPS
-
-            env[DVCLIVE_PATH] = relpath(out.fs_path, self.wdir)
-            if isinstance(out.live, dict):
-                config = project(out.live, LIVE_PROPS)
-
-                env[DVCLIVE_SUMMARY] = str(
-                    int(config.get(Output.PARAM_LIVE_SUMMARY, True))
-                )
-                env[DVCLIVE_HTML] = str(
-                    int(config.get(Output.PARAM_LIVE_HTML, True))
-                )
-        elif out.checkpoint and checkpoint_func:
+        if out.checkpoint and checkpoint_func:
             from dvc.env import DVC_CHECKPOINT
 
             env.update({DVC_CHECKPOINT: "1"})
         return env
 
     def env(self, checkpoint_func=None) -> Env:
         from dvc.env import DVC_ROOT
@@ -302,386 +315,467 @@
                 env.get(key) != current.get(key)
                 for key in set(env.keys()).intersection(current.keys())
             ):
                 raise DvcException("Conflicting values for env variable")
             env.update(current)
         return env
 
-    def changed_deps(self):
+    def changed_deps(self, allow_missing: bool = False) -> bool:
         if self.frozen:
             return False
 
         if self.is_callback or self.always_changed or self.is_checkpoint:
             return True
 
-        return self._changed_deps()
+        return self._changed_deps(allow_missing=allow_missing)
 
     @rwlocked(read=["deps"])
-    def _changed_deps(self):
+    def _changed_deps(self, allow_missing: bool = False) -> bool:
         for dep in self.deps:
             status = dep.status()
             if status:
+                if allow_missing and status[str(dep)] == "deleted":
+                    continue
                 logger.debug(
-                    "Dependency '{dep}' of {stage} changed because it is "
-                    "'{status}'.".format(
-                        dep=dep, stage=self, status=status[str(dep)]
-                    )
+                    "Dependency '%s' of %s changed because it is '%s'.",
+                    dep,
+                    self,
+                    status[str(dep)],
                 )
                 return True
         return False
 
     @rwlocked(read=["outs"])
-    def changed_outs(self):
+    def changed_outs(self, allow_missing: bool = False) -> bool:
         for out in self.outs:
             status = out.status()
             if status:
+                if allow_missing and status[str(out)] == "not in cache":
+                    continue
                 logger.debug(
-                    "Output '{out}' of {stage} changed because it is "
-                    "'{status}'".format(
-                        out=out, stage=self, status=status[str(out)]
-                    )
+                    "Output '%s' of %s changed because it is '%s'.",
+                    out,
+                    self,
+                    status[str(out)],
                 )
                 return True
 
         return False
 
-    def changed_stage(self):
+    def changed_stage(self) -> bool:
         changed = self.md5 != self.compute_md5()
         if changed:
             logger.debug(self._changed_stage_entry())
         return changed
 
     @rwlocked(read=["deps", "outs"])
-    def changed(self):
+    def changed(self, allow_missing: bool = False) -> bool:
         is_changed = (
             # Short-circuit order: stage md5 is fast,
             # deps are expected to change
             self.changed_stage()
-            or self.changed_deps()
-            or self.changed_outs()
+            or self.changed_deps(allow_missing=allow_missing)
+            or self.changed_outs(allow_missing=allow_missing)
         )
         if is_changed:
             logger.debug("%s changed.", self)
         return is_changed
 
     @rwlocked(write=["outs"])
-    def remove_outs(self, ignore_remove=False, force=False):
+    def remove_outs(self, ignore_remove=False, force=False) -> None:
         """Used mainly for `dvc remove --outs` and :func:`Stage.reproduce`."""
         for out in self.outs:
-            if (out.persist or out.checkpoint or out.live) and not force:
+            if (out.persist or out.checkpoint) and not force:
                 out.unprotect()
                 continue
 
-            logger.debug(f"Removing output '{out}' of {self}.")
+            logger.debug("Removing output '%s' of %s.", out, self)
             out.remove(ignore_remove=ignore_remove)
 
-    def unprotect_outs(self):
+    def unprotect_outs(self) -> None:
         for out in self.outs:
             out.unprotect()
 
-    def ignore_remove_outs(self):
+    def ignore_remove_outs(self) -> None:
         for out in self.outs:
             out.ignore_remove()
 
     @rwlocked(write=["outs"])
-    def remove(self, force=False, remove_outs=True, purge=True):
+    def remove(self, force=False, remove_outs=True, purge=True) -> None:
         if remove_outs:
             self.remove_outs(ignore_remove=True, force=force)
         else:
             self.unprotect_outs()
             self.ignore_remove_outs()
         if purge:
             self.dvcfile.remove_stage(self)
 
     def transfer(
         self,
         source: str,
-        odb: "ObjectDB" = None,
+        odb: Optional["ObjectDB"] = None,
         to_remote: bool = False,
         **kwargs: Any,
     ) -> None:
         assert len(self.outs) == 1
         (out,) = self.outs
         out.transfer(source, odb=odb, jobs=kwargs.get("jobs"))
         if not to_remote:
-            out.checkout()
+            out.checkout(force=kwargs.get("force"))
+            out.ignore()
 
     @rwlocked(read=["deps"], write=["outs"])
-    def reproduce(self, interactive=False, **kwargs):
-        if not (kwargs.get("force", False) or self.changed()):
+    def reproduce(self, interactive=False, **kwargs) -> Optional["Stage"]:
+        if not (
+            kwargs.get("force", False)
+            or self.changed(kwargs.get("allow_missing", False))
+        ):
             if not isinstance(self, PipelineStage) and self.is_data_source:
                 logger.info("'%s' didn't change, skipping", self.addressing)
             else:
-                logger.info(
-                    "Stage '%s' didn't change, skipping", self.addressing
-                )
+                logger.info("Stage '%s' didn't change, skipping", self.addressing)
             return None
 
-        msg = (
-            "Going to reproduce {stage}. "
-            "Are you sure you want to continue?".format(stage=self)
+        msg = "Going to reproduce {stage}. Are you sure you want to continue?".format(
+            stage=self
         )
 
         if interactive and not prompt.confirm(msg):
             raise DvcException("reproduction aborted by the user")
 
         self.run(**kwargs)
 
-        logger.debug(f"{self} was reproduced")
+        logger.debug("%s was reproduced", self)
 
         return self
 
-    def update(self, rev=None, to_remote=False, remote=None, jobs=None):
+    def update(
+        self,
+        rev=None,
+        to_remote=False,
+        remote=None,
+        no_download=None,
+        jobs=None,
+    ) -> None:
         if not (self.is_repo_import or self.is_import):
             raise StageUpdateError(self.relpath)
         update_import(
-            self, rev=rev, to_remote=to_remote, remote=remote, jobs=jobs
+            self,
+            rev=rev,
+            to_remote=to_remote,
+            remote=remote,
+            no_download=no_download,
+            jobs=jobs,
         )
 
-    def reload(self):
+    def reload(self) -> "Stage":
         return self.dvcfile.stage
 
-    def dumpd(self):
-        return get_dump(self)
+    def dumpd(self, **kwargs) -> Dict[str, Any]:
+        return get_dump(self, **kwargs)
 
-    def compute_md5(self):
+    def compute_md5(self) -> Optional[str]:
         # `dvc add`ed files don't need stage md5
         if self.is_data_source and not (self.is_import or self.is_repo_import):
             m = None
         else:
             m = compute_md5(self)
-        logger.debug(f"Computed {self} md5: '{m}'")
+        logger.debug("Computed %s md5: '%s'", self, m)
         return m
 
-    def save(self, allow_missing=False):
+    def save(self, allow_missing: bool = False, run_cache: bool = True):
         self.save_deps(allow_missing=allow_missing)
+
         self.save_outs(allow_missing=allow_missing)
+
         self.md5 = self.compute_md5()
 
-        self.repo.stage_cache.save(self)
+        if run_cache:
+            self.repo.stage_cache.save(self)
 
     def save_deps(self, allow_missing=False):
         from dvc.dependency.base import DependencyDoesNotExistError
 
         for dep in self.deps:
             try:
                 dep.save()
             except DependencyDoesNotExistError:
                 if not allow_missing:
                     raise
 
-    def save_outs(self, allow_missing=False):
+    def get_versioned_outs(self) -> Dict[str, "Output"]:
+        from .exceptions import StageFileDoesNotExistError, StageNotFound
+
+        try:
+            old = self.reload()
+        except (StageFileDoesNotExistError, StageNotFound):
+            return {}
+
+        return {
+            out.def_path: out
+            for out in old.outs
+            if out.files is not None
+            or (out.meta is not None and out.meta.version_id is not None)
+        }
+
+    def save_outs(self, allow_missing: bool = False):
         from dvc.output import OutputDoesNotExistError
 
+        old_versioned_outs = self.get_versioned_outs()
         for out in self.outs:
             try:
                 out.save()
             except OutputDoesNotExistError:
                 if not (allow_missing or out.checkpoint):
                     raise
 
-    def ignore_outs(self):
+            if old_out := old_versioned_outs.get(out.def_path):
+                out.merge_version_meta(old_out)
+
+    def ignore_outs(self) -> None:
         for out in self.outs:
             out.ignore()
 
     @staticmethod
-    def _changed_entries(entries):
+    def _changed_entries(entries) -> List[str]:
         return [str(entry) for entry in entries if entry.workspace_status()]
 
-    def _changed_stage_entry(self):
+    def _changed_stage_entry(self) -> str:
         return f"'md5' of {self} changed."
 
-    def changed_entries(self):
+    def changed_entries(self) -> ChangedEntries:
         changed_deps = self._changed_entries(self.deps)
         changed_outs = self._changed_entries(self.outs)
         return (
             changed_deps,
             changed_outs,
             self._changed_stage_entry() if self.changed_stage() else None,
         )
 
     @rwlocked(write=["outs"])
-    def commit(self, allow_missing=False, filter_info=None):
+    def commit(self, allow_missing=False, filter_info=None, **kwargs) -> None:
         from dvc.output import OutputDoesNotExistError
 
         link_failures = []
         for out in self.filter_outs(filter_info):
             try:
-                out.commit(filter_info=filter_info)
+                out.commit(filter_info=filter_info, **kwargs)
             except OutputDoesNotExistError:
                 if not (allow_missing or out.checkpoint):
                     raise
             except CacheLinkError:
                 link_failures.append(out.fs_path)
         if link_failures:
             raise CacheLinkError(link_failures)
 
+    @rwlocked(write=["outs"])
+    def add_outs(  # noqa: C901
+        self, filter_info=None, allow_missing: bool = False, **kwargs
+    ):
+        from dvc.output import OutputDoesNotExistError
+
+        link_failures = []
+        old_versioned_outs = self.get_versioned_outs()
+        for out in self.filter_outs(filter_info):
+            try:
+                out.add(filter_info, **kwargs)
+            except (FileNotFoundError, OutputDoesNotExistError):
+                if not (allow_missing or out.checkpoint):
+                    raise
+            except CacheLinkError:
+                link_failures.append(filter_info or out.fs_path)
+
+            if old_out := old_versioned_outs.get(out.def_path):
+                out.merge_version_meta(old_out)
+
+        if link_failures:
+            raise CacheLinkError(link_failures)
+
     @rwlocked(read=["deps", "outs"])
-    def run(
+    def run(  # noqa: C901
         self,
         dry=False,
         no_commit=False,
         force=False,
         allow_missing=False,
+        no_download=False,
         **kwargs,
-    ):
+    ) -> None:
         if (self.cmd or self.is_import) and not self.frozen and not dry:
             self.remove_outs(ignore_remove=False, force=False)
 
-        if not self.frozen and self.is_import:
-            jobs = kwargs.get("jobs", None)
-            self._sync_import(dry, force, jobs)
+        if (
+            self.is_import and (not self.frozen or kwargs.get("pull"))
+        ) or self.is_partial_import:
+            self._sync_import(dry, force, kwargs.get("jobs", None), no_download)
         elif not self.frozen and self.cmd:
-            self._run_stage(dry, force, **kwargs)
-        else:
-            args = (
-                ("outputs", "frozen ") if self.frozen else ("data sources", "")
-            )
+            self._run_stage(dry, force, allow_missing=allow_missing, **kwargs)
+        elif kwargs.get("pull"):
+            logger.info("Pulling data for %s", self)
+            self.repo.pull(self.addressing, jobs=kwargs.get("jobs", None))
+            self.checkout()
+        elif not dry:
+            args = ("outputs", "frozen ") if self.frozen else ("data sources", "")
             logger.info("Verifying %s in %s%s", *args, self)
-            if not dry:
-                self._check_missing_outputs()
+            self._check_missing_outputs()
 
         if not dry:
-            if kwargs.get("checkpoint_func", None):
+            if kwargs.get("checkpoint_func", None) or no_download:
                 allow_missing = True
-            self.save(allow_missing=allow_missing)
+
+            no_cache_outs = any(
+                not out.use_cache
+                for out in self.outs
+                if not (out.is_metric or out.is_plot)
+            )
+            self.save(
+                allow_missing=allow_missing,
+                run_cache=not no_commit and not no_cache_outs,
+            )
+
+            if no_download:
+                self.ignore_outs()
             if not no_commit:
                 self.commit(allow_missing=allow_missing)
 
     @rwlocked(read=["deps"], write=["outs"])
-    def _run_stage(self, dry, force, **kwargs):
+    def _run_stage(self, dry, force, **kwargs) -> None:
         return run_stage(self, dry, force, **kwargs)
 
     @rwlocked(read=["deps"], write=["outs"])
-    def _sync_import(self, dry, force, jobs):
-        sync_import(self, dry, force, jobs)
+    def _sync_import(self, dry, force, jobs, no_download) -> None:
+        sync_import(self, dry, force, jobs, no_download)
 
     @rwlocked(read=["outs"])
-    def _check_missing_outputs(self):
+    def _check_missing_outputs(self) -> None:
         check_missing_outputs(self)
 
-    def filter_outs(self, fs_path):
+    def filter_outs(self, fs_path) -> Iterable["Output"]:
         def _func(o):
             return o.fs.path.isin_or_eq(fs_path, o.fs_path)
 
         return filter(_func, self.outs) if fs_path else self.outs
 
     @rwlocked(write=["outs"])
-    def checkout(self, allow_missing=False, **kwargs):
-        stats = defaultdict(list)
+    def checkout(
+        self, allow_missing: bool = False, **kwargs
+    ) -> Dict[str, List["StrPath"]]:
+        stats: Dict[str, List["StrPath"]] = defaultdict(list)
+        if self.is_partial_import:
+            return stats
+
         for out in self.filter_outs(kwargs.get("filter_info")):
             key, outs = self._checkout(
                 out,
                 allow_missing=allow_missing or self.is_checkpoint,
                 **kwargs,
             )
             if key:
                 stats[key].extend(outs)
         return stats
 
     @staticmethod
-    def _checkout(out, **kwargs):
+    def _checkout(out, **kwargs) -> Tuple[Optional[str], List[str]]:
         try:
             result = out.checkout(**kwargs)
             added, modified = result or (None, None)
             if not (added or modified):
                 return None, []
             return "modified" if modified else "added", [str(out)]
         except CheckoutError as exc:
             return "failed", exc.target_infos
 
     @rwlocked(read=["deps", "outs"])
-    def status(self, check_updates=False, filter_info=None):
-        ret = []
-        show_import = self.is_repo_import and check_updates
+    def status(
+        self, check_updates: bool = False, filter_info: Optional[bool] = None
+    ) -> Dict[str, List[Union[str, Dict[str, str]]]]:
+        ret: List[Union[str, Dict[str, str]]] = []
+        show_import = (
+            self.is_repo_import or self.is_versioned_import
+        ) and check_updates
 
         if not self.frozen or show_import:
             self._status_deps(ret)
         self._status_outs(ret, filter_info=filter_info)
         self._status_always_changed(ret)
         self._status_stage(ret)
         return {self.addressing: ret} if ret else {}
 
     @staticmethod
-    def _status(entries):
+    def _status(entries: Iterable["Output"]) -> Dict[str, str]:
         ret = {}
 
         for entry in entries:
             ret.update(entry.status())
 
         return ret
 
-    def _status_deps(self, ret):
+    def _status_deps(self, ret) -> None:
         deps_status = self._status(self.deps)
         if deps_status:
             ret.append({"changed deps": deps_status})
 
-    def _status_outs(self, ret, filter_info):
+    def _status_outs(self, ret, filter_info) -> None:
         filter_outs = self.filter_outs(filter_info)
         outs_status = self._status(filter_outs)
         if outs_status:
             ret.append({"changed outs": outs_status})
 
-    def _status_always_changed(self, ret):
+    def _status_always_changed(self, ret) -> None:
         if self.is_callback or self.always_changed or self.is_checkpoint:
             ret.append("always changed")
 
-    def _status_stage(self, ret):
+    def _status_stage(self, ret) -> None:
         if self.changed_stage():
             ret.append("changed checksum")
 
-    def already_cached(self):
-        return (
-            not self.changed_stage()
-            and self.deps_cached()
-            and self.outs_cached()
-        )
+    def already_cached(self) -> bool:
+        return not self.changed_stage() and self.deps_cached() and self.outs_cached()
 
-    def deps_cached(self):
+    def deps_cached(self) -> bool:
         return all(not dep.changed() for dep in self.deps)
 
-    def outs_cached(self):
+    def outs_cached(self) -> bool:
         return all(
             not out.changed_cache() if out.use_cache else not out.changed()
             for out in self.outs
         )
 
-    def get_all_files_number(self, filter_info=None):
+    def get_all_files_number(self, filter_info=None) -> int:
         return sum(
-            out.get_files_number(filter_info)
-            for out in self.filter_outs(filter_info)
+            out.get_files_number(filter_info) for out in self.filter_outs(filter_info)
         )
 
     def get_used_objs(
         self, *args, **kwargs
     ) -> Dict[Optional["ObjectDB"], Set["HashInfo"]]:
         """Return set of object IDs used by this stage."""
+        if self.is_partial_import and not self.is_repo_import:
+            return {}
+
         used_objs = defaultdict(set)
         for out in self.filter_outs(kwargs.get("filter_info")):
             for odb, objs in out.get_used_objs(*args, **kwargs).items():
                 used_objs[odb].update(objs)
         return used_objs
 
     @staticmethod
-    def _check_can_merge(stage, ancestor_out=None):
+    def _check_can_merge(stage, ancestor_out=None) -> None:
         if isinstance(stage, PipelineStage):
             raise MergeError("unable to auto-merge pipeline stages")
 
         if not stage.is_data_source or stage.deps or len(stage.outs) > 1:
             raise MergeError(
-                "unable to auto-merge DVC files that weren't "
-                "created by `dvc add`"
+                "unable to auto-merge DVC files that weren't created by `dvc add`"
             )
 
         if ancestor_out and not stage.outs:
-            raise MergeError(
-                "unable to auto-merge DVC files with deleted outputs"
-            )
+            raise MergeError("unable to auto-merge DVC files with deleted outputs")
 
-    def merge(self, ancestor, other):
+    def merge(self, ancestor, other, allowed=None) -> None:
         assert other
 
         if not other.outs:
             return
 
         if not self.outs:
             self.outs = other.outs
@@ -693,51 +787,56 @@
             ancestor_out = outs[0] if outs else None
         else:
             ancestor_out = None
 
         self._check_can_merge(self, ancestor_out)
         self._check_can_merge(other, ancestor_out)
 
-        self.outs[0].merge(ancestor_out, other.outs[0])
+        self.outs[0].merge(ancestor_out, other.outs[0], allowed=allowed)
 
-    def dump(self, **kwargs):
+    def dump(self, **kwargs) -> None:
         self.dvcfile.dump(self, **kwargs)
 
 
 class PipelineStage(Stage):
-    def __init__(self, *args, name=None, **kwargs):
+    def __init__(self, *args, name: Optional[str] = None, **kwargs):
         super().__init__(*args, **kwargs)
         self.name = name
         self.cmd_changed = False
-        self.tracked_vars = {}
+        self.tracked_vars: Dict[str, Dict[str, Dict[str, str]]] = {}
 
     def __eq__(self, other):
         return super().__eq__(other) and self.name == other.name
 
-    def __hash__(self):
+    def __hash__(self) -> int:
         return hash((self.path_in_repo, self.name))
 
     @property
     def addressing(self):
-        from dvc.dvcfile import PIPELINE_FILE
+        from dvc.dvcfile import PROJECT_FILE
 
-        if self.path and self.relpath == PIPELINE_FILE:
+        if self.path and self.relpath == PROJECT_FILE:
             return self.name
         return f"{super().addressing}:{self.name}"
 
-    def reload(self):
+    def reload(self) -> Stage:
+        from dvc.dvcfile import ProjectFile
+
+        assert isinstance(self.dvcfile, ProjectFile)
+
+        self.dvcfile._reset()  # pylint: disable=protected-access
         return self.dvcfile.stages[self.name]
 
-    def _status_stage(self, ret):
+    def _status_stage(self, ret) -> None:
         if self.cmd_changed:
             ret.append("changed command")
 
-    def changed_stage(self):
+    def changed_stage(self) -> bool:
         if self.cmd_changed:
             logger.debug(self._changed_stage_entry())
         return self.cmd_changed
 
-    def _changed_stage_entry(self):
+    def _changed_stage_entry(self) -> str:
         return f"'cmd' of {self} has changed."
 
-    def merge(self, ancestor, other):
+    def merge(self, ancestor, other, allowed=None):
         raise NotImplementedError
```

### Comparing `dvc-2.9.5/dvc/stage/cache.py` & `dvc-3.0.0a0/dvc/stage/cache.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,50 +1,54 @@
 import logging
 import os
-import tempfile
 from contextlib import contextmanager
-from typing import TYPE_CHECKING, Optional
+from typing import TYPE_CHECKING, List, Optional, Tuple
 
-from funcy import cached_property, first
+from funcy import first
 
+from dvc import fs
 from dvc.exceptions import DvcException
 from dvc.utils import dict_sha256, relpath
 
 if TYPE_CHECKING:
-    from dvc.objects.db.base import ObjectDB
+    from dvc_objects.db import ObjectDB
 
 logger = logging.getLogger(__name__)
 
 
 class RunCacheNotFoundError(DvcException):
     def __init__(self, stage):
         super().__init__(f"No run-cache for {stage.addressing}")
 
 
+class RunCacheNotSupported(DvcException):
+    pass
+
+
 def _get_cache_hash(cache, key=False):
-    from dvc.data.meta import Meta
+    from dvc_data.hashfile.meta import Meta
 
     if key:
         cache["outs"] = [out["path"] for out in cache.get("outs", [])]
     return dict_sha256(cache, exclude=[Meta.PARAM_SIZE, Meta.PARAM_NFILES])
 
 
 def _can_hash(stage):
     if stage.is_callback or stage.always_changed:
         return False
 
     if not all([stage.cmd, stage.deps, stage.outs]):
         return False
 
     for dep in stage.deps:
-        if not (dep.scheme == "local" and dep.def_path and dep.get_hash()):
+        if not (dep.protocol == "local" and dep.def_path and dep.get_hash()):
             return False
 
     for out in stage.outs:
-        if out.scheme != "local" or not out.def_path or out.persist:
+        if out.protocol != "local" or not out.def_path or out.persist:
             return False
 
     return True
 
 
 def _get_stage_hash(stage):
     from .serialize import to_single_stage_lockfile
@@ -52,18 +56,15 @@
     assert _can_hash(stage)
     return _get_cache_hash(to_single_stage_lockfile(stage), key=True)
 
 
 class StageCache:
     def __init__(self, repo):
         self.repo = repo
-
-    @cached_property
-    def cache_dir(self):
-        return os.path.join(self.repo.odb.local.cache_dir, "runs")
+        self.cache_dir = os.path.join(self.repo.cache.local.path, "runs")
 
     def _get_cache_dir(self, key):
         return os.path.join(self.cache_dir, key[:2], key)
 
     def _get_cache_path(self, key, value):
         return os.path.join(self._get_cache_dir(key), value)
 
@@ -114,30 +115,28 @@
             external=True,
         )
         StageLoader.fill_from_lock(stage, cache)
         return stage
 
     @contextmanager
     def _cache_type_copy(self):
-        cache_types = self.repo.odb.local.cache_types
-        self.repo.odb.local.cache_types = ["copy"]
+        cache_types = self.repo.cache.local.cache_types
+        self.repo.cache.local.cache_types = ["copy"]
         try:
             yield
         finally:
-            self.repo.odb.local.cache_types = cache_types
+            self.repo.cache.local.cache_types = cache_types
 
     def _uncached_outs(self, stage, cache):
         # NOTE: using temporary stage to avoid accidentally modifying original
         # stage and to workaround `commit/checkout` not working for uncached
         # outputs.
         cached_stage = self._create_stage(cache, wdir=stage.wdir)
 
-        outs_no_cache = [
-            out.def_path for out in stage.outs if not out.use_cache
-        ]
+        outs_no_cache = [out.def_path for out in stage.outs if not out.use_cache]
 
         # NOTE: using copy link to make it look like a git-tracked file
         with self._cache_type_copy():
             for out in cached_stage.outs:
                 if out.def_path in outs_no_cache:
                     yield out
 
@@ -163,102 +162,112 @@
         from dvc.schema import COMPILED_LOCK_FILE_STAGE_SCHEMA
         from dvc.utils.serialize import dump_yaml
 
         # sanity check
         COMPILED_LOCK_FILE_STAGE_SCHEMA(cache)
 
         path = self._get_cache_path(cache_key, cache_value)
-        parent = self.repo.odb.local.fs.path.parent(path)
-        self.repo.odb.local.makedirs(parent)
-        tmp = tempfile.NamedTemporaryFile(delete=False, dir=parent).name
+        local_fs = self.repo.cache.local.fs
+        parent = local_fs.path.parent(path)
+        self.repo.cache.local.makedirs(parent)
+        tmp = local_fs.path.join(parent, fs.utils.tmp_fname())
         assert os.path.exists(parent)
         assert os.path.isdir(parent)
         dump_yaml(tmp, cache)
-        self.repo.odb.local.move(tmp, path)
+        self.repo.cache.local.move(tmp, path)
 
-    def restore(self, stage, run_cache=True, pull=False):
+    def restore(self, stage, run_cache=True, pull=False, dry=False):
         from .serialize import to_single_stage_lockfile
 
         if not _can_hash(stage):
             raise RunCacheNotFoundError(stage)
 
         if (
             not stage.changed_stage()
             and stage.deps_cached()
             and all(bool(out.hash_info) for out in stage.outs)
         ):
             cache = to_single_stage_lockfile(stage)
         else:
             if not run_cache:  # backward compatibility
                 raise RunCacheNotFoundError(stage)
-            stage.save_deps()
+            if not dry:
+                stage.save_deps()
             cache = self._load(stage)
             if not cache:
                 raise RunCacheNotFoundError(stage)
 
         cached_stage = self._create_stage(cache, wdir=stage.wdir)
 
-        if pull:
+        if pull and not dry:
             for objs in cached_stage.get_used_objs().values():
                 self.repo.cloud.pull(objs)
 
         if not cached_stage.outs_cached():
             raise RunCacheNotFoundError(stage)
 
         logger.info(
             "Stage '%s' is cached - skipping run, checking out outputs",
             stage.addressing,
         )
-        cached_stage.checkout()
+        if not dry:
+            cached_stage.checkout()
+
+    def transfer(self, from_odb, to_odb):
+        from dvc.fs import HTTPFileSystem, LocalFileSystem
+        from dvc.fs.callbacks import Callback
+
+        from_fs = from_odb.fs
+        to_fs = to_odb.fs
+        func = fs.generic.log_exceptions(fs.generic.copy)
+        runs = from_fs.path.join(from_odb.path, "runs")
+
+        http_odb = next(
+            (odb for odb in (from_odb, to_odb) if isinstance(odb.fs, HTTPFileSystem)),
+            None,
+        )
+        if http_odb:
+            path = http_odb.path
+            message = f"run-cache is not supported for http filesystem: {path}"
+            raise RunCacheNotSupported(message)
+
+        ret: List[Tuple[str, str]] = []
+        if not from_fs.exists(runs):
+            return ret
+
+        for src in from_fs.find(runs):
+            rel = from_fs.path.relpath(src, from_odb.path)
+            if not isinstance(to_fs, LocalFileSystem):
+                rel = from_fs.path.as_posix(rel)
 
-    @staticmethod
-    def _transfer(func, from_remote, to_remote):
-        ret = []
-
-        runs = from_remote.fs.path.join(from_remote.fs_path, "runs")
-        if not from_remote.fs.exists(runs):
-            return []
-
-        from_path = from_remote.fs.path
-        for src in from_remote.fs.find(runs):
-            rel = from_path.relpath(src, from_remote.fs_path)
-            dst = to_remote.fs.path.join(to_remote.fs_path, rel)
-            key = to_remote.fs.path.parent(dst)
+            dst = to_fs.path.join(to_odb.path, rel)
+            key = to_fs.path.parent(dst)
             # check if any build cache already exists for this key
             # TODO: check if MaxKeys=1 or something like that applies
             # or otherwise this will take a lot of time!
-            if to_remote.fs.exists(key) and first(to_remote.fs.find(key)):
+            if to_fs.exists(key) and first(to_fs.find(key)):
                 continue
-            func(src, dst)
-            ret.append(
-                (from_path.name(from_path.parent(src)), from_path.name(src))
-            )
 
+            src_name = from_fs.path.name(src)
+            parent_name = from_fs.path.name(from_fs.path.parent(src))
+            with Callback.as_tqdm_callback(
+                desc=src_name,
+                bytes=True,
+            ) as cb:
+                func(from_fs, src, to_fs, dst, callback=cb)
+            ret.append((parent_name, src_name))
         return ret
 
     def push(self, remote: Optional[str], odb: Optional["ObjectDB"] = None):
-        from dvc.data.transfer import _log_exceptions
+        dest_odb = odb or self.repo.cloud.get_remote_odb(remote, "push --run-cache")
+        return self.transfer(self.repo.cache.local, dest_odb)
 
-        if odb is None:
-            odb = self.repo.cloud.get_remote_odb(remote)
-        return self._transfer(
-            _log_exceptions(odb.fs.upload),
-            self.repo.odb.local,
-            odb,
-        )
-
-    def pull(self, remote: Optional[str]):
-        from dvc.data.transfer import _log_exceptions
-
-        odb = self.repo.cloud.get_remote_odb(remote)
-        return self._transfer(
-            _log_exceptions(odb.fs.download),
-            odb,
-            self.repo.odb.local,
-        )
+    def pull(self, remote: Optional[str], odb: Optional["ObjectDB"] = None):
+        odb = odb or self.repo.cloud.get_remote_odb(remote, "fetch --run-cache")
+        return self.transfer(odb, self.repo.cache.local)
 
     def get_used_objs(self, used_run_cache, *args, **kwargs):
         """Return used cache for the specified run-cached stages."""
         from collections import defaultdict
 
         used_objs = defaultdict(set)
         for key, value in used_run_cache:
```

### Comparing `dvc-2.9.5/dvc/stage/decorators.py` & `dvc-3.0.0a0/dvc/stage/decorators.py`

 * *Files 21% similar despite different names*

```diff
@@ -28,15 +28,22 @@
             # There is no need to lock RepoDependency deps, as there is no
             # corresponding OutputREPO, so we can't even write it.
             if not isinstance(item, RepoDependency)
         ]
 
     cmd = " ".join(sys.argv)
 
-    with rwlock(stage.repo.tmp_dir, cmd, _chain(read), _chain(write)):
+    with rwlock(
+        stage.repo.tmp_dir,
+        stage.repo.fs,
+        cmd,
+        _chain(read),
+        _chain(write),
+        stage.repo.config["core"].get("hardlink_lock", False),
+    ):
         return call()
 
 
 def unlocked_repo(f):
     @wraps(f)
     def wrapper(stage, *args, **kwargs):
         stage.repo.lock.unlock()
```

### Comparing `dvc-2.9.5/dvc/stage/exceptions.py` & `dvc-3.0.0a0/dvc/stage/exceptions.py`

 * *Files 24% similar despite different names*

```diff
@@ -5,14 +5,22 @@
     def __init__(self, cmd, status=None):
         msg = f"failed to run: {cmd}"
         if status is not None:
             msg += f", exited with {status}"
         super().__init__(msg)
 
 
+class CheckpointKilledError(DvcException):
+    def __init__(self, cmd, status=None):
+        msg = f"failed to finish: {cmd}"
+        if status is not None:
+            msg += f", exited with {status}"
+        super().__init__(msg)
+
+
 class StageFileDoesNotExistError(DvcException):
     DVC_IGNORED = "is dvc-ignored"
     DOES_NOT_EXIST = "does not exist"
 
     def __init__(self, fname, dvc_ignored=False):
         self.file = fname
         message = self.DVC_IGNORED if dvc_ignored else self.DOES_NOT_EXIST
@@ -27,15 +35,15 @@
     def __init__(self, fname):
         from dvc.dvcfile import DVC_FILE_SUFFIX, is_dvc_file
 
         msg = f"'{fname}' is not a .dvc file"
 
         sname = fname + DVC_FILE_SUFFIX
         if is_dvc_file(sname):
-            msg += f" Do you mean '{sname}'?"
+            msg += f". Do you mean '{sname}'?"
 
         super().__init__(msg)
 
 
 class StageFileBadNameError(DvcException):
     pass
 
@@ -58,39 +66,39 @@
 
 class StageExternalOutputsError(DvcException):
     pass
 
 
 class StageUpdateError(DvcException):
     def __init__(self, path):
-        super().__init__(
-            "update is not supported for '{}' that is not an "
-            "import.".format(path)
-        )
+        super().__init__(f"update is not supported for '{path}' that is not an import.")
 
 
 class MissingDataSource(DvcException):
     def __init__(self, missing_files):
         assert len(missing_files) > 0
 
         source = "source"
         if len(missing_files) > 1:
             source += "s"
 
         msg = "missing data '{}': {}".format(source, ", ".join(missing_files))
         super().__init__(msg)
 
 
+class DataSourceChanged(DvcException):
+    def __init__(self, path: str):
+        super().__init__(f"data source changed: {path}")
+
+
 class StageNotFound(DvcException, KeyError):
     def __init__(self, file, name):
         self.file = file.relpath
         self.name = name
-        super().__init__(
-            f"Stage '{self.name}' not found inside '{self.file}' file"
-        )
+        super().__init__(f"Stage '{self.name}' not found inside '{self.file}' file")
 
     def __str__(self):
         # `KeyError` quotes the message
         # see: https://bugs.python.org/issue2651
         return self.msg
```

### Comparing `dvc-2.9.5/dvc/stage/loader.py` & `dvc-3.0.0a0/dvc/stage/loader.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,61 +1,65 @@
 import logging
 from collections.abc import Mapping
 from copy import deepcopy
 from itertools import chain
+from typing import TYPE_CHECKING, Any, Dict, Iterable, Optional, Tuple
 
-from funcy import cached_property, get_in, lcat, once, project
+from funcy import get_in, lcat, once, project
 
 from dvc import dependency, output
-from dvc.data.meta import Meta
-from dvc.hash_info import HashInfo
-from dvc.parsing import FOREACH_KWD, JOIN, DataResolver, EntryNotFound
-from dvc.parsing.versions import LOCKFILE_VERSION
+from dvc.parsing import FOREACH_KWD, JOIN, EntryNotFound
+from dvc.utils.objects import cached_property
+from dvc_data.hashfile.hash_info import HashInfo
+from dvc_data.hashfile.meta import Meta
 
 from . import PipelineStage, Stage, loads_from
 from .exceptions import StageNameUnspecified, StageNotFound
 from .params import StageParams
 from .utils import fill_stage_dependencies, resolve_paths
 
+if TYPE_CHECKING:
+    from dvc.dvcfile import ProjectFile, SingleStageFile
+    from dvc.output import Output
+
 logger = logging.getLogger(__name__)
 
 
 class StageLoader(Mapping):
-    def __init__(self, dvcfile, data, lockfile_data=None):
+    def __init__(
+        self,
+        dvcfile: "ProjectFile",
+        data,
+        lockfile_data=None,
+    ):
         self.dvcfile = dvcfile
+        self.resolver = self.dvcfile.resolver
         self.data = data or {}
         self.stages_data = self.data.get("stages", {})
         self.repo = self.dvcfile.repo
 
         lockfile_data = lockfile_data or {}
-        version = LOCKFILE_VERSION.from_dict(lockfile_data)
-        if version == LOCKFILE_VERSION.V1:
-            self._lockfile_data = lockfile_data
-        else:
-            self._lockfile_data = lockfile_data.get("stages", {})
-
-    @cached_property
-    def resolver(self):
-        wdir = self.repo.fs.path.parent(self.dvcfile.path)
-        return DataResolver(self.repo, wdir, self.data)
+        self._lockfile_data = lockfile_data.get("stages", {})
 
     @cached_property
-    def lockfile_data(self):
+    def lockfile_data(self) -> Dict[str, Any]:
         if not self._lockfile_data:
             logger.debug("Lockfile for '%s' not found", self.dvcfile.relpath)
         return self._lockfile_data
 
     @staticmethod
     def fill_from_lock(stage, lock_data=None):
         """Fill values for params, checksums for outs and deps from lock."""
         if not lock_data:
             return
 
+        from dvc.output import merge_file_meta_from_cloud
+
         assert isinstance(lock_data, dict)
-        items = chain(
+        items: Iterable[Tuple[str, "Output"]] = chain(
             ((StageParams.PARAM_DEPS, dep) for dep in stage.deps),
             ((StageParams.PARAM_OUTS, out) for out in stage.outs),
         )
 
         checksums = {
             key: {item["path"]: item for item in lock_data.get(key, {})}
             for key in [StageParams.PARAM_DEPS, StageParams.PARAM_OUTS]
@@ -64,24 +68,32 @@
             path = item.def_path
             if isinstance(item, dependency.ParamsDependency):
                 item.fill_values(get_in(lock_data, [stage.PARAM_PARAMS, path]))
                 continue
             info = get_in(checksums, [key, path], {})
             info = info.copy()
             info.pop("path", None)
-            item.meta = Meta.from_dict(info)
-            item.hash_info = HashInfo.from_dict(info)
+
+            item.meta = Meta.from_dict(merge_file_meta_from_cloud(info))
+            hash_value = getattr(item.meta, item.hash_name, None)
+            item.hash_info = HashInfo(item.hash_name, hash_value)
+            files = get_in(checksums, [key, path, item.PARAM_FILES], None)
+            if files:
+                item.files = [merge_file_meta_from_cloud(f) for f in files]
+            # pylint: disable-next=protected-access
+            item._compute_meta_hash_info_from_files()
 
     @classmethod
-    def load_stage(cls, dvcfile, name, stage_data, lock_data=None):
+    def load_stage(cls, dvcfile: "ProjectFile", name, stage_data, lock_data=None):
         assert all([name, dvcfile, dvcfile.repo, dvcfile.path])
-        assert stage_data and isinstance(stage_data, dict)
+        assert stage_data
+        assert isinstance(stage_data, dict)
 
         path, wdir = resolve_paths(
-            dvcfile.path, stage_data.get(Stage.PARAM_WDIR)
+            dvcfile.repo.fs, dvcfile.path, stage_data.get(Stage.PARAM_WDIR)
         )
         stage = loads_from(PipelineStage, dvcfile.repo, path, wdir, stage_data)
         stage.name = name
         stage.desc = stage_data.get(Stage.PARAM_DESC)
         stage.meta = stage_data.get(Stage.PARAM_META)
 
         deps = project(stage_data, [stage.PARAM_DEPS, stage.PARAM_PARAMS])
@@ -89,15 +101,14 @@
 
         outs = project(
             stage_data,
             [
                 stage.PARAM_OUTS,
                 stage.PARAM_METRICS,
                 stage.PARAM_PLOTS,
-                stage.PARAM_LIVE,
             ],
         )
         stage.outs = lcat(
             output.load_from_pipeline(stage, data, typ=key)
             for key, data in outs.items()
         )
 
@@ -118,15 +129,15 @@
     def __getitem__(self, name):
         if not name:
             raise StageNameUnspecified(self.dvcfile)
 
         try:
             resolved_data = self.resolver.resolve_one(name)
         except EntryNotFound:
-            raise StageNotFound(self.dvcfile, name)
+            raise StageNotFound(self.dvcfile, name)  # noqa: B904
 
         if self.lockfile_data and name not in self.lockfile_data:
             self.lockfile_needs_update()
             logger.trace(  # type: ignore[attr-defined]
                 "No lock entry found for '%s:%s'", self.dvcfile.relpath, name
             )
 
@@ -139,61 +150,65 @@
         )
 
         stage.tracked_vars = self.resolver.tracked_vars.get(name, {})
         group, *keys = name.rsplit(JOIN, maxsplit=1)
         if group and keys and name not in self.stages_data:
             stage.raw_data.generated_from = group
 
-        stage.raw_data.parametrized = (
-            self.stages_data.get(name, {}) != resolved_stage
-        )
+        stage.raw_data.parametrized = self.stages_data.get(name, {}) != resolved_stage
         return stage
 
     def __iter__(self):
         return iter(self.resolver.get_keys())
 
     def __len__(self):
         return len(self.resolver.get_keys())
 
     def __contains__(self, name):
         return self.resolver.has_key(name)  # noqa: W601
 
     def is_foreach_generated(self, name: str):
-        return (
-            name in self.stages_data and FOREACH_KWD in self.stages_data[name]
-        )
+        return name in self.stages_data and FOREACH_KWD in self.stages_data[name]
 
 
 class SingleStageLoader(Mapping):
-    def __init__(self, dvcfile, stage_data, stage_text=None):
+    def __init__(
+        self,
+        dvcfile: "SingleStageFile",
+        stage_data: Dict[Any, str],
+        stage_text: Optional[str] = None,
+    ):
         self.dvcfile = dvcfile
         self.stage_data = stage_data or {}
         self.stage_text = stage_text
 
     def __getitem__(self, item):
         if item:
             logger.warning(
                 "Ignoring name '%s' for single stage in '%s'.",
                 item,
                 self.dvcfile,
             )
         # during `load`, we remove attributes from stage data, so as to
         # not duplicate, therefore, for MappingView, we need to deepcopy.
-        return self.load_stage(
-            self.dvcfile, deepcopy(self.stage_data), self.stage_text
-        )
+        return self.load_stage(self.dvcfile, deepcopy(self.stage_data), self.stage_text)
 
     @classmethod
-    def load_stage(cls, dvcfile, d, stage_text):
-        path, wdir = resolve_paths(dvcfile.path, d.get(Stage.PARAM_WDIR))
-        stage = loads_from(Stage, dvcfile.repo, path, wdir, d)
-        stage._stage_text = stage_text  # noqa, pylint:disable=protected-access
-        stage.deps = dependency.loadd_from(
-            stage, d.get(Stage.PARAM_DEPS) or []
+    def load_stage(
+        cls,
+        dvcfile: "SingleStageFile",
+        d: Dict[str, Any],
+        stage_text: Optional[str],
+    ) -> Stage:
+        path, wdir = resolve_paths(
+            dvcfile.repo.fs, dvcfile.path, d.get(Stage.PARAM_WDIR)
         )
+        stage = loads_from(Stage, dvcfile.repo, path, wdir, d)
+        stage._stage_text = stage_text  # pylint: disable=protected-access
+        stage.deps = dependency.loadd_from(stage, d.get(Stage.PARAM_DEPS) or [])
         stage.outs = output.loadd_from(stage, d.get(Stage.PARAM_OUTS) or [])
         return stage
 
     def __iter__(self):
         return iter([None])
 
     def __contains__(self, item):
```

### Comparing `dvc-2.9.5/dvc/stage/monitor.py` & `dvc-3.0.0a0/dvc/stage/monitor.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,50 +1,39 @@
 import functools
 import logging
 import os
-import subprocess
+import subprocess  # nosec B404
 import threading
 from dataclasses import dataclass
 from typing import TYPE_CHECKING, Callable, List
 
-from dvc.repo.live import create_summary
 from dvc.stage.decorators import relock_repo
-from dvc.stage.exceptions import StageCmdFailedError
-from dvc.ui import ui
+from dvc.stage.exceptions import CheckpointKilledError
 
 if TYPE_CHECKING:
-    from dvc.output import Output
     from dvc.stage import Stage
 
 
 logger = logging.getLogger(__name__)
 
 
-class CheckpointKilledError(StageCmdFailedError):
-    pass
-
-
-class LiveKilledError(StageCmdFailedError):
-    pass
-
-
 @dataclass
 class MonitorTask:
     stage: "Stage"
     execute: Callable
     proc: subprocess.Popen
     done: threading.Event = threading.Event()
-    killed: threading.Event = threading.Event()
+    updated: threading.Event = threading.Event()
 
     @property
     def name(self) -> str:
         raise NotImplementedError
 
     @property
-    def SIGNAL_FILE(self) -> str:
+    def SIGNAL_FILE(self) -> str:  # noqa: N802
         raise NotImplementedError
 
     @property
     def error_cls(self) -> type:
         raise NotImplementedError
 
     @property
@@ -56,57 +45,35 @@
 
 
 class CheckpointTask(MonitorTask):
     name = "checkpoint"
     SIGNAL_FILE = "DVC_CHECKPOINT"
     error_cls = CheckpointKilledError
 
-    def __init__(
-        self, stage: "Stage", callback_func: Callable, proc: subprocess.Popen
-    ):
+    def __init__(self, stage: "Stage", callback_func: Callable, proc: subprocess.Popen):
         super().__init__(
             stage,
-            functools.partial(
-                CheckpointTask._run_callback, stage, callback_func
-            ),
+            functools.partial(CheckpointTask._run_callback, stage, callback_func),
             proc,
         )
 
     @staticmethod
     @relock_repo
     def _run_callback(stage, callback_func):
         stage.save(allow_missing=True)
         stage.commit(allow_missing=True)
         stage.unprotect_outs()
         logger.debug("Running checkpoint callback for stage '%s'", stage)
         callback_func()
 
 
-class LiveTask(MonitorTask):
-    name = "live"
-    SIGNAL_FILE = "DVC_LIVE"
-    error_cls = LiveKilledError
-
-    def __init__(self, stage: "Stage", out: "Output", proc: subprocess.Popen):
-        self.output_path = os.path.join(
-            os.getcwd(), out.fs_path + "_dvc_plots", "index.html"
-        )
-        ui.write(f"Live summary will be created at:\n{self.output_path}")
-        super().__init__(stage, functools.partial(create_summary, out), proc)
-
-    def after_run(self):
-        # make sure summary is prepared for all the data
-        self.execute()
-        ui.write(f"Live summary has been created at:\n{self.output_path}")
-
-
 class Monitor:
     AWAIT: float = 1.0
 
-    def __init__(self, tasks: List[MonitorTask]):
+    def __init__(self, tasks: List[CheckpointTask]):
         self.done = threading.Event()
         self.tasks = tasks
         self.monitor_thread = threading.Thread(
             target=Monitor._loop, args=(self.tasks, self.done)
         )
 
     def __enter__(self):
@@ -125,31 +92,32 @@
         proc.terminate()
         proc.wait()
 
     @staticmethod
     def _kill_nt(proc):
         # windows stages are spawned with shell=True, proc is the shell process
         # and not the actual stage process - we have to kill the entire tree
-        subprocess.call(["taskkill", "/F", "/T", "/PID", str(proc.pid)])
+        subprocess.call(  # nosec B607, B603
+            ["taskkill", "/F", "/T", "/PID", str(proc.pid)]
+        )
 
     @staticmethod
     def _loop(tasks: List[MonitorTask], done: threading.Event):
         while True:
             for task in tasks:
                 if os.path.exists(task.signal_path):
                     try:
                         task.execute()
+                        task.updated.set()
                     except Exception:  # pylint: disable=broad-except
                         logger.exception(
                             "Error running '%s' task, '%s' will be aborted",
                             task.name,
                             task.stage,
                         )
                         Monitor.kill(task.proc)
-                        task.killed.set()
                     finally:
-                        logger.debug(
-                            "Removing signal file for '%s' task", task.name
-                        )
+                        logger.debug("Removing signal file for '%s' task", task.name)
                         os.remove(task.signal_path)
+
             if done.wait(Monitor.AWAIT):
                 return
```

### Comparing `dvc-2.9.5/dvc/stage/run.py` & `dvc-3.0.0a0/dvc/stage/run.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,37 +1,36 @@
 import logging
 import os
 import signal
-import subprocess
+import subprocess  # nosec B404
 import threading
+from typing import TYPE_CHECKING, List
 
-from funcy import first
-
-from dvc.stage.monitor import Monitor
+from dvc.stage.monitor import CheckpointTask, Monitor
 from dvc.utils import fix_env
 
 from .decorators import unlocked_repo
 from .exceptions import StageCmdFailedError
 
+if TYPE_CHECKING:
+    from dvc.stage import Stage
+
 logger = logging.getLogger(__name__)
 
 
 def _make_cmd(executable, cmd):
     if executable is None:
         return cmd
     opts = {"zsh": ["--no-rcs"], "bash": ["--noprofile", "--norc"]}
     name = os.path.basename(executable).lower()
-    return [executable] + opts.get(name, []) + ["-c", cmd]
+    return [executable, *opts.get(name, []), "-c", cmd]
 
 
 def warn_if_fish(executable):
-    if (
-        executable is None
-        or os.path.basename(os.path.realpath(executable)) != "fish"
-    ):
+    if executable is None or os.path.basename(os.path.realpath(executable)) != "fish":
         return
 
     logger.warning(
         "DVC detected that you are using fish as your default "
         "shell. Be aware that it might cause problems by overwriting "
         "your current environment variables with values defined "
         "in '.fishrc', which might affect your command. See "
@@ -60,101 +59,108 @@
     # configs and modifying env, which may change the behavior or the
     # command we are running. See [2] for more info.
     #
     # [1] https://github.com/python/cpython/blob/3.7/Lib/subprocess.py
     #                                                            #L1426
     # [2] https://github.com/iterative/dvc/issues/2506
     #                                           #issuecomment-535396799
-    kwargs["shell"] = True if os.name == "nt" else False
+    kwargs["shell"] = os.name == "nt"
     return kwargs
 
 
 def display_command(cmd):
     logger.info("%s %s", ">", cmd)
 
 
 def get_executable():
     return (os.getenv("SHELL") or "/bin/sh") if os.name != "nt" else None
 
 
-def _run(stage, executable, cmd, checkpoint_func, **kwargs):
+def _run(stage: "Stage", executable, cmd, checkpoint_func, **kwargs):
+    # pylint: disable=protected-access
     main_thread = isinstance(
         threading.current_thread(),
-        threading._MainThread,  # pylint: disable=protected-access
+        threading._MainThread,  # type: ignore[attr-defined]
     )
+    old_handler = None
 
     exec_cmd = _make_cmd(executable, cmd)
-    old_handler = None
 
     try:
-        p = subprocess.Popen(exec_cmd, **kwargs)
+        p = subprocess.Popen(exec_cmd, **kwargs)  # nosec B603
         if main_thread:
             old_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)
 
         tasks = _get_monitor_tasks(stage, checkpoint_func, p)
 
         if tasks:
             with Monitor(tasks):
                 p.communicate()
         else:
             p.communicate()
 
         if p.returncode != 0:
             for t in tasks:
-                if t.killed.is_set():
+                if t.updated.is_set():
                     raise t.error_cls(cmd, p.returncode)
             raise StageCmdFailedError(cmd, p.returncode)
     finally:
         if old_handler:
             signal.signal(signal.SIGINT, old_handler)
 
 
-def _get_monitor_tasks(stage, checkpoint_func, proc):
-
+def _get_monitor_tasks(stage, checkpoint_func, proc) -> List[CheckpointTask]:
     result = []
     if checkpoint_func:
-        from .monitor import CheckpointTask
-
         result.append(CheckpointTask(stage, checkpoint_func, proc))
 
-    live = first(o for o in stage.outs if (o.live and o.live["html"]))
-    if live:
-        from .monitor import LiveTask
-
-        result.append(LiveTask(stage, live, proc))
-
     return result
 
 
 def cmd_run(stage, dry=False, checkpoint_func=None, run_env=None):
     logger.info("Running stage '%s':", stage.addressing)
     commands = _enforce_cmd_list(stage.cmd)
-    kwargs = prepare_kwargs(
-        stage, checkpoint_func=checkpoint_func, run_env=run_env
-    )
+    kwargs = prepare_kwargs(stage, checkpoint_func=checkpoint_func, run_env=run_env)
     executable = get_executable()
 
     if not dry:
         warn_if_fish(executable)
 
     for cmd in commands:
         display_command(cmd)
         if dry:
             continue
 
         _run(stage, executable, cmd, checkpoint_func=checkpoint_func, **kwargs)
 
 
+def _pull_missing_deps(stage):
+    for dep in stage.deps:
+        if not dep.exists:
+            stage.repo.pull(dep.def_path)
+
+
 def run_stage(
-    stage, dry=False, force=False, checkpoint_func=None, run_env=None, **kwargs
+    stage,
+    dry=False,
+    force=False,
+    checkpoint_func=None,
+    run_env=None,
+    allow_missing: bool = False,
+    **kwargs,
 ):
-    if not (dry or force or checkpoint_func):
+    if not (force or checkpoint_func):
+        if allow_missing and kwargs.get("pull") and not dry:
+            _pull_missing_deps(stage)
+
         from .cache import RunCacheNotFoundError
 
         try:
-            stage.repo.stage_cache.restore(stage, **kwargs)
-            return
+            stage.repo.stage_cache.restore(stage, dry=dry, **kwargs)
+            if not dry:
+                return
         except RunCacheNotFoundError:
-            stage.save_deps()
+            if not dry:
+                stage.save_deps()
 
     run = cmd_run if dry else unlocked_repo(cmd_run)
     run(stage, dry=dry, checkpoint_func=checkpoint_func, run_env=run_env)
```

### Comparing `dvc-2.9.5/dvc/stage/serialize.py` & `dvc-3.0.0a0/dvc/stage/serialize.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,16 +1,24 @@
 from collections import OrderedDict
-from functools import partial
 from operator import attrgetter
-from typing import TYPE_CHECKING, List, no_type_check
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Dict,
+    Iterable,
+    List,
+    Optional,
+    Union,
+    no_type_check,
+)
 
 from funcy import post_processing
 
 from dvc.dependency import ParamsDependency
-from dvc.output import Output
+from dvc.output import Annotation, Output
 from dvc.utils.collections import apply_diff
 from dvc.utils.serialize import parse_yaml_for_update
 
 from .params import StageParams
 from .utils import resolve_wdir, split_params_deps
 
 if TYPE_CHECKING:
@@ -23,119 +31,108 @@
 PARAM_OUTS = StageParams.PARAM_OUTS
 
 PARAM_CACHE = Output.PARAM_CACHE
 PARAM_METRIC = Output.PARAM_METRIC
 PARAM_PLOT = Output.PARAM_PLOT
 PARAM_PERSIST = Output.PARAM_PERSIST
 PARAM_CHECKPOINT = Output.PARAM_CHECKPOINT
-PARAM_DESC = Output.PARAM_DESC
+PARAM_DESC = Annotation.PARAM_DESC
 PARAM_REMOTE = Output.PARAM_REMOTE
+PARAM_PUSH = Output.PARAM_PUSH
 
 DEFAULT_PARAMS_FILE = ParamsDependency.DEFAULT_PARAMS_FILE
 
 
-sort_by_path = partial(sorted, key=attrgetter("def_path"))
-
-
 @post_processing(OrderedDict)
 def _get_flags(out):
-    if out.desc:
-        yield PARAM_DESC, out.desc
+    annot = out.annot.to_dict()
+    yield from annot.items()
+
     if not out.use_cache:
         yield PARAM_CACHE, False
     if out.checkpoint:
         yield PARAM_CHECKPOINT, True
     if out.persist:
         yield PARAM_PERSIST, True
     if out.plot and isinstance(out.plot, dict):
         # notice `out.plot` is not sorted
         # `out.plot` is in the same order as is in the file when read
         # and, should be dumped as-is without any sorting
         yield from out.plot.items()
-    if out.live and isinstance(out.live, dict):
-        yield from out.live.items()
     if out.remote:
         yield PARAM_REMOTE, out.remote
+    if not out.can_push:
+        yield PARAM_PUSH, False
 
 
 def _serialize_out(out):
     flags = _get_flags(out)
     return out.def_path if not flags else {out.def_path: flags}
 
 
 @no_type_check
 def _serialize_outs(outputs: List[Output]):
-    outs, metrics, plots, live = [], [], [], None
-    for out in sort_by_path(outputs):
+    outs, metrics, plots = [], [], []
+    for out in sorted(outputs, key=attrgetter("def_path")):
         bucket = outs
         if out.plot:
             bucket = plots
         elif out.metric:
             bucket = metrics
-        elif out.live:
-            assert live is None
-            live = _serialize_out(out)
-            continue
         bucket.append(_serialize_out(out))
-    return outs, metrics, plots, live
+    return outs, metrics, plots
 
 
-def _serialize_params_keys(params):
+def _serialize_params_keys(params: Iterable["ParamsDependency"]):
     """
     Returns the following format of data:
      ['lr', 'train', {'params2.yaml': ['lr']}]
 
     The output is sorted, with keys of params from default params file being
     at the first, and then followed by entry of other files in lexicographic
     order. The keys of those custom files are also sorted in the same order.
     """
-    keys = []
-    for param_dep in sort_by_path(params):
-        dump = param_dep.dumpd()
-        path, params = dump[PARAM_PATH], dump[PARAM_PARAMS]
-        assert isinstance(params, (dict, list))
+    keys: List[Union[str, Dict[str, Optional[List[str]]]]] = []
+    for param_dep in sorted(params, key=attrgetter("def_path")):
         # when on no_exec, params are not filled and are saved as list
-        k = sorted(params.keys() if isinstance(params, dict) else params)
-        if not k:
-            continue
-
-        if path == DEFAULT_PARAMS_FILE:
-            keys = k + keys
+        k: List[str] = sorted(param_dep.params)
+        if k and param_dep.def_path == DEFAULT_PARAMS_FILE:
+            keys = k + keys  # type: ignore[operator,assignment]
         else:
-            keys.append({path: k})
+            keys.append({param_dep.def_path: k or None})
     return keys
 
 
 @no_type_check
 def _serialize_params_values(params: List[ParamsDependency]):
     """Returns output of following format, used for lockfile:
         {'params.yaml': {'lr': '1', 'train': 2}, {'params2.yaml': {'lr': '1'}}
 
     Default params file are always kept at the start, followed by others in
     alphabetical order. The param values are sorted too(not recursively though)
     """
     key_vals = OrderedDict()
-    for param_dep in sort_by_path(params):
+    for param_dep in sorted(params, key=attrgetter("def_path")):
         dump = param_dep.dumpd()
         path, params = dump[PARAM_PATH], dump[PARAM_PARAMS]
         if isinstance(params, dict):
             kv = [(key, params[key]) for key in sorted(params.keys())]
             key_vals[path] = OrderedDict(kv)
             if path == DEFAULT_PARAMS_FILE:
                 key_vals.move_to_end(path, last=False)
     return key_vals
 
 
 def to_pipeline_file(stage: "PipelineStage"):
     wdir = resolve_wdir(stage.wdir, stage.path)
-    params, deps = split_params_deps(stage)
-    deps = sorted(d.def_path for d in deps)
-    params = _serialize_params_keys(params)
+    param_objs, deps_objs = split_params_deps(stage)
+    deps = sorted(d.def_path for d in deps_objs)
+    params = _serialize_params_keys(param_objs)
 
-    outs, metrics, plots, live = _serialize_outs(stage.outs)
+    outs, metrics, plots = _serialize_outs(stage.outs)
 
     cmd = stage.cmd
     assert cmd, (
         f"'{stage.PARAM_CMD}' cannot be empty for stage '{stage.name}', "
         f"got: '{cmd}'(type: '{type(cmd).__name__}')"
     )
     res = [
@@ -143,66 +140,75 @@
         (stage.PARAM_CMD, stage.cmd),
         (stage.PARAM_WDIR, wdir),
         (stage.PARAM_DEPS, deps),
         (stage.PARAM_PARAMS, params),
         (stage.PARAM_OUTS, outs),
         (stage.PARAM_METRICS, metrics),
         (stage.PARAM_PLOTS, plots),
-        (stage.PARAM_LIVE, live),
         (stage.PARAM_FROZEN, stage.frozen),
         (stage.PARAM_ALWAYS_CHANGED, stage.always_changed),
         (stage.PARAM_META, stage.meta),
     ]
-    return {
-        stage.name: OrderedDict([(key, value) for key, value in res if value])
-    }
+    return {stage.name: OrderedDict([(key, value) for key, value in res if value])}
 
 
-def to_single_stage_lockfile(stage: "Stage") -> dict:
-    assert stage.cmd
+def to_single_stage_lockfile(stage: "Stage", **kwargs) -> dict:
+    from dvc.output import _serialize_tree_obj_to_files, split_file_meta_from_cloud
+    from dvc_data.hashfile.tree import Tree
 
-    def _dumpd(item):
-        ret = [
-            (item.PARAM_PATH, item.def_path),
-            *item.hash_info.to_dict().items(),
-            *item.meta.to_dict().items(),
-        ]
+    assert stage.cmd
 
-        return OrderedDict(ret)
+    def _dumpd(item: "Output"):
+        ret: Dict[str, Any] = {item.PARAM_PATH: item.def_path}
+        if item.hash_info.isdir and kwargs.get("with_files"):
+            obj = item.obj or item.get_obj()
+            if obj:
+                assert isinstance(obj, Tree)
+                ret[item.PARAM_FILES] = [
+                    split_file_meta_from_cloud(f)
+                    for f in _serialize_tree_obj_to_files(obj)
+                ]
+        else:
+            meta_d = item.meta.to_dict()
+            meta_d.pop("isdir", None)
+            ret.update(item.hash_info.to_dict())
+            ret.update(split_file_meta_from_cloud(meta_d))
+        return ret
 
     res = OrderedDict([("cmd", stage.cmd)])
     params, deps = split_params_deps(stage)
     deps, outs = (
-        [_dumpd(item) for item in sort_by_path(items)]
+        [_dumpd(item) for item in sorted(items, key=attrgetter("def_path"))]
         for items in [deps, stage.outs]
     )
     params = _serialize_params_values(params)
     if deps:
         res[PARAM_DEPS] = deps
     if params:
         res[PARAM_PARAMS] = params
     if outs:
         res[PARAM_OUTS] = outs
 
     return res
 
 
-def to_lockfile(stage: "PipelineStage") -> dict:
+def to_lockfile(stage: "PipelineStage", **kwargs) -> dict:
     assert stage.name
-    return {stage.name: to_single_stage_lockfile(stage)}
+    return {stage.name: to_single_stage_lockfile(stage, **kwargs)}
 
 
-def to_single_stage_file(stage: "Stage"):
-    state = stage.dumpd()
+def to_single_stage_file(stage: "Stage", **kwargs):
+    state = stage.dumpd(**kwargs)
 
     # When we load a stage we parse yaml with a fast parser, which strips
     # off all the comments and formatting. To retain those on update we do
     # a trick here:
     # - reparse the same yaml text with a slow but smart ruamel yaml parser
     # - apply changes to a returned structure
     # - serialize it
-    text = stage._stage_text  # noqa, pylint: disable=protected-access
-    if text is not None:
-        saved_state = parse_yaml_for_update(text, stage.path)
-        apply_diff(state, saved_state)
-        state = saved_state
-    return state
+    text = stage._stage_text  # pylint: disable=protected-access
+    if text is None:
+        return state
+
+    saved_state = parse_yaml_for_update(text, stage.path)
+    apply_diff(state, saved_state)
+    return saved_state
```

### Comparing `dvc-2.9.5/dvc/stage/utils.py` & `dvc-3.0.0a0/dvc/stage/utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 import os
 import pathlib
-from typing import TYPE_CHECKING, Union
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union
 
-from funcy import concat, first, lsplit, rpartial, without
+from funcy import concat, first, lsplit, rpartial
 
-from dvc.data.meta import Meta
+from dvc.annotations import ANNOTATION_FIELDS
 from dvc.exceptions import InvalidArgumentError
+from dvc_data.hashfile.meta import Meta
 
 from .exceptions import (
     MissingDataSource,
     StageExternalOutputsError,
     StagePathNotDirectoryError,
     StagePathNotFoundError,
     StagePathOutsideError,
 )
 
 if TYPE_CHECKING:
+    from dvc.dependency import Dependency, ParamsDependency
     from dvc.repo import Repo
 
     from . import PipelineStage, Stage
 
 
 def check_stage_path(repo, path, is_wdir=False):
     from dvc.utils.fs import path_isin
@@ -46,73 +48,47 @@
     from dvc.output import loads_from
 
     assert not stage.outs
 
     keys = [
         "outs_persist",
         "outs_persist_no_cache",
-        "metrics_no_cache",
         "metrics",
-        "plots_no_cache",
+        "metrics_persist",
+        "metrics_no_cache",
+        "metrics_persist_no_cache",
         "plots",
+        "plots_persist",
+        "plots_no_cache",
+        "plots_persist_no_cache",
         "outs_no_cache",
         "outs",
         "checkpoints",
     ]
 
     stage.outs = []
 
-    stage.outs += _load_live_output(stage, **kwargs)
-
     for key in keys:
         stage.outs += loads_from(
             stage,
             kwargs.get(key, []),
             use_cache="no_cache" not in key,
             persist="persist" in key,
             metric="metrics" in key,
             plot="plots" in key,
             checkpoint="checkpoints" in key,
         )
 
 
-def _load_live_output(
-    stage,
-    live=None,
-    live_no_cache=None,
-    live_summary=False,
-    live_html=False,
-    **kwargs,
-):
-    from dvc.output import Output, loads_from
-
-    outs = []
-    if live or live_no_cache:
-        assert bool(live) != bool(live_no_cache)
-
-        path = live or live_no_cache
-        outs += loads_from(
-            stage,
-            [path],
-            use_cache=not bool(live_no_cache),
-            live={
-                Output.PARAM_LIVE_SUMMARY: live_summary,
-                Output.PARAM_LIVE_HTML: live_html,
-            },
-        )
-
-    return outs
-
-
-def fill_stage_dependencies(stage, deps=None, erepo=None, params=None):
+def fill_stage_dependencies(stage, deps=None, erepo=None, params=None, fs_config=None):
     from dvc.dependency import loads_from, loads_params
 
     assert not stage.deps
     stage.deps = []
-    stage.deps += loads_from(stage, deps or [], erepo=erepo)
+    stage.deps += loads_from(stage, deps or [], erepo=erepo, fs_config=fs_config)
     stage.deps += loads_params(stage, params or [])
 
 
 def check_no_externals(stage):
     from urllib.parse import urlparse
 
     from dvc.utils import format_link
@@ -130,16 +106,15 @@
     outs = [str(out) for out in stage.outs if _is_external(out)]
     if not outs:
         return
 
     str_outs = ", ".join(outs)
     link = format_link("https://dvc.org/doc/user-guide/managing-external-data")
     raise StageExternalOutputsError(
-        f"Output(s) outside of DVC project: {str_outs}. "
-        f"See {link} for more info."
+        f"Output(s) outside of DVC project: {str_outs}. See {link} for more info."
     )
 
 
 def check_circular_dependency(stage):
     from dvc.exceptions import CircularDependencyError
 
     circular_dependencies = {d.fs_path for d in stage.deps} & {
@@ -166,16 +141,15 @@
     paths = [str(out) for out in stage.outs if not out.exists]
     if paths:
         raise MissingDataSource(paths)
 
 
 def compute_md5(stage):
     from dvc.output import Output
-
-    from ..utils import dict_md5
+    from dvc.utils import dict_md5
 
     d = stage.dumpd()
 
     # Remove md5 and meta, these should not affect stage md5
     d.pop(stage.PARAM_MD5, None)
     d.pop(stage.PARAM_META, None)
     d.pop(stage.PARAM_DESC, None)
@@ -186,72 +160,74 @@
     # didn't have WDIR in their DVC files.
     if d.get(stage.PARAM_WDIR) == ".":
         del d[stage.PARAM_WDIR]
 
     return dict_md5(
         d,
         exclude=[
+            *ANNOTATION_FIELDS,
             stage.PARAM_LOCKED,  # backward compatibility
             stage.PARAM_FROZEN,
-            Output.PARAM_DESC,
             Output.PARAM_METRIC,
             Output.PARAM_PERSIST,
             Output.PARAM_CHECKPOINT,
             Meta.PARAM_ISEXEC,
             Meta.PARAM_SIZE,
             Meta.PARAM_NFILES,
         ],
     )
 
 
 def resolve_wdir(wdir, path):
-    from ..utils import relpath
+    from dvc.utils import relpath
 
     rel_wdir = relpath(wdir, os.path.dirname(path))
     return pathlib.PurePath(rel_wdir).as_posix() if rel_wdir != "." else None
 
 
-def resolve_paths(path, wdir=None):
-    path = os.path.abspath(path)
+def resolve_paths(fs, path, wdir=None):
+    path = fs.path.abspath(path)
     wdir = wdir or os.curdir
-    wdir = os.path.abspath(os.path.join(os.path.dirname(path), wdir))
+    wdir = fs.path.abspath(fs.path.join(fs.path.dirname(path), wdir))
     return path, wdir
 
 
-def get_dump(stage):
+def get_dump(stage: "Stage", **kwargs):
     return {
         key: value
         for key, value in {
             stage.PARAM_DESC: stage.desc,
             stage.PARAM_MD5: stage.md5,
             stage.PARAM_CMD: stage.cmd,
             stage.PARAM_WDIR: resolve_wdir(stage.wdir, stage.path),
             stage.PARAM_FROZEN: stage.frozen,
-            stage.PARAM_DEPS: [d.dumpd() for d in stage.deps],
-            stage.PARAM_OUTS: [o.dumpd() for o in stage.outs],
+            stage.PARAM_DEPS: [d.dumpd(**kwargs) for d in stage.deps],
+            stage.PARAM_OUTS: [o.dumpd(**kwargs) for o in stage.outs],
             stage.PARAM_ALWAYS_CHANGED: stage.always_changed,
             stage.PARAM_META: stage.meta,
         }.items()
         if value
     }
 
 
-def split_params_deps(stage):
-    from ..dependency import ParamsDependency
+def split_params_deps(
+    stage: "Stage",
+) -> Tuple[List["ParamsDependency"], List["Dependency"]]:
+    from dvc.dependency import ParamsDependency
 
     return lsplit(rpartial(isinstance, ParamsDependency), stage.deps)
 
 
-def is_valid_name(name: str):
+def is_valid_name(name: str) -> bool:
     from . import INVALID_STAGENAME_CHARS
 
     return not INVALID_STAGENAME_CHARS & set(name)
 
 
-def prepare_file_path(kwargs):
+def prepare_file_path(kwargs) -> str:
     """Determine file path from the first output name.
 
     Used in creating .dvc files.
     """
     from dvc.dvcfile import DVC_FILE, DVC_FILE_SUFFIX
 
     out = first(
@@ -261,77 +237,56 @@
             kwargs.get("metrics", []),
             kwargs.get("metrics_no_cache", []),
             kwargs.get("plots", []),
             kwargs.get("plots_no_cache", []),
             kwargs.get("outs_persist", []),
             kwargs.get("outs_persist_no_cache", []),
             kwargs.get("checkpoints", []),
-            without([kwargs.get("live", None)], None),
         )
     )
 
     return (
-        os.path.basename(os.path.normpath(out)) + DVC_FILE_SUFFIX
-        if out
-        else DVC_FILE
+        os.path.basename(os.path.normpath(out)) + DVC_FILE_SUFFIX if out else DVC_FILE
     )
 
 
-def check_stage_exists(
-    repo: "Repo", stage: Union["Stage", "PipelineStage"], path: str
-):
-    from dvc.dvcfile import make_dvcfile
+def check_stage_exists(repo: "Repo", stage: Union["Stage", "PipelineStage"], path: str):
+    from dvc.dvcfile import load_file
     from dvc.stage import PipelineStage
-    from dvc.stage.exceptions import (
-        DuplicateStageName,
-        StageFileAlreadyExistsError,
-    )
+    from dvc.stage.exceptions import DuplicateStageName, StageFileAlreadyExistsError
 
-    dvcfile = make_dvcfile(repo, path)
+    dvcfile = load_file(repo, path)
     if not dvcfile.exists():
         return
 
     hint = "Use '--force' to overwrite."
     if not isinstance(stage, PipelineStage):
-        raise StageFileAlreadyExistsError(
-            f"'{stage.relpath}' already exists. {hint}"
-        )
-    elif stage.name and stage.name in dvcfile.stages:
+        raise StageFileAlreadyExistsError(f"'{stage.relpath}' already exists. {hint}")
+    if stage.name and stage.name in dvcfile.stages:
         raise DuplicateStageName(
             f"Stage '{stage.name}' already exists in '{stage.relpath}'. {hint}"
         )
 
 
-def validate_kwargs(single_stage: bool = False, fname: str = None, **kwargs):
+def validate_kwargs(
+    single_stage: bool = False, fname: Optional[str] = None, **kwargs
+) -> Dict[str, Any]:
     """Prepare, validate and process kwargs passed from cli"""
     cmd = kwargs.get("cmd")
     if not cmd and not single_stage:
         raise InvalidArgumentError("command is not specified")
 
     stage_name = kwargs.get("name")
     if stage_name and single_stage:
-        raise InvalidArgumentError(
-            "`-n|--name` is incompatible with `--single-stage`"
-        )
+        raise InvalidArgumentError("`-n|--name` is incompatible with `--single-stage`")
     if stage_name and fname:
         raise InvalidArgumentError(
             "`--file` is currently incompatible with `-n|--name` "
             "and requires `--single-stage`"
         )
     if not stage_name and not single_stage:
         raise InvalidArgumentError("`-n|--name` is required")
 
     if single_stage:
         kwargs.pop("name", None)
 
-    if kwargs.get("live") and kwargs.get("live_no_cache"):
-        raise InvalidArgumentError(
-            "cannot specify both `--live` and `--live-no-cache`"
-        )
-
-    kwargs.update(
-        {
-            "live_summary": not kwargs.pop("live_no_summary", False),
-            "live_html": not kwargs.pop("live_no_html", False),
-        }
-    )
     return kwargs
```

### Comparing `dvc-2.9.5/dvc/testing/cloud.py` & `dvc-3.0.0a0/dvc/testing/cloud.py`

 * *Files 2% similar despite different names*

```diff
@@ -61,15 +61,15 @@
     def gen(self, struct, text=""):
         if isinstance(struct, (str, bytes, pathlib.PurePath)):
             struct = {struct: text}
 
         self._gen(struct)
         return struct.keys()
 
-    def close(self):
+    def close(self):  # noqa: B027
         pass
 
     @staticmethod
     def should_test():
         return True
 
     @staticmethod
```

### Comparing `dvc-2.9.5/dvc/testing/fixtures.py` & `dvc-3.0.0a0/dvc/testing/fixtures.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,83 +1,71 @@
 import os
-import subprocess
+import subprocess  # nosec B404
+from typing import Dict, Tuple
 
 import pytest
 
+# pylint: disable=redefined-outer-name,unused-argument
+
 __all__ = [
     "make_tmp_dir",
     "tmp_dir",
     "scm",
     "dvc",
     "make_cloud",
+    "make_cloud_version_aware",
     "make_local",
     "cloud",
     "local_cloud",
     "make_remote",
+    "make_remote_version_aware",
+    "make_remote_worktree",
     "remote",
+    "remote_version_aware",
+    "remote_worktree",
     "local_remote",
     "workspace",
     "make_workspace",
     "local_workspace",
-    "docker",
-    "docker_compose",
     "docker_compose_project_name",
     "docker_services",
 ]
 
-CACHE = {}  # type: ignore
-
-
-def _fs_copy(src, dst, ignore=None):
-    import shutil
-
-    if os.path.isdir(src):
-        shutil.copytree(src, dst, ignore=ignore)
-    else:
-        shutil.copy2(src, dst)
+CACHE: Dict[Tuple[bool, bool, bool], str] = {}
 
 
 @pytest.fixture(scope="session")
 def make_tmp_dir(tmp_path_factory, request, worker_id):
-    def make(
-        name, *, scm=False, dvc=False, subdir=False
-    ):  # pylint: disable=W0621
-        from shutil import ignore_patterns
-
-        from scmrepo.git import Git
+    def make(name, *, scm=False, dvc=False, subdir=False):  # pylint: disable=W0621
+        from shutil import copytree, ignore_patterns
 
         from dvc.repo import Repo
+        from dvc.scm import Git
 
         from .tmp_dir import TmpDir
 
         cache = CACHE.get((scm, dvc, subdir))
         if not cache:
-            cache = tmp_path_factory.mktemp("dvc-test-cache" + worker_id)
-            TmpDir(cache).init(scm=scm, dvc=dvc, subdir=subdir)
-            CACHE[(scm, dvc, subdir)] = os.fspath(cache)
+            cache_dir = tmp_path_factory.mktemp("dvc-test-cache" + worker_id)
+            TmpDir(cache_dir).init(scm=scm, dvc=dvc, subdir=subdir)
+            CACHE[(scm, dvc, subdir)] = cache = os.fspath(cache_dir)
+
+        assert cache
         path = tmp_path_factory.mktemp(name) if isinstance(name, str) else name
 
         # ignore sqlite files from .dvc/tmp. We might not be closing the cache
         # connection resulting in PermissionErrors in Windows.
         ignore = ignore_patterns("cache.db*")
-        for entry in os.listdir(cache):
-            # shutil.copytree's dirs_exist_ok is only available in >=3.8
-            _fs_copy(
-                os.path.join(cache, entry),
-                os.path.join(path, entry),
-                ignore=ignore,
-            )
+        copytree(cache, path, dirs_exist_ok=True, ignore=ignore)
         new_dir = TmpDir(path)
         str_path = os.fspath(new_dir)
         if dvc:
             new_dir.dvc = Repo(str_path)
         if scm:
-            new_dir.scm = (
-                new_dir.dvc.scm if hasattr(new_dir, "dvc") else Git(str_path)
-            )
+            new_dir.scm = new_dir.dvc.scm if hasattr(new_dir, "dvc") else Git(str_path)
         request.addfinalizer(new_dir.close)
         return new_dir
 
     return make
 
 
 @pytest.fixture
@@ -111,63 +99,107 @@
     def _make_cloud(typ):
         return request.getfixturevalue(f"make_{typ}")()
 
     return _make_cloud
 
 
 @pytest.fixture
+def make_cloud_version_aware(request):
+    def _make_cloud(typ):
+        return request.getfixturevalue(f"make_{typ}_version_aware")()
+
+    return _make_cloud
+
+
+@pytest.fixture
 def cloud(make_cloud, request):
     typ = getattr(request, "param", "local")
     return make_cloud(typ)
 
 
 @pytest.fixture
 def local_cloud(make_cloud):
     return make_cloud("local")
 
 
 @pytest.fixture
-def make_remote(tmp_dir, dvc, make_cloud):
+def make_remote(tmp_dir, dvc, make_cloud):  # noqa: ARG001
     def _make_remote(name, typ="local", **kwargs):
         cloud = make_cloud(typ)  # pylint: disable=W0621
         tmp_dir.add_remote(name=name, config=cloud.config, **kwargs)
         return cloud
 
     return _make_remote
 
 
 @pytest.fixture
+def make_remote_version_aware(tmp_dir, dvc, make_cloud_version_aware):  # noqa: ARG001
+    def _make_remote(name, typ="local", **kwargs):
+        cloud = make_cloud_version_aware(typ)  # pylint: disable=W0621
+        config = dict(cloud.config)
+        config["version_aware"] = True
+        tmp_dir.add_remote(name=name, config=config, **kwargs)
+        return cloud
+
+    return _make_remote
+
+
+@pytest.fixture
+def make_remote_worktree(tmp_dir, dvc, make_cloud_version_aware):  # noqa: ARG001
+    def _make_remote(name, typ="local", **kwargs):
+        cloud = make_cloud_version_aware(typ)  # pylint: disable=W0621
+        config = dict(cloud.config)
+        config["worktree"] = True
+        tmp_dir.add_remote(name=name, config=config, **kwargs)
+        return cloud
+
+    return _make_remote
+
+
+@pytest.fixture
 def remote(make_remote, request):
     typ = getattr(request, "param", "local")
     return make_remote("upstream", typ=typ)
 
 
 @pytest.fixture
+def remote_version_aware(make_remote_version_aware, request):
+    typ = getattr(request, "param", "local")
+    return make_remote_version_aware("upstream", typ=typ)
+
+
+@pytest.fixture
+def remote_worktree(make_remote_worktree, request):
+    typ = getattr(request, "param", "local")
+    return make_remote_worktree("upstream", typ=typ)
+
+
+@pytest.fixture
 def local_remote(make_remote):
     return make_remote("upstream", typ="local")
 
 
 @pytest.fixture
 def make_workspace(tmp_dir, dvc, make_cloud):
     def _make_workspace(name, typ="local"):
-        from dvc.data.db import ODBManager
+        from dvc.cachemgr import CacheManager
 
         cloud = make_cloud(typ)  # pylint: disable=W0621
 
         tmp_dir.add_remote(name=name, config=cloud.config, default=False)
         tmp_dir.add_remote(
             name=f"{name}-cache", url="remote://workspace/cache", default=False
         )
 
         scheme = getattr(cloud, "scheme", "local")
         if scheme != "http":
             with dvc.config.edit() as conf:
                 conf["cache"][scheme] = f"{name}-cache"
 
-            dvc.odb = ODBManager(dvc)
+            dvc.cache = CacheManager(dvc)
 
         return cloud
 
     return _make_workspace
 
 
 @pytest.fixture
@@ -178,53 +210,52 @@
 
 @pytest.fixture
 def local_workspace(make_workspace):
     return make_workspace("workspace", typ="local")
 
 
 @pytest.fixture(scope="session")
-def docker():
-    # See https://travis-ci.community/t/docker-linux-containers-on-windows/301
-    if os.environ.get("CI") and os.name == "nt":
-        pytest.skip("disabled for Windows on Github Actions")
-
-    try:
-        subprocess.check_output("docker ps", shell=True)
-    except (subprocess.CalledProcessError, OSError):
-        pytest.skip("no docker installed")
-
-
-@pytest.fixture(scope="session")
-def docker_compose(docker):
-    try:
-        subprocess.check_output("docker-compose version", shell=True)
-    except (subprocess.CalledProcessError, OSError):
-        pytest.skip("no docker-compose installed")
-
-
-@pytest.fixture(scope="session")
 def docker_compose_project_name():
     return "pytest-dvc-test"
 
 
 @pytest.fixture(scope="session")
 def docker_services(
-    docker_compose_file, docker_compose_project_name, tmp_path_factory
+    tmp_path_factory,
+    docker_compose_command,
+    docker_compose_file,
+    docker_compose_project_name,
+    docker_setup,
 ):
-    # overriding `docker_services` fixture from `pytest_docker` plugin to
-    # only launch docker images once.
-
     from filelock import FileLock
     from pytest_docker.plugin import DockerComposeExecutor, Services
 
+    if os.environ.get("CI") and os.name == "nt":
+        pytest.skip("disabled for Windows on CI")
+
+    try:
+        subprocess.check_output(  # nosec B607, B602
+            "docker ps", stderr=subprocess.STDOUT, shell=True
+        )
+    except subprocess.CalledProcessError as err:
+        out = (err.output or b"").decode("utf-8")
+        pytest.skip(f"docker is not installed or the daemon is not running: {out}")
+
+    try:
+        cmd = "docker-compose version"
+        subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True)  # nosec
+    except subprocess.CalledProcessError as err:
+        out = (err.output or b"").decode("utf-8")
+        pytest.skip(f"docker-compose is not installed: {out}")
+
     executor = DockerComposeExecutor(
-        docker_compose_file, docker_compose_project_name
+        docker_compose_command, docker_compose_file, docker_compose_project_name
     )
 
     # making sure we don't accidentally launch docker-compose in parallel,
     # as it might result in network conflicts. Inspired by:
     # https://github.com/pytest-dev/pytest-xdist#making-session-scoped-fixtures-execute-only-once
     lockfile = tmp_path_factory.getbasetemp().parent / "docker-compose.lock"
-    with FileLock(str(lockfile)):  # pylint:disable=abstract-class-instantiated
-        executor.execute("up --build -d")
-
-    return Services(executor)
+    with FileLock(os.fspath(lockfile)):
+        executor.execute(docker_setup)
+        # note: we are not tearing down the containers here
+        return Services(executor)
```

### Comparing `dvc-2.9.5/dvc/testing/path_info.py` & `dvc-3.0.0a0/dvc/testing/path_info.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,29 +1,29 @@
 # pylint: disable=protected-access
 import os
 import pathlib
 import posixpath
 from typing import Callable
 from urllib.parse import urlparse
 
-from funcy import cached_property
-
 from dvc.utils import relpath
+from dvc.utils.objects import cached_property
 
 
 class _BasePath:
     def overlaps(self, other):
         if isinstance(other, (str, bytes)):
-            other = self.__class__(other)
+            other = self.__class__(other)  # type: ignore[call-arg]
         elif self.__class__ != other.__class__:
             return False
         return self.isin_or_eq(other) or other.isin(self)
 
     def isin_or_eq(self, other):
-        return self == other or self.isin(other)  # pylint: disable=no-member
+        # pylint: disable-next=no-member
+        return self == other or self.isin(other)  # type: ignore[attr-defined]
 
 
 class PathInfo(pathlib.PurePath, _BasePath):
     # Use __slots__ in PathInfo objects following PurePath implementation.
     # This makes objects smaller and speeds up attribute access.
     # We don't add any fields so it's empty.
     __slots__ = ()
@@ -32,18 +32,19 @@
     def __new__(cls, *args):
         # Construct a proper subclass depending on current os
         if cls is PathInfo:
             cls = (  # pylint: disable=self-cls-assignment
                 WindowsPathInfo if os.name == "nt" else PosixPathInfo
             )
 
-        return cls._from_parts(args)
+        return cls._from_parts(args)  # type: ignore[attr-defined]
 
     def as_posix(self):
-        f = self._flavour  # pylint: disable=no-member
+        # pylint: disable-next=no-member
+        f = self._flavour  # type: ignore[attr-defined]
         # Unlike original implementation [1] that uses `str()` we actually need
         # to use `fspath`, because we've overridden `__str__` method to return
         # relative paths, which will break original `as_posix`.
         #
         # [1] https://github.com/python/cpython/blob/v3.7.0/Lib/pathlib.py#L692
         return self.fspath.replace(f.sep, "/")
 
@@ -56,15 +57,15 @@
 
     # This permits passing it to file utils directly in Python 3.6+
     def __fspath__(self):
         return pathlib.PurePath.__str__(self)
 
     @property
     def fspath(self):
-        return self.__fspath__()
+        return os.fspath(self)
 
     url = fspath
 
     path = fspath
 
     def relpath(self, other):
         return self.__class__(relpath(self, other))
@@ -72,15 +73,18 @@
     def isin(self, other):
         if isinstance(other, (str, bytes)):
             other = self.__class__(other)
         elif self.__class__ != other.__class__:
             return False
         # Use cached casefolded parts to compare paths
         n = len(other._cparts)
-        return len(self._cparts) > n and self._cparts[:n] == other._cparts
+        return (
+            len(self._cparts) > n  # type: ignore[attr-defined]
+            and self._cparts[:n] == other._cparts  # type: ignore[attr-defined]
+        )
 
     def relative_to(self, other):  # pylint: disable=arguments-differ
         # pathlib relative_to raises exception when one path is not a direct
         # descendant of the other when os.path.relpath would return abspath.
         # For DVC PathInfo we only need the relpath behavior.
         # See: https://bugs.python.org/issue40358
         try:
@@ -121,15 +125,17 @@
 
 
 class URLInfo(_BasePath):
     DEFAULT_PORTS = {"http": 80, "https": 443, "ssh": 22, "hdfs": 0}
 
     def __init__(self, url):
         p = urlparse(url)
-        assert not p.query and not p.params and not p.fragment
+        assert not p.query
+        assert not p.params
+        assert not p.fragment
         assert p.password is None
         self._fill_parts(p.scheme, p.hostname, p.username, p.port, p.path)
 
     @classmethod
     def from_parts(
         cls, scheme=None, host=None, user=None, port=None, path="", netloc=None
     ):
@@ -150,30 +156,30 @@
         self.port = int(port) if port else self.DEFAULT_PORTS.get(self.scheme)
 
         if isinstance(path, _URLPathInfo):
             self._spath = str(path)
             self._path = path
         else:
             if path and path[0] != "/":
-                path = "/" + path
+                path = "/" + path  # type: ignore[operator]
             self._spath = path
 
     @property
     def _base_parts(self):
         return (self.scheme, self.host, self.user, self.port)
 
     @property
     def parts(self):
         return self._base_parts + self._path.parts
 
     def replace(self, path=None):
-        return self.from_parts(*self._base_parts, path=path)
+        return self.from_parts(*self._base_parts, path=path)  # type: ignore[misc]
 
     @cached_property
-    def url(self):
+    def url(self) -> str:
         return f"{self.scheme}://{self.netloc}{self._spath}"
 
     def __str__(self):
         return self.url
 
     def __repr__(self):
         return f"{type(self).__name__}: '{self}'"
@@ -199,32 +205,34 @@
     __truediv__ = __div__
 
     @property
     def path(self):
         return self._spath
 
     @cached_property
-    def _path(self):  # false-positive, pylint: disable=method-hidden
+    def _path(  # pylint: disable=method-hidden
+        self,
+    ) -> "_URLPathInfo":
         return _URLPathInfo(self._spath)
 
     @property
-    def name(self):
+    def name(self) -> str:
         return self._path.name
 
     @cached_property
-    def netloc(self):
+    def netloc(self) -> str:
         netloc = self.host
         if self.user:
             netloc = self.user + "@" + netloc
         if self.port and int(self.port) != self.DEFAULT_PORTS.get(self.scheme):
             netloc += ":" + str(self.port)
         return netloc
 
     @property
-    def bucket(self):
+    def bucket(self) -> str:
         return self.netloc
 
     @property
     def parent(self):
         return self.replace(path=self._path.parent)
 
     @property
@@ -243,17 +251,15 @@
         return self._path.relative_to(other._path)
 
     def isin(self, other):
         if isinstance(other, (str, bytes)):
             other = self.__class__(other)
         elif self.__class__ != other.__class__:
             return False
-        return self._base_parts == other._base_parts and self._path.isin(
-            other._path
-        )
+        return self._base_parts == other._base_parts and self._path.isin(other._path)
 
 
 class CloudURLInfo(URLInfo):
     @property
     def path(self):
         return self._spath.lstrip("/")
 
@@ -266,15 +272,15 @@
         stripped = p._replace(params=None, query=None, fragment=None)
         super().__init__(stripped.geturl())
         self.params = p.params
         self.query = p.query
         self.fragment = p.fragment
 
     def replace(self, path=None):
-        return self.from_parts(
+        return self.from_parts(  # type: ignore[misc]
             *self._base_parts,
             params=self.params,
             query=self.query,
             fragment=self.fragment,
             path=path,
         )
 
@@ -317,15 +323,15 @@
         return (self.params, self.query, self.fragment)
 
     @property
     def parts(self):
         return self._base_parts + self._path.parts + self._extra_parts
 
     @cached_property
-    def url(self):
+    def url(self) -> str:
         return "{}://{}{}{}{}{}".format(
             self.scheme,
             self.netloc,
             self._spath,
             (";" + self.params) if self.params else "",
             ("?" + self.query) if self.query else "",
             ("#" + self.fragment) if self.fragment else "",
@@ -340,11 +346,11 @@
             and self._path == other._path
             and self._extra_parts == other._extra_parts
         )
 
 
 class WebDAVURLInfo(URLInfo):
     @cached_property
-    def url(self):
+    def url(self) -> str:
         return "{}://{}{}".format(
             self.scheme.replace("webdav", "http"), self.netloc, self._spath
         )
```

### Comparing `dvc-2.9.5/dvc/testing/test_workspace.py` & `dvc-3.0.0a0/tests/unit/test_dvcfile.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,136 +1,127 @@
-import os
-
 import pytest
 
-
-class TestImport:
-    def test_import(self, tmp_dir, dvc, workspace):
-        workspace.gen("file", "file")
-        assert not (tmp_dir / "file").exists()  # sanity check
-        dvc.imp_url("remote://workspace/file")
-        assert (tmp_dir / "file").read_text() == "file"
-        assert dvc.status() == {}
-
-    @pytest.fixture
-    def stage_md5(self):
-        pytest.skip()
-
-    @pytest.fixture
-    def dir_md5(self):
-        pytest.skip()
-
-    def test_import_dir(self, tmp_dir, dvc, workspace, stage_md5, dir_md5):
-        from dvc.data.db import ODBManager
-
-        workspace.gen(
-            {"dir": {"file": "file", "subdir": {"subfile": "subfile"}}}
-        )
-
-        # remove external cache to make sure that we don't need it
-        # to import dirs
-        with dvc.config.edit() as conf:
-            del conf["cache"]
-        dvc.odb = ODBManager(dvc)
-
-        assert not (tmp_dir / "dir").exists()  # sanity check
-        dvc.imp_url("remote://workspace/dir")
-        assert set(os.listdir(tmp_dir / "dir")) == {"file", "subdir"}
-        assert (tmp_dir / "dir" / "file").read_text() == "file"
-        assert list(os.listdir(tmp_dir / "dir" / "subdir")) == ["subfile"]
-        assert (
-            tmp_dir / "dir" / "subdir" / "subfile"
-        ).read_text() == "subfile"
-
-        assert dvc.status() == {}
-
-        if stage_md5 is not None and dir_md5 is not None:
-            assert (tmp_dir / "dir.dvc").read_text() == (
-                f"md5: {stage_md5}\n"
-                "frozen: true\n"
-                "deps:\n"
-                f"- md5: {dir_md5}\n"
-                "  size: 11\n"
-                "  nfiles: 2\n"
-                "  path: remote://workspace/dir\n"
-                "outs:\n"
-                "- md5: b6dcab6ccd17ca0a8bf4a215a37d14cc.dir\n"
-                "  size: 11\n"
-                "  nfiles: 2\n"
-                "  path: dir\n"
-            )
-
-    @pytest.fixture
-    def is_object_storage(self):
-        pytest.skip()
-
-    def test_import_empty_dir(
-        self, tmp_dir, dvc, workspace, is_object_storage
-    ):
-        # prefix based storage services (e.g s3) doesn't have the real concept
-        # of directories. So instead we create an empty file that ends with a
-        # trailing slash in order to actually support this operation
-        if is_object_storage:
-            contents = ""
-        else:
-            contents = {}
-
-        workspace.gen({"empty_dir/": contents})
-
-        dvc.imp_url("remote://workspace/empty_dir/")
-
-        empty_dir = tmp_dir / "empty_dir"
-        assert empty_dir.is_dir()
-        assert tuple(empty_dir.iterdir()) == ()
-
-
-class TestAdd:
-    @pytest.fixture
-    def hash_name(self):
-        pytest.skip()
-
-    @pytest.fixture
-    def hash_value(self):
-        pytest.skip()
-
-    @pytest.fixture
-    def dir_hash_value(self):
-        pytest.skip()
-
-    def test_add(self, tmp_dir, dvc, workspace, hash_name, hash_value):
-        from dvc.stage.exceptions import StageExternalOutputsError
-
-        workspace.gen("file", "file")
-
-        with pytest.raises(StageExternalOutputsError):
-            dvc.add(workspace.url)
-
-        dvc.add("remote://workspace/file")
-        assert (tmp_dir / "file.dvc").read_text() == (
-            "outs:\n"
-            f"- {hash_name}: {hash_value}\n"
-            "  size: 4\n"
-            "  path: remote://workspace/file\n"
-        )
-        assert (workspace / "file").read_text() == "file"
-        assert (
-            workspace / "cache" / hash_value[:2] / hash_value[2:]
-        ).read_text() == "file"
-
-        assert dvc.status() == {}
-
-    def test_add_dir(self, tmp_dir, dvc, workspace, hash_name, dir_hash_value):
-        workspace.gen(
-            {"dir": {"file": "file", "subdir": {"subfile": "subfile"}}}
-        )
-
-        dvc.add("remote://workspace/dir")
-        assert (tmp_dir / "dir.dvc").read_text() == (
-            "outs:\n"
-            f"- {hash_name}: {dir_hash_value}\n"
-            "  size: 11\n"
-            "  nfiles: 2\n"
-            "  path: remote://workspace/dir\n"
-        )
-        assert (
-            workspace / "cache" / dir_hash_value[:2] / dir_hash_value[2:]
-        ).is_file()
+from dvc.dvcfile import (
+    LOCK_FILE,
+    PROJECT_FILE,
+    FileIsGitIgnored,
+    ProjectFile,
+    SingleStageFile,
+    load_file,
+)
+from dvc.stage import PipelineStage
+from dvc.stage.exceptions import StageFileDoesNotExistError, StageFileIsNotDvcFileError
+from dvc.utils.fs import remove
+from dvc.utils.serialize import EncodingError
+from dvc.utils.strictyaml import YAMLValidationError
+
+
+@pytest.mark.parametrize(
+    "path",
+    [
+        "pipelines.yaml",
+        "pipelines.yml",
+        "custom-pipelines.yml",
+        "custom-pipelines.yaml",
+        "../models/pipelines.yml",
+    ],
+)
+def test_pipelines_file(path):
+    file_obj = load_file(object(), path)
+    assert isinstance(file_obj, ProjectFile)
+
+
+@pytest.mark.parametrize("path", ["Dvcfile", "stage.dvc", "../models/stage.dvc"])
+def test_pipelines_single_stage_file(path):
+    file_obj = load_file(object(), path)
+    assert isinstance(file_obj, SingleStageFile)
+
+
+@pytest.mark.parametrize("file", ["stage.dvc", "dvc.yaml"])
+@pytest.mark.parametrize("is_dvcignored", [True, False])
+def test_stage_load_on_not_existing_file(tmp_dir, dvc, file, is_dvcignored):
+    dvcfile = load_file(dvc, file)
+    if is_dvcignored:
+        (tmp_dir / ".dvcignore").write_text(file)
+
+    assert not dvcfile.exists()
+    with pytest.raises(StageFileDoesNotExistError) as exc_info:
+        assert dvcfile.stages.values()
+
+    assert str(exc_info.value) == f"'{file}' does not exist"
+
+
+@pytest.mark.parametrize("file", ["stage.dvc", "dvc.yaml"])
+def test_stage_load_on_non_file(tmp_dir, dvc, file):
+    (tmp_dir / file).mkdir()
+    dvcfile = load_file(dvc, file)
+    with pytest.raises(StageFileIsNotDvcFileError):
+        assert dvcfile.stages.values()
+
+
+@pytest.mark.parametrize("file", ["stage.dvc", "dvc.yaml"])
+def test_stage_load_on_invalid_data(tmp_dir, dvc, file):
+    data = {"is_this_a_valid_dvcfile": False}
+    (tmp_dir / file).dump(data)
+    dvcfile = load_file(dvc, file)
+    with pytest.raises(YAMLValidationError):
+        assert dvcfile.stages
+    with pytest.raises(YAMLValidationError):
+        assert dvcfile.validate(data, file)
+
+
+def test_dump_stage(tmp_dir, dvc):
+    stage = PipelineStage(dvc, cmd="command", name="stage_name", path="dvc.yaml")
+    dvcfile = load_file(dvc, "dvc.yaml")
+
+    dvcfile.dump(stage, update_lock=False, update_pipeline=False)
+    assert not (tmp_dir / PROJECT_FILE).exists()
+    assert not (tmp_dir / LOCK_FILE).exists()
+
+    dvcfile.dump(stage, update_pipeline=False)
+    assert not (tmp_dir / PROJECT_FILE).exists()
+    assert (tmp_dir / LOCK_FILE).exists()
+    assert dvcfile._lockfile.load()
+
+    remove(tmp_dir / LOCK_FILE)
+
+    dvcfile.dump(stage)
+    assert (tmp_dir / PROJECT_FILE).exists()
+    assert (tmp_dir / LOCK_FILE).exists()
+    assert list(dvcfile.stages.values()) == [stage]
+
+
+@pytest.mark.parametrize("file", ["stage.dvc", "dvc.yaml"])
+def test_stage_load_file_exists_but_dvcignored(tmp_dir, dvc, scm, file):
+    (tmp_dir / file).write_text("")
+    (tmp_dir / ".dvcignore").write_text(file)
+
+    dvc._reset()
+    dvcfile = load_file(dvc, file)
+    with pytest.raises(StageFileDoesNotExistError) as exc_info:
+        assert dvcfile.stages.values()
+
+    assert str(exc_info.value) == f"'{file}' is dvc-ignored"
+
+
+@pytest.mark.parametrize("file", ["foo.dvc", "dvc.yaml"])
+def test_try_loading_dvcfile_that_is_gitignored(tmp_dir, dvc, scm, file):
+    with open(tmp_dir / ".gitignore", "a+", encoding="utf-8") as fd:
+        fd.write(file)
+
+    # create a file just to avoid other checks
+    (tmp_dir / file).write_text("")
+    scm._reset()
+
+    dvcfile = load_file(dvc, file)
+    with pytest.raises(FileIsGitIgnored) as exc_info:
+        dvcfile._load()
+
+    assert str(exc_info.value) == f"bad DVC file name '{file}' is git-ignored."
+
+
+def test_dvcfile_encoding_error(tmp_dir, dvc):
+    tmp_dir.gen(PROJECT_FILE, b"\x80some: stuff")
+
+    dvcfile = load_file(dvc, PROJECT_FILE)
+    with pytest.raises(EncodingError):
+        dvcfile._load()
```

### Comparing `dvc-2.9.5/dvc/ui/__init__.py` & `dvc-3.0.0a0/dvc/ui/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,61 +1,83 @@
-from contextlib import contextmanager
+from contextlib import contextmanager, nullcontext
 from typing import (
     TYPE_CHECKING,
     Any,
     Callable,
     Dict,
     Iterable,
     Iterator,
     Optional,
     Sequence,
     TextIO,
     Type,
     Union,
 )
 
-from funcy import cached_property
+import colorama
+
+from dvc.utils.objects import cached_property
 
 if TYPE_CHECKING:
+    from rich.console import Console as RichConsole
+    from rich.console import JustifyMethod, OverflowMethod
     from rich.status import Status
+    from rich.style import Style
     from rich.text import Text as RichText
 
     from dvc.progress import Tqdm
     from dvc.types import StrPath
     from dvc.ui.table import Headers, Styles, TableData
 
 
+@contextmanager
+def disable_colorama():
+    import sys
+
+    colorama.deinit()
+    try:
+        yield
+    finally:
+        if sys.stdout:
+            sys.stdout.flush()
+        if sys.stderr:
+            sys.stderr.flush()
+        colorama.reinit()
+
+
 class Formatter:
-    def __init__(self, theme: Dict = None, defaults: Dict = None) -> None:
+    def __init__(
+        self, theme: Optional[Dict] = None, defaults: Optional[Dict] = None
+    ) -> None:
         from collections import defaultdict
 
         theme = theme or {
             "success": {"color": "green", "style": "bold"},
             "warn": {"color": "yellow"},
             "error": {"color": "red", "style": "bold"},
         }
         self.theme = defaultdict(lambda: defaults or {}, theme)
 
-    def format(self, message: str, style: str = None, **kwargs) -> str:
+    def format(  # noqa: A003
+        self, message: str, style: Optional[str] = None, **kwargs
+    ) -> str:
         from dvc.utils import colorize
 
         return colorize(message, **self.theme[style])
 
 
 class Console:
     def __init__(
-        self, formatter: Formatter = None, enable: bool = False
+        self, formatter: Optional[Formatter] = None, enable: bool = False
     ) -> None:
         from contextvars import ContextVar
 
         self.formatter: Formatter = formatter or Formatter()
         self._enabled: bool = enable
-        self._paginate: ContextVar[bool] = ContextVar(
-            "_paginate", default=False
-        )
+        self._paginate: ContextVar[bool] = ContextVar("_paginate", default=False)
 
     def enable(self) -> None:
         self._enabled = True
 
     def success(self, message: str) -> None:
         self.write(message, style="success")
 
@@ -64,89 +86,151 @@
 
     def warn(self, message: str) -> None:
         self.error_write(message, style="warn")
 
     def error_write(
         self,
         *objects: Any,
-        style: str = None,
-        sep: str = None,
-        end: str = None,
+        style: Optional[str] = None,
+        sep: Optional[str] = None,
+        end: Optional[str] = None,
         styled: bool = False,
         force: bool = True,
     ) -> None:
         return self.write(
             *objects,
             style=style,
             sep=sep,
             end=end,
             stderr=True,
             force=force,
             styled=styled,
         )
 
-    def write_json(
+    def write_json(  # noqa: PLR0913
         self,
         data: Any,
-        indent: int = None,
-        highlight: bool = None,
+        indent: Optional[int] = None,
+        highlight: Optional[bool] = None,
         stderr: bool = False,
         skip_keys: bool = False,
         ensure_ascii: bool = True,
         check_circular: bool = True,
         allow_nan: bool = True,
         default: Optional[Callable[[Any], Any]] = None,
         sort_keys: bool = False,
     ) -> None:
         if highlight is None:
             highlight = self.isatty()
         if indent is None and self.isatty():
             indent = 2
 
-        console = self.error_console if stderr else self.rich_console
-        return console.print_json(
+        from rich.json import JSON
+
+        json = JSON.from_data(
             data=data,
             indent=indent,
             highlight=bool(highlight),
             skip_keys=skip_keys,
             ensure_ascii=ensure_ascii,
             check_circular=check_circular,
             allow_nan=allow_nan,
             default=default,
             sort_keys=sort_keys,
         )
+        if not highlight:
+            import os
+
+            # we don't need colorama to try to strip ansi codes
+            # when highlighting is disabled
+            ctx = nullcontext() if "DVC_TEST" in os.environ else disable_colorama()
+            with ctx:
+                return self.write(json.text, stderr=stderr)
+        return self.rich_print(json, stderr=stderr, soft_wrap=True)
+
+    def rich_print(
+        self,
+        *objects: Any,
+        sep: str = " ",
+        end: str = "\n",
+        stderr: bool = False,
+        style: Optional[Union[str, "Style"]] = None,
+        justify: Optional["JustifyMethod"] = None,
+        overflow: Optional["OverflowMethod"] = None,
+        no_wrap: Optional[bool] = None,
+        emoji: Optional[bool] = None,
+        markup: Optional[bool] = None,
+        highlight: Optional[bool] = None,
+        width: Optional[int] = None,
+        height: Optional[int] = None,
+        crop: bool = True,
+        soft_wrap: Optional[bool] = None,
+        new_line_start: bool = False,
+    ) -> None:
+        if stderr:
+            console = self.error_console
+        else:
+            console = self.rich_console
+        return console.print(
+            *objects,
+            sep=sep,
+            end=end,
+            style=style,
+            justify=justify,
+            overflow=overflow,
+            no_wrap=no_wrap,
+            emoji=emoji,
+            markup=markup,
+            highlight=highlight,
+            width=width,
+            height=height,
+            crop=crop,
+            soft_wrap=soft_wrap,
+            new_line_start=new_line_start,
+        )
 
     def write(
         self,
         *objects: Any,
-        style: str = None,
-        sep: str = None,
-        end: str = None,
+        style: Optional[str] = None,
+        sep: Optional[str] = None,
+        end: Optional[str] = None,
         stderr: bool = False,
         force: bool = False,
         styled: bool = False,
-        file: TextIO = None,
+        file: Optional[TextIO] = None,
     ) -> None:
         import sys
 
         from dvc.progress import Tqdm
 
         sep = " " if sep is None else sep
         end = "\n" if end is None else end
         if not self._enabled and not force:
             return
 
         file = file or (sys.stderr if stderr else sys.stdout)
         with Tqdm.external_write_mode(file=file):
             # if we are inside pager context, send the output to rich's buffer
             if styled or self._paginate.get():
-                console = self.error_console if stderr else self.rich_console
                 if styled:
-                    return console.print(*objects, sep=sep, end=end)
-                return console.out(*objects, sep=sep, end=end, highlight=False)
+                    return self.rich_print(*objects, sep=sep, end=end, stderr=stderr)
+                return self.rich_print(
+                    sep.join(str(_object) for _object in objects),
+                    style=None,
+                    highlight=False,
+                    emoji=False,
+                    markup=False,
+                    no_wrap=True,
+                    overflow="ignore",
+                    crop=False,
+                    sep=sep,
+                    end=end,
+                    stderr=stderr,
+                )
 
             values = (self.formatter.format(obj, style) for obj in objects)
             return print(*values, sep=sep, end=end, file=file)
 
     @property
     def rich_text(self) -> "Type[RichText]":
         from rich.text import Text
@@ -167,15 +251,18 @@
         try:
             with self.rich_console.pager(pager=DvcPager(), styles=styles):
                 yield
         finally:
             self._paginate.reset(tok)
 
     def prompt(
-        self, text: str, choices: Iterable[str] = None, password: bool = False
+        self,
+        text: str,
+        choices: Optional[Iterable[str]] = None,
+        password: bool = False,
     ) -> Optional[str]:
         while True:
             try:
                 response = self.rich_console.input(
                     text + " ", markup=False, password=password
                 )
             except EOFError:
@@ -184,54 +271,51 @@
             answer = response.lower()
             if not choices:
                 return answer
 
             if answer in choices:
                 return answer
 
-            self.write(
-                f"Your response must be one of: {choices}. "
-                "Please try again."
-            )
+            self.write(f"Your response must be one of: {choices}. Please try again.")
 
     def confirm(self, statement: str) -> bool:
         """Ask the user for confirmation about the specified statement.
 
         Args:
             statement: statement to ask the user confirmation about.
         """
         text = f"{statement} [y/n]:"
         answer = self.prompt(text, choices=["yes", "no", "y", "n"])
         if not answer:
             return False
         return answer.startswith("y")
 
     @cached_property
-    def rich_console(self):
+    def rich_console(self) -> "RichConsole":
         """rich_console is only set to stdout for now."""
         from rich import console
 
         return console.Console()
 
     @cached_property
-    def error_console(self):
+    def error_console(self) -> "RichConsole":
         from rich import console
 
         return console.Console(stderr=True)
 
     def table(
         self,
         data: "TableData",
-        headers: "Headers" = None,
+        headers: Optional["Headers"] = None,
         markdown: bool = False,
         rich_table: bool = False,
         force: bool = True,
         pager: bool = False,
-        header_styles: Union[Dict[str, "Styles"], Sequence["Styles"]] = None,
-        row_styles: Sequence["Styles"] = None,
+        header_styles: Optional[Union[Dict[str, "Styles"], Sequence["Styles"]]] = None,
+        row_styles: Optional[Sequence["Styles"]] = None,
         borders: Union[bool, str] = False,
     ) -> None:
         from dvc.ui import table as t
 
         if not data and not markdown:
             return
 
@@ -265,24 +349,20 @@
         import webbrowser
         from pathlib import Path
         from platform import uname
 
         from dvc.utils import relpath
 
         path = Path(file).resolve()
-        url = (
-            relpath(path) if "Microsoft" in uname().release else path.as_uri()
-        )
+        url = relpath(path) if "microsoft" in uname().release.lower() else path.as_uri()
 
         opened = webbrowser.open(url)
 
         if not opened:
-            ui.error_write(
-                f"Failed to open {url}. " "Please try opening it manually."
-            )
+            ui.error_write(f"Failed to open {url}. Please try opening it manually.")
             return 1
 
         return 0
 
 
 ui = Console()
 
@@ -292,10 +372,8 @@
 
     ui.write("No default remote set")
     ui.success("Everything is up to date.")
     ui.warn("Run queued experiments will be removed.")
     ui.error("too few arguments.")
 
     ui.table([("scores.json", "0.5674")], headers=["Path", "auc"])
-    ui.table(
-        [("scores.json", "0.5674")], headers=["Path", "auc"], markdown=True
-    )
+    ui.table([("scores.json", "0.5674")], headers=["Path", "auc"], markdown=True)
```

### Comparing `dvc-2.9.5/dvc/ui/pager.py` & `dvc-3.0.0a0/dvc/ui/pager.py`

 * *Files 8% similar despite different names*

```diff
@@ -55,41 +55,39 @@
         return None
 
     # pylint: disable=redefined-outer-name
     pager = os.getenv(DVC_PAGER)
     if not pager:
         pager = os.getenv(PAGER_ENV)
     if not pager:
-        if os.system(f"({DEFAULT_PAGER}) 2>{os.devnull}") != 0:
+        if os.system(f"({DEFAULT_PAGER}) 2>{os.devnull}") != 0:  # nosec B605
             logger.warning(
-                "Unable to find `less` in the PATH. Check out "
-                "{} for more info.".format(
-                    format_link("https://man.dvc.org/pipeline/show")
-                )
+                "Unable to find `less` in the PATH. Check out %s for more info.",
+                format_link("https://man.dvc.org/pipeline/show"),
             )
         else:
             pager = DEFAULT_PAGER
 
     if pager == DEFAULT_PAGER:
         # if pager is less (i.e. default), regardless of `$LESS`, apply `-RS`.
         # `-R` is required to render ansi escape sequences for exp show
         # and, `-S` is required for horizontal scrolling.
         less_env = bool(os.getenv(LESS))
-        pager = prepare_default_pager(
+        return prepare_default_pager(
             ansi_escapes=True,
             chop_long_lines=True,
             quit_if_one_screen=not less_env,
             no_init=not less_env,
         )
 
     return pager
 
 
 def pager(text: str) -> None:
     _pager = find_pager()
-    logger.trace(f"Using pager: '{_pager}'")  # type: ignore[attr-defined]
+    logger.trace("Using pager: '%s'", _pager)  # type: ignore[attr-defined]
     make_pager(_pager)(text)
 
 
 class DvcPager(Pager):
     def show(self, content: str) -> None:
         pager(content)
```

### Comparing `dvc-2.9.5/dvc/ui/table.py` & `dvc-3.0.0a0/dvc/ui/table.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,34 +1,34 @@
 from collections import abc
 from contextlib import ExitStack, contextmanager
 from itertools import zip_longest
-from typing import TYPE_CHECKING, Dict, Iterator, Sequence, Union
+from typing import TYPE_CHECKING, Dict, Iterator, Optional, Sequence, Union
 
 from dvc.types import DictStrAny
 
 if TYPE_CHECKING:
     from rich.console import Console as RichConsole
     from rich.table import Table
 
     from dvc.ui import Console, RichText
 
 SHOW_MAX_WIDTH = 1024
 
 
-CellT = Union[str, "RichText"]  # RichText is mostly compatible with str
+CellT = Union[str, "RichText", None]  # RichText is mostly compatible with str
 Row = Sequence[CellT]
 TableData = Sequence[Row]
 Headers = Sequence[str]
 Styles = DictStrAny
 
 
 def plain_table(
     ui: "Console",
     data: TableData,
-    headers: Headers = None,
+    headers: Optional[Headers] = None,
     markdown: bool = False,
     pager: bool = False,
     force: bool = True,
 ) -> None:
     from funcy import nullcontext
     from tabulate import tabulate
 
@@ -46,17 +46,15 @@
 
     cm = ui.pager() if pager else nullcontext()
     with cm:
         ui.write(text, force=force)
 
 
 @contextmanager
-def console_width(
-    table: "Table", console: "RichConsole", val: int
-) -> Iterator[None]:
+def console_width(table: "Table", console: "RichConsole", val: int) -> Iterator[None]:
     # NOTE: rich does not have native support for unlimited width
     # via pager. we override rich table compression by setting
     # console width to the full width of the table
     # pylint: disable=protected-access
 
     console_options = console.options
     original = console_options.max_width
@@ -72,18 +70,18 @@
         console_options.max_width = original
         console._width = con_width
 
 
 def rich_table(
     ui: "Console",
     data: TableData,
-    headers: Headers = None,
+    headers: Optional[Headers] = None,
     pager: bool = False,
-    header_styles: Union[Dict[str, Styles], Sequence[Styles]] = None,
-    row_styles: Sequence[Styles] = None,
+    header_styles: Optional[Union[Dict[str, Styles], Sequence[Styles]]] = None,
+    row_styles: Optional[Sequence[Styles]] = None,
     borders: Union[bool, str] = False,
 ) -> None:
     from rich import box
 
     from dvc.utils.table import Table
 
     border_style = {
@@ -106,15 +104,13 @@
 
     rs: Sequence[Styles] = row_styles or []
     for row, style in zip_longest(data, rs):
         table.add_row(*row, **(style or {}))
 
     stack = ExitStack()
     if pager:
-        stack.enter_context(
-            console_width(table, ui.rich_console, SHOW_MAX_WIDTH)
-        )
+        stack.enter_context(console_width(table, ui.rich_console, SHOW_MAX_WIDTH))
         stack.enter_context(ui.pager())
 
     with stack:
         ui.write(table, styled=True)
         return
```

### Comparing `dvc-2.9.5/dvc/utils/__init__.py` & `dvc-3.0.0a0/dvc/utils/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,80 +1,25 @@
 """Helpers for other modules."""
 
 import hashlib
 import json
 import logging
-import math
 import os
 import re
-import stat
 import sys
-import time
 from typing import Dict, List, Optional, Tuple
 
 import colorama
 
 logger = logging.getLogger(__name__)
 
-LOCAL_CHUNK_SIZE = 2**20  # 1 MB
-LARGE_FILE_SIZE = 2**30  # 1 GB
 LARGE_DIR_SIZE = 100
 TARGET_REGEX = re.compile(r"(?P<path>.*?)(:(?P<name>[^\\/:]*))??$")
 
 
-def dos2unix(data):
-    return data.replace(b"\r\n", b"\n")
-
-
-def _fobj_md5(fobj, hash_md5, binary, progress_func=None):
-    while True:
-        data = fobj.read(LOCAL_CHUNK_SIZE)
-        if not data:
-            break
-
-        if binary:
-            chunk = data
-        else:
-            chunk = dos2unix(data)
-
-        hash_md5.update(chunk)
-        if progress_func:
-            progress_func(len(data))
-
-
-def file_md5(fname, fs):
-    """get the (md5 hexdigest, md5 digest) of a file"""
-    from dvc.istextfile import istextfile
-    from dvc.progress import Tqdm
-
-    hash_md5 = hashlib.md5()
-    binary = not istextfile(fname, fs=fs)
-    size = fs.getsize(fname) or 0
-    no_progress_bar = True
-    if size >= LARGE_FILE_SIZE:
-        no_progress_bar = False
-        msg = (
-            f"Computing md5 for a large file '{fname}'. "
-            "This is only done once."
-        )
-        logger.info(msg)
-
-    with Tqdm(
-        desc=str(fname),
-        disable=no_progress_bar,
-        total=size,
-        bytes=True,
-        leave=False,
-    ) as pbar:
-        with fs.open(fname, "rb") as fobj:
-            _fobj_md5(fobj, hash_md5, binary, pbar.update)
-
-    return hash_md5.hexdigest()
-
-
 def bytes_hash(byts, typ):
     hasher = getattr(hashlib, typ)()
     hasher.update(byts)
     return hasher.hexdigest()
 
 
 def dict_filter(d, exclude=()):
@@ -83,17 +28,15 @@
     """
     if not exclude or not isinstance(d, (list, dict)):
         return d
 
     if isinstance(d, list):
         return [dict_filter(e, exclude) for e in d]
 
-    return {
-        k: dict_filter(v, exclude) for k, v in d.items() if k not in exclude
-    }
+    return {k: dict_filter(v, exclude) for k, v in d.items() if k not in exclude}
 
 
 def dict_hash(d, typ, exclude=()):
     filtered = dict_filter(d, exclude)
     byts = json.dumps(filtered, sort_keys=True).encode("utf-8")
     return bytes_hash(byts, typ)
 
@@ -109,36 +52,14 @@
 def _split(list_to_split, chunk_size):
     return [
         list_to_split[i : i + chunk_size]
         for i in range(0, len(list_to_split), chunk_size)
     ]
 
 
-def _to_chunks_by_chunks_number(list_to_split, num_chunks):
-    chunk_size = int(math.ceil(float(len(list_to_split)) / num_chunks))
-
-    if len(list_to_split) == 1:
-        return [list_to_split]
-
-    if chunk_size == 0:
-        chunk_size = 1
-
-    return _split(list_to_split, chunk_size)
-
-
-def to_chunks(list_to_split, num_chunks=None, chunk_size=None):
-    if (num_chunks and chunk_size) or (not num_chunks and not chunk_size):
-        raise ValueError(
-            "Only one among `num_chunks` or `chunk_size` must be defined."
-        )
-    if chunk_size:
-        return _split(list_to_split, chunk_size)
-    return _to_chunks_by_chunks_number(list_to_split, num_chunks)
-
-
 # NOTE: Check if we are in a bundle
 # https://pythonhosted.org/PyInstaller/runtime-information.html
 def is_binary():
     return getattr(sys, "frozen", False)
 
 
 def fix_env(env=None):
@@ -207,27 +128,14 @@
                 del parts[0]
 
         env["PATH"] = ":".join(parts)
 
     return env
 
 
-def tmp_fname(fname=""):
-    """Temporary name for a partial download"""
-    from shortuuid import uuid
-
-    return os.fspath(fname) + "." + uuid() + ".tmp"
-
-
-def current_timestamp():
-    import nanotime
-
-    return int(nanotime.timestamp(time.time()))
-
-
 def colorize(message, color=None, style=None):
     """Returns a message in a specified color."""
     if not color:
         return message
 
     styles = {"dim": colorama.Style.DIM, "bold": colorama.Style.BRIGHT}
 
@@ -282,22 +190,20 @@
             border=colorize(chars["vertical"], color=border_color),
             space=chars["empty"] * padding_horizontal,
             content=_visual_center(line, max_width),
         )
         for line in lines
     ]
 
-    box_str = "{margin}{padding}{content}{padding}{margin}".format(
+    return "{margin}{padding}{content}{padding}{margin}".format(
         margin=colorize(margin, color=border_color),
         padding="".join(padding_lines),
         content="".join(content_lines),
     )
 
-    return box_str
-
 
 def _visual_width(line):
     """Get the the number of columns required to display a string"""
 
     return len(re.sub(colorama.ansitowin32.AnsiToWin32.ANSI_CSI_RE, "", line))
 
 
@@ -328,50 +234,58 @@
         path = resolve_network_drive_windows(os.path.abspath(path))
         start = resolve_network_drive_windows(start)
         if not os.path.commonprefix([start, path]):
             return path
     return os.path.relpath(path, start)
 
 
+def as_posix(path: str) -> str:
+    import ntpath
+    import posixpath
+
+    return path.replace(ntpath.sep, posixpath.sep)
+
+
 def env2bool(var, undefined=False):
     """
     undefined: return value if env var is unset
     """
     var = os.getenv(var, None)
     if var is None:
         return undefined
     return bool(re.search("1|y|yes|true", var, flags=re.I))
 
 
-def resolve_output(inp, out):
-    import errno
+def resolve_output(inp, out, force=False):
     from urllib.parse import urlparse
 
+    from dvc.exceptions import FileExistsLocallyError
+
     name = os.path.basename(os.path.normpath(urlparse(inp).path))
     if not out:
         ret = name
     elif os.path.isdir(out):
         ret = os.path.join(out, name)
     else:
         ret = out
 
-    if os.path.exists(ret):
-        raise FileExistsError(errno.EEXIST, os.strerror(errno.EEXIST), ret)
+    if os.path.exists(ret) and not force:
+        hint = "\nTo override it, re-run with '--force'."
+        raise FileExistsLocallyError(ret, hint=hint)
 
     return ret
 
 
 def resolve_paths(repo, out, always_local=False):
     from urllib.parse import urlparse
 
-    from dvc.fs.local import localfs
+    from dvc.dvcfile import DVC_FILE_SUFFIX
+    from dvc.exceptions import DvcException
+    from dvc.fs import localfs
 
-    from ..dvcfile import DVC_FILE_SUFFIX
-    from ..exceptions import DvcException
-    from ..system import System
     from .fs import contains_symlink_up_to
 
     abspath = os.path.abspath(out)
     dirname = os.path.dirname(abspath)
     base = os.path.basename(os.path.normpath(out))
 
     scheme = urlparse(out).scheme
@@ -379,23 +293,21 @@
     if os.name == "nt" and scheme == os.path.splitdrive(abspath)[0][0].lower():
         # urlparse interprets windows drive letters as URL scheme
         scheme = ""
 
     if scheme or not localfs.path.isin_or_eq(abspath, repo.root_dir):
         wdir = os.getcwd()
     elif contains_symlink_up_to(dirname, repo.root_dir) or (
-        os.path.isdir(abspath) and System.is_symlink(abspath)
+        os.path.isdir(abspath) and localfs.is_symlink(abspath)
     ):
         msg = (
             "Cannot add files inside symlinked directories to DVC. "
             "See {} for more information."
         ).format(
-            format_link(
-                "https://dvc.org/doc/user-guide/troubleshooting#add-symlink"
-            )
+            format_link("https://dvc.org/doc/user-guide/troubleshooting#add-symlink")
         )
         raise DvcException(msg)
     else:
         wdir = dirname
         out = base
 
     if always_local:
@@ -413,24 +325,24 @@
 
 
 def error_link(name):
     return format_link(f"https://error.dvc.org/{name}")
 
 
 def parse_target(
-    target: str, default: str = None, isa_glob: bool = False
+    target: str, default: Optional[str] = None, isa_glob: bool = False
 ) -> Tuple[Optional[str], Optional[str]]:
-    from dvc.dvcfile import PIPELINE_FILE, PIPELINE_LOCK, is_valid_filename
+    from dvc.dvcfile import LOCK_FILE, PROJECT_FILE, is_valid_filename
     from dvc.exceptions import DvcException
     from dvc.parsing import JOIN
 
     if not target:
         return None, None
 
-    default = default or PIPELINE_FILE
+    default = default or PROJECT_FILE
     if isa_glob:
         path, _, glob = target.rpartition(":")
         return path or default, glob or None
 
     # look for first "@", so as not to assume too much about stage name
     # eg: it might contain ":" in a generated stages from dict which might
     # affect further parsing with the regex.
@@ -442,59 +354,59 @@
 
     path, name = (match.group("path"), match.group("name"))
 
     if name and key:
         name += f"{JOIN}{key}"
 
     if path:
-        if os.path.basename(path) == PIPELINE_LOCK:
+        if os.path.basename(path) == LOCK_FILE:
             raise DvcException(
-                "Did you mean: `{}`?".format(
-                    target.replace(".lock", ".yaml", 1)
-                )
+                "Did you mean: `{}`?".format(target.replace(".lock", ".yaml", 1))
             )
         if not name:
             ret = (target, None)
             return ret if is_valid_filename(target) else ret[::-1]
 
     if not path:
-        logger.trace(  # type: ignore[attr-defined]
-            "Assuming file to be '%s'", default
-        )
+        logger.trace("Assuming file to be '%s'", default)  # type: ignore[attr-defined]
 
     return path or default, name
 
 
-def is_exec(mode):
-    return bool(mode & (stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH))
-
-
 def glob_targets(targets, glob=True, recursive=True):
+    from dvc.exceptions import DvcException
+
     if not glob:
         return targets
 
     from glob import iglob
 
-    return [
+    results = [
         exp_target
         for target in targets
         for exp_target in iglob(target, recursive=recursive)
     ]
 
+    if not results:
+        msg = f"Glob {targets} has no matches."
+        raise DvcException(msg)
+
+    return results
+
 
 def error_handler(func):
     def wrapper(*args, **kwargs):
         onerror = kwargs.get("onerror", None)
         result = {}
 
         try:
             vals = func(*args, **kwargs)
             if vals:
                 result["data"] = vals
-        except Exception as e:  # pylint: disable=broad-except
+        except Exception as e:  # noqa: BLE001, pylint: disable=broad-except
             if onerror is not None:
                 onerror(result, e, **kwargs)
         return result
 
     return wrapper
```

### Comparing `dvc-2.9.5/dvc/utils/cli_parse.py` & `dvc-3.0.0a0/dvc/utils/cli_parse.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 from collections import defaultdict
-from typing import Any, Dict, Iterable, List
+from typing import Dict, Iterable, List
 
 
 def parse_params(path_params: Iterable[str]) -> List[Dict[str, List[str]]]:
     """Normalizes the shape of params from the CLI to dict."""
     from dvc.dependency.param import ParamsDependency
 
     ret: Dict[str, List[str]] = defaultdict(list)
@@ -13,39 +13,25 @@
         params = filter(bool, params_str.split(","))
         if not path:
             path = ParamsDependency.DEFAULT_PARAMS_FILE
         ret[path].extend(params)
     return [{path: params} for path, params in ret.items()]
 
 
-def loads_param_overrides(
+def to_path_overrides(
     path_params: Iterable[str],
-) -> Dict[str, Dict[str, Any]]:
-    """Loads the content of params from the cli as Python object."""
-    from ruamel.yaml import YAMLError
-
+) -> Dict[str, List[str]]:
+    """Group overrides by path"""
     from dvc.dependency.param import ParamsDependency
-    from dvc.exceptions import InvalidArgumentError
-
-    from .serialize import loads_yaml
-
-    ret: Dict[str, Dict[str, Any]] = defaultdict(dict)
 
+    path_overrides = defaultdict(list)
     for path_param in path_params:
-        param_name, _, param_value = path_param.partition("=")
-        if not param_value:
-            raise InvalidArgumentError(
-                f"Must provide a value for parameter '{param_name}'"
-            )
-        path, _, param_name = param_name.partition(":")
-        if not param_name:
-            param_name = path
+        path_and_name = path_param.partition("=")[0]
+        if ":" not in path_and_name:
+            override = path_param
             path = ParamsDependency.DEFAULT_PARAMS_FILE
+        else:
+            path, _, override = path_param.partition(":")
 
-        try:
-            ret[path][param_name] = loads_yaml(param_value)
-        except (ValueError, YAMLError):
-            raise InvalidArgumentError(
-                f"Invalid parameter value for '{param_name}': '{param_value}"
-            )
+        path_overrides[path].append(override)
 
-    return ret
+    return dict(path_overrides)
```

### Comparing `dvc-2.9.5/dvc/utils/collections.py` & `dvc-3.0.0a0/dvc/utils/collections.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,81 +1,103 @@
 import inspect
-import os
 from collections.abc import Mapping
 from functools import wraps
-from typing import Callable, Dict, Iterable, List, TypeVar, Union
+from typing import Callable, Dict, Iterable, List, TypeVar, Union, no_type_check
 
-from pygtrie import StringTrie as _StringTrie
 
-from dvc.exceptions import DvcException
-
-
-class NewParamsFound(DvcException):
-    """Thrown if new params were found during merge_params"""
-
-    def __init__(self, new_params: List, *args):
-        self.new_params = new_params
-        super().__init__("New params found during merge", *args)
-
-
-class PathStringTrie(_StringTrie):
-    """Trie based on platform-dependent separator for pathname components."""
-
-    def __init__(self, *args, **kwargs):
-        kwargs["separator"] = os.sep
-        super().__init__(*args, **kwargs)
-
-
-def apply_diff(src, dest):
+@no_type_check
+def apply_diff(src, dest):  # noqa: C901
     """Recursively apply changes from src to dest.
 
     Preserves dest type and hidden info in dest structure,
     like ruamel.yaml leaves when parses files. This includes comments,
     ordering and line foldings.
 
     Used in Stage load/dump cycle to preserve comments and custom formatting.
     """
-    Seq = (list, tuple)
-    Container = (Mapping, list, tuple)
+    Seq = (list, tuple)  # noqa: N806
+    Container = (Mapping, list, tuple)  # noqa: N806
 
     def is_same_type(a, b):
         return any(
-            isinstance(a, t) and isinstance(b, t)
-            for t in [str, Mapping, Seq, bool]
+            isinstance(a, t) and isinstance(b, t) for t in [str, Mapping, Seq, bool]
         )
 
     if isinstance(src, Mapping) and isinstance(dest, Mapping):
         for key, value in src.items():
-            if isinstance(value, Container) and is_same_type(
-                value, dest.get(key)
-            ):
+            if isinstance(value, Container) and is_same_type(value, dest.get(key)):
                 apply_diff(value, dest[key])
             elif key not in dest or value != dest[key]:
                 dest[key] = value
         for key in set(dest) - set(src):
             del dest[key]
     elif isinstance(src, Seq) and isinstance(dest, Seq):
         if len(src) != len(dest):
             dest[:] = src
         else:
             for i, value in enumerate(src):
-                if isinstance(value, Container) and is_same_type(
-                    value, dest[i]
-                ):
+                if isinstance(value, Container) and is_same_type(value, dest[i]):
                     apply_diff(value, dest[i])
                 elif value != dest[i]:
                     dest[i] = value
     else:
-        raise AssertionError(
+        raise AssertionError(  # noqa: TRY004
             "Can't apply diff from {} to {}".format(
                 src.__class__.__name__, dest.__class__.__name__
             )
         )
 
 
+def to_omegaconf(item):
+    """
+    Some parsers return custom classes (i.e. parse_yaml_for_update)
+    that can mess up with omegaconf logic.
+    Cast the custom classes to Python primitives.
+    """
+    if isinstance(item, dict):
+        return {k: to_omegaconf(v) for k, v in item.items()}
+    if isinstance(item, list):
+        return [to_omegaconf(x) for x in item]
+    return item
+
+
+def remove_missing_keys(src, to_update):
+    keys = list(src.keys())
+    for key in keys:
+        if key not in to_update:
+            del src[key]
+        elif isinstance(src[key], dict):
+            remove_missing_keys(src[key], to_update[key])
+
+    return src
+
+
+def _merge_item(d, key, value):
+    if key in d:
+        item = d.get(key, None)
+        if isinstance(item, dict) and isinstance(value, dict):
+            merge_dicts(item, value)
+        else:
+            d[key] = value
+    else:
+        d[key] = value
+
+
+def merge_dicts(src: Dict, to_update: Dict) -> Dict:
+    """Recursively merges dictionaries.
+
+    Args:
+        src (dict): source dictionary of parameters
+        to_update (dict): dictionary of parameters to merge into src
+    """
+    for key, value in to_update.items():
+        _merge_item(src, key, value)
+    return src
+
+
 def ensure_list(item: Union[Iterable[str], str, None]) -> List[str]:
     if item is None:
         return []
     if isinstance(item, str):
         return [item]
     return list(item)
 
@@ -86,39 +108,14 @@
 
 def chunk_dict(d: Dict[_KT, _VT], size: int = 1) -> List[Dict[_KT, _VT]]:
     from funcy import chunks
 
     return [{key: d[key] for key in chunk} for chunk in chunks(size, d)]
 
 
-def merge_params(src: Dict, to_update: Dict, allow_new: bool = True) -> Dict:
-    """
-    Recursively merges params with benedict's syntax support in-place.
-
-    Args:
-        src (dict): source dictionary of parameters
-        to_update (dict): dictionary of parameters to merge into src
-        allow_new (bool): if False, raises an error if new keys would be
-            added to src
-    """
-    from ._benedict import benedict
-
-    data = benedict(src)
-
-    if not allow_new:
-        new_params = list(
-            set(to_update.keys()) - set(data.keypaths(indexes=True))
-        )
-        if new_params:
-            raise NewParamsFound(new_params)
-
-    data.merge(to_update, overwrite=True)
-    return src
-
-
 class _NamespacedDict(dict):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.__dict__ = self
 
 
 def validate(*validators: Callable, post: bool = False):
@@ -150,15 +147,15 @@
     def wrapped(func: Callable):
         sig = inspect.signature(func)
 
         @wraps(func)
         def inner(*args, **kwargs):
             ba = sig.bind(*args, **kwargs)
             ba.apply_defaults()
-            ba.arguments = _NamespacedDict(ba.arguments)
+            ba.arguments = _NamespacedDict(ba.arguments)  # type: ignore[assignment]
 
             if not post:
                 for validator in validators:
                     validator(ba.arguments)
 
             result = func(*ba.args, **ba.kwargs)
             if post:
@@ -172,11 +169,10 @@
 
 
 def nested_contains(dictionary: Dict, phrase: str) -> bool:
     for key, val in dictionary.items():
         if key == phrase and val:
             return True
 
-        if isinstance(val, dict):
-            if nested_contains(val, phrase):
-                return True
+        if isinstance(val, dict) and nested_contains(val, phrase):
+            return True
     return False
```

### Comparing `dvc-2.9.5/dvc/utils/diff.py` & `dvc-3.0.0a0/dvc/utils/diff.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import json
 from collections import defaultdict
+from typing import Dict
 
 from .flatten import flatten
 
 
 def _parse(raw):
     if raw is None or isinstance(raw, (dict, list, int, float)):
         return raw
@@ -12,27 +13,24 @@
     try:
         return json.loads(raw)
     except json.JSONDecodeError:
         return raw
 
 
 def _diff_vals(old, new, with_unchanged):
-    if (
-        isinstance(new, list)
-        and isinstance(old, list)
-        and len(old) == len(new) == 1
-    ):
+    if isinstance(new, list) and isinstance(old, list) and len(old) == len(new) == 1:
         return _diff_vals(old[0], new[0], with_unchanged)
 
     if not with_unchanged and old == new:
         return {}
 
     res = {"old": old, "new": new}
     if isinstance(new, (int, float)) and isinstance(old, (int, float)):
         res["diff"] = new - old
+
     return res
 
 
 def _flatten(d):
     if not d:
         return defaultdict(lambda: None)
 
@@ -42,15 +40,15 @@
     return defaultdict(lambda: "unable to parse")
 
 
 def _diff_dicts(old_dict, new_dict, with_unchanged):
     new = _flatten(new_dict)
     old = _flatten(old_dict)
 
-    res = defaultdict(dict)
+    res: Dict[str, Dict] = defaultdict(dict)
 
     xpaths = set(old.keys())
     xpaths.update(set(new.keys()))
     for xpath in xpaths:
         old_val = old[xpath]
         new_val = new[xpath]
         val_diff = _diff_vals(old_val, new_val, with_unchanged)
@@ -73,15 +71,15 @@
     return {}
 
 
 def diff(old, new, with_unchanged=False):
     paths = set(old.keys())
     paths.update(set(new.keys()))
 
-    res = defaultdict(dict)
+    res: Dict[str, Dict] = defaultdict(dict)
     for path in paths:
         path_diff = _diff(
             old.get(path, {}).get("data", {}),
             new.get(path, {}).get("data", {}),
             with_unchanged,
         )
         if path_diff:
```

### Comparing `dvc-2.9.5/dvc/utils/humanize.py` & `dvc-3.0.0a0/dvc/utils/humanize.py`

 * *Files 14% similar despite different names*

```diff
@@ -3,37 +3,31 @@
 
 def join(words):
     words = list(words)
     if not words:
         return ""
 
     return (
-        "{before} and {after}".format(
-            before=", ".join(words[:-1]), after=words[-1]
-        )
+        "{before} and {after}".format(before=", ".join(words[:-1]), after=words[-1])
         if len(words) > 1
         else words[0]
     )
 
 
 def get_summary(stats):
     status = (
-        (state, len(data) if is_seq(data) else data)
-        for state, data in stats
-        if data
+        (state, len(data) if is_seq(data) else data) for state, data in stats if data
     )
     return join(
         "{} file{} {}".format(num, "s" if num > 1 else "", state)
         for state, num in status
     )
 
 
 ELLIPSIS = "…"
 
 
-def truncate_text(
-    text: str, max_length: int, with_ellipsis: bool = True
-) -> str:
+def truncate_text(text: str, max_length: int, with_ellipsis: bool = True) -> str:
     if with_ellipsis and len(text) > max_length:
         return text[: max_length - 1] + ELLIPSIS
 
     return text[:max_length]
```

### Comparing `dvc-2.9.5/dvc/utils/serialize/__init__.py` & `dvc-3.0.0a0/dvc/utils/serialize/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -6,24 +6,34 @@
 from ._py import *  # noqa, pylint: disable=wildcard-import
 from ._toml import *  # noqa, pylint: disable=wildcard-import
 from ._yaml import *  # noqa, pylint: disable=wildcard-import
 
 LOADERS: DefaultDict[str, LoaderFn] = defaultdict(  # noqa: F405
     lambda: load_yaml  # noqa: F405
 )
-LOADERS.update(
-    {".toml": load_toml, ".json": load_json, ".py": load_py}  # noqa: F405
+LOADERS.update({".toml": load_toml, ".json": load_json, ".py": load_py})  # noqa: F405
+
+PARSERS: DefaultDict[str, ParserFn] = defaultdict(  # noqa: F405
+    lambda: parse_yaml  # noqa: F405
+)
+PARSERS.update(
+    {".toml": parse_toml, ".json": parse_json, ".py": parse_py}  # noqa: F405
 )
 
+
+def load_path(fs_path, fs):
+    suffix = fs.path.suffix(fs_path).lower()
+    loader = LOADERS[suffix]
+    return loader(fs_path, fs=fs)
+
+
 DUMPERS: DefaultDict[str, DumperFn] = defaultdict(  # noqa: F405
     lambda: dump_yaml  # noqa: F405
 )
-DUMPERS.update(
-    {".toml": dump_toml, ".json": dump_json, ".py": dump_py}  # noqa: F405
-)
+DUMPERS.update({".toml": dump_toml, ".json": dump_json, ".py": dump_py})  # noqa: F405
 
 MODIFIERS: DefaultDict[str, ModifierFn] = defaultdict(  # noqa: F405
     lambda: modify_yaml  # noqa: F405
 )
 MODIFIERS.update(
     {
         ".toml": modify_toml,  # noqa: F405
```

### Comparing `dvc-2.9.5/dvc/utils/serialize/_common.py` & `dvc-3.0.0a0/dvc/utils/serialize/_common.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,89 +1,102 @@
 """Common utilities for serialize."""
 import os
 from contextlib import contextmanager
-from typing import TYPE_CHECKING, Any, Callable, ContextManager, Dict, Union
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Callable,
+    ContextManager,
+    Dict,
+    Optional,
+    Protocol,
+    TextIO,
+    Union,
+)
 
 from funcy import reraise
-from typing_extensions import Protocol
 
 from dvc.exceptions import DvcException
 
 if TYPE_CHECKING:
-    from dvc.fs.base import FileSystem
-    from dvc.types import AnyPath
+    from dvc.fs import FileSystem
+    from dvc.types import StrPath
 
 
 class DumperFn(Protocol):
     def __call__(
-        self, path: "AnyPath", data: Any, fs: "FileSystem" = None
+        self, path: "StrPath", data: Any, fs: Optional["FileSystem"] = None
     ) -> Any:
         ...
 
 
+class DumpersFn(Protocol):
+    def __call__(self, data: Any, stream: TextIO) -> Any:
+        ...
+
+
 class ModifierFn(Protocol):
     def __call__(
-        self, path: "AnyPath", fs: "FileSystem" = None
+        self, path: "StrPath", fs: Optional["FileSystem"] = None
     ) -> ContextManager[Dict]:
         ...
 
 
 class LoaderFn(Protocol):
-    def __call__(self, path: "AnyPath", fs: "FileSystem" = None) -> Any:
+    def __call__(self, path: "StrPath", fs: Optional["FileSystem"] = None) -> Any:
         ...
 
 
 ReadType = Union[bytes, None, str]
-ParserFn = Callable[[ReadType, "AnyPath"], dict]
+ParserFn = Callable[[ReadType, "StrPath"], dict]
 
 
 class ParseError(DvcException):
     """Errors while parsing files"""
 
-    def __init__(self, path: "AnyPath", message: str):
+    def __init__(self, path: "StrPath", message: str):
         from dvc.utils import relpath
 
         path = relpath(path)
         self.path = path
         super().__init__(f"unable to read: '{path}', {message}")
 
 
 class EncodingError(ParseError):
     """We could not read a file with the given encoding"""
 
-    def __init__(self, path: "AnyPath", encoding: str):
+    def __init__(self, path: "StrPath", encoding: str):
         self.encoding = encoding
         super().__init__(path, f"is not valid {encoding}")
 
 
-def _load_data(path: "AnyPath", parser: ParserFn, fs: "FileSystem" = None):
+def _load_data(path: "StrPath", parser: ParserFn, fs: Optional["FileSystem"] = None):
     open_fn = fs.open if fs else open
     encoding = "utf-8"
-    with open_fn(path, encoding=encoding) as fd:  # type: ignore
+    with open_fn(path, encoding=encoding) as fd:  # type: ignore[operator]
         with reraise(UnicodeDecodeError, EncodingError(path, encoding)):
             return parser(fd.read(), path)
 
 
 def _dump_data(
     path,
     data: Any,
-    dumper: DumperFn,
-    fs: "FileSystem" = None,
+    dumper: DumpersFn,
+    fs: Optional["FileSystem"] = None,
     **dumper_args,
 ):
     open_fn = fs.open if fs else open
-    with open_fn(path, "w+", encoding="utf-8") as fd:  # type: ignore
+    with open_fn(path, "w+", encoding="utf-8") as fd:  # type: ignore[operator]
         dumper(data, fd, **dumper_args)
 
 
 @contextmanager
 def _modify_data(
-    path: "AnyPath",
+    path: "StrPath",
     parser: ParserFn,
-    dumper: DumperFn,
-    fs: "FileSystem" = None,
+    dumper: DumpersFn,
+    fs: Optional["FileSystem"] = None,
 ):
-    exists_fn = fs.exists if fs else os.path.exists
-    file_exists = exists_fn(path)  # type: ignore
+    file_exists = fs.exists(os.fspath(path)) if fs else os.path.exists(path)
     data = _load_data(path, parser=parser, fs=fs) if file_exists else {}
     yield data
-    dumper(path, data, fs=fs)
+    _dump_data(path, data, dumper=dumper, fs=fs)
```

### Comparing `dvc-2.9.5/dvc/utils/serialize/_json.py` & `dvc-3.0.0a0/dvc/utils/serialize/_json.py`

 * *Files 16% similar despite different names*

```diff
@@ -15,21 +15,25 @@
 
 
 def parse_json(text, path, **kwargs):
     with reraise(json.JSONDecodeError, JSONFileCorruptedError(path)):
         return json.loads(text, **kwargs) or {}
 
 
+def _dump_json(data, stream, **kwargs):
+    return json.dump(data, stream, **kwargs)
+
+
 def dump_json(path, data, fs=None, **kwargs):
-    return _dump_data(path, data, dumper=json.dump, fs=fs, **kwargs)
+    return _dump_data(path, data, dumper=_dump_json, fs=fs, **kwargs)
 
 
 @contextmanager
 def modify_json(path, fs=None):
-    with _modify_data(path, parse_json, dump_json, fs=fs) as d:
+    with _modify_data(path, parse_json, _dump_json, fs=fs) as d:
         yield d
 
 
 def encode_exception(o):
     if isinstance(o, Exception):
         return {"type": type(o).__name__, "msg": str(o)}
     raise TypeError
```

### Comparing `dvc-2.9.5/dvc/utils/serialize/_py.py` & `dvc-3.0.0a0/dvc/utils/serialize/_py.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import ast
 from contextlib import contextmanager
+from typing import Any
 
 from funcy import reraise
 
 from ._common import ParseError, _dump_data, _load_data, _modify_data
 
 _PARAMS_KEY = "__params_old_key_for_update__"
 _PARAMS_TEXT_KEY = "__params_text_key_for_update__"
@@ -19,31 +20,29 @@
 
 
 def parse_py(text, path):
     """Parses text from .py file into Python structure."""
     with reraise(SyntaxError, PythonFileCorruptedError(path)):
         tree = ast.parse(text, filename=path)
 
-    result = _ast_tree_to_dict(tree)
-    return result
+    return _ast_tree_to_dict(tree)
 
 
 def parse_py_for_update(text, path):
     """Parses text into dict for update params."""
     with reraise(SyntaxError, PythonFileCorruptedError(path)):
         tree = ast.parse(text, filename=path)
 
     result = _ast_tree_to_dict(tree)
     result.update({_PARAMS_KEY: _ast_tree_to_dict(tree, lineno=True)})
     result.update({_PARAMS_TEXT_KEY: text})
     return result
 
 
 def _dump(data, stream):
-
     old_params = data[_PARAMS_KEY]
     new_params = {
         key: value
         for key, value in data.items()
         if key not in [_PARAMS_KEY, _PARAMS_TEXT_KEY]
     }
     old_lines = data[_PARAMS_TEXT_KEY].splitlines(True)
@@ -63,30 +62,30 @@
 
     new_lines = _update_lines(old_lines, old_params, new_params)
     new_text = "".join(new_lines)
 
     try:
         ast.parse(new_text)
     except SyntaxError:
-        raise PythonFileCorruptedError(
+        raise PythonFileCorruptedError(  # noqa: B904
             stream.name,
             "Python file structure is corrupted after update params",
         )
 
     stream.write(new_text)
     stream.close()
 
 
 def dump_py(path, data, fs=None):
     return _dump_data(path, data, dumper=_dump, fs=fs)
 
 
 @contextmanager
 def modify_py(path, fs=None):
-    with _modify_data(path, parse_py_for_update, dump_py, fs=fs) as d:
+    with _modify_data(path, parse_py_for_update, _dump, fs=fs) as d:
         yield d
 
 
 def _ast_tree_to_dict(tree, only_self_params=False, lineno=False):
     """Parses ast trees to dict.
 
     :param tree: ast.Tree
@@ -94,56 +93,50 @@
     :param lineno: add params line number (needed for update)
     :return:
     """
     result = {}
     for _body in tree.body:
         try:
             if isinstance(_body, (ast.Assign, ast.AnnAssign)):
-                result.update(
-                    _ast_assign_to_dict(_body, only_self_params, lineno)
-                )
+                result.update(_ast_assign_to_dict(_body, only_self_params, lineno))
             elif isinstance(_body, ast.ClassDef):
+                result.update({_body.name: _ast_tree_to_dict(_body, lineno=lineno)})
+            elif isinstance(_body, ast.FunctionDef) and _body.name == "__init__":
                 result.update(
-                    {_body.name: _ast_tree_to_dict(_body, lineno=lineno)}
-                )
-            elif (
-                isinstance(_body, ast.FunctionDef) and _body.name == "__init__"
-            ):
-                result.update(
-                    _ast_tree_to_dict(
-                        _body, only_self_params=True, lineno=lineno
-                    )
+                    _ast_tree_to_dict(_body, only_self_params=True, lineno=lineno)
                 )
         except ValueError:
             continue
         except AttributeError:
             continue
     return result
 
 
-def _ast_assign_to_dict(assign, only_self_params=False, lineno=False):
+def _ast_assign_to_dict(assign, only_self_params=False, lineno=False):  # noqa: PLR0912
     result = {}
 
     if isinstance(assign, ast.AnnAssign):
         name = _get_ast_name(assign.target, only_self_params)
     elif len(assign.targets) == 1:
         name = _get_ast_name(assign.targets[0], only_self_params)
     else:
         raise AttributeError
 
+    value: Any
     if isinstance(assign.value, ast.Dict):
         value = {}
         for key, val in zip(assign.value.keys, assign.value.values):
             if lineno:
-                value[ast.literal_eval(key)] = {
+                value[ast.literal_eval(key)] = {  # type: ignore[arg-type]
                     "lineno": assign.lineno - 1,
                     "value": ast.literal_eval(val),
                 }
             else:
-                value[ast.literal_eval(key)] = ast.literal_eval(val)
+                v = ast.literal_eval(val)
+                value[ast.literal_eval(key)] = v  # type: ignore[arg-type]
     elif isinstance(assign.value, ast.List):
         value = [ast.literal_eval(val) for val in assign.value.elts]
     elif isinstance(assign.value, ast.Set):
         values = [ast.literal_eval(val) for val in assign.value.elts]
         value = set(values)
     elif isinstance(assign.value, ast.Tuple):
         values = [ast.literal_eval(val) for val in assign.value.elts]
@@ -157,13 +150,11 @@
         result[name] = value
 
     return result
 
 
 def _get_ast_name(target, only_self_params=False):
     if hasattr(target, "id") and not only_self_params:
-        result = target.id
-    elif hasattr(target, "attr") and target.value.id == "self":
-        result = target.attr
-    else:
-        raise AttributeError
-    return result
+        return target.id
+    if hasattr(target, "attr") and target.value.id == "self":
+        return target.attr
+    raise AttributeError
```

### Comparing `dvc-2.9.5/dvc/utils/serialize/_toml.py` & `dvc-3.0.0a0/dvc/utils/serialize/_toml.py`

 * *Files 15% similar despite different names*

```diff
@@ -10,41 +10,42 @@
         super().__init__(path, "TOML file structure is corrupted")
 
 
 def load_toml(path, fs=None):
     return _load_data(path, parser=parse_toml, fs=fs)
 
 
-def parse_toml(text, path, decoder=None):
-    from toml import TomlDecodeError, loads
+def _parse_toml(text, path):
+    from tomlkit import loads
+    from tomlkit.exceptions import ParseError as TomlkitParseError
 
-    with reraise(TomlDecodeError, TOMLFileCorruptedError(path)):
-        return loads(text, decoder=decoder)
+    with reraise(TomlkitParseError, TOMLFileCorruptedError(path)):
+        return loads(text)
 
 
-def parse_toml_for_update(text, path):
-    """Parses text into Python structure.
+def parse_toml(text, path, preserve_comments=False):
+    rval = _parse_toml(text, path)
+
+    if preserve_comments:
+        return rval
 
-    NOTE: Python toml package does not currently use ordered dicts, so
-    keys may be re-ordered between load/dump, but this function will at
-    least preserve comments.
-    """
-    from toml import TomlPreserveCommentDecoder
+    return rval.unwrap()
 
-    decoder = TomlPreserveCommentDecoder()
-    return parse_toml(text, path, decoder=decoder)
+
+def parse_toml_for_update(text, path):
+    return parse_toml(text, path, preserve_comments=True)
 
 
-def _dump(data, stream):
-    import toml
+def _dump(data, stream, sort_keys=False):
+    import tomlkit
 
-    return toml.dump(data, stream, encoder=toml.TomlPreserveCommentEncoder())
+    return tomlkit.dump(data, stream, sort_keys=sort_keys)
 
 
 def dump_toml(path, data, fs=None, **kwargs):
     return _dump_data(path, data, dumper=_dump, fs=fs, **kwargs)
 
 
 @contextmanager
 def modify_toml(path, fs=None):
-    with _modify_data(path, parse_toml_for_update, dump_toml, fs=fs) as d:
+    with _modify_data(path, parse_toml_for_update, _dump, fs=fs) as d:
         yield d
```

### Comparing `dvc-2.9.5/dvc/utils/serialize/_yaml.py` & `dvc-3.0.0a0/dvc/utils/serialize/_yaml.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 import io
 from collections import OrderedDict
 from contextlib import contextmanager
+from typing import Any, TextIO
 
 from funcy import reraise
 
 from ._common import ParseError, _dump_data, _load_data, _modify_data
 
 
 class YAMLError(ParseError):
@@ -49,15 +50,15 @@
 
     # tell Dumper to represent OrderedDict as normal dict
     yaml_repr_cls = yaml.Representer
     yaml_repr_cls.add_representer(OrderedDict, yaml_repr_cls.represent_dict)
     return yaml
 
 
-def _dump(data, stream):
+def _dump(data: Any, stream: TextIO) -> Any:
     yaml = _get_yaml()
     return yaml.dump(data, stream)
 
 
 def dump_yaml(path, data, fs=None, **kwargs):
     return _dump_data(path, data, dumper=_dump, fs=fs, **kwargs)
 
@@ -72,9 +73,9 @@
     stream = io.StringIO()
     _dump(d, stream)
     return stream.getvalue()
 
 
 @contextmanager
 def modify_yaml(path, fs=None):
-    with _modify_data(path, parse_yaml_for_update, dump_yaml, fs=fs) as d:
+    with _modify_data(path, parse_yaml_for_update, _dump, fs=fs) as d:
         yield d
```

### Comparing `dvc-2.9.5/dvc/utils/strictyaml.py` & `dvc-3.0.0a0/dvc/utils/strictyaml.py`

 * *Files 9% similar despite different names*

```diff
@@ -21,15 +21,15 @@
 )
 
 if TYPE_CHECKING:
     from rich.syntax import Syntax
     from ruamel.yaml import StreamMark
     from voluptuous import MultipleInvalid
 
-    from dvc.fs.base import FileSystem
+    from dvc.fs import FileSystem
     from dvc.ui import RichText
 
 
 _T = TypeVar("_T")
 merge_conflict_marker = re.compile("^([<=>]{7}) .*$", re.MULTILINE)
 
 
@@ -49,17 +49,15 @@
     return ui.rich_text(message, style="red")
 
 
 def _prepare_cause(cause: str) -> "RichText":
     return ui.rich_text(cause, style="bold")
 
 
-def _prepare_code_snippets(
-    code: str, start_line: int = 1, **kwargs: Any
-) -> "Syntax":
+def _prepare_code_snippets(code: str, start_line: int = 1, **kwargs: Any) -> "Syntax":
     from rich.syntax import Syntax
 
     kwargs.setdefault("start_line", start_line)
     return Syntax(
         code,
         "yaml",
         theme="ansi_dark",
@@ -67,77 +65,82 @@
         line_numbers=True,
         indent_guides=True,
         **kwargs,
     )
 
 
 class YAMLSyntaxError(PrettyDvcException, YAMLFileCorruptedError):
-    def __init__(self, path: str, yaml_text: str, exc: Exception) -> None:
+    def __init__(
+        self,
+        path: str,
+        yaml_text: str,
+        exc: Exception,
+        rev: Optional[str] = None,
+    ) -> None:
         self.path: str = path
         self.yaml_text: str = yaml_text
         self.exc: Exception = exc
 
         merge_conflicts = merge_conflict_marker.search(self.yaml_text)
         self.hint = " (possible merge conflicts)" if merge_conflicts else ""
+        self.rev: Optional[str] = rev
         super().__init__(self.path)
 
-    def __pretty_exc__(self, **kwargs: Any) -> None:
+    def __pretty_exc__(self, **kwargs: Any) -> None:  # noqa: C901
         from ruamel.yaml.error import MarkedYAMLError
 
         exc = self.exc.__cause__
 
         if not isinstance(exc, MarkedYAMLError):
-            raise ValueError("nothing to pretty-print here. :)")
+            raise ValueError("nothing to pretty-print here.")  # noqa: TRY004
 
         source = self.yaml_text.splitlines()
 
         def prepare_linecol(mark: "StreamMark") -> str:
             return f"in line {mark.line + 1}, column {mark.column + 1}"
 
         def prepare_message(
-            message: str, mark: "StreamMark" = None
+            message: str, mark: Optional["StreamMark"] = None
         ) -> "RichText":
             cause = ", ".join(
                 [message.capitalize(), prepare_linecol(mark) if mark else ""]
             )
             return _prepare_cause(cause)
 
         def prepare_code(mark: "StreamMark") -> "Syntax":
             line = mark.line + 1
             code = "" if line > len(source) else source[line - 1]
             return _prepare_code_snippets(code, line)
 
         lines: List[object] = []
         if hasattr(exc, "context"):
             if exc.context_mark is not None:
-                lines.append(
-                    prepare_message(str(exc.context), exc.context_mark)
-                )
+                lines.append(prepare_message(str(exc.context), exc.context_mark))
             if exc.context_mark is not None and (
                 exc.problem is None
                 or exc.problem_mark is None
                 or exc.context_mark.name != exc.problem_mark.name
                 or exc.context_mark.line != exc.problem_mark.line
                 or exc.context_mark.column != exc.problem_mark.column
             ):
                 lines.extend([prepare_code(exc.context_mark), ""])
             if exc.problem is not None:
-                lines.append(
-                    prepare_message(str(exc.problem), exc.problem_mark)
-                )
+                lines.append(prepare_message(str(exc.problem), exc.problem_mark))
             if exc.problem_mark is not None:
                 lines.append(prepare_code(exc.problem_mark))
 
         if lines:
             # we should not add a newline after the main message
             # if there are no other outputs
             lines.insert(0, "")
 
         rel = make_relpath(self.path)
-        lines.insert(0, _prepare_message(f"'{rel}' is invalid{self.hint}."))
+        rev_msg = f" in revision '{self.rev[:7]}'" if self.rev else ""
+        msg_fmt = f"'{rel}' is invalid{self.hint}{rev_msg}."
+        lines.insert(0, _prepare_message(msg_fmt))
         for line in lines:
             ui.error_write(line, styled=True)
 
 
 def determine_linecol(
     data, paths, max_steps=5
 ) -> Tuple[Optional[int], Optional[int], int]:
@@ -162,45 +165,47 @@
     This may end up being not accurate, so we try to show the same amount of
     lines of code for `n` number of steps taken upwards. In a worst case,
     it may be just 1 step (as non-collection item cannot have child items),
     but `schema validator` may provide us arbitrary path. So, this caps the
     number of steps upward to just 5. If it does not find any linecols, it'll
     abort.
     """
-    from dpath.util import get
+    from dpath import get
 
     step = 1
     line, col = None, None
     while paths and step < max_steps:
         value = get(data, paths, default=None)
         if value is not None:
             with suppress(AttributeError, TypeError):
-                line = value.lc.line + 1
-                col = value.lc.col + 1
+                line = value.lc.line + 1  # type: ignore[attr-defined]
+                col = value.lc.col + 1  # type: ignore[attr-defined]
                 break
         step += 1
         *paths, _ = paths
 
     return line, col, step
 
 
 class YAMLValidationError(PrettyDvcException):
     def __init__(
         self,
         exc: "MultipleInvalid",
-        path: str = None,
-        text: str = None,
+        path: Optional[str] = None,
+        text: Optional[str] = None,
+        rev: Optional[str] = None,
     ) -> None:
         self.text = text or ""
         self.exc = exc
 
         rel = make_relpath(path) if path else ""
         self.path = path or ""
 
         message = f"'{rel}' validation failed"
+        message += f" in revision '{rev[:7]}'" if rev else ""
         if len(self.exc.errors) > 1:
             message += f": {len(self.exc.errors)} errors"
         super().__init__(f"{message}")
 
     def _prepare_context(self, data: typing.Mapping) -> List[object]:
         lines: List[object] = []
         for index, error in enumerate(self.exc.errors):
@@ -248,41 +253,44 @@
         for line in lines:
             ui.error_write(line, styled=True)
 
 
 def validate(
     data: _T,
     schema: Callable[[_T], _T],
-    text: str = None,
-    path: str = None,
+    text: Optional[str] = None,
+    path: Optional[str] = None,
+    rev: Optional[str] = None,
 ) -> _T:
     from voluptuous import MultipleInvalid
 
     try:
         return schema(data)
     except MultipleInvalid as exc:
-        raise YAMLValidationError(exc, path, text) from exc
+        raise YAMLValidationError(exc, path, text, rev=rev) from exc
 
 
 def load(
     path: str,
-    schema: Callable[[_T], _T] = None,
-    fs: "FileSystem" = None,
+    schema: Optional[Callable[[_T], _T]] = None,
+    fs: Optional["FileSystem"] = None,
     encoding: str = "utf-8",
     round_trip: bool = False,
 ) -> Any:
     open_fn = fs.open if fs else open
+    rev = getattr(fs, "rev", None)
+
     try:
-        with open_fn(path, encoding=encoding) as fd:  # type: ignore
+        with open_fn(path, encoding=encoding) as fd:  # type: ignore[operator]
             text = fd.read()
         data = parse_yaml(text, path, typ="rt" if round_trip else "safe")
     except UnicodeDecodeError as exc:
         raise EncodingError(path, encoding) from exc
     except YAMLFileCorruptedError as exc:
         cause = exc.__cause__
-        raise YAMLSyntaxError(path, text, exc) from cause
+        raise YAMLSyntaxError(path, text, exc, rev=rev) from cause
 
     if schema:
         # not returning validated data, as it may remove
         # details from CommentedMap that we get from roundtrip parser
-        validate(data, schema, text=text, path=path)
+        validate(data, schema, text=text, path=path, rev=rev)
     return data, text
```

### Comparing `dvc-2.9.5/dvc/utils/table.py` & `dvc-3.0.0a0/dvc/utils/table.py`

 * *Files 6% similar despite different names*

```diff
@@ -22,18 +22,15 @@
         Adjacent collapsed columns will be removed until there is only a single
         truncated column remaining.
         """
         widths = super()._calculate_column_widths(console, options)
         last_collapsed = -1
         columns = self.columns
         for i in range(len(columns) - 1, -1, -1):
-            if (
-                widths[i] == 0
-                and columns[i].collapse  # type: ignore[attr-defined]
-            ):
+            if widths[i] == 0 and columns[i].collapse:  # type: ignore[attr-defined]
                 if last_collapsed >= 0:
                     del widths[last_collapsed]
                     del columns[last_collapsed]
                     if self.box:
                         options.max_width += 1
                     for column in columns[last_collapsed:]:
                         column._index -= 1
@@ -47,25 +44,28 @@
                     # single unicode ellipsis in this column
                     widths[i] = 1 + padding
             else:
                 last_collapsed = -1
         return widths
 
     def _collapse_widths(  # type: ignore[override]
-        self, widths: List[int], wrapable: List[bool], max_width: int
+        # pylint: disable=arguments-differ
+        self,
+        widths: List[int],
+        wrapable: List[bool],
+        max_width: int,
     ) -> List[int]:
         """Collapse columns right-to-left if possible to fit table into
         max_width.
 
         If table is still too wide after collapsing, rich's automatic overflow
         handling will be used.
         """
         collapsible = [
-            column.collapse  # type: ignore[attr-defined]
-            for column in self.columns
+            column.collapse for column in self.columns  # type: ignore[attr-defined]
         ]
         total_width = sum(widths)
         excess_width = total_width - max_width
         if any(collapsible):
             for i in range(len(widths) - 1, -1, -1):
                 if collapsible[i]:
                     excess_width -= widths[i]
```

### Comparing `dvc-2.9.5/dvc.egg-info/PKG-INFO` & `dvc-3.0.0a0/README.rst`

 * *Files 26% similar despite different names*

```diff
@@ -1,287 +1,230 @@
-Metadata-Version: 2.1
-Name: dvc
-Version: 2.9.5
-Summary: Git for data scientists - manage your code and data together
-Home-page: http://dvc.org
-Download-URL: https://github.com/iterative/dvc
-Author: Dmitry Petrov
-Author-email: dmitry@dvc.org
-Maintainer: Iterative
-Maintainer-email: support@dvc.org
-License: Apache License 2.0
-Project-URL: Documentation, https://dvc.org/doc
-Project-URL: Source, https://github.com/iterative/dvc
-Keywords: data-science, data-version-control, machine-learning, git,developer-tools, reproducibility, collaboration, ai
-Platform: UNKNOWN
-Classifier: Development Status :: 4 - Beta
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Requires-Python: >=3.7
-Description-Content-Type: text/x-rst
-Provides-Extra: all
-Provides-Extra: dev
-Provides-Extra: azure
-Provides-Extra: gdrive
-Provides-Extra: gs
-Provides-Extra: hdfs
-Provides-Extra: oss
-Provides-Extra: s3
-Provides-Extra: ssh
-Provides-Extra: ssh_gssapi
-Provides-Extra: webdav
-Provides-Extra: webhdfs
-Provides-Extra: webdhfs_kerberos
-Provides-Extra: terraform
-Provides-Extra: tests
-License-File: LICENSE
-
 |Banner|
 
 `Website <https://dvc.org>`_
 • `Docs <https://dvc.org/doc>`_
 • `Blog <http://blog.dataversioncontrol.com>`_
-• `Twitter <https://twitter.com/DVCorg>`_
-• `Chat (Community & Support) <https://dvc.org/chat>`_
 • `Tutorial <https://dvc.org/doc/get-started>`_
-• `Mailing List <https://sweedom.us10.list-manage.com/subscribe/post?u=a08bf93caae4063c4e6a351f6&id=24c0ecc49a>`_
+• `Related Technologies <https://dvc.org/doc/user-guide/related-technologies>`_
+• `How DVC works`_
+• `VS Code Extension`_
+• `Installation`_
+• `Contributing`_
+• `Community and Support`_
 
-|CI| |Maintainability| |Coverage| |Donate| |DOI|
+|CI| |Python Version| |Coverage| |VS Code| |DOI|
 
-|PyPI| |Packages| |Brew| |Conda| |Choco| |Snap|
+|PyPI| |PyPI Downloads| |Packages| |Brew| |Conda| |Choco| |Snap|
 
 |
 
-**Data Version Control** or **DVC** is an **open-source** tool for data science and machine
-learning projects. Key features:
+**Data Version Control** or **DVC** is a command line tool and `VS Code Extension`_ to help you develop reproducible machine learning projects:
+
+#. **Version** your data and models.
+   Store them in your cloud storage but keep their version info in your Git repo.
+
+#. **Iterate** fast with lightweight pipelines.
+   When you make changes, only run the steps impacted by those changes.
 
-#. Simple **command line** Git-like experience. Does not require installing and maintaining
-   any databases. Does not depend on any proprietary online services.
+#. **Track** experiments in your local Git repo (no servers needed).
 
-#. Management and versioning of **datasets** and **machine learning
-   models**. Data can be saved in S3, Google cloud, Azure, Alibaba 
-   cloud, SSH server, HDFS, or even local HDD RAID.
+#. **Compare** any data, code, parameters, model, or performance plots.
+
+#. **Share** experiments and automatically reproduce anyone's experiment.
+
+Quick start
+===========
 
-#. Makes projects **reproducible** and **shareable**; helping to answer questions about how
-   a model was built.
+    Please read our `Command Reference <https://dvc.org/doc/command-reference>`_ for a complete list.
 
-#. Helps manage experiments with Git tags/branches and **metrics** tracking.
+A common CLI workflow includes:
 
-**DVC** aims to replace spreadsheet and document sharing tools (such as Excel or Google Docs)
-frequently used as both knowledge repositories and team ledgers.
-DVC also replaces both ad-hoc scripts to track, move, and deploy different model versions
-and ad-hoc data file suffixes and prefixes.
 
-.. contents:: **Contents**
-  :backlinks: none
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Task                              | Terminal                                                                                           |
++===================================+====================================================================================================+
+| Track data                        | | ``$ git add train.py params.yaml``                                                               |
+|                                   | | ``$ dvc add images/``                                                                            |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Connect code and data             | | ``$ dvc stage add -n featurize -d images/ -o features/ python featurize.py``                     |
+|                                   | | ``$ dvc stage add -n train -d features/ -d train.py -o model.p -M metrics.json python train.py`` |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Make changes and experiment       | | ``$ dvc exp run -n exp-baseline``                                                                |
+|                                   | | ``$ vi train.py``                                                                                |
+|                                   | | ``$ dvc exp run -n exp-code-change``                                                             |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Compare and select experiments    | | ``$ dvc exp show``                                                                               |
+|                                   | | ``$ dvc exp apply exp-baseline``                                                                 |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Share code                        | | ``$ git add .``                                                                                  |
+|                                   | | ``$ git commit -m 'The baseline model'``                                                         |
+|                                   | | ``$ git push``                                                                                   |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
+| Share data and ML models          | | ``$ dvc remote add myremote -d s3://mybucket/image_cnn``                                         |
+|                                   | | ``$ dvc push``                                                                                   |
++-----------------------------------+----------------------------------------------------------------------------------------------------+
 
 How DVC works
 =============
 
-We encourage you to read our `Get Started <https://dvc.org/doc/get-started>`_ guide to better understand what DVC
-is and how it can fit your scenarios.
+    We encourage you to read our `Get Started
+    <https://dvc.org/doc/get-started>`_ docs to better understand what DVC
+    does and how it can fit your scenarios.
+
+The closest *analogies* to describe the main DVC features are these:
+
+#. **Git for data**: Store and share data artifacts (like Git-LFS but without a server) and models, connecting them with a Git repository. Data management meets GitOps!
+#. **Makefiles** for ML: Describes how data or model artifacts are built from other data and code in a standard format. Now you can version your data pipelines with Git.
+#. Local **experiment tracking**: Turn your machine into an ML experiment management platform, and collaborate with others using existing Git hosting (Github, Gitlab, etc.).
+
+Git is employed as usual to store and version code (including DVC meta-files as placeholders for data).
+DVC `stores data and model files <https://dvc.org/doc/start/data-management>`_ seamlessly in a cache outside of Git, while preserving almost the same user experience as if they were in the repo.
+To share and back up the *data cache*, DVC supports multiple remote storage platforms - any cloud (S3, Azure, Google Cloud, etc.) or on-premise network storage (via SSH, for example).
 
-The easiest (but not perfect!) *analogy* to describe it: DVC is Git (or Git-LFS to be precise) & Makefiles
-made right and tailored specifically for ML and Data Science scenarios.
+|Flowchart|
 
-#. ``Git/Git-LFS`` part - DVC helps store and share data artifacts and models, connecting them with a Git repository.
-#. ``Makefile``\ s part - DVC describes how one data or model artifact was built from other data and code.
+`DVC pipelines <https://dvc.org/doc/start/data-management/data-pipelines>`_ (computational graphs) connect code and data together.
+They specify all steps required to produce a model: input dependencies including code, data, commands to run; and output information to be saved.
 
-DVC usually runs along with Git. Git is used as usual to store and version code (including DVC meta-files). DVC helps
-to store data and model files seamlessly out of Git, while preserving almost the same user experience as if they
-were stored in Git itself. To store and share the data cache, DVC supports multiple remotes - any cloud (S3, Azure,
-Google Cloud, etc) or any on-premise network storage (via SSH, for example).
+Last but not least, `DVC Experiment Versioning <https://dvc.org/doc/start/experiments>`_ lets you prepare and run a large number of experiments.
+Their results can be filtered and compared based on hyperparameters and metrics, and visualized with multiple plots.
 
-|Flowchart|
+.. _`VS Code Extension`:
 
-The DVC pipelines (computational graph) feature connects code and data together. It is possible to explicitly
-specify all steps required to produce a model: input dependencies including data, commands to run,
-and output information to be saved. See the quick start section below or
-the `Get Started <https://dvc.org/doc/get-started>`_ tutorial to learn more.
+Visual Studio Code Extension
+============================
 
-Quick start
-===========
+|VS Code|
+
+To use DVC as a GUI right from your VS Code IDE, install the `DVC Extension <https://marketplace.visualstudio.com/items?itemName=Iterative.dvc>`_ from the Marketplace.
+It currently features experiment tracking and data management, and more features (data pipeline support, etc.) are coming soon!
 
-Please read `Get Started <https://dvc.org/doc/get-started>`_ guide for a full version. Common workflow commands include:
+|VS Code Extension Overview|
 
-+-----------------------------------+----------------------------------------------------------------------------+
-| Step                              | Command                                                                    |
-+===================================+============================================================================+
-| Track data                        | | ``$ git add train.py``                                                   |
-|                                   | | ``$ dvc add images.zip``                                                 |
-+-----------------------------------+----------------------------------------------------------------------------+
-| Connect code and data by commands | | ``$ dvc run -n prepare -d images.zip -o images/ unzip -q images.zip``    |
-|                                   | | ``$ dvc run -n train -d images/ -d train.py -o model.p python train.py`` |
-+-----------------------------------+----------------------------------------------------------------------------+
-| Make changes and reproduce        | | ``$ vi train.py``                                                        |
-|                                   | | ``$ dvc repro model.p.dvc``                                              |
-+-----------------------------------+----------------------------------------------------------------------------+
-| Share code                        | | ``$ git add .``                                                          |
-|                                   | | ``$ git commit -m 'The baseline model'``                                 |
-|                                   | | ``$ git push``                                                           |
-+-----------------------------------+----------------------------------------------------------------------------+
-| Share data and ML models          | | ``$ dvc remote add myremote -d s3://mybucket/image_cnn``                 |
-|                                   | | ``$ dvc push``                                                           |
-+-----------------------------------+----------------------------------------------------------------------------+
+    Note: You'll have to install core DVC on your system separately (as detailed
+    below). The Extension will guide you if needed.
 
 Installation
 ============
 
-There are four options to install DVC: ``pip``, Homebrew, Conda (Anaconda) or an OS-specific package.
+There are several ways to install DVC: in VS Code; using ``snap``, ``choco``, ``brew``, ``conda``, ``pip``; or with an OS-specific package.
 Full instructions are `available here <https://dvc.org/doc/get-started/install>`_.
 
-Snap (Snapcraft/Linux)
-----------------------
+Snapcraft (Linux)
+-----------------
 
 |Snap|
 
 .. code-block:: bash
 
    snap install dvc --classic
 
 This corresponds to the latest tagged release.
-Add ``--beta`` for the latest tagged release candidate,
-or ``--edge`` for the latest ``main`` version.
+Add ``--beta`` for the latest tagged release candidate, or ``--edge`` for the latest ``main`` version.
 
-Choco (Chocolatey/Windows)
---------------------------
+Chocolatey (Windows)
+--------------------
 
 |Choco|
 
 .. code-block:: bash
 
    choco install dvc
 
-Brew (Homebrew/Mac OS)
-----------------------
+Brew (mac OS)
+-------------
 
 |Brew|
 
 .. code-block:: bash
 
    brew install dvc
 
-Conda (Anaconda)
-----------------
+Anaconda (Any platform)
+-----------------------
 
 |Conda|
 
 .. code-block:: bash
 
    conda install -c conda-forge mamba # installs much faster than conda
    mamba install -c conda-forge dvc
 
-Depending on the remote storage type you plan to use to keep and share your data, you might need to
-install optional dependencies: `dvc-s3`, `dvc-azure`, `dvc-gdrive`, `dvc-gs`, `dvc-oss`, `dvc-ssh`.
+Depending on the remote storage type you plan to use to keep and share your data, you might need to install optional dependencies: `dvc-s3`, `dvc-azure`, `dvc-gdrive`, `dvc-gs`, `dvc-oss`, `dvc-ssh`.
 
-pip (PyPI)
-----------
+PyPI (Python)
+-------------
 
 |PyPI|
 
 .. code-block:: bash
 
    pip install dvc
 
-Depending on the remote storage type you plan to use to keep and share your data, you might need to specify
-one of the optional dependencies: ``s3``, ``gs``, ``azure``, ``oss``, ``ssh``. Or ``all`` to include them all.
-The command should look like this: ``pip install dvc[s3]`` (in this case AWS S3 dependencies such as ``boto3``
-will be installed automatically).
+Depending on the remote storage type you plan to use to keep and share your data, you might need to specify one of the optional dependencies: ``s3``, ``gs``, ``azure``, ``oss``, ``ssh``. Or ``all`` to include them all.
+The command should look like this: ``pip install 'dvc[s3]'`` (in this case AWS S3 dependencies such as ``boto3`` will be installed automatically).
 
 To install the development version, run:
 
 .. code-block:: bash
 
    pip install git+git://github.com/iterative/dvc
 
-Package
--------
+Package (Platform-specific)
+---------------------------
 
 |Packages|
 
-Self-contained packages for Linux, Windows, and Mac are available. The latest version of the packages
-can be found on the GitHub `releases page <https://github.com/iterative/dvc/releases>`_.
+Self-contained packages for Linux, Windows, and Mac are available.
+The latest version of the packages can be found on the GitHub `releases page <https://github.com/iterative/dvc/releases>`_.
 
 Ubuntu / Debian (deb)
 ^^^^^^^^^^^^^^^^^^^^^
 .. code-block:: bash
 
    sudo wget https://dvc.org/deb/dvc.list -O /etc/apt/sources.list.d/dvc.list
-   sudo apt-get update
-   sudo apt-get install dvc
+   wget -qO - https://dvc.org/deb/iterative.asc | sudo apt-key add -
+   sudo apt update
+   sudo apt install dvc
 
 Fedora / CentOS (rpm)
 ^^^^^^^^^^^^^^^^^^^^^
 .. code-block:: bash
 
    sudo wget https://dvc.org/rpm/dvc.repo -O /etc/yum.repos.d/dvc.repo
+   sudo rpm --import https://dvc.org/rpm/iterative.asc
    sudo yum update
    sudo yum install dvc
 
-Comparison to related technologies
-==================================
-
-#. Data Engineering tools such as `AirFlow <https://airflow.apache.org/>`_,
-   `Luigi <https://github.com/spotify/luigi>`_, and others - in DVC data,
-   model and ML pipelines represent a single ML project focused on data
-   scientists' experience.  Data engineering tools orchestrate multiple data
-   projects and focus on efficient execution. A DVC project can be used from
-   existing data pipelines as a single execution step.
-
-#. `Git-annex <https://git-annex.branchable.com/>`_ - DVC uses the idea of storing the content of large files (which should
-   not be in a Git repository) in a local key-value store, and uses file hardlinks/symlinks instead of
-   copying/duplicating files.
-
-#. `Git-LFS <https://git-lfs.github.com/>`_ - DVC is compatible with many
-   remote storage services (S3, Google Cloud, Azure, SSH, etc). DVC also
-   uses reflinks or hardlinks to avoid copy operations on checkouts; thus
-   handling large data files much more efficiently.
-
-#. Makefile (and analogues including ad-hoc scripts) - DVC tracks
-   dependencies (in a directed acyclic graph).
-
-#. `Workflow Management Systems <https://en.wikipedia.org/wiki/Workflow_management_system>`_ - DVC is a workflow
-   management system designed specifically to manage machine learning experiments. DVC is built on top of Git.
-
-#. `DAGsHub <https://dagshub.com/>`_ - online service to host DVC
-   projects.  It provides a useful UI around DVC repositories and integrates
-   other tools.
-
-#. `DVC Studio <https://studio.iterative.ai/>`_ - official online
-   platform for DVC projects.  It can be used to manage data and models, run
-   and track experiments, and visualize and share results.  Also, it
-   integrates with `CML (CI/CD for ML) <https://cml.dev/>`__ for training
-   models in the cloud or Kubernetes.
-
-
 Contributing
 ============
 
-|Maintainability| |Donate|
+|Maintainability|
 
-Contributions are welcome! Please see our `Contributing Guide <https://dvc.org/doc/user-guide/contributing/core>`_ for more
-details. Thanks to all our contributors!
+Contributions are welcome!
+Please see our `Contributing Guide <https://dvc.org/doc/user-guide/contributing/core>`_ for more details.
+Thanks to all our contributors!
 
 |Contribs|
 
-Mailing List
-============
+Community and Support
+=====================
 
-Want to stay up to date? Want to help improve DVC by participating in our occasional polls? Subscribe to our `mailing list <https://sweedom.us10.list-manage.com/subscribe/post?u=a08bf93caae4063c4e6a351f6&id=24c0ecc49a>`_. No spam, really low traffic.
+* `Twitter <https://twitter.com/DVCorg>`_
+* `Forum <https://discuss.dvc.org/>`_
+* `Discord Chat <https://dvc.org/chat>`_
+* `Email <mailto:support@dvc.org>`_
+* `Mailing List <https://sweedom.us10.list-manage.com/subscribe/post?u=a08bf93caae4063c4e6a351f6&id=24c0ecc49a>`_
 
 Copyright
 =========
 
 This project is distributed under the Apache license version 2.0 (see the LICENSE file in the project root).
 
-By submitting a pull request to this project, you agree to license your contribution under the Apache license version
-2.0 to this project.
+By submitting a pull request to this project, you agree to license your contribution under the Apache license version 2.0 to this project.
 
 Citation
 ========
 
 |DOI|
 
 Iterative, *DVC: Data Version Control - Git for Data & Models* (2020)
@@ -290,30 +233,33 @@
 Barrak, A., Eghan, E.E. and Adams, B. `On the Co-evolution of ML Pipelines and Source Code - Empirical Study of DVC Projects <https://mcis.cs.queensu.ca/publications/2021/saner.pdf>`_ , in Proceedings of the 28th IEEE International Conference on Software Analysis, Evolution, and Reengineering, SANER 2021. Hawaii, USA.
 
 
 .. |Banner| image:: https://dvc.org/img/logo-github-readme.png
    :target: https://dvc.org
    :alt: DVC logo
 
+.. |VS Code Extension Overview| image:: https://raw.githubusercontent.com/iterative/vscode-dvc/main/extension/docs/overview.gif
+   :alt: DVC Extension for VS Code
+
 .. |CI| image:: https://github.com/iterative/dvc/workflows/Tests/badge.svg?branch=main
    :target: https://github.com/iterative/dvc/actions
    :alt: GHA Tests
 
 .. |Maintainability| image:: https://codeclimate.com/github/iterative/dvc/badges/gpa.svg
    :target: https://codeclimate.com/github/iterative/dvc
    :alt: Code Climate
 
+.. |Python Version| image:: https://img.shields.io/pypi/pyversions/dvc
+   :target: https://pypi.org/project/dvc
+   :alt: Python Version
+
 .. |Coverage| image:: https://codecov.io/gh/iterative/dvc/branch/main/graph/badge.svg
    :target: https://codecov.io/gh/iterative/dvc
    :alt: Codecov
 
-.. |Donate| image:: https://img.shields.io/badge/patreon-donate-green.svg?logo=patreon
-   :target: https://www.patreon.com/DVCorg/overview
-   :alt: Donate
-
 .. |Snap| image:: https://img.shields.io/badge/snap-install-82BEA0.svg?logo=snapcraft
    :target: https://snapcraft.io/dvc
    :alt: Snapcraft
 
 .. |Choco| image:: https://img.shields.io/chocolatey/v/dvc?label=choco
    :target: https://chocolatey.org/packages/dvc
    :alt: Chocolatey
@@ -326,14 +272,18 @@
    :target: https://anaconda.org/conda-forge/dvc
    :alt: Conda-forge
 
 .. |PyPI| image:: https://img.shields.io/pypi/v/dvc.svg?label=pip&logo=PyPI&logoColor=white
    :target: https://pypi.org/project/dvc
    :alt: PyPI
 
+.. |PyPI Downloads| image:: https://img.shields.io/pypi/dm/dvc.svg?color=blue&label=Downloads&logo=pypi&logoColor=gold
+   :target: https://pypi.org/project/dvc
+   :alt: PyPI Downloads
+
 .. |Packages| image:: https://img.shields.io/github/v/release/iterative/dvc?label=deb|pkg|rpm|exe&logo=GitHub
    :target: https://github.com/iterative/dvc/releases/latest
    :alt: deb|pkg|rpm|exe
 
 .. |DOI| image:: https://img.shields.io/badge/DOI-10.5281/zenodo.3677553-blue.svg
    :target: https://doi.org/10.5281/zenodo.3677553
    :alt: DOI
@@ -342,8 +292,10 @@
    :target: https://dvc.org/img/flow.gif
    :alt: how_dvc_works
 
 .. |Contribs| image:: https://contrib.rocks/image?repo=iterative/dvc
    :target: https://github.com/iterative/dvc/graphs/contributors
    :alt: Contributors
 
-
+.. |VS Code| image:: https://img.shields.io/visual-studio-marketplace/v/Iterative.dvc?color=blue&label=VSCode&logo=visualstudiocode&logoColor=blue
+   :target: https://marketplace.visualstudio.com/items?itemName=Iterative.dvc
+   :alt: VS Code Extension
```

### Comparing `dvc-2.9.5/dvc.egg-info/SOURCES.txt` & `dvc-3.0.0a0/dvc.egg-info/SOURCES.txt`

 * *Files 11% similar despite different names*

```diff
@@ -1,110 +1,109 @@
-.codecov.yml
-.coveragerc
+.dvcignore
+.flake8
+.git-blame-ignore-revs
 .git_archival.txt
 .gitattributes
 .gitignore
 .mailmap
 .pre-commit-config.yaml
 .pre-commit-hooks.yaml
 .zenodo.json
 CODE_OF_CONDUCT.md
 CONTRIBUTING.md
 LICENSE
-MANIFEST.in
 README.rst
 pyproject.toml
-setup.cfg
-setup.py
 .dvc/.gitignore
 .dvc/config
-.github/CODEOWNERS
-.github/FUNDING.yml
 .github/PULL_REQUEST_TEMPLATE.md
+.github/codecov.yml
 .github/dependabot.yml
 .github/mergify.yml
-.github/release-drafter.yml
+.github/release.yml
 .github/ISSUE_TEMPLATE/bug_report.md
 .github/ISSUE_TEMPLATE/config.yml
+.github/ISSUE_TEMPLATE/epic_story.md
 .github/ISSUE_TEMPLATE/feature_request.md
 .github/workflows/benchmarks.yaml
+.github/workflows/codeql.yml
 .github/workflows/packages.yaml
+.github/workflows/plugin_tests.yaml
 .github/workflows/tests.yaml
-bin/dvc
-bin/dvc.bat
 dvc/__init__.py
 dvc/__main__.py
 dvc/_debug.py
 dvc/_dvc_version.py
 dvc/analytics.py
-dvc/api.py
+dvc/annotations.py
+dvc/cachemgr.py
 dvc/compare.py
 dvc/config.py
 dvc/config_schema.py
 dvc/daemon.py
 dvc/dagascii.py
 dvc/data_cloud.py
+dvc/dirs.py
 dvc/dvcfile.py
 dvc/env.py
 dvc/exceptions.py
-dvc/external_repo.py
-dvc/hash_info.py
 dvc/ignore.py
 dvc/info.py
-dvc/istextfile.py
 dvc/lock.py
 dvc/logger.py
-dvc/main.py
 dvc/output.py
 dvc/pathspec_math.py
 dvc/progress.py
 dvc/prompt.py
 dvc/rwlock.py
 dvc/schema.py
-dvc/scheme.py
 dvc/scm.py
-dvc/state.py
-dvc/system.py
 dvc/types.py
 dvc/updater.py
 dvc/version.py
 dvc.egg-info/PKG-INFO
 dvc.egg-info/SOURCES.txt
 dvc.egg-info/dependency_links.txt
 dvc.egg-info/entry_points.txt
-dvc.egg-info/not-zip-safe
 dvc.egg-info/requires.txt
 dvc.egg-info/top_level.txt
+dvc/api/__init__.py
+dvc/api/data.py
+dvc/api/experiments.py
+dvc/api/scm.py
+dvc/api/show.py
 dvc/cli/__init__.py
 dvc/cli/command.py
+dvc/cli/completion.py
 dvc/cli/parser.py
 dvc/cli/utils.py
 dvc/commands/__init__.py
 dvc/commands/add.py
 dvc/commands/cache.py
 dvc/commands/check_ignore.py
 dvc/commands/checkout.py
 dvc/commands/commit.py
 dvc/commands/completion.py
 dvc/commands/config.py
 dvc/commands/daemon.py
 dvc/commands/dag.py
+dvc/commands/data.py
 dvc/commands/data_sync.py
 dvc/commands/destroy.py
 dvc/commands/diff.py
 dvc/commands/freeze.py
 dvc/commands/gc.py
 dvc/commands/get.py
 dvc/commands/get_url.py
 dvc/commands/git_hook.py
 dvc/commands/imp.py
 dvc/commands/imp_url.py
 dvc/commands/init.py
 dvc/commands/install.py
-dvc/commands/live.py
+dvc/commands/ls_url.py
 dvc/commands/machine.py
 dvc/commands/metrics.py
 dvc/commands/move.py
 dvc/commands/params.py
 dvc/commands/plots.py
 dvc/commands/remote.py
 dvc/commands/remove.py
@@ -115,213 +114,214 @@
 dvc/commands/status.py
 dvc/commands/unprotect.py
 dvc/commands/update.py
 dvc/commands/version.py
 dvc/commands/experiments/__init__.py
 dvc/commands/experiments/apply.py
 dvc/commands/experiments/branch.py
+dvc/commands/experiments/clean.py
 dvc/commands/experiments/diff.py
 dvc/commands/experiments/exec_run.py
-dvc/commands/experiments/gc.py
-dvc/commands/experiments/init.py
 dvc/commands/experiments/ls.py
 dvc/commands/experiments/pull.py
 dvc/commands/experiments/push.py
+dvc/commands/experiments/queue_worker.py
 dvc/commands/experiments/remove.py
 dvc/commands/experiments/run.py
+dvc/commands/experiments/save.py
 dvc/commands/experiments/show.py
 dvc/commands/ls/__init__.py
 dvc/commands/ls/ls_colors.py
-dvc/data/__init__.py
-dvc/data/checkout.py
-dvc/data/diff.py
-dvc/data/gc.py
-dvc/data/meta.py
-dvc/data/reference.py
-dvc/data/slow_link_detection.py
-dvc/data/stage.py
-dvc/data/status.py
-dvc/data/transfer.py
-dvc/data/tree.py
-dvc/data/db/__init__.py
-dvc/data/db/index.py
-dvc/data/db/local.py
-dvc/data/db/reference.py
+dvc/commands/queue/__init__.py
+dvc/commands/queue/kill.py
+dvc/commands/queue/logs.py
+dvc/commands/queue/remove.py
+dvc/commands/queue/start.py
+dvc/commands/queue/status.py
+dvc/commands/queue/stop.py
 dvc/dependency/__init__.py
 dvc/dependency/base.py
 dvc/dependency/param.py
 dvc/dependency/repo.py
 dvc/fs/__init__.py
-dvc/fs/_callback.py
-dvc/fs/azure.py
-dvc/fs/base.py
+dvc/fs/callbacks.py
+dvc/fs/data.py
 dvc/fs/dvc.py
-dvc/fs/fsspec_wrapper.py
-dvc/fs/gdrive.py
 dvc/fs/git.py
-dvc/fs/gs.py
-dvc/fs/hdfs.py
-dvc/fs/http.py
-dvc/fs/https.py
-dvc/fs/local.py
-dvc/fs/memory.py
-dvc/fs/oss.py
-dvc/fs/path.py
-dvc/fs/pool.py
-dvc/fs/repo.py
-dvc/fs/s3.py
-dvc/fs/ssh.py
-dvc/fs/utils.py
-dvc/fs/webdav.py
-dvc/fs/webhdfs.py
 dvc/machine/__init__.py
 dvc/machine/backend/__init__.py
 dvc/machine/backend/base.py
 dvc/machine/backend/terraform.py
-dvc/objects/__init__.py
-dvc/objects/db.py
-dvc/objects/errors.py
-dvc/objects/file.py
 dvc/parsing/__init__.py
 dvc/parsing/context.py
 dvc/parsing/interpolate.py
-dvc/parsing/versions.py
-dvc/proc/__init__.py
-dvc/proc/exceptions.py
-dvc/proc/manager.py
-dvc/proc/process.py
 dvc/render/__init__.py
-dvc/render/base.py
-dvc/render/data.py
-dvc/render/html.py
-dvc/render/image.py
-dvc/render/plotly.py
-dvc/render/utils.py
-dvc/render/vega.py
+dvc/render/convert.py
+dvc/render/match.py
+dvc/render/converter/__init__.py
+dvc/render/converter/image.py
+dvc/render/converter/vega.py
 dvc/repo/__init__.py
 dvc/repo/add.py
+dvc/repo/artifacts.py
 dvc/repo/brancher.py
 dvc/repo/checkout.py
 dvc/repo/collect.py
 dvc/repo/commit.py
+dvc/repo/data.py
 dvc/repo/destroy.py
 dvc/repo/diff.py
 dvc/repo/fetch.py
 dvc/repo/freeze.py
 dvc/repo/gc.py
 dvc/repo/get.py
 dvc/repo/get_url.py
 dvc/repo/graph.py
 dvc/repo/imp.py
 dvc/repo/imp_url.py
+dvc/repo/imports.py
 dvc/repo/index.py
 dvc/repo/init.py
 dvc/repo/install.py
-dvc/repo/live.py
 dvc/repo/ls.py
+dvc/repo/ls_url.py
 dvc/repo/move.py
+dvc/repo/open_repo.py
 dvc/repo/pull.py
 dvc/repo/push.py
 dvc/repo/remove.py
 dvc/repo/reproduce.py
 dvc/repo/run.py
 dvc/repo/scm_context.py
 dvc/repo/stage.py
 dvc/repo/status.py
 dvc/repo/trie.py
 dvc/repo/update.py
+dvc/repo/worktree.py
 dvc/repo/experiments/__init__.py
 dvc/repo/experiments/apply.py
-dvc/repo/experiments/base.py
 dvc/repo/experiments/branch.py
+dvc/repo/experiments/cache.py
+dvc/repo/experiments/clean.py
+dvc/repo/experiments/collect.py
 dvc/repo/experiments/diff.py
 dvc/repo/experiments/exceptions.py
-dvc/repo/experiments/gc.py
-dvc/repo/experiments/init.py
 dvc/repo/experiments/ls.py
 dvc/repo/experiments/pull.py
 dvc/repo/experiments/push.py
+dvc/repo/experiments/refs.py
 dvc/repo/experiments/remove.py
 dvc/repo/experiments/run.py
+dvc/repo/experiments/save.py
+dvc/repo/experiments/serialize.py
 dvc/repo/experiments/show.py
+dvc/repo/experiments/stash.py
 dvc/repo/experiments/utils.py
 dvc/repo/experiments/executor/__init__.py
 dvc/repo/experiments/executor/base.py
 dvc/repo/experiments/executor/local.py
 dvc/repo/experiments/executor/ssh.py
-dvc/repo/experiments/executor/manager/__init__.py
-dvc/repo/experiments/executor/manager/base.py
-dvc/repo/experiments/executor/manager/local.py
-dvc/repo/experiments/executor/manager/ssh.py
+dvc/repo/experiments/queue/__init__.py
+dvc/repo/experiments/queue/base.py
+dvc/repo/experiments/queue/celery.py
+dvc/repo/experiments/queue/exceptions.py
+dvc/repo/experiments/queue/remove.py
+dvc/repo/experiments/queue/tasks.py
+dvc/repo/experiments/queue/tempdir.py
+dvc/repo/experiments/queue/utils.py
+dvc/repo/experiments/queue/workspace.py
 dvc/repo/metrics/__init__.py
 dvc/repo/metrics/diff.py
 dvc/repo/metrics/show.py
 dvc/repo/params/__init__.py
 dvc/repo/params/diff.py
 dvc/repo/params/show.py
 dvc/repo/plots/__init__.py
 dvc/repo/plots/diff.py
-dvc/repo/plots/template.py
-dvc/repo/plots/templates/confusion.json
-dvc/repo/plots/templates/confusion_normalized.json
-dvc/repo/plots/templates/linear.json
-dvc/repo/plots/templates/scatter.json
-dvc/repo/plots/templates/simple.json
-dvc/repo/plots/templates/smooth.json
 dvc/stage/__init__.py
 dvc/stage/cache.py
 dvc/stage/decorators.py
 dvc/stage/exceptions.py
 dvc/stage/imports.py
 dvc/stage/loader.py
 dvc/stage/monitor.py
 dvc/stage/params.py
 dvc/stage/run.py
 dvc/stage/serialize.py
 dvc/stage/utils.py
+dvc/testing/README.rst
 dvc/testing/__init__.py
+dvc/testing/api_tests.py
 dvc/testing/cloud.py
 dvc/testing/conftest.py
 dvc/testing/fixtures.py
 dvc/testing/path_info.py
-dvc/testing/test_api.py
-dvc/testing/test_remote.py
-dvc/testing/test_workspace.py
+dvc/testing/plugin.py
+dvc/testing/remote_tests.py
 dvc/testing/tmp_dir.py
+dvc/testing/workspace_tests.py
+dvc/testing/benchmarks/__init__.py
+dvc/testing/benchmarks/conftest.py
+dvc/testing/benchmarks/fixtures.py
+dvc/testing/benchmarks/plugin.py
+dvc/testing/benchmarks/api/__init__.py
+dvc/testing/benchmarks/cli/__init__.py
+dvc/testing/benchmarks/cli/commands/__init__.py
+dvc/testing/benchmarks/cli/commands/test_add.py
+dvc/testing/benchmarks/cli/commands/test_checkout.py
+dvc/testing/benchmarks/cli/commands/test_data_status.py
+dvc/testing/benchmarks/cli/commands/test_diff.py
+dvc/testing/benchmarks/cli/commands/test_exp_show.py
+dvc/testing/benchmarks/cli/commands/test_gc.py
+dvc/testing/benchmarks/cli/commands/test_get.py
+dvc/testing/benchmarks/cli/commands/test_get_url.py
+dvc/testing/benchmarks/cli/commands/test_help.py
+dvc/testing/benchmarks/cli/commands/test_import.py
+dvc/testing/benchmarks/cli/commands/test_import_url.py
+dvc/testing/benchmarks/cli/commands/test_init.py
+dvc/testing/benchmarks/cli/commands/test_ls.py
+dvc/testing/benchmarks/cli/commands/test_plots.py
+dvc/testing/benchmarks/cli/commands/test_pull.py
+dvc/testing/benchmarks/cli/commands/test_push.py
+dvc/testing/benchmarks/cli/commands/test_status.py
+dvc/testing/benchmarks/cli/commands/test_update.py
+dvc/testing/benchmarks/cli/stories/__init__.py
+dvc/testing/benchmarks/cli/stories/test_modify_data.py
+dvc/testing/benchmarks/cli/stories/use_cases/__init__.py
+dvc/testing/benchmarks/cli/stories/use_cases/test_sharing.py
 dvc/ui/__init__.py
+dvc/ui/_rich_progress.py
 dvc/ui/pager.py
-dvc/ui/prompt.py
 dvc/ui/table.py
 dvc/utils/.gitignore
 dvc/utils/__init__.py
-dvc/utils/_benedict.py
 dvc/utils/build.py
 dvc/utils/cli_parse.py
 dvc/utils/collections.py
-dvc/utils/conversions.py
-dvc/utils/decorators.py
 dvc/utils/diff.py
 dvc/utils/flatten.py
 dvc/utils/fs.py
 dvc/utils/humanize.py
+dvc/utils/hydra.py
+dvc/utils/objects.py
 dvc/utils/pkg.py
-dvc/utils/stream.py
+dvc/utils/plots.py
 dvc/utils/strictyaml.py
+dvc/utils/studio.py
 dvc/utils/table.py
 dvc/utils/threadpool.py
 dvc/utils/serialize/__init__.py
 dvc/utils/serialize/_common.py
 dvc/utils/serialize/_json.py
 dvc/utils/serialize/_py.py
 dvc/utils/serialize/_toml.py
 dvc/utils/serialize/_yaml.py
 scripts/build-requirements.txt
 scripts/build.py
 scripts/build_package.sh
-scripts/pypi_upload.sh
 scripts/fpm/.gitignore
 scripts/fpm/after-install.sh
 scripts/fpm/after-remove.sh
 scripts/fpm/build.py
 scripts/fpm/notarize.py
 scripts/fpm/sign.py
 scripts/innosetup/.gitignore
@@ -333,56 +333,58 @@
 scripts/innosetup/dvc_up.bmp.dvc
 scripts/innosetup/modpath.iss
 scripts/innosetup/setup.iss
 scripts/pyinstaller/.gitignore
 scripts/pyinstaller/build.py
 scripts/pyinstaller/entitlements.plist
 scripts/pyinstaller/sign.py
+scripts/pyinstaller/hooks/hook-asyncssh.py
+scripts/pyinstaller/hooks/hook-celery.py
 scripts/pyinstaller/hooks/hook-dvc.py
 scripts/pyinstaller/hooks/hook-dvc.system.py
 scripts/pyinstaller/hooks/hook-dvc.tree.gs.py
 scripts/pyinstaller/hooks/hook-dvc.utils.flatten.py
+scripts/pyinstaller/hooks/hook-dvc_task.py
+scripts/pyinstaller/hooks/hook-fsspec.py
 scripts/pyinstaller/hooks/hook-google_compute_engine.logger.py
-scripts/pyinstaller/hooks/hook-googleapiclient.model.py
 scripts/pyinstaller/hooks/hook-pydrive2.py
 tests/__init__.py
 tests/__main__.py
-tests/basic_env.py
 tests/conftest.py
 tests/dir_helpers.py
 tests/docker-compose.yml
-tests/pylint_plugin_disable.py
 tests/remotes_env.sample
+tests/ruff.toml
+tests/scripts.py
 tests/func/__init__.py
-tests/func/conftest.py
 tests/func/test_add.py
 tests/func/test_analytics.py
-tests/func/test_api.py
 tests/func/test_check_ignore.py
 tests/func/test_checkout.py
 tests/func/test_cli.py
 tests/func/test_commit.py
 tests/func/test_config.py
 tests/func/test_data_cloud.py
+tests/func/test_data_status.py
 tests/func/test_diff.py
 tests/func/test_dvcfile.py
 tests/func/test_external_repo.py
 tests/func/test_fs.py
 tests/func/test_gc.py
 tests/func/test_get.py
 tests/func/test_get_url.py
 tests/func/test_ignore.py
 tests/func/test_import.py
 tests/func/test_import_url.py
 tests/func/test_init.py
 tests/func/test_install.py
-tests/func/test_live.py
 tests/func/test_lock.py
 tests/func/test_lockfile.py
 tests/func/test_ls.py
+tests/func/test_ls_url.py
 tests/func/test_merge_driver.py
 tests/func/test_move.py
 tests/func/test_odb.py
 tests/func/test_remote.py
 tests/func/test_remove.py
 tests/func/test_repo.py
 tests/func/test_repo_index.py
@@ -396,190 +398,208 @@
 tests/func/test_scm_context.py
 tests/func/test_stage.py
 tests/func/test_stage_load.py
 tests/func/test_state.py
 tests/func/test_status.py
 tests/func/test_unprotect.py
 tests/func/test_update.py
+tests/func/test_used_objs.py
 tests/func/test_utils.py
 tests/func/test_version.py
+tests/func/test_virtual_directory.py
+tests/func/api/__init__.py
+tests/func/api/test_data.py
+tests/func/api/test_experiments.py
+tests/func/api/test_scm.py
+tests/func/api/test_show.py
+tests/func/artifacts/__init__.py
+tests/func/artifacts/test_artifacts.py
+tests/func/data/__init__.py
+tests/func/data/db/__init__.py
+tests/func/data/db/test_index.py
 tests/func/experiments/__init__.py
 tests/func/experiments/conftest.py
+tests/func/experiments/test_apply.py
 tests/func/experiments/test_checkpoints.py
 tests/func/experiments/test_diff.py
 tests/func/experiments/test_experiments.py
-tests/func/experiments/test_gc.py
-tests/func/experiments/test_init.py
+tests/func/experiments/test_queue.py
 tests/func/experiments/test_remote.py
 tests/func/experiments/test_remove.py
+tests/func/experiments/test_save.py
+tests/func/experiments/test_set_params.py
 tests/func/experiments/test_show.py
+tests/func/experiments/test_stash_exp.py
+tests/func/experiments/test_utils.py
 tests/func/experiments/executor/__init__.py
 tests/func/experiments/executor/test_ssh.py
 tests/func/machine/__init__.py
 tests/func/machine/conftest.py
 tests/func/machine/test_machine_config.py
 tests/func/machine/test_machine_status.py
 tests/func/metrics/__init__.py
 tests/func/metrics/test_diff.py
 tests/func/metrics/test_show.py
-tests/func/objects/__init__.py
-tests/func/objects/db/__init__.py
-tests/func/objects/db/test_index.py
 tests/func/params/__init__.py
 tests/func/params/test_diff.py
 tests/func/params/test_show.py
 tests/func/parsing/__init__.py
 tests/func/parsing/test_errors.py
 tests/func/parsing/test_foreach.py
 tests/func/parsing/test_interpolated_entry.py
 tests/func/parsing/test_resolver.py
 tests/func/plots/__init__.py
 tests/func/plots/test_diff.py
 tests/func/plots/test_modify.py
 tests/func/plots/test_show.py
 tests/func/utils/__init__.py
-tests/func/utils/test_fs.py
+tests/func/utils/test_hydra.py
 tests/func/utils/test_strict_yaml.py
+tests/integration/__init__.py
+tests/integration/conftest.py
+tests/integration/test_studio_live_experiments.py
+tests/integration/plots/__init__.py
+tests/integration/plots/conftest.py
+tests/integration/plots/test_plots.py
+tests/integration/plots/test_repo_plots_api.py
 tests/remotes/__init__.py
 tests/remotes/git_server.py
 tests/remotes/user.key
 tests/remotes/user.key.pub
 tests/remotes/git-init/git.sh
 tests/unit/__init__.py
 tests/unit/test_analytics.py
+tests/unit/test_api.py
 tests/unit/test_collect.py
 tests/unit/test_compare.py
 tests/unit/test_config.py
 tests/unit/test_context.py
 tests/unit/test_daemon.py
 tests/unit/test_dvcfile.py
-tests/unit/test_external_repo.py
+tests/unit/test_hashinfo.py
 tests/unit/test_ignore.py
 tests/unit/test_imports.py
 tests/unit/test_info.py
 tests/unit/test_interpolate.py
 tests/unit/test_lockfile.py
 tests/unit/test_logger.py
 tests/unit/test_metrics.py
 tests/unit/test_params.py
 tests/unit/test_pathspec_math.py
-tests/unit/test_plots.py
 tests/unit/test_progress.py
 tests/unit/test_prompt.py
 tests/unit/test_run.py
 tests/unit/test_rwlock.py
+tests/unit/test_scm.py
 tests/unit/test_tabular_data.py
 tests/unit/test_updater.py
+tests/unit/cli/__init__.py
+tests/unit/cli/test_main.py
 tests/unit/command/__init__.py
 tests/unit/command/test_add.py
 tests/unit/command/test_cache.py
 tests/unit/command/test_checkout.py
 tests/unit/command/test_compat_flag.py
 tests/unit/command/test_config.py
 tests/unit/command/test_dag.py
+tests/unit/command/test_data_status.py
 tests/unit/command/test_data_sync.py
-tests/unit/command/test_debug.py
 tests/unit/command/test_diff.py
 tests/unit/command/test_experiments.py
+tests/unit/command/test_gc.py
 tests/unit/command/test_get.py
 tests/unit/command/test_get_url.py
 tests/unit/command/test_git_hook.py
 tests/unit/command/test_imp.py
 tests/unit/command/test_imp_url.py
-tests/unit/command/test_live.py
+tests/unit/command/test_ls_url.py
 tests/unit/command/test_machine.py
 tests/unit/command/test_metrics.py
 tests/unit/command/test_params.py
 tests/unit/command/test_plots.py
+tests/unit/command/test_queue.py
 tests/unit/command/test_repro.py
 tests/unit/command/test_run.py
 tests/unit/command/test_stage.py
 tests/unit/command/test_status.py
 tests/unit/command/test_update.py
 tests/unit/command/ls/__init__.py
 tests/unit/command/ls/test_ls.py
 tests/unit/command/ls/test_ls_colors.py
+tests/unit/data/__init__.py
+tests/unit/data/db/__init__.py
+tests/unit/data/db/test_local.py
 tests/unit/dependency/__init__.py
 tests/unit/dependency/test_dependency.py
 tests/unit/dependency/test_params.py
 tests/unit/fs/__init__.py
 tests/unit/fs/test_azure.py
 tests/unit/fs/test_base.py
+tests/unit/fs/test_data.py
 tests/unit/fs/test_dvc.py
+tests/unit/fs/test_dvc_info.py
 tests/unit/fs/test_fs.py
-tests/unit/fs/test_hdfs.py
 tests/unit/fs/test_path.py
-tests/unit/fs/test_repo.py
-tests/unit/fs/test_repo_info.py
 tests/unit/fs/test_s3.py
-tests/unit/fs/test_ssh.py
 tests/unit/fs/test_tree.py
 tests/unit/machine/__init__.py
 tests/unit/machine/test_machine.py
-tests/unit/objects/__init__.py
-tests/unit/objects/test_tree.py
-tests/unit/objects/db/__init__.py
-tests/unit/objects/db/test_local.py
 tests/unit/output/__init__.py
+tests/unit/output/test_annotations.py
 tests/unit/output/test_load.py
 tests/unit/output/test_local.py
 tests/unit/output/test_output.py
-tests/unit/proc/__init__.py
-tests/unit/proc/test_manager.py
-tests/unit/proc/test_process.py
 tests/unit/remote/__init__.py
-tests/unit/remote/test_base.py
-tests/unit/remote/test_gdrive.py
-tests/unit/remote/test_http.py
-tests/unit/remote/test_index.py
 tests/unit/remote/test_oss.py
 tests/unit/remote/test_remote.py
-tests/unit/remote/test_slow_link_detection.py
 tests/unit/remote/test_webdav.py
 tests/unit/remote/test_webhdfs.py
 tests/unit/render/__init__.py
-tests/unit/render/test_data.py
-tests/unit/render/test_html.py
-tests/unit/render/test_image.py
-tests/unit/render/test_parallel_coordinates.py
-tests/unit/render/test_render.py
-tests/unit/render/test_renderer.py
-tests/unit/render/test_vega.py
+tests/unit/render/test_convert.py
+tests/unit/render/test_image_converter.py
+tests/unit/render/test_match.py
+tests/unit/render/test_vega_converter.py
 tests/unit/repo/__init__.py
+tests/unit/repo/test_open_repo.py
 tests/unit/repo/test_repo.py
 tests/unit/repo/test_reproduce.py
 tests/unit/repo/test_run.py
 tests/unit/repo/test_scm_context.py
 tests/unit/repo/experiments/__init__.py
+tests/unit/repo/experiments/conftest.py
+tests/unit/repo/experiments/test_executor_status.py
+tests/unit/repo/experiments/test_remove.py
 tests/unit/repo/experiments/test_utils.py
+tests/unit/repo/experiments/queue/__init__.py
+tests/unit/repo/experiments/queue/test_celery.py
+tests/unit/repo/experiments/queue/test_remove.py
 tests/unit/repo/plots/__init__.py
 tests/unit/repo/plots/test_diff.py
-tests/unit/repo/plots/test_templates.py
+tests/unit/scm/__init__.py
 tests/unit/scm/test_scm.py
 tests/unit/stage/__init__.py
 tests/unit/stage/test_cache.py
 tests/unit/stage/test_loader_pipeline_file.py
 tests/unit/stage/test_run.py
 tests/unit/stage/test_serialize_pipeline_file.py
 tests/unit/stage/test_serialize_pipeline_lock.py
 tests/unit/stage/test_stage.py
 tests/unit/stage/test_utils.py
 tests/unit/ui/__init__.py
 tests/unit/ui/test_console.py
 tests/unit/ui/test_pager.py
-tests/unit/ui/test_prompt.py
 tests/unit/ui/test_table.py
 tests/unit/utils/__init__.py
 tests/unit/utils/test_cli_parse.py
 tests/unit/utils/test_collections.py
-tests/unit/utils/test_conversions.py
-tests/unit/utils/test_decorators.py
+tests/unit/utils/test_executors.py
 tests/unit/utils/test_fs.py
 tests/unit/utils/test_humanize.py
-tests/unit/utils/test_stream.py
+tests/unit/utils/test_studio.py
 tests/unit/utils/test_utils.py
 tests/unit/utils/serialize/__init__.py
 tests/unit/utils/serialize/test_python.py
+tests/unit/utils/serialize/test_toml.py
 tests/unit/utils/serialize/test_yaml.py
 tests/utils/__init__.py
 tests/utils/asserts.py
-tests/utils/scriptify.py
+tests/utils/plots.py
```

### Comparing `dvc-2.9.5/scripts/build.py` & `dvc-3.0.0a0/scripts/build.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,63 +1,46 @@
 import argparse
-import importlib
 import pathlib
-import textwrap
+import sys
 from subprocess import STDOUT, check_call
 
 path = pathlib.Path(__file__).parent.absolute()
 dvc = path.parent / "dvc"
 pyinstaller = path / "pyinstaller"
 innosetup = path / "innosetup"
 fpm = path / "fpm"
 
 parser = argparse.ArgumentParser()
-parser.add_argument(
-    "pkg", choices=["deb", "rpm", "osxpkg", "exe"], help="package type"
-)
+parser.add_argument("pkg", choices=["deb", "rpm", "osxpkg", "exe"], help="package type")
 parser.add_argument("--sign-application", default=False, action="store_true")
 parser.add_argument("--application-id")
 parser.add_argument("--sign-installer", default=False, action="store_true")
 parser.add_argument("--installer-id")
 parser.add_argument("--notarize", default=False, action="store_true")
 parser.add_argument("--apple-id-username")
 parser.add_argument("--apple-id-password")
 args = parser.parse_args()
 
 (dvc / "utils" / "build.py").write_text(f'PKG = "{args.pkg}"')
 
-# Autogenerate version, similar to what we do in setup.py
-spec = importlib.util.spec_from_file_location(
-    "dvc.version", dvc / "version.py"
-)
-dvc_version = importlib.util.module_from_spec(spec)
-spec.loader.exec_module(dvc_version)
-version = dvc_version.__version__
-
-(dvc / "version.py").write_text(
-    textwrap.dedent(
-        f"""\
-        # AUTOGENERATED at build time by scripts/build.py
-        __version__ = "{version}"
-    """
-    )
-)
+if not (dvc / "_dvc_version.py").exists():
+    raise Exception("no version info found")
 
 check_call(
     ["python", "build.py"],
     cwd=pyinstaller,
     stderr=STDOUT,
 )
 
 if args.sign_application:
     if args.pkg != "osxpkg":
         raise NotImplementedError
     if not args.application_id:
         print("--sign-application requires --application-id")
-        exit(1)
+        sys.exit(1)
     check_call(
         ["python", "sign.py", "--application-id", args.application_id],
         cwd=pyinstaller,
         stderr=STDOUT,
     )
 
 if args.pkg == "exe":
@@ -72,33 +55,29 @@
         cwd=fpm,
         stderr=STDOUT,
     )
 
 if args.sign_installer:
     if args.pkg != "osxpkg":
         raise NotImplementedError
-    if not all(
-        [args.installer_id, args.apple_id_username, args.apple_id_password]
-    ):
+    if not all([args.installer_id, args.apple_id_username, args.apple_id_password]):
         print("--sign-installer requires --installer-id")
-        exit(1)
+        sys.exit(1)
     check_call(
         ["python", "sign.py", "--installer-id", args.installer_id],
         cwd=fpm,
         stderr=STDOUT,
     )
 
 if args.notarize:
     if args.pkg != "osxpkg":
         raise NotImplementedError
     if not all([args.apple_id_username, args.apple_id_password]):
-        print(
-            "--notarize requires --apple-id-username and --apple-id-password"
-        )
-        exit(1)
+        print("--notarize requires --apple-id-username and --apple-id-password")
+        sys.exit(1)
     check_call(
         [
             "python",
             "notarize.py",
             "--apple-id-username",
             args.apple_id_username,
             "--apple-id-password",
```

### Comparing `dvc-2.9.5/scripts/fpm/notarize.py` & `dvc-3.0.0a0/scripts/fpm/notarize.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,17 +10,15 @@
 
 parser = argparse.ArgumentParser()
 parser.add_argument(
     "path",
     nargs="?",
     help="Path to the osxpkg to notarize. If not specified - try to find one.",
 )
-parser.add_argument(
-    "--apple-id-username", required=True, help="Apple ID username."
-)
+parser.add_argument("--apple-id-username", required=True, help="Apple ID username.")
 parser.add_argument(
     "--apple-id-password",
     required=True,
     help=(
         "Apple ID app-specific password. Note that this is not a regular "
         "Apple ID password, so you need to generate one at "
         "https://appleid.apple.com/account/manage"
@@ -32,19 +30,19 @@
 
 if args.path:
     pkg = pathlib.Path(args.path)
 else:
     pkgs = list(path.glob("*.pkg"))
     if not pkgs:
         print("No pkgs found")
-        exit(1)
+        sys.exit(1)
 
     if len(pkgs) > 1:
         print("Too many packages")
-        exit(1)
+        sys.exit(1)
 
     (pkg,) = pkgs
 
 
 config = {
     "notarize": {
         "path": os.fspath(pkg),
```

### Comparing `dvc-2.9.5/scripts/fpm/sign.py` & `dvc-3.0.0a0/scripts/fpm/sign.py`

 * *Files 3% similar despite different names*

```diff
@@ -24,19 +24,19 @@
 
 if args.path:
     pkg = pathlib.Path(args.path)
 else:
     pkgs = list(path.glob("*.pkg"))
     if not pkgs:
         print("No pkgs found")
-        exit(1)
+        sys.exit(1)
 
     if len(pkgs) > 1:
         print("Too many packages")
-        exit(1)
+        sys.exit(1)
 
     (pkg,) = pkgs
 
 unsigned = pkg.with_suffix(".unsigned")
 os.rename(pkg, unsigned)
 check_call(
     [
```

### Comparing `dvc-2.9.5/scripts/innosetup/addSymLinkPermissions.ps1` & `dvc-3.0.0a0/scripts/innosetup/addSymLinkPermissions.ps1`

 * *Files 3% similar despite different names*

```diff
@@ -20,26 +20,26 @@
     if( [string]::IsNullOrEmpty($sidstr) ) {
         Write-Host "Account not found!" -ForegroundColor Red
         exit -1
     }
     Write-Host "Account SID: $($sidstr)" -ForegroundColor DarkCyan
     $tmp = [System.IO.Path]::GetTempFileName()
     Write-Host "Export current Local Security Policy" -ForegroundColor DarkCyan
-    secedit.exe /export /cfg "$($tmp)" 
-    $c = Get-Content -Path $tmp 
+    secedit.exe /export /cfg "$($tmp)"
+    $c = Get-Content -Path $tmp
     $currentSetting = ""
     foreach($s in $c) {
         if( $s -like "SECreateSymbolicLinkPrivilege*") {
             $x = $s.split("=",[System.StringSplitOptions]::RemoveEmptyEntries)
             $currentSetting = $x[1].Trim()
         }
     }
     if( $currentSetting -notlike "*$($sidstr)*" ) {
         Write-Host "Need to add permissions to SymLink" -ForegroundColor Yellow
-        
+
         Write-Host "Modify Setting ""Create SymLink""" -ForegroundColor DarkCyan
 
         if( [string]::IsNullOrEmpty($currentSetting) ) {
             $currentSetting = "*$($sidstr)"
         } else {
             $currentSetting = "*$($sidstr),$($currentSetting)"
         }
@@ -54,16 +54,16 @@
 SECreateSymbolicLinkPrivilege = $($currentSetting)
 "@
     $tmp2 = [System.IO.Path]::GetTempFileName()
         Write-Host "Import new settings to Local Security Policy" -ForegroundColor DarkCyan
         $outfile | Set-Content -Path $tmp2 -Encoding Unicode -Force
         Push-Location (Split-Path $tmp2)
         try {
-            secedit.exe /configure /db "secedit.sdb" /cfg "$($tmp2)" /areas USER_RIGHTS 
-        } finally { 
+            secedit.exe /configure /db "secedit.sdb" /cfg "$($tmp2)" /areas USER_RIGHTS
+        } finally {
             Pop-Location
         }
     } else {
         Write-Host "NO ACTIONS REQUIRED! Account already in ""Create SymLink""" -ForegroundColor DarkCyan
         Write-Host "Account $accountToAdd already has permissions to SymLink" -ForegroundColor Green
         return $true;
     }
```

### Comparing `dvc-2.9.5/scripts/innosetup/addsymlink.iss` & `dvc-3.0.0a0/scripts/innosetup/addsymlink.iss`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/scripts/innosetup/build.py` & `dvc-3.0.0a0/scripts/innosetup/build.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/scripts/innosetup/modpath.iss` & `dvc-3.0.0a0/scripts/innosetup/modpath.iss`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/scripts/innosetup/setup.iss` & `dvc-3.0.0a0/scripts/innosetup/setup.iss`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/scripts/pyinstaller/hooks/hook-dvc.py` & `dvc-3.0.0a0/scripts/pyinstaller/hooks/hook-dvc.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,8 @@
-from PyInstaller.utils.hooks import (  # pylint:disable=import-error
-    copy_metadata,
-)
+from PyInstaller.utils.hooks import copy_metadata  # pylint:disable=import-error
 
 # needed for `dvc doctor` to show dep versions
 datas = copy_metadata("adlfs", recursive=True)
 datas += copy_metadata("knack")
 datas += copy_metadata("gcsfs")
 datas += copy_metadata("pyarrow")
 datas += copy_metadata("pydrive2")
@@ -12,9 +10,19 @@
 datas += copy_metadata("boto3")
 datas += copy_metadata("ossfs")
 datas += copy_metadata("sshfs")
 datas += copy_metadata("webdav4")
 datas += copy_metadata("aiohttp")
 datas += copy_metadata("aiohttp_retry")
 
-# https://github.com/pypa/setuptools/issues/1963
-hiddenimports = ["pkg_resources.py2_warn"]
+hiddenimports = [
+    "dvc_azure",
+    "dvc_gdrive",
+    "dvc_gs",
+    "dvc_hdfs",
+    "dvc_oss",
+    "dvc_s3",
+    "dvc_webdav",
+    "dvc_webhdfs",
+    # https://github.com/pypa/setuptools/issues/1963
+    "pkg_resources.py2_warn",
+]
```

### Comparing `dvc-2.9.5/scripts/pyinstaller/sign.py` & `dvc-3.0.0a0/scripts/pyinstaller/sign.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/conftest.py` & `dvc-3.0.0a0/tests/conftest.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,18 +1,20 @@
+import json
 import os
 import sys
 from contextlib import suppress
 
 import pytest
 
+from dvc import env
 from dvc.testing.fixtures import *  # noqa, pylint: disable=wildcard-import
 
 from .dir_helpers import *  # noqa, pylint: disable=wildcard-import
 from .remotes import *  # noqa, pylint: disable=wildcard-import
-from .utils.scriptify import scriptify
+from .scripts import *  # noqa, pylint: disable=wildcard-import
 
 # Prevent updater and analytics from running their processes
 os.environ["DVC_TEST"] = "true"
 # Ensure progress output even when not outputting to raw sys.stderr console
 os.environ["DVC_IGNORE_ISATTY"] = "true"
 # Disable system git config
 os.environ["GIT_CONFIG_NOSYSTEM"] = "1"
@@ -55,28 +57,19 @@
     from dvc.ui import ui
 
     ui.enable()
 
 
 @pytest.fixture(autouse=True)
 def clean_repos():
-    # pylint: disable=redefined-outer-name
-    from dvc.external_repo import clean_repos
+    from dvc.repo.open_repo import clean_repos
 
     clean_repos()
 
 
-@pytest.fixture(scope="session", autouse=True)
-def _close_pools():
-    from dvc.fs.pool import close_pools
-
-    yield
-    close_pools()
-
-
 def _get_opt(remote_name, action):
     return f"--{action}-{remote_name}"
 
 
 def pytest_addoption(parser):
     """Adds remote-related flags to selectively disable/enable for tests
     Eg: If some remotes, eg: ssh is enabled to be tested for by default
@@ -122,18 +115,15 @@
     # Apply test markers to skip tests selectively
     # NOTE: this only works on individual tests,
     # for fixture, use `test_config` fixture and
     # run `test_config.requires(remote_name)`.
     for marker in item.iter_markers():
         item.config.dvc_config.apply_marker(marker)
 
-    if (
-        "CI" in os.environ
-        and item.get_closest_marker("needs_internet") is not None
-    ):
+    if "CI" in os.environ and item.get_closest_marker("needs_internet") is not None:
         # remotes that need internet connection might be flaky,
         # so we rerun them in case it fails.
         item.add_marker(pytest.mark.flaky(max_runs=5, min_passes=1))
 
 
 @pytest.fixture(scope="session")
 def test_config(request):
@@ -168,54 +158,40 @@
             enabled_remotes.discard(remote_name)
         if enabled:
             enabled_remotes.add(remote_name)
 
 
 @pytest.fixture
 def custom_template(tmp_dir, dvc):
-    try:
-        import importlib_resources
-    except ImportError:
-        import importlib.resources as importlib_resources
-
-    content = (
-        importlib_resources.files("dvc.repo.plots")
-        / "templates"
-        / "simple.json"
-    ).read_text()
+    from dvc_render.vega_templates import SimpleLinearTemplate
 
     template = tmp_dir / "custom_template.json"
-    template.write_text(content)
+    template.write_text(json.dumps(SimpleLinearTemplate.DEFAULT_CONTENT))
     return template
 
 
-scriptify_fixture = pytest.fixture(lambda: scriptify, name="scriptify")
-
-
 @pytest.fixture(autouse=True)
 def mocked_webbrowser_open(mocker):
     mocker.patch("webbrowser.open")
 
 
-@pytest.fixture(autouse=True)
-def isolate(tmp_path_factory, monkeypatch) -> None:
+@pytest.fixture(scope="session", autouse=True)
+def isolate(tmp_path_factory):
     path = tmp_path_factory.mktemp("mock")
     home_dir = path / "home"
     home_dir.mkdir()
 
+    monkeypatch = pytest.MonkeyPatch()
     if sys.platform == "win32":
         home_drive, home_path = os.path.splitdrive(home_dir)
         monkeypatch.setenv("USERPROFILE", str(home_dir))
         monkeypatch.setenv("HOMEDRIVE", home_drive)
         monkeypatch.setenv("HOMEPATH", home_path)
 
-        for env_var, sub_path in (
-            ("APPDATA", "Roaming"),
-            ("LOCALAPPDATA", "Local"),
-        ):
+        for env_var, sub_path in (("APPDATA", "Roaming"), ("LOCALAPPDATA", "Local")):
             path = home_dir / "AppData" / sub_path
             path.mkdir(parents=True)
             monkeypatch.setenv(env_var, os.fspath(path))
     else:
         monkeypatch.setenv("HOME", str(home_dir))
 
     monkeypatch.setenv("GIT_CONFIG_NOSYSTEM", "1")
@@ -227,7 +203,50 @@
 defaultBranch=master
 """
     (home_dir / ".gitconfig").write_bytes(contents)
 
     import pygit2
 
     pygit2.settings.search_path[pygit2.GIT_CONFIG_LEVEL_GLOBAL] = str(home_dir)
+
+    monkeypatch.setenv(env.DVC_SYSTEM_CONFIG_DIR, os.fspath(path / "system"))
+    monkeypatch.setenv(env.DVC_GLOBAL_CONFIG_DIR, os.fspath(path / "global"))
+    monkeypatch.setenv(env.DVC_SITE_CACHE_DIR, os.fspath(path / "site_cache_dir"))
+
+    yield
+
+    monkeypatch.undo()
+
+
+@pytest.fixture
+def run_copy_metrics(tmp_dir, copy_script):
+    def run(
+        file1,
+        file2,
+        commit=None,
+        tag=None,
+        single_stage=True,
+        name=None,
+        **kwargs,
+    ):
+        if name:
+            single_stage = False
+
+        stage = tmp_dir.dvc.run(
+            cmd=f"python copy.py {file1} {file2}",
+            deps=[file1],
+            single_stage=single_stage,
+            name=name,
+            **kwargs,
+        )
+
+        if hasattr(tmp_dir.dvc, "scm"):
+            files = [stage.path]
+            files += [out.fs_path for out in stage.outs if not out.use_cache]
+            tmp_dir.dvc.scm.add(files)
+            if commit:
+                tmp_dir.dvc.scm.commit(commit)
+            if tag:
+                tmp_dir.dvc.scm.tag(tag)
+        return stage
+
+    return run
```

### Comparing `dvc-2.9.5/tests/dir_helpers.py` & `dvc-3.0.0a0/tests/dir_helpers.py`

 * *Files 19% similar despite different names*

```diff
@@ -39,49 +39,32 @@
 from global repo template to creating everything inplace, which:
 
     - makes all path references local to test, enhancing readability
     - allows using telling filenames, e.g. "git_tracked_file" instead of "foo"
     - does not create unnecessary files
 """
 
-# pylint: disable=redefined-outer-name, attribute-defined-outside-init
 
 import os
 import pathlib
-from textwrap import dedent
 
 import pytest
 
-from dvc.logger import disable_other_loggers
-
 __all__ = [
     "run_copy",
     "run_head",
     "erepo_dir",
     "git_dir",
     "git_upstream",
     "git_downstream",
 ]
 
-# see https://github.com/iterative/dvc/issues/3167
-disable_other_loggers()
-
 
 @pytest.fixture
-def run_copy(tmp_dir, dvc):
-    tmp_dir.gen(
-        "copy.py",
-        (
-            "import sys, shutil, os\n"
-            "shutil.copyfile(sys.argv[1], sys.argv[2]) "
-            "if os.path.isfile(sys.argv[1]) "
-            "else shutil.copytree(sys.argv[1], sys.argv[2])"
-        ),
-    )
-
+def run_copy(tmp_dir, copy_script, dvc):
     def run_copy(src, dst, **run_kwargs):
         wdir = pathlib.Path(run_kwargs.get("wdir", "."))
         wdir = pathlib.Path("../" * len(wdir.parts))
         script_path = wdir / "copy.py"
 
         return dvc.run(
             cmd=f"python {script_path} {src} {dst}",
@@ -90,31 +73,15 @@
             **run_kwargs,
         )
 
     return run_copy
 
 
 @pytest.fixture
-def run_head(tmp_dir, dvc):
-    """Output first line of each file to different file with '-1' appended.
-    Useful for tracking multiple outputs/dependencies which are not a copy
-    of each others.
-    """
-    tmp_dir.gen(
-        {
-            "head.py": dedent(
-                """
-        import sys
-        for file in sys.argv[1:]:
-            with open(file) as f, open(file +"-1","w+") as w:
-                w.write(f.readline())
-        """
-            )
-        }
-    )
+def run_head(tmp_dir, head_script, dvc):
     script = os.path.abspath(tmp_dir / "head.py")
 
     def run(*args, **run_kwargs):
         return dvc.run(
             **{
                 "cmd": "python {} {}".format(script, " ".join(args)),
                 "outs": [dep + "-1" for dep in args],
@@ -144,18 +111,18 @@
         self.remote = name
         self.url = url
 
 
 @pytest.fixture
 def git_upstream(tmp_dir, erepo_dir, git_dir, request):
     remote = erepo_dir if "dvc" in request.fixturenames else git_dir
-    url = "file://{}".format(remote.resolve().as_posix())
+    url = f"file://{remote.resolve().as_posix()}"
     tmp_dir.scm.gitpython.repo.create_remote("upstream", url)
     return GitRemote(remote, "upstream", url)
 
 
 @pytest.fixture
 def git_downstream(tmp_dir, erepo_dir, git_dir, request):
     remote = erepo_dir if "dvc" in request.fixturenames else git_dir
-    url = "file://{}".format(tmp_dir.resolve().as_posix())
+    url = f"file://{tmp_dir.resolve().as_posix()}"
     remote.scm.gitpython.repo.create_remote("upstream", url)
     return GitRemote(remote, "upstream", url)
```

### Comparing `dvc-2.9.5/tests/func/experiments/executor/test_ssh.py` & `dvc-3.0.0a0/tests/func/experiments/executor/test_ssh.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 import posixpath
 from contextlib import contextmanager
 from functools import partial
 from urllib.parse import urlparse
 
 import pytest
+from dvc_ssh import SSHFileSystem
 from dvc_ssh.tests.cloud import TEST_SSH_KEY_PATH, TEST_SSH_USER
 
-from dvc.fs.ssh import SSHFileSystem
-from dvc.repo.experiments.base import EXEC_HEAD, EXEC_MERGE
 from dvc.repo.experiments.executor.base import ExecutorInfo, ExecutorResult
 from dvc.repo.experiments.executor.ssh import SSHExecutor
+from dvc.repo.experiments.refs import EXEC_HEAD, EXEC_MERGE
 from tests.func.machine.conftest import *  # noqa, pylint: disable=wildcard-import
 
 
 @contextmanager
 def _ssh_factory(cloud):
     yield SSHFileSystem(
         host=cloud.host,
@@ -25,45 +25,45 @@
 
 def test_init_from_stash(tmp_dir, scm, dvc, machine_instance, mocker):
     mock = mocker.patch.object(SSHExecutor, "_from_stash_entry")
     mock_entry = mocker.Mock()
     mock_entry.name = ""
     SSHExecutor.from_stash_entry(
         dvc,
-        "abc123",
         mock_entry,
         machine_name="foo",
     )
     _args, kwargs = mock.call_args
     assert kwargs["host"] == machine_instance["instance_ip"]
 
 
 @pytest.mark.needs_internet
 @pytest.mark.parametrize("cloud", [pytest.lazy_fixture("git_ssh")])
-def test_init_git(tmp_dir, scm, cloud):
+def test_init_git(tmp_dir, dvc, scm, cloud, mocker):
     tmp_dir.scm_gen({"foo": "foo", "dir": {"bar": "bar"}}, commit="init")
     baseline_rev = scm.get_rev()
-    scm.set_ref(EXEC_HEAD, baseline_rev)
     tmp_dir.gen("foo", "stashed")
     scm.gitpython.git.stash()
     rev = scm.resolve_rev("stash@{0}")
-    scm.set_ref(EXEC_MERGE, rev)
+
+    mock = mocker.Mock(baseline_rev=baseline_rev, head_rev=baseline_rev)
 
     root_url = cloud / SSHExecutor.gen_dirname()
 
     executor = SSHExecutor(
         root_dir=root_url.path,
         dvc_dir=".dvc",
         baseline_rev=baseline_rev,
         host=root_url.host,
         port=root_url.port,
         username=TEST_SSH_USER,
         fs_factory=partial(_ssh_factory, cloud),
     )
-    executor.init_git(scm)
+    infofile = str((root_url / "foo.run").path)
+    executor.init_git(dvc, scm, rev, mock, infofile=infofile)
     assert root_url.path == executor._repo_abspath
 
     fs = cloud._ssh
     assert fs.exists(posixpath.join(executor._repo_abspath, "foo"))
     assert fs.exists(posixpath.join(executor._repo_abspath, "dir"))
     assert fs.exists(posixpath.join(executor._repo_abspath, "dir", "bar"))
 
@@ -96,15 +96,15 @@
         )
     )
 
 
 @pytest.mark.needs_internet
 @pytest.mark.parametrize("cloud", [pytest.lazy_fixture("git_ssh")])
 def test_reproduce(tmp_dir, scm, dvc, cloud, exp_stage, mocker):
-    from sshfs import SSHFileSystem as _sshfs
+    from sshfs import SSHFileSystem as _sshfs  # noqa: N813
 
     rev = scm.get_rev()
     root_url = cloud / SSHExecutor.gen_dirname()
     mocker.patch.object(SSHFileSystem, "exists", return_value=True)
     mock_execute = mocker.patch.object(_sshfs, "execute")
     info = ExecutorInfo(
         str(root_url),
@@ -113,15 +113,14 @@
         str(root_url.path),
         ".dvc",
     )
     infofile = str((root_url / "foo.run").path)
     SSHExecutor.reproduce(
         info,
         rev,
-        infofile=infofile,
         fs_factory=partial(_ssh_factory, cloud),
     )
     assert mock_execute.called_once()
     _name, args, _kwargs = mock_execute.mock_calls[0]
     assert f"dvc exp exec-run --infofile {infofile}" in args[0]
```

### Comparing `dvc-2.9.5/tests/func/experiments/test_checkpoints.py` & `dvc-3.0.0a0/tests/func/experiments/test_checkpoints.py`

 * *Files 7% similar despite different names*

```diff
@@ -2,67 +2,57 @@
 
 import pytest
 from funcy import first
 
 from dvc.config import NoRemoteError
 from dvc.env import DVC_EXP_AUTO_PUSH, DVC_EXP_GIT_REMOTE
 from dvc.exceptions import DvcException
-from dvc.repo.experiments import MultipleBranchError
-from dvc.repo.experiments.base import EXEC_APPLY, EXEC_CHECKPOINT
+from dvc.repo.experiments.exceptions import MultipleBranchError
 from dvc.repo.experiments.executor.base import BaseExecutor
+from dvc.repo.experiments.refs import EXEC_APPLY, EXEC_CHECKPOINT
 from dvc.repo.experiments.utils import exp_refs_by_rev
-from dvc.scm import InvalidRemoteSCMRepo
+from dvc.scm import InvalidRemoteSCMRepo, RevError
 
 
-@pytest.mark.parametrize("workspace", [True, False])
 @pytest.mark.parametrize("links", ["reflink,copy", "hardlink,symlink"])
-def test_new_checkpoint(
-    tmp_dir, scm, dvc, checkpoint_stage, mocker, workspace, links
-):
+def test_new_checkpoint(tmp_dir, scm, dvc, checkpoint_stage, mocker, workspace, links):
     with dvc.config.edit() as conf:
         conf["cache"]["type"] = links
 
     new_mock = mocker.spy(dvc.experiments, "new")
     results = dvc.experiments.run(
         checkpoint_stage.addressing, params=["foo=2"], tmp_dir=not workspace
     )
     exp = first(results)
 
     new_mock.assert_called_once()
-    for rev in dvc.brancher([exp]):
-        if rev == "workspace":
-            continue
-        fs = dvc.repo_fs
-        with fs.open((tmp_dir / "foo").fs_path) as fobj:
+    with dvc.switch(exp):
+        fs = dvc.dvcfs
+        with fs.open("foo") as fobj:
             assert fobj.read().strip() == str(checkpoint_stage.iterations)
-        with fs.open((tmp_dir / "metrics.yaml").fs_path) as fobj:
+        with fs.open("metrics.yaml") as fobj:
             assert fobj.read().strip() == "foo: 2"
 
     if workspace:
         assert scm.get_ref(EXEC_APPLY) == exp
     assert scm.get_ref(EXEC_CHECKPOINT) == exp
     if workspace:
-        assert (tmp_dir / "foo").read_text().strip() == str(
-            checkpoint_stage.iterations
-        )
+        assert (tmp_dir / "foo").read_text().strip() == str(checkpoint_stage.iterations)
         assert (tmp_dir / "metrics.yaml").read_text().strip() == "foo: 2"
 
 
-@pytest.mark.parametrize(
-    "checkpoint_resume, workspace",
-    [(None, True), (None, False), ("foo", True), ("foo", False)],
-)
+@pytest.mark.parametrize("checkpoint_resume", [None, "foo"])
 def test_resume_checkpoint(
     tmp_dir, scm, dvc, checkpoint_stage, checkpoint_resume, workspace
 ):
     results = dvc.experiments.run(
         checkpoint_stage.addressing, params=["foo=2"], tmp_dir=not workspace
     )
 
-    with pytest.raises(DvcException):
+    with pytest.raises(RevError, match="unknown Git revision 'abc1234'"):
         dvc.experiments.run(
             checkpoint_stage.addressing,
             checkpoint_resume="abc1234",
             tmp_dir=not workspace,
         )
 
     if checkpoint_resume:
@@ -73,60 +63,49 @@
     results = dvc.experiments.run(
         checkpoint_stage.addressing,
         checkpoint_resume=checkpoint_resume,
         tmp_dir=not workspace,
     )
     exp = first(results)
 
-    for rev in dvc.brancher([exp]):
-        if rev == "workspace":
-            continue
-        fs = dvc.repo_fs
-        with fs.open((tmp_dir / "foo").fs_path) as fobj:
+    with dvc.switch(exp):
+        fs = dvc.dvcfs
+        with fs.open("foo") as fobj:
             assert fobj.read().strip() == str(2 * checkpoint_stage.iterations)
-        with fs.open((tmp_dir / "metrics.yaml").fs_path) as fobj:
+        with fs.open("metrics.yaml") as fobj:
             assert fobj.read().strip() == "foo: 2"
 
     if workspace:
         assert scm.get_ref(EXEC_APPLY) == exp
     assert scm.get_ref(EXEC_CHECKPOINT) == exp
 
 
-@pytest.mark.parametrize("workspace", [True, False])
-def test_reset_checkpoint(
-    tmp_dir, scm, dvc, checkpoint_stage, caplog, workspace
-):
-    dvc.experiments.run(
-        checkpoint_stage.addressing, name="foo", tmp_dir=not workspace
-    )
+def test_reset_checkpoint(tmp_dir, scm, dvc, checkpoint_stage, caplog, workspace):
+    dvc.experiments.run(checkpoint_stage.addressing, tmp_dir=not workspace)
 
     results = dvc.experiments.run(
         checkpoint_stage.addressing,
         params=["foo=2"],
-        name="foo",
         tmp_dir=not workspace,
         reset=True,
     )
     exp = first(results)
 
-    for rev in dvc.brancher([exp]):
-        if rev == "workspace":
-            continue
-        fs = dvc.repo_fs
-        with fs.open((tmp_dir / "foo").fs_path) as fobj:
+    with dvc.switch(exp):
+        fs = dvc.dvcfs
+        with fs.open("foo") as fobj:
             assert fobj.read().strip() == str(checkpoint_stage.iterations)
-        with fs.open((tmp_dir / "metrics.yaml").fs_path) as fobj:
+        with fs.open("metrics.yaml") as fobj:
             assert fobj.read().strip() == "foo: 2"
 
     if workspace:
         assert scm.get_ref(EXEC_APPLY) == exp
     assert scm.get_ref(EXEC_CHECKPOINT) == exp
 
 
-@pytest.mark.parametrize("workspace", [True, False])
 def test_resume_branch(tmp_dir, scm, dvc, checkpoint_stage, workspace):
     results = dvc.experiments.run(
         checkpoint_stage.addressing, params=["foo=2"], tmp_dir=not workspace
     )
     branch_rev = first(results)
     if not workspace:
         dvc.experiments.apply(branch_rev)
@@ -143,55 +122,48 @@
         checkpoint_stage.addressing,
         checkpoint_resume=branch_rev,
         params=["foo=100"],
         tmp_dir=not workspace,
     )
     checkpoint_b = first(results)
 
-    for rev in dvc.brancher([checkpoint_a]):
-        if rev == "workspace":
-            continue
-        fs = dvc.repo_fs
-        with fs.open((tmp_dir / "foo").fs_path) as fobj:
+    with dvc.switch(checkpoint_a):
+        fs = dvc.dvcfs
+        with fs.open("foo") as fobj:
             assert fobj.read().strip() == str(2 * checkpoint_stage.iterations)
-        with fs.open((tmp_dir / "metrics.yaml").fs_path) as fobj:
+        with fs.open("metrics.yaml") as fobj:
             assert fobj.read().strip() == "foo: 2"
 
-    for rev in dvc.brancher([checkpoint_b]):
-        if rev == "workspace":
-            continue
-        fs = dvc.repo_fs
-        with fs.open((tmp_dir / "foo").fs_path) as fobj:
+    with dvc.switch(checkpoint_b):
+        fs = dvc.dvcfs
+        with fs.open("foo") as fobj:
             assert fobj.read().strip() == str(2 * checkpoint_stage.iterations)
-        with fs.open((tmp_dir / "metrics.yaml").fs_path) as fobj:
+        with fs.open("metrics.yaml") as fobj:
             assert fobj.read().strip() == "foo: 100"
 
     with pytest.raises(MultipleBranchError):
         dvc.experiments.get_branch_by_rev(branch_rev)
 
     assert branch_rev == dvc.experiments.scm.gitpython.repo.git.merge_base(
         checkpoint_a, checkpoint_b
     )
 
 
-@pytest.mark.parametrize("workspace", [True, False])
-def test_resume_non_head_checkpoint(
-    tmp_dir, scm, dvc, checkpoint_stage, workspace
-):
+def test_resume_non_head_checkpoint(tmp_dir, scm, dvc, checkpoint_stage, workspace):
     orig_head = scm.get_rev()
     results = dvc.experiments.run(
         checkpoint_stage.addressing, params=["foo=2"], tmp_dir=not workspace
     )
     checkpoint_head = first(results)
     orig_branch = dvc.experiments.get_branch_by_rev(checkpoint_head)
 
     rev = list(scm.branch_revs(checkpoint_head, orig_head))[-1]
     dvc.experiments.apply(rev)
 
-    with pytest.raises(DvcException):
+    with pytest.raises(DvcException, match="Nothing to do for unchanged checkpoint"):
         dvc.experiments.run(checkpoint_stage.addressing, tmp_dir=not workspace)
 
     results = dvc.experiments.run(
         checkpoint_stage.addressing, params=["foo=100"], tmp_dir=not workspace
     )
     new_head = first(results)
     assert orig_branch != dvc.experiments.get_branch_by_rev(new_head)
@@ -231,15 +203,16 @@
     monkeypatch.setenv(DVC_EXP_AUTO_PUSH, "true")
     results = dvc.experiments.run(checkpoint_stage.addressing)
     assert (tmp_dir / "foo").read_text() == "4"
     exp = first(results)
     ref_info = first(exp_refs_by_rev(scm, exp))
     assert git_upstream.tmp_dir.scm.get_ref(str(ref_info)) == exp
 
-    assert auto_push_spy.call_count == 2
+    # Assert 3 pushes: 2 checkpoints and final commit
+    assert auto_push_spy.call_count == 3
     assert auto_push_spy.call_args[0][2] == remote
 
 
 def test_auto_push_error_url(
     dvc, scm, checkpoint_stage, local_remote, monkeypatch, clear_env
 ):
     monkeypatch.setenv(DVC_EXP_GIT_REMOTE, "true")
@@ -266,19 +239,31 @@
     caplog,
     monkeypatch,
     clear_env,
 ):
     root_dir = str(tmp_dir)
     monkeypatch.setenv(DVC_EXP_GIT_REMOTE, root_dir)
     monkeypatch.setenv(DVC_EXP_AUTO_PUSH, "true")
-    assert (
-        dvc.experiments.run(checkpoint_stage.addressing, params=["foo=2"])
-        != {}
-    )
+    assert dvc.experiments.run(checkpoint_stage.addressing, params=["foo=2"]) != {}
 
     with caplog.at_level(logging.WARNING, logger="dvc.repo.experiments"):
         assert (
             f"'{root_dir}' points to the current Git repo, experiment "
             "Git refs will not be pushed. But DVC cache and run cache will "
             "automatically be pushed to the default DVC remote (if any) "
             "on each experiment commit." in caplog.text
         )
+
+
+def test_tmp_dir_failed_checkpoint(tmp_dir, scm, dvc, failed_checkpoint_stage, caplog):
+    dvc.experiments.run(
+        failed_checkpoint_stage.addressing, params=["foo=2"], tmp_dir=True
+    )
+
+    result = dvc.experiments.show()[1]
+    assert len(result.experiments) == 1
+    # Assert 2 checkpoints and final commit
+    assert len(result.experiments[0]) == 3
+    assert (
+        "Checkpoint stage 'failed-checkpoint-file' was interrupted remaining "
+        "stages in pipeline will not be reproduced." in caplog.text
+    )
```

### Comparing `dvc-2.9.5/tests/func/experiments/test_diff.py` & `dvc-3.0.0a0/tests/func/experiments/test_diff.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/func/experiments/test_experiments.py` & `dvc-3.0.0a0/tests/func/experiments/test_experiments.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,65 +1,73 @@
 import itertools
 import logging
 import os
 import stat
+from textwrap import dedent
 
 import pytest
+from configobj import ConfigObj
 from funcy import first
 
-from dvc.dvcfile import PIPELINE_FILE
-from dvc.exceptions import DvcException
+from dvc.dvcfile import PROJECT_FILE
+from dvc.env import (
+    DVC_EXP_BASELINE_REV,
+    DVC_EXP_NAME,
+    DVC_STUDIO_OFFLINE,
+    DVC_STUDIO_REPO_URL,
+    DVC_STUDIO_TOKEN,
+    DVC_STUDIO_URL,
+)
+from dvc.exceptions import DvcException, ReproductionError
+from dvc.repo.experiments.exceptions import ExperimentExistsError
+from dvc.repo.experiments.queue.base import BaseStashQueue
+from dvc.repo.experiments.refs import CELERY_STASH
 from dvc.repo.experiments.utils import exp_refs_by_rev
-from dvc.scm import resolve_rev
+from dvc.scm import SCMError, resolve_rev
 from dvc.stage.exceptions import StageFileDoesNotExistError
 from dvc.utils.serialize import PythonFileCorruptedError
-from tests.func.test_repro_multistage import COPY_SCRIPT
+from tests.scripts import COPY_SCRIPT
 
 
-@pytest.mark.parametrize(
-    "name,workspace",
-    [(None, True), (None, False), ("foo", True), ("foo", False)],
-)
+@pytest.mark.parametrize("name", [None, "foo"])
 def test_new_simple(tmp_dir, scm, dvc, exp_stage, mocker, name, workspace):
     baseline = scm.get_rev()
     tmp_dir.gen("params.yaml", "foo: 2")
 
     new_mock = mocker.spy(dvc.experiments, "new")
     results = dvc.experiments.run(
         exp_stage.addressing, name=name, tmp_dir=not workspace
     )
     exp = first(results)
     ref_info = first(exp_refs_by_rev(scm, exp))
-    assert ref_info and ref_info.baseline_sha == baseline
+    assert ref_info
+    assert ref_info.baseline_sha == baseline
 
     new_mock.assert_called_once()
     fs = scm.get_fs(exp)
-    with fs.open(tmp_dir / "metrics.yaml", mode="r", encoding="utf-8") as fobj:
+    with fs.open("metrics.yaml", mode="r", encoding="utf-8") as fobj:
         assert fobj.read().strip() == "foo: 2"
 
     if workspace:
         assert (tmp_dir / "metrics.yaml").read_text().strip() == "foo: 2"
 
     exp_name = name if name else ref_info.name
-    assert dvc.experiments.get_exact_name(exp) == exp_name
+    assert dvc.experiments.get_exact_name([exp])[exp] == exp_name
     assert resolve_rev(scm, exp_name) == exp
 
 
-@pytest.mark.parametrize("workspace", [True, False])
 def test_experiment_exists(tmp_dir, scm, dvc, exp_stage, mocker, workspace):
-    from dvc.repo.experiments.base import ExperimentExistsError
-
     dvc.experiments.run(
         exp_stage.addressing,
         name="foo",
         params=["foo=2"],
         tmp_dir=not workspace,
     )
 
-    new_mock = mocker.spy(dvc.experiments, "_stash_exp")
+    new_mock = mocker.spy(BaseStashQueue, "_stash_exp")
     with pytest.raises(ExperimentExistsError):
         dvc.experiments.run(
             exp_stage.addressing,
             name="foo",
             params=["foo=3"],
             tmp_dir=not workspace,
         )
@@ -71,15 +79,15 @@
         params=["foo=3"],
         force=True,
         tmp_dir=not workspace,
     )
     exp = first(results)
 
     fs = scm.get_fs(exp)
-    with fs.open(tmp_dir / "metrics.yaml", mode="r", encoding="utf-8") as fobj:
+    with fs.open("metrics.yaml", mode="r", encoding="utf-8") as fobj:
         assert fobj.read().strip() == "foo: 3"
 
 
 @pytest.mark.skipif(os.name == "nt", reason="Not supported for Windows.")
 def test_file_permissions(tmp_dir, scm, dvc, exp_stage, mocker):
     mode = 0o755
     os.chmod(tmp_dir / "copy.py", mode)
@@ -87,170 +95,56 @@
     scm.commit("set exec")
 
     tmp_dir.gen("params.yaml", "foo: 2")
     dvc.experiments.run(exp_stage.addressing)
     assert stat.S_IMODE(os.stat(tmp_dir / "copy.py").st_mode) == mode
 
 
-def test_failed_exp(tmp_dir, scm, dvc, exp_stage, mocker, caplog):
-    from dvc.exceptions import ReproductionError
-
+def test_failed_exp_workspace(
+    tmp_dir,
+    scm,
+    dvc,
+    failed_exp_stage,
+    mocker,
+    capsys,
+):
     tmp_dir.gen("params.yaml", "foo: 2")
-
-    mocker.patch(
-        "concurrent.futures.Future.exception",
-        return_value=ReproductionError(exp_stage.addressing),
-    )
-    with caplog.at_level(logging.ERROR):
-        dvc.experiments.run(exp_stage.addressing, tmp_dir=True)
-        assert "Failed to reproduce experiment" in caplog.text
-
-
-@pytest.mark.parametrize(
-    "changes, expected",
-    [
-        [["foo=baz"], "{foo: baz, goo: {bag: 3}, lorem: false}"],
-        [["foo=baz", "goo=bar"], "{foo: baz, goo: bar, lorem: false}"],
-        [
-            ["goo.bag=4"],
-            "{foo: [bar: 1, baz: 2], goo: {bag: 4}, lorem: false}",
-        ],
-        [["foo[0]=bar"], "{foo: [bar, baz: 2], goo: {bag: 3}, lorem: false}"],
-        [
-            ["foo[1].baz=3"],
-            "{foo: [bar: 1, baz: 3], goo: {bag: 3}, lorem: false}",
-        ],
-        [
-            ["foo[1]=[baz, goo]"],
-            "{foo: [bar: 1, [baz, goo]], goo: {bag: 3}, lorem: false}",
-        ],
-    ],
-)
-def test_modify_params(tmp_dir, scm, dvc, mocker, changes, expected):
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
-    tmp_dir.gen(
-        "params.yaml", "{foo: [bar: 1, baz: 2], goo: {bag: 3}, lorem: false}"
-    )
-    stage = dvc.run(
-        cmd="python copy.py params.yaml metrics.yaml",
-        metrics_no_cache=["metrics.yaml"],
-        params=["foo", "goo", "lorem"],
-        name="copy-file",
-    )
-    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
-    scm.commit("init")
-
-    new_mock = mocker.spy(dvc.experiments, "new")
-    results = dvc.experiments.run(stage.addressing, params=changes)
-    exp = first(results)
-
-    new_mock.assert_called_once()
-    fs = scm.get_fs(exp)
-    with fs.open(tmp_dir / "metrics.yaml", mode="r") as fobj:
-        assert fobj.read().strip() == expected
-
-
-@pytest.mark.parametrize(
-    "changes",
-    [["lorem.ipsum=3"], ["foo[0].bazar=3"]],
-)
-def test_add_params(tmp_dir, scm, dvc, changes):
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
-    tmp_dir.gen(
-        "params.yaml", "{foo: [bar: 1, baz: 2], goo: {bag: 3}, lorem: false}"
-    )
-    stage = dvc.run(
-        cmd="python copy.py params.yaml metrics.yaml",
-        metrics_no_cache=["metrics.yaml"],
-        params=["foo", "goo", "lorem"],
-        name="copy-file",
-    )
-    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
-    scm.commit("init")
-
-    with pytest.raises(DvcException):
-        dvc.experiments.run(stage.addressing, params=changes)
-
-
-@pytest.mark.parametrize("queue", [True, False])
-def test_apply(tmp_dir, scm, dvc, exp_stage, queue):
-    from dvc.exceptions import InvalidArgumentError
-    from dvc.repo.experiments.base import ApplyConflictError
-
-    metrics_original = (tmp_dir / "metrics.yaml").read_text().strip()
-    results = dvc.experiments.run(
-        exp_stage.addressing, params=["foo=2"], queue=queue, tmp_dir=True
-    )
-    exp_a = first(results)
-
-    results = dvc.experiments.run(
-        exp_stage.addressing, params=["foo=3"], queue=queue, tmp_dir=True
-    )
-    exp_b = first(results)
-
-    with pytest.raises(InvalidArgumentError):
-        dvc.experiments.apply("foo")
-
-    dvc.experiments.apply(exp_a)
-    assert (tmp_dir / "params.yaml").read_text().strip() == "foo: 2"
-    assert (
-        (tmp_dir / "metrics.yaml").read_text().strip() == metrics_original
-        if queue
-        else "foo: 2"
-    )
-
-    with pytest.raises(ApplyConflictError):
-        dvc.experiments.apply(exp_b, force=False)
-        # failed apply should revert everything to prior state
-        assert (tmp_dir / "params.yaml").read_text().strip() == "foo: 2"
-        assert (
-            (tmp_dir / "metrics.yaml").read_text().strip() == metrics_original
-            if queue
-            else "foo: 2"
-        )
-
-    dvc.experiments.apply(exp_b)
-    assert (tmp_dir / "params.yaml").read_text().strip() == "foo: 3"
-    assert (
-        (tmp_dir / "metrics.yaml").read_text().strip() == metrics_original
-        if queue
-        else "foo: 3"
+    with pytest.raises(ReproductionError):
+        dvc.experiments.run(failed_exp_stage.addressing)
+    assert not dvc.fs.exists(
+        os.path.join(dvc.experiments.workspace_queue.pid_dir, "workspace")
     )
 
 
 def test_get_baseline(tmp_dir, scm, dvc, exp_stage):
-    from dvc.repo.experiments.base import EXPS_STASH
-
     init_rev = scm.get_rev()
     assert dvc.experiments.get_baseline(init_rev) is None
 
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
     exp_rev = first(results)
     assert dvc.experiments.get_baseline(exp_rev) == init_rev
 
     dvc.experiments.run(exp_stage.addressing, params=["foo=3"], queue=True)
-    assert dvc.experiments.get_baseline(f"{EXPS_STASH}@{{0}}") == init_rev
+    assert dvc.experiments.get_baseline(f"{CELERY_STASH}@{{0}}") == init_rev
 
     scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
     scm.commit("promote exp")
     promote_rev = scm.get_rev()
     assert dvc.experiments.get_baseline(promote_rev) is None
 
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=4"])
     exp_rev = first(results)
     assert dvc.experiments.get_baseline(exp_rev) == promote_rev
 
     dvc.experiments.run(exp_stage.addressing, params=["foo=5"], queue=True)
-    assert dvc.experiments.get_baseline(f"{EXPS_STASH}@{{0}}") == promote_rev
-    print("stash 1")
-    assert dvc.experiments.get_baseline(f"{EXPS_STASH}@{{1}}") == init_rev
+    assert dvc.experiments.get_baseline(f"{CELERY_STASH}@{{0}}") == promote_rev
+    assert dvc.experiments.get_baseline(f"{CELERY_STASH}@{{1}}") == init_rev
 
 
-def test_update_py_params(tmp_dir, scm, dvc):
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
+def test_update_py_params(tmp_dir, scm, dvc, session_queue, copy_script):
     tmp_dir.gen("params.py", "INT = 1\n")
     stage = dvc.run(
         cmd="python copy.py params.py metrics.py",
         metrics_no_cache=["metrics.py"],
         params=["params.py:INT"],
         name="copy-file",
     )
@@ -259,24 +153,26 @@
 
     results = dvc.experiments.run(
         stage.addressing, params=["params.py:INT=2"], tmp_dir=True
     )
     exp_a = first(results)
 
     fs = scm.get_fs(exp_a)
-    with fs.open(tmp_dir / "params.py", mode="r", encoding="utf-8") as fobj:
+    with fs.open("params.py", mode="r", encoding="utf-8") as fobj:
         assert fobj.read().strip() == "INT = 2"
-    with fs.open(tmp_dir / "metrics.py", mode="r", encoding="utf-8") as fobj:
+    with fs.open("metrics.py", mode="r", encoding="utf-8") as fobj:
         assert fobj.read().strip() == "INT = 2"
 
     tmp_dir.gen(
         "params.py",
-        "INT = 1\nFLOAT = 0.001\nDICT = {'a': 1}\n\n"
-        "class Train:\n    seed = 2020\n\n"
-        "class Klass:\n    def __init__(self):\n        self.a = 111\n",
+        (
+            "INT = 1\nFLOAT = 0.001\nDICT = {'a': 1}\n\n"
+            "class Train:\n    seed = 2020\n\n"
+            "class Klass:\n    def __init__(self):\n        self.a = 111\n"
+        ),
     )
     stage = dvc.run(
         cmd="python copy.py params.py metrics.py",
         metrics_no_cache=["metrics.py"],
         params=["params.py:INT,FLOAT,DICT,Train,Klass"],
         name="copy-file",
     )
@@ -305,33 +201,31 @@
             return text
 
         # NOTE: git on windows will use CRLF, so we have to convert it to LF
         # in order to compare with the original
         return text.replace("\r\n", "\n")
 
     fs = scm.get_fs(exp_a)
-    with fs.open(tmp_dir / "params.py", mode="r", encoding="utf-8") as fobj:
+    with fs.open("params.py", mode="r", encoding="utf-8") as fobj:
         assert _dos2unix(fobj.read().strip()) == result
-    with fs.open(tmp_dir / "metrics.py", mode="r", encoding="utf-8") as fobj:
+    with fs.open("metrics.py", mode="r", encoding="utf-8") as fobj:
         assert _dos2unix(fobj.read().strip()) == result
 
     tmp_dir.gen("params.py", "INT = 1\n")
     stage = dvc.run(
         cmd="python copy.py params.py metrics.py",
         metrics_no_cache=["metrics.py"],
         params=["params.py:INT"],
         name="copy-file",
     )
     scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.py", "metrics.py"])
     scm.commit("init")
 
     with pytest.raises(PythonFileCorruptedError):
-        dvc.experiments.run(
-            stage.addressing, params=["params.py:INT=2a"], tmp_dir=True
-        )
+        dvc.experiments.run(stage.addressing, params=["params.py:INT=2a"], tmp_dir=True)
 
 
 def test_detached_parent(tmp_dir, scm, dvc, exp_stage, mocker):
     detached_rev = scm.get_rev()
 
     tmp_dir.gen("params.yaml", "foo: 2")
     dvc.reproduce(exp_stage.addressing)
@@ -351,34 +245,31 @@
     from dvc.exceptions import InvalidArgumentError
 
     with pytest.raises(InvalidArgumentError):
         dvc.experiments.branch("foo", "branch")
 
     scm.branch("branch-exists")
 
-    results = dvc.experiments.run(
-        exp_stage.addressing, params=["foo=2"], name="foo"
-    )
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"], name="foo")
     exp_a = first(results)
     ref_a = dvc.experiments.get_branch_by_rev(exp_a)
 
     with pytest.raises(InvalidArgumentError):
         dvc.experiments.branch("foo", "branch-exists")
+    dvc.experiments.branch("foo")
     dvc.experiments.branch("foo", "branch-name")
     dvc.experiments.branch(exp_a, "branch-rev")
     dvc.experiments.branch(ref_a, "branch-ref")
 
-    for name in ["branch-name", "branch-rev", "branch-ref"]:
+    for name in ["foo-branch", "branch-name", "branch-rev", "branch-ref"]:
         assert name in scm.list_branches()
         assert scm.resolve_rev(name) == exp_a
 
     tmp_dir.scm_gen({"new_file": "new_file"}, commit="new baseline")
-    results = dvc.experiments.run(
-        exp_stage.addressing, params=["foo=2"], name="foo"
-    )
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"], name="foo")
     exp_b = first(results)
     ref_b = dvc.experiments.get_branch_by_rev(exp_b)
 
     with pytest.raises(InvalidArgumentError):
         dvc.experiments.branch("foo", "branch-name")
     dvc.experiments.branch(ref_b, "branch-ref-b")
 
@@ -403,136 +294,145 @@
         "pull",
         "ls",
     ]:
         with pytest.raises(NoSCMError):
             getattr(dvc.experiments, cmd)()
 
 
-@pytest.mark.parametrize("workspace", [True, False])
-def test_untracked(tmp_dir, scm, dvc, caplog, workspace):
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
+def test_untracked(tmp_dir, scm, dvc, caplog, workspace, copy_script):
     tmp_dir.scm_gen("params.yaml", "foo: 1", commit="track params")
     stage = dvc.run(
         cmd="python copy.py params.yaml metrics.yaml",
         metrics_no_cache=["metrics.yaml"],
         params=["foo"],
         deps=["copy.py"],
         name="copy-file",
         no_exec=True,
     )
 
     # copy.py is untracked
-    with caplog.at_level(logging.ERROR):
-        results = dvc.experiments.run(
-            stage.addressing, params=["foo=2"], tmp_dir=True
-        )
-        assert "Failed to reproduce experiment" in caplog.text
-        assert not results
+    # with caplog.at_level(logging.ERROR):
+    #     results = dvc.experiments.run(
+    #         stage.addressing, params=["foo=2"], tmp_dir=True
+    #     )
+    #     assert "Failed to reproduce experiment" in caplog.text
+    #     assert not results
 
     # dvc.yaml, copy.py are staged as new file but not committed
     scm.add(["dvc.yaml", "copy.py"])
     results = dvc.experiments.run(
         stage.addressing, params=["foo=2"], tmp_dir=not workspace
     )
     exp = first(results)
     fs = scm.get_fs(exp)
     assert fs.exists("dvc.yaml")
     assert fs.exists("dvc.lock")
     assert fs.exists("copy.py")
-    with fs.open(tmp_dir / "metrics.yaml", mode="r", encoding="utf-8") as fobj:
+    with fs.open("metrics.yaml", mode="r", encoding="utf-8") as fobj:
         assert fobj.read().strip() == "foo: 2"
 
 
 def test_packed_args_exists(tmp_dir, scm, dvc, exp_stage, caplog):
     from dvc.repo.experiments.executor.base import BaseExecutor
 
     tmp_dir.scm_gen(
         tmp_dir / ".dvc" / "tmp" / BaseExecutor.PACKED_ARGS_FILE,
         "",
         commit="commit args file",
+        force=True,
     )
 
     with caplog.at_level(logging.WARNING):
         dvc.experiments.run(exp_stage.addressing)
         assert "Temporary DVC file" in caplog.text
+    assert not (tmp_dir / ".dvc" / "tmp" / BaseExecutor.PACKED_ARGS_FILE).exists()
 
 
 def test_list(tmp_dir, scm, dvc, exp_stage):
     baseline_a = scm.get_rev()
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
     exp_a = first(results)
     ref_info_a = first(exp_refs_by_rev(scm, exp_a))
 
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
     exp_b = first(results)
     ref_info_b = first(exp_refs_by_rev(scm, exp_b))
 
     tmp_dir.scm_gen("new", "new", commit="new")
-    baseline_c = scm.get_rev()
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=4"])
     exp_c = first(results)
     ref_info_c = first(exp_refs_by_rev(scm, exp_c))
 
-    assert dvc.experiments.ls() == {baseline_c: [ref_info_c.name]}
+    assert dvc.experiments.ls() == {"master": [ref_info_c.name]}
 
     exp_list = dvc.experiments.ls(rev=ref_info_a.baseline_sha)
     assert {key: set(val) for key, val in exp_list.items()} == {
-        baseline_a: {ref_info_a.name, ref_info_b.name}
+        baseline_a[:7]: {ref_info_a.name, ref_info_b.name}
     }
 
-    exp_list = dvc.experiments.ls(all_=True)
+    exp_list = dvc.experiments.ls(rev=[baseline_a, scm.get_rev()])
     assert {key: set(val) for key, val in exp_list.items()} == {
-        baseline_a: {ref_info_a.name, ref_info_b.name},
-        baseline_c: {ref_info_c.name},
+        baseline_a[:7]: {ref_info_a.name, ref_info_b.name},
+        "master": {ref_info_c.name},
+    }
+
+    exp_list = dvc.experiments.ls(all_commits=True)
+    assert {key: set(val) for key, val in exp_list.items()} == {
+        baseline_a[:7]: {ref_info_a.name, ref_info_b.name},
+        "master": {ref_info_c.name},
+    }
+
+    scm.checkout("branch", True)
+    exp_list = dvc.experiments.ls(all_commits=True)
+    assert {key: set(val) for key, val in exp_list.items()} == {
+        baseline_a[:7]: {ref_info_a.name, ref_info_b.name},
+        "branch": {ref_info_c.name},
     }
 
 
-@pytest.mark.parametrize("workspace", [True, False])
 def test_subdir(tmp_dir, scm, dvc, workspace):
     subdir = tmp_dir / "dir"
     subdir.gen("copy.py", COPY_SCRIPT)
     subdir.gen("params.yaml", "foo: 1")
 
     with subdir.chdir():
         dvc.run(
             cmd="python copy.py params.yaml metrics.yaml",
             metrics_no_cache=["metrics.yaml"],
             params=["foo"],
             name="copy-file",
             no_exec=True,
         )
-        scm.add(
-            [subdir / "dvc.yaml", subdir / "copy.py", subdir / "params.yaml"]
-        )
+        scm.add([subdir / "dvc.yaml", subdir / "copy.py", subdir / "params.yaml"])
         scm.commit("init")
 
         results = dvc.experiments.run(
-            PIPELINE_FILE, params=["foo=2"], tmp_dir=not workspace
+            PROJECT_FILE, params=["foo=2"], tmp_dir=not workspace
         )
         assert results
 
     exp = first(results)
     ref_info = first(exp_refs_by_rev(scm, exp))
 
     fs = scm.get_fs(exp)
     for fname in ["metrics.yaml", "dvc.lock"]:
-        assert fs.exists(subdir / fname)
-    with fs.open(subdir / "metrics.yaml", mode="r", encoding="utf-8") as fobj:
+        assert fs.exists(f"dir/{fname}")
+    with fs.open("dir/metrics.yaml", mode="r", encoding="utf-8") as fobj:
         assert fobj.read().strip() == "foo: 2"
 
-    assert dvc.experiments.get_exact_name(exp) == ref_info.name
+    assert dvc.experiments.get_exact_name([exp])[exp] == ref_info.name
     assert resolve_rev(scm, ref_info.name) == exp
 
 
-@pytest.mark.parametrize("workspace", [True, False])
-def test_subrepo(tmp_dir, scm, workspace):
-    from tests.unit.fs.test_repo import make_subrepo
+def test_subrepo(tmp_dir, request, scm, workspace):
+    from dvc.testing.tmp_dir import make_subrepo
 
     subrepo = tmp_dir / "dir" / "repo"
     make_subrepo(subrepo, scm)
+    request.addfinalizer(subrepo.dvc.close)
 
     subrepo.gen("copy.py", COPY_SCRIPT)
     subrepo.gen("params.yaml", "foo: 1")
 
     with subrepo.chdir():
         subrepo.dvc.run(
             cmd="python copy.py params.yaml metrics.yaml",
@@ -547,58 +447,55 @@
                 subrepo / "copy.py",
                 subrepo / "params.yaml",
             ]
         )
         scm.commit("init")
 
         results = subrepo.dvc.experiments.run(
-            PIPELINE_FILE, params=["foo=2"], tmp_dir=not workspace
+            PROJECT_FILE, params=["foo=2"], tmp_dir=not workspace
         )
         assert results
 
     exp = first(results)
     ref_info = first(exp_refs_by_rev(scm, exp))
 
     fs = scm.get_fs(exp)
     for fname in ["metrics.yaml", "dvc.lock"]:
-        assert fs.exists(subrepo / fname)
-    with fs.open(subrepo / "metrics.yaml", mode="r", encoding="utf-8") as fobj:
+        assert fs.exists(f"dir/repo/{fname}")
+    with fs.open("dir/repo/metrics.yaml", mode="r", encoding="utf-8") as fobj:
         assert fobj.read().strip() == "foo: 2"
 
-    assert subrepo.dvc.experiments.get_exact_name(exp) == ref_info.name
+    assert subrepo.dvc.experiments.get_exact_name([exp])[exp] == ref_info.name
     assert resolve_rev(scm, ref_info.name) == exp
 
 
-def test_queue(tmp_dir, scm, dvc, exp_stage, mocker):
+def test_run_celery(tmp_dir, scm, dvc, exp_stage, mocker):
+    """Test running with full (non-pytest-celery) dvc-task queue."""
     dvc.experiments.run(exp_stage.addressing, params=["foo=2"], queue=True)
     dvc.experiments.run(exp_stage.addressing, params=["foo=3"], queue=True)
     assert len(dvc.experiments.stash_revs) == 2
 
-    repro_mock = mocker.spy(dvc.experiments, "_reproduce_revs")
+    repro_spy = mocker.spy(dvc.experiments, "reproduce_celery")
     results = dvc.experiments.run(run_all=True)
     assert len(results) == 2
-    repro_mock.assert_called_with(jobs=1)
+    repro_spy.assert_called_once_with(jobs=1)
 
     expected = {"foo: 2", "foo: 3"}
     metrics = set()
     for exp in results:
         fs = scm.get_fs(exp)
-        with fs.open(
-            tmp_dir / "metrics.yaml", mode="r", encoding="utf-8"
-        ) as fobj:
+        with fs.open("metrics.yaml", mode="r", encoding="utf-8") as fobj:
             metrics.add(fobj.read().strip())
     assert expected == metrics
 
 
 def test_run_metrics(tmp_dir, scm, dvc, exp_stage, mocker):
     from dvc.cli import main
 
-    mocker.patch.object(
-        dvc.experiments, "run", return_value={"abc123": "abc123"}
-    )
+    mocker.patch.object(dvc.experiments, "run", return_value={"abc123": "abc123"})
     show_mock = mocker.patch.object(dvc.metrics, "show", return_value={})
 
     main(["exp", "run", "-m"])
     assert show_mock.called_once()
 
 
 def test_checkout_targets_deps(tmp_dir, scm, dvc, exp_stage):
@@ -620,33 +517,32 @@
     assert (tmp_dir / "foo").exists()
     assert (tmp_dir / "foo").read_text() == "foo"
     assert not (tmp_dir / "bar").exists()
 
 
 @pytest.mark.parametrize("tail", ["", "~1", "^"])
 def test_fix_exp_head(tmp_dir, scm, tail):
-    from dvc.repo.experiments.base import EXEC_BASELINE
+    from dvc.repo.experiments.refs import EXEC_BASELINE
     from dvc.repo.experiments.utils import fix_exp_head
 
     head = "HEAD" + tail
     assert head == fix_exp_head(scm, head)
 
     rev = "1" * 40
     scm.set_ref(EXEC_BASELINE, rev)
     assert EXEC_BASELINE + tail == fix_exp_head(scm, head)
     assert "foo" + tail == fix_exp_head(scm, "foo" + tail)
 
 
 @pytest.mark.parametrize(
-    "workspace, params, target",
-    itertools.product((True, False), ("foo: 1", "foo: 2"), (True, False)),
+    "params, target",
+    itertools.product(("foo: 1", "foo: 2"), (True, False)),
 )
-def test_modified_data_dep(tmp_dir, scm, dvc, workspace, params, target):
+def test_modified_data_dep(tmp_dir, scm, dvc, workspace, params, target, copy_script):
     tmp_dir.dvc_gen("data", "data")
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
     tmp_dir.gen("params.yaml", "foo: 1")
     exp_stage = dvc.run(
         cmd="python copy.py params.yaml metrics.yaml",
         metrics_no_cache=["metrics.yaml"],
         params=["foo"],
         name="copy-file",
         deps=["copy.py", "data"],
@@ -671,17 +567,17 @@
         exp_stage.addressing if target else None, tmp_dir=not workspace
     )
     exp = first(results)
 
     for rev in dvc.brancher(revs=[exp]):
         if rev != exp:
             continue
-        with dvc.repo_fs.open((tmp_dir / "metrics.yaml").fs_path) as fobj:
+        with dvc.dvcfs.open("metrics.yaml") as fobj:
             assert fobj.read().strip() == params
-        with dvc.repo_fs.open((tmp_dir / "data").fs_path) as fobj:
+        with dvc.dvcfs.open("data") as fobj:
             assert fobj.read().strip() == "modified"
 
     if workspace:
         assert (tmp_dir / "metrics.yaml").read_text().strip() == params
         assert (tmp_dir / "data").read_text().strip() == "modified"
 
 
@@ -693,15 +589,15 @@
     assert dvc.experiments.run(".", recursive=True)
     assert (tmp_dir / "metric.json").parse() == {"foo": 1}
 
 
 def test_experiment_name_invalid(tmp_dir, scm, dvc, exp_stage, mocker):
     from dvc.exceptions import InvalidArgumentError
 
-    new_mock = mocker.spy(dvc.experiments, "_stash_exp")
+    new_mock = mocker.spy(BaseStashQueue, "_stash_exp")
     with pytest.raises(InvalidArgumentError):
         dvc.experiments.run(
             exp_stage.addressing,
             name="fo^o",
             params=["foo=3"],
         )
     new_mock.assert_not_called()
@@ -713,7 +609,217 @@
     Instead it should just leave it to be handled in the main entrypoints.
     """
     with caplog.at_level(logging.ERROR):
         with pytest.raises(StageFileDoesNotExistError):
             dvc.experiments.run()
 
     assert not caplog.text
+
+
+@pytest.mark.vscode
+def test_run_env(tmp_dir, dvc, scm, mocker):
+    dump_run_env = dedent(
+        """\
+        import os
+        from dvc.env import (
+            DVC_EXP_BASELINE_REV,
+            DVC_EXP_NAME,
+            DVC_STUDIO_OFFLINE,
+            DVC_STUDIO_REPO_URL,
+            DVC_STUDIO_TOKEN,
+            DVC_STUDIO_URL
+        )
+        for v in (
+            DVC_EXP_BASELINE_REV,
+            DVC_EXP_NAME,
+            DVC_STUDIO_OFFLINE,
+            DVC_STUDIO_REPO_URL,
+            DVC_STUDIO_TOKEN,
+            DVC_STUDIO_URL
+        ):
+            with open(v, "w") as f:
+                f.write(os.environ.get(v, ""))
+        """
+    )
+    mocker.patch(
+        "dvc.repo.experiments.queue.base.get_studio_config",
+        return_value={
+            "token": "TOKEN",
+            "repo_url": "REPO_URL",
+            "url": "BASE_URL",
+            "offline": "false",
+        },
+    )
+    (tmp_dir / "dump_run_env.py").write_text(dump_run_env)
+    baseline = scm.get_rev()
+    dvc.stage.add(
+        cmd="python dump_run_env.py",
+        name="run_env",
+    )
+    dvc.experiments.run()
+    assert (tmp_dir / DVC_EXP_BASELINE_REV).read_text().strip() == baseline
+    assert (tmp_dir / DVC_EXP_NAME).read_text().strip()
+    assert (tmp_dir / DVC_STUDIO_TOKEN).read_text().strip() == "TOKEN"
+    assert (tmp_dir / DVC_STUDIO_REPO_URL).read_text().strip() == "REPO_URL"
+    assert (tmp_dir / DVC_STUDIO_URL).read_text().strip() == "BASE_URL"
+    assert (tmp_dir / DVC_STUDIO_OFFLINE).read_text().strip() == "false"
+
+    dvc.experiments.run(name="foo")
+    assert (tmp_dir / DVC_EXP_BASELINE_REV).read_text().strip() == baseline
+    assert (tmp_dir / DVC_EXP_NAME).read_text().strip() == "foo"
+
+
+def test_experiment_unchanged(tmp_dir, scm, dvc, exp_stage):
+    dvc.experiments.run(exp_stage.addressing)
+    dvc.experiments.run(exp_stage.addressing)
+
+    assert len(dvc.experiments.ls()["master"]) == 2
+
+
+def test_experiment_run_dry(tmp_dir, scm, dvc, exp_stage):
+    dvc.experiments.run(exp_stage.addressing, dry=True)
+
+    assert len(dvc.experiments.ls()["master"]) == 0
+
+
+def test_clean(tmp_dir, scm, dvc, mocker):
+    clean = mocker.spy(dvc.experiments.celery_queue.celery, "clean")
+    dvc.experiments.clean()
+    clean.assert_called_once_with()
+
+
+def test_experiment_no_commit(tmp_dir):
+    from scmrepo.git import Git
+
+    from dvc.repo import Repo
+
+    Git.init(tmp_dir.fs_path).close()
+
+    repo = Repo.init()
+    assert repo.scm.no_commits
+
+    try:
+        with pytest.raises(SCMError):  # noqa: PT011
+            repo.experiments.ls()
+    finally:
+        repo.close()
+
+
+def test_local_config_is_propagated_to_tmp(tmp_dir, scm, dvc):
+    with dvc.config.edit("local") as conf:
+        conf["cache"]["type"] = "hardlink"
+
+    stage = dvc.stage.add(
+        cmd="cat .dvc/config.local > file", name="foo", outs_no_cache=["file"]
+    )
+    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")
+
+    results = dvc.experiments.run(stage.addressing, tmp_dir=True)
+    exp = first(results)
+    fs = scm.get_fs(exp)
+
+    with fs.open("file") as fobj:
+        conf_obj = ConfigObj(fobj)
+        assert conf_obj["cache"]["type"] == "hardlink"
+
+
+@pytest.mark.parametrize("tmp", [True, False])
+def test_untracked_top_level_files_are_included_in_exp(tmp_dir, scm, dvc, tmp):
+    (tmp_dir / "dvc.yaml").dump(
+        {
+            "metrics": ["metrics.json"],
+            "params": ["params.yaml"],
+            "plots": ["plots.csv"],
+        }
+    )
+    stage = dvc.stage.add(
+        cmd="touch metrics.json && touch params.yaml && touch plots.csv",
+        name="top-level",
+    )
+    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")
+    results = dvc.experiments.run(stage.addressing, tmp_dir=tmp)
+    exp = first(results)
+    fs = scm.get_fs(exp)
+    for file in ["metrics.json", "params.yaml", "plots.csv"]:
+        assert fs.exists(file)
+
+
+@pytest.mark.parametrize("tmp", [True, False])
+def test_copy_paths(tmp_dir, scm, dvc, tmp):
+    stage = dvc.stage.add(
+        cmd="cat file && ls dir",
+        name="foo",
+    )
+    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")
+
+    (tmp_dir / "dir").mkdir()
+    (tmp_dir / "dir" / "file").write_text("dir/file")
+    scm.ignore(tmp_dir / "dir")
+    (tmp_dir / "file").write_text("file")
+    scm.ignore(tmp_dir / "file")
+
+    results = dvc.experiments.run(
+        stage.addressing, tmp_dir=tmp, copy_paths=["dir", "file"]
+    )
+    exp = first(results)
+    fs = scm.get_fs(exp)
+    assert not fs.exists("dir")
+    assert not fs.exists("file")
+
+
+def test_copy_paths_errors(tmp_dir, scm, dvc, mocker):
+    stage = dvc.stage.add(
+        cmd="echo foo",
+        name="foo",
+    )
+    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")
+
+    with pytest.raises(DvcException, match="Unable to copy"):
+        dvc.experiments.run(stage.addressing, tmp_dir=True, copy_paths=["foo"])
+
+    (tmp_dir / "foo").write_text("foo")
+    mocker.patch("shutil.copy", side_effect=OSError)
+
+    with pytest.raises(DvcException, match="Unable to copy"):
+        dvc.experiments.run(stage.addressing, tmp_dir=True, copy_paths=["foo"])
+
+
+def test_mixed_git_dvc_out(tmp_dir, scm, dvc, exp_stage):
+    (tmp_dir / "dir").mkdir()
+    dir_metrics = os.path.join("dir", "metrics.yaml")
+    dvc.stage.add(
+        cmd=f"python copy.py params.yaml {dir_metrics}",
+        metrics=[dir_metrics],
+        params=["foo"],
+        name="copy-file",
+        deps=["copy.py"],
+        force=True,
+    )
+    dvc.stage.add(
+        cmd=f"python copy.py {dir_metrics} metrics.yaml",
+        metrics_no_cache=["metrics.yaml"],
+        name="copy-dir-file",
+        deps=["dir"],
+    )
+    scm.add(["dvc.yaml", "dvc.lock"])
+    scm.commit("add dir stage")
+
+    exp = first(dvc.experiments.run())
+    assert (tmp_dir / "dir" / "metrics.yaml").exists()
+    git_fs = scm.get_fs(exp)
+    assert not git_fs.exists("dir/metrics.yaml")
+
+
+@pytest.mark.parametrize("tmp", [True, False])
+def test_custom_commit_message(tmp_dir, scm, dvc, tmp):
+    stage = dvc.stage.add(
+        cmd="echo foo",
+        name="foo",
+    )
+    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")
+
+    exp = first(
+        dvc.experiments.run(
+            stage.addressing, tmp_dir=tmp, message="custom commit message"
+        )
+    )
+    assert scm.gitpython.repo.commit(exp).message == "custom commit message"
```

### Comparing `dvc-2.9.5/tests/func/experiments/test_gc.py` & `dvc-3.0.0a0/tests/func/experiments/test_set_params.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,120 +1,144 @@
 import pytest
-from funcy import first
 
-from tests.func.test_repro_multistage import COPY_SCRIPT
+from dvc.exceptions import InvalidArgumentError
+from tests.func.utils.test_hydra import hydra_setup
 
 
-@pytest.mark.parametrize("queued, expected", [(True, 0), (False, 1)])
-def test_workspace(tmp_dir, scm, dvc, queued, expected):
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
-    tmp_dir.gen("params.yaml", "foo: 1")
-
-    stage = dvc.run(
-        cmd="python copy.py params.yaml metrics.yaml",
-        metrics_no_cache=["metrics.yaml"],
-        params=["foo"],
-        name="foo",
-    )
-    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
-    scm.commit("v1")
-
-    dvc.experiments.run(stage.addressing, params=["foo=2"])
-    dvc.experiments.run(stage.addressing, params=["foo=3"], queue=True)
-
-    removed = dvc.experiments.gc(workspace=True, queued=queued)
-    assert removed == expected
-
-
-@pytest.mark.parametrize("queued, expected", [(True, 0), (False, 2)])
-def test_all_commits(tmp_dir, scm, dvc, queued, expected):
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
-    tmp_dir.gen("params.yaml", "foo: 1")
-
-    stage = dvc.run(
-        cmd="python copy.py params.yaml metrics.yaml",
-        metrics_no_cache=["metrics.yaml"],
-        params=["foo"],
-        name="foo",
-    )
-    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
-    scm.commit("v1")
-
-    results = dvc.experiments.run(stage.addressing, params=["foo=2"])
-    exp_rev = first(results)
-    dvc.experiments.run(stage.addressing, params=["foo=3"], queue=True)
-
-    dvc.experiments.apply(exp_rev)
-    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
-    scm.commit("v2")
-
-    dvc.experiments.run(stage.addressing, params=["foo=4"])
-    dvc.experiments.run(stage.addressing, params=["foo=5"], queue=True)
-
-    removed = dvc.experiments.gc(all_commits=True, queued=queued)
-    assert removed == expected
-
-
-@pytest.mark.parametrize("queued, expected", [(True, 2), (False, 3)])
-def test_all_branches(tmp_dir, scm, dvc, queued, expected):
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
-    tmp_dir.gen("params.yaml", "foo: 1")
-
-    stage = dvc.run(
-        cmd="python copy.py params.yaml metrics.yaml",
-        metrics_no_cache=["metrics.yaml"],
-        params=["foo"],
-        name="foo",
-    )
-    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
-    scm.commit("v1")
-
-    results = dvc.experiments.run(stage.addressing, params=["foo=2"])
-    exp_rev = first(results)
-    dvc.experiments.run(stage.addressing, params=["foo=3"], queue=True)
-
-    dvc.experiments.apply(exp_rev)
-    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
-    scm.commit("v2")
-
-    scm.branch("branch")
-
-    results = dvc.experiments.run(stage.addressing, params=["foo=4"])
-    dvc.experiments.run(stage.addressing, params=["foo=5"], queue=True)
-    exp_rev = first(results)
-
-    dvc.experiments.apply(exp_rev)
-    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
-    scm.commit("v3")
-
-    removed = dvc.experiments.gc(all_branches=True, queued=queued)
-    assert removed == expected
-
-
-@pytest.mark.parametrize("queued, expected", [(True, 2), (False, 3)])
-def test_all_tags(tmp_dir, scm, dvc, queued, expected):
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
-    tmp_dir.gen("params.yaml", "foo: 1")
-
-    stage = dvc.run(
-        cmd="python copy.py params.yaml metrics.yaml",
-        metrics_no_cache=["metrics.yaml"],
-        params=["foo"],
-        name="foo",
+@pytest.mark.parametrize(
+    "changes, expected",
+    [
+        [["foo=baz"], "foo: baz\ngoo:\n  bag: 3.0\nlorem: false"],
+        [["params.yaml:foo=baz"], "foo: baz\ngoo:\n  bag: 3.0\nlorem: false"],
+    ],
+)
+def test_modify_params(params_repo, dvc, changes, expected):
+    dvc.experiments.run(params=changes)
+    with open("params.yaml") as fobj:
+        assert fobj.read().strip() == expected
+
+
+@pytest.mark.parametrize("hydra_enabled", [True, False])
+@pytest.mark.parametrize(
+    "config_dir,config_name",
+    [
+        (None, None),
+        (None, "bar"),
+        ("conf", "bar"),
+    ],
+)
+def test_hydra_compose_and_dump(
+    tmp_dir, params_repo, dvc, hydra_enabled, config_dir, config_name
+):
+    hydra_setup(
+        tmp_dir,
+        config_dir=config_dir or "conf",
+        config_name=config_name or "config",
     )
-    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
-    scm.commit("v1")
-
-    results = dvc.experiments.run(stage.addressing, params=["foo=2"])
-    exp_rev = first(results)
-    dvc.experiments.run(stage.addressing, params=["foo=3"], queue=True)
-
-    dvc.experiments.apply(exp_rev)
-    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
-    scm.commit("v2")
-    scm.tag("tag-v2")
-
-    dvc.experiments.run(stage.addressing, params=["foo=4"])
-    dvc.experiments.run(stage.addressing, params=["foo=5"], queue=True)
 
-    removed = dvc.experiments.gc(all_tags=True, queued=queued)
-    assert removed == expected
+    dvc.experiments.run()
+    assert (tmp_dir / "params.yaml").parse() == {
+        "foo": [{"bar": 1}, {"baz": 2}],
+        "goo": {"bag": 3.0},
+        "lorem": False,
+    }
+
+    with dvc.config.edit() as conf:
+        if hydra_enabled:
+            conf["hydra"]["enabled"] = True
+        if config_dir is not None:
+            conf["hydra"]["config_dir"] = config_dir
+        if config_name is not None:
+            conf["hydra"]["config_name"] = config_name
+
+    dvc.experiments.run()
+
+    if hydra_enabled:
+        assert (tmp_dir / "params.yaml").parse() == {
+            "db": {"driver": "mysql", "user": "omry", "pass": "secret"},
+        }
+
+        dvc.experiments.run(params=["db=postgresql"])
+        assert (tmp_dir / "params.yaml").parse() == {
+            "db": {
+                "driver": "postgresql",
+                "user": "foo",
+                "pass": "bar",
+                "timeout": 10,
+            }
+        }
+    else:
+        assert (tmp_dir / "params.yaml").parse() == {
+            "foo": [{"bar": 1}, {"baz": 2}],
+            "goo": {"bag": 3.0},
+            "lorem": False,
+        }
+
+
+@pytest.mark.parametrize(
+    "hydra_enabled,overrides,expected",
+    [
+        (
+            True,
+            ["db=mysql,postgresql"],
+            [
+                {"params.yaml": ["db=mysql"]},
+                {"params.yaml": ["db=postgresql"]},
+            ],
+        ),
+        (
+            False,
+            ["foo=bar,baz"],
+            [{"params.yaml": ["foo=bar"]}, {"params.yaml": ["foo=baz"]}],
+        ),
+        (
+            False,
+            [],
+            [{}],
+        ),
+    ],
+)
+def test_hydra_sweep(
+    tmp_dir, params_repo, dvc, mocker, hydra_enabled, overrides, expected
+):
+    patched = mocker.patch.object(dvc.experiments, "queue_one")
+
+    if hydra_enabled:
+        hydra_setup(
+            tmp_dir,
+            config_dir="conf",
+            config_name="config",
+        )
+        with dvc.config.edit() as conf:
+            conf["hydra"]["enabled"] = True
+
+    dvc.experiments.run(params=overrides, queue=True)
+
+    assert patched.call_count == len(expected)
+    for e in expected:
+        patched.assert_any_call(
+            mocker.ANY,
+            params=e,
+            reset=True,
+            targets=None,
+            copy_paths=None,
+            message=None,
+        )
+
+
+def test_hydra_sweep_requires_queue(params_repo, dvc):
+    with pytest.raises(
+        InvalidArgumentError,
+        match="Sweep overrides can't be used without `--queue`",
+    ):
+        dvc.experiments.run(params=["db=mysql,postgresql"])
+
+
+def test_hydra_sweep_prefix_name(tmp_dir, params_repo, dvc):
+    prefix = "foo"
+    db_values = ["mysql", "postgresql"]
+    param = "+db=" + ",".join(db_values)
+    dvc.experiments.run(params=[param], queue=True, name=prefix)
+    expected_names = [f"{prefix}-{i+1}" for i, _ in enumerate(db_values)]
+    exp_names = [entry.name for entry in dvc.experiments.celery_queue.iter_queued()]
+    for name, expected in zip(exp_names, expected_names):
+        assert name == expected
```

### Comparing `dvc-2.9.5/tests/func/experiments/test_init.py` & `dvc-3.0.0a0/tests/func/test_repro_multistage.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,458 +1,459 @@
-import io
 import os
+from copy import deepcopy
+from textwrap import dedent
 
 import pytest
+from funcy import lsplit
 
 from dvc.cli import main
-from dvc.commands.experiments.init import CmdExperimentsInit
-from dvc.exceptions import DvcException
-from dvc.repo.experiments.init import init
-from dvc.stage.exceptions import DuplicateStageName
+from dvc.dvcfile import LOCK_FILE, PROJECT_FILE
+from dvc.exceptions import CyclicGraphError, ReproductionError
+from dvc.stage import PipelineStage
+from dvc.stage.exceptions import StageNotFound
+from dvc.utils.fs import remove
+
+
+def test_non_existing_stage_name(tmp_dir, dvc, run_copy):
+    tmp_dir.gen("file1", "file1")
+    run_copy("file1", "file2", name="copy-file1-file2")
+
+    with pytest.raises(StageNotFound):
+        dvc.freeze(":copy-file1-file3")
+
+    assert main(["freeze", ":copy-file1-file3"]) != 0
+
+
+def test_repro_frozen(tmp_dir, dvc, run_copy):
+    (data_stage,) = tmp_dir.dvc_gen("data", "foo")
+    stage0 = run_copy("data", "stage0", name="copy-data-stage0")
+    run_copy("stage0", "stage1", name="copy-data-stage1")
+    run_copy("stage1", "stage2", name="copy-data-stage2")
+
+    dvc.freeze("copy-data-stage1")
+
+    tmp_dir.gen("data", "bar")
+    stages = dvc.reproduce()
+    assert stages == [data_stage, stage0]
+
+
+def test_downstream(tmp_dir, dvc):
+    # The dependency graph should look like this:
+    #
+    #       E
+    #      / \
+    #     D   F
+    #    / \   \
+    #   B   C   G
+    #    \ /
+    #     A
+    #
+    assert main(["run", "-n", "A-gen", "-o", "A", "echo A>A"]) == 0
+    assert main(["run", "-n", "B-gen", "-d", "A", "-o", "B", "echo B>B"]) == 0
+    assert main(["run", "--single-stage", "-d", "A", "-o", "C", "echo C>C"]) == 0
+    assert (
+        main(["run", "-n", "D-gen", "-d", "B", "-d", "C", "-o", "D", "echo D>D"]) == 0
+    )
+    assert main(["run", "--single-stage", "-o", "G", "echo G>G"]) == 0
+    assert main(["run", "-n", "F-gen", "-d", "G", "-o", "F", "echo F>F"]) == 0
+    assert (
+        main(
+            [
+                "run",
+                "--single-stage",
+                "-d",
+                "D",
+                "-d",
+                "F",
+                "-o",
+                "E",
+                "echo E>E",
+            ]
+        )
+        == 0
+    )
 
-# the tests may hang on prompts on failure
-pytestmark = pytest.mark.timeout(3, func_only=True)
+    # We want the evaluation to move from B to E
+    #
+    #       E
+    #      /
+    #     D
+    #    /
+    #   B
+    #
+    evaluation = dvc.reproduce(PROJECT_FILE + ":B-gen", downstream=True, force=True)
 
+    assert len(evaluation) == 3
+    assert isinstance(evaluation[0], PipelineStage)
+    assert evaluation[0].relpath == PROJECT_FILE
+    assert evaluation[0].name == "B-gen"
 
-@pytest.mark.timeout(5, func_only=True)
-def test_init_simple(tmp_dir, scm, dvc, capsys):
-    tmp_dir.gen(
-        {
-            CmdExperimentsInit.CODE: {"copy.py": ""},
-            "data": "data",
-            "params.yaml": '{"foo": 1}',
-            "dvclive": {},
-            "plots": {},
-        }
-    )
-    code_path = os.path.join(CmdExperimentsInit.CODE, "copy.py")
-    script = f"python {code_path}"
-
-    capsys.readouterr()
-    assert main(["exp", "init", script]) == 0
-    out, err = capsys.readouterr()
-    assert not err
-    assert "Created train stage in dvc.yaml" in out
-    assert (tmp_dir / "dvc.yaml").parse() == {
-        "stages": {
-            "train": {
-                "cmd": script,
-                "deps": ["data", "src"],
-                "metrics": [{"metrics.json": {"cache": False}}],
-                "outs": ["models"],
-                "params": ["foo"],
-                "plots": [{"plots": {"cache": False}}],
-            }
-        }
-    }
-    assert (tmp_dir / "data").read_text() == "data"
-    assert (tmp_dir / "src").is_dir()
+    assert isinstance(evaluation[1], PipelineStage)
+    assert evaluation[1].relpath == PROJECT_FILE
+    assert evaluation[1].name == "D-gen"
 
+    assert not isinstance(evaluation[2], PipelineStage)
+    assert evaluation[2].relpath == "E.dvc"
 
-@pytest.mark.parametrize("interactive", [True, False])
-def test_when_stage_already_exists_with_same_name(tmp_dir, dvc, interactive):
-    (tmp_dir / "dvc.yaml").dump({"stages": {"train": {"cmd": "test"}}})
-    with pytest.raises(DuplicateStageName) as exc:
-        init(
-            dvc,
-            interactive=interactive,
-            overrides={"cmd": "true"},
-            defaults=CmdExperimentsInit.DEFAULTS,
-        )
-    assert (
-        str(exc.value) == "Stage 'train' already exists in 'dvc.yaml'. "
-        "Use '--force' to overwrite."
+    # B, C should be run (in any order) before D
+    # See https://github.com/iterative/dvc/issues/3602
+    evaluation = dvc.reproduce(PROJECT_FILE + ":A-gen", downstream=True, force=True)
+
+    assert len(evaluation) == 5
+    assert isinstance(evaluation[0], PipelineStage)
+    assert evaluation[0].relpath == PROJECT_FILE
+    assert evaluation[0].name == "A-gen"
+
+    names = set()
+    for stage in evaluation[1:3]:
+        if isinstance(stage, PipelineStage):
+            assert stage.relpath == PROJECT_FILE
+            names.add(stage.name)
+        else:
+            names.add(stage.relpath)
+    assert names == {"B-gen", "C.dvc"}
+
+    assert isinstance(evaluation[3], PipelineStage)
+    assert evaluation[3].relpath == PROJECT_FILE
+    assert evaluation[3].name == "D-gen"
+
+    assert not isinstance(evaluation[4], PipelineStage)
+    assert evaluation[4].relpath == "E.dvc"
+
+
+def test_repro_when_cmd_changes(tmp_dir, dvc, run_copy, mocker):
+    from dvc.dvcfile import ProjectFile
+
+    tmp_dir.gen("foo", "foo")
+    stage = run_copy("foo", "bar", name="copy-file")
+    target = "copy-file"
+    assert not dvc.reproduce(target)
+
+    from dvc.stage.run import cmd_run
+
+    m = mocker.patch("dvc.stage.run.cmd_run", wraps=cmd_run)
+    stage.cmd = "  ".join(stage.cmd.split())  # change cmd spacing by two
+    ProjectFile(dvc, PROJECT_FILE)._dump_pipeline_file(stage)
+
+    assert dvc.status([target]) == {target: ["changed command"]}
+    assert dvc.reproduce(target)[0] == stage
+    m.assert_called_once_with(stage, checkpoint_func=None, dry=False, run_env=None)
+
+
+def test_repro_when_new_deps_is_added_in_dvcfile(tmp_dir, dvc, run_copy, copy_script):
+    from dvc.dvcfile import load_file
+
+    tmp_dir.gen({"foo": "foo", "bar": "bar"})
+    stage = dvc.run(
+        cmd="python copy.py {} {}".format("foo", "foobar"),
+        outs=["foobar"],
+        deps=["foo"],
+        name="copy-file",
     )
+    target = PROJECT_FILE + ":copy-file"
+    assert not dvc.reproduce(target)
+
+    dvcfile = load_file(dvc, stage.path)
+    data, _ = dvcfile._load()
+    data["stages"]["copy-file"]["deps"] += ["copy.py"]
+    (tmp_dir / stage.path).dump(data)
 
+    assert dvc.reproduce(target)[0] == stage
 
-def test_when_stage_force_if_already_exists(tmp_dir, dvc):
-    (tmp_dir / "params.yaml").dump({"foo": 1})
-    (tmp_dir / "dvc.yaml").dump({"stages": {"train": {"cmd": "test"}}})
-    init(
-        dvc,
-        force=True,
-        overrides={"cmd": "true"},
-        defaults=CmdExperimentsInit.DEFAULTS,
+
+def test_repro_when_new_outs_is_added_in_dvcfile(tmp_dir, dvc, copy_script):
+    from dvc.dvcfile import load_file
+
+    tmp_dir.gen({"foo": "foo", "bar": "bar"})
+    stage = dvc.run(
+        cmd="python copy.py {} {}".format("foo", "foobar"),
+        outs=[],  # scenario where user forgot to add
+        deps=["foo"],
+        name="copy-file",
     )
-    d = (tmp_dir / "dvc.yaml").parse()
-    assert d["stages"]["train"]["cmd"] == "true"
+    target = ":copy-file"
+    assert not dvc.reproduce(target)
 
+    dvcfile = load_file(dvc, stage.path)
+    data, _ = dvcfile._load()
+    data["stages"]["copy-file"]["outs"] = ["foobar"]
+    (tmp_dir / stage.path).dump(data)
 
-def test_with_a_custom_name(tmp_dir, dvc):
-    init(dvc, name="custom", overrides={"cmd": "cmd"})
-    assert (tmp_dir / "dvc.yaml").parse() == {
-        "stages": {"custom": {"cmd": "cmd"}}
-    }
+    assert dvc.reproduce(target)[0] == stage
 
 
-def test_init_with_no_defaults_non_interactive(tmp_dir, scm, dvc):
-    init(dvc, defaults={}, overrides={"cmd": "python script.py"})
+def test_repro_when_new_deps_is_moved(tmp_dir, dvc, copy_script):
+    from dvc.dvcfile import load_file
 
-    assert (tmp_dir / "dvc.yaml").parse() == {
-        "stages": {"train": {"cmd": "python script.py"}}
-    }
-    scm._reset()
-    assert not (tmp_dir / "dvc.lock").exists()
-    assert scm.is_tracked("dvc.yaml")
+    tmp_dir.gen({"foo": "foo", "bar": "foo"})
+    stage = dvc.run(
+        cmd="python copy.py {} {}".format("foo", "foobar"),
+        outs=["foobar"],
+        deps=["foo"],
+        name="copy-file",
+    )
+    target = ":copy-file"
+    assert not dvc.reproduce(target)
 
+    # hardcode values in source code, ignore sys.argv
+    tmp_dir.gen(
+        "copy.py",
+        """
+import shutil
 
-def test_abort_confirmation(tmp_dir, dvc):
-    (tmp_dir / "param").dump({"foo": 1})
-    inp = io.StringIO("./script\nscript\ndata\nmodel\nparam\nmetric\nplt\nn")
-    with pytest.raises(DvcException) as exc:
-        init(
-            dvc,
-            interactive=True,
-            defaults=CmdExperimentsInit.DEFAULTS,
-            stream=inp,
-        )
-    assert str(exc.value) == "Aborting ..."
-    assert not (tmp_dir / "dvc.yaml").exists()
-    assert not (tmp_dir / "dvc.lock").exists()
-
-
-@pytest.mark.parametrize(
-    "extra_overrides, inp",
-    [
-        ({"cmd": "cmd"}, io.StringIO()),
-        ({}, io.StringIO("cmd")),
-    ],
-)
-def test_init_interactive_when_no_path_prompts_need_to_be_asked(
-    tmp_dir, dvc, extra_overrides, inp
-):
-    """When we pass everything that's required of, it should not prompt us."""
-    (tmp_dir / "params.yaml").dump({"foo": 1})
-    init(
-        dvc,
-        interactive=True,
-        defaults=CmdExperimentsInit.DEFAULTS,
-        overrides={**CmdExperimentsInit.DEFAULTS, **extra_overrides},
-        stream=inp,  # we still need to confirm
-    )
-    assert (tmp_dir / "dvc.yaml").parse() == {
-        "stages": {
-            "train": {
-                "cmd": "cmd",
-                "deps": ["data", "src"],
-                "live": {"dvclive": {"html": True, "summary": True}},
-                "metrics": [{"metrics.json": {"cache": False}}],
-                # we specify `live` through `overrides`,
-                # so it creates checkpoint-based output.
-                "outs": [{"models": {"checkpoint": True}}],
-                "params": ["foo"],
-                "plots": [{"plots": {"cache": False}}],
-            }
-        }
-    }
-    assert (tmp_dir / "src").is_dir()
-    assert (tmp_dir / "data").is_dir()
+shutil.copyfile('bar', 'foobar')
+""",
+    )
+    from shutil import move
 
+    move("foo", "bar")
 
-def test_when_params_is_omitted_in_interactive_mode(tmp_dir, scm, dvc):
-    (tmp_dir / "params.yaml").dump({"foo": 1})
-    inp = io.StringIO("python script.py\nscript.py\ndata\nmodels\nn")
-
-    init(
-        dvc, interactive=True, stream=inp, defaults=CmdExperimentsInit.DEFAULTS
-    )
-
-    assert (tmp_dir / "dvc.yaml").parse() == {
-        "stages": {
-            "train": {
-                "cmd": "python script.py",
-                "deps": ["data", "script.py"],
-                "metrics": [{"metrics.json": {"cache": False}}],
-                "outs": ["models"],
-                "plots": [{"plots": {"cache": False}}],
-            }
-        }
-    }
-    assert not (tmp_dir / "dvc.lock").exists()
-    assert (tmp_dir / "script.py").read_text() == ""
-    assert (tmp_dir / "data").is_dir()
-    scm._reset()
-    assert scm.is_tracked("dvc.yaml")
-    assert not scm.is_tracked("params.yaml")
-    assert scm.is_tracked(".gitignore")
-    assert scm.is_ignored("models")
-
-
-def test_init_interactive_params_validation(tmp_dir, dvc, capsys):
-    tmp_dir.gen({"data": {"foo": "foo"}})
-    (tmp_dir / "params.yaml").dump({"foo": 1})
-    inp = io.StringIO(
-        "python script.py\nscript.py\ndata\nmodels\nparams.json\ndata\n"
-    )
-
-    init(
-        dvc, stream=inp, interactive=True, defaults=CmdExperimentsInit.DEFAULTS
-    )
-
-    assert (tmp_dir / "dvc.yaml").parse() == {
-        "stages": {
-            "train": {
-                "cmd": "python script.py",
-                "deps": ["data", "script.py"],
-                "metrics": [{"metrics.json": {"cache": False}}],
-                "outs": ["models"],
-                "params": ["foo"],
-                "plots": [{"plots": {"cache": False}}],
-            }
-        }
-    }
-    assert (tmp_dir / "script.py").read_text() == ""
-    assert (tmp_dir / "data").is_dir()
+    dvcfile = load_file(dvc, stage.path)
+    data, _ = dvcfile._load()
+    data["stages"]["copy-file"]["deps"] = ["bar"]
+    (tmp_dir / stage.path).dump(data)
+
+    assert dvc.reproduce(target)[0] == stage
 
-    out, err = capsys.readouterr()
-    assert (
-        "Path to a parameters file [params.yaml, n to omit]: "
-        "'params.json' does not exist. "
-        "Please retry with an existing parameters file.\n"
-        "Path to a parameters file [params.yaml, n to omit]: "
-        "'data' is a directory. "
-        "Please retry with an existing parameters file.\n"
-        "Path to a parameters file [params.yaml, n to omit]:"
-    ) in err
-    assert out == "Created script.py.\n"
-
-
-def test_init_with_no_defaults_interactive(tmp_dir, dvc):
-    inp = io.StringIO("script.py\n" "data\n" "model\n" "n\n" "metric\n" "n\n")
-    init(
-        dvc,
-        defaults={},
-        overrides={"cmd": "python script.py"},
-        interactive=True,
-        stream=inp,
-    )
-    assert (tmp_dir / "dvc.yaml").parse() == {
-        "stages": {
-            "train": {
-                "cmd": "python script.py",
-                "deps": ["data", "script.py"],
-                "metrics": [{"metric": {"cache": False}}],
-                "outs": ["model"],
-            }
-        }
-    }
-    assert (tmp_dir / "script.py").read_text() == ""
-    assert (tmp_dir / "data").is_dir()
 
+def test_repro_when_new_out_overlaps_others_stage_outs(tmp_dir, dvc):
+    from dvc.exceptions import OverlappingOutputPathsError
 
-@pytest.mark.parametrize(
-    "interactive, overrides, inp",
-    [
-        (False, {"cmd": "python script.py", "code": "script.py"}, None),
-        (
-            True,
-            {},
-            io.StringIO(
-                "python script.py\n"
-                "script.py\n"
-                "data\n"
-                "models\n"
-                "params.yaml\n"
-                "metrics.json\n"
-                "plots\n"
-                "y"
-            ),
-        ),
-    ],
-    ids=["non-interactive", "interactive"],
-)
-def test_init_default(tmp_dir, scm, dvc, interactive, overrides, inp, capsys):
-    (tmp_dir / "params.yaml").dump({"foo": {"bar": 1}})
-
-    init(
-        dvc,
-        interactive=interactive,
-        defaults=CmdExperimentsInit.DEFAULTS,
-        overrides=overrides,
-        stream=inp,
-    )
-
-    assert (tmp_dir / "dvc.yaml").parse() == {
-        "stages": {
-            "train": {
-                "cmd": "python script.py",
-                "deps": ["data", "script.py"],
-                "metrics": [{"metrics.json": {"cache": False}}],
-                "outs": ["models"],
-                "params": ["foo"],
-                "plots": [{"plots": {"cache": False}}],
+    tmp_dir.gen({"dir": {"file1": "file1"}, "foo": "foo"})
+    dvc.add("dir")
+    (tmp_dir / PROJECT_FILE).dump(
+        {
+            "stages": {
+                "run-copy": {
+                    "cmd": "python copy {} {}".format("foo", "dir/foo"),
+                    "deps": ["foo"],
+                    "outs": ["dir/foo"],
+                }
             }
-        }
-    }
-    assert not (tmp_dir / "dvc.lock").exists()
-    assert (tmp_dir / "script.py").read_text() == ""
-    assert (tmp_dir / "data").is_dir()
-    scm._reset()
-    assert scm.is_tracked("dvc.yaml")
-    assert scm.is_tracked("params.yaml")
-    assert scm.is_tracked(".gitignore")
-    assert scm.is_ignored("models")
-    out, err = capsys.readouterr()
-
-    if interactive:
-        assert "'script.py' does not exist, the file will be created." in err
-        assert "'data' does not exist, the directory will be created." in err
-    else:
-        assert "Using experiment project structure: " in out
-    assert "Created script.py and data" in out
-
-
-@pytest.mark.timeout(5, func_only=True)
-@pytest.mark.parametrize(
-    "interactive, overrides, inp",
-    [
-        (False, {"cmd": "python script.py", "code": "script.py"}, None),
-        (
-            True,
-            {},
-            io.StringIO(
-                "python script.py\n"
-                "script.py\n"
-                "data\n"
-                "models\n"
-                "params.yaml\n"
-                "dvclive\n"
-                "y"
-            ),
-        ),
-        (
-            True,
-            {"cmd": "python script.py"},
-            io.StringIO(
-                "script.py\n"
-                "data\n"
-                "models\n"
-                "params.yaml\n"
-                "dvclive\n"
-                "y"
-            ),
-        ),
-        (
-            True,
-            {"cmd": "python script.py", "models": "models"},
-            io.StringIO("script.py\ndata\nparams.yaml\ndvclive\ny"),
-        ),
-    ],
-    ids=[
-        "non-interactive",
-        "interactive",
-        "interactive-cmd-provided",
-        "interactive-cmd-models-provided",
-    ],
-)
-def test_init_interactive_live(
-    tmp_dir, scm, dvc, interactive, overrides, inp, capsys
-):
-    (tmp_dir / "params.yaml").dump({"foo": {"bar": 1}})
-
-    init(
-        dvc,
-        type="dl",
-        interactive=interactive,
-        defaults=CmdExperimentsInit.DEFAULTS,
-        overrides=overrides,
-        stream=inp,
-    )
-    assert (tmp_dir / "dvc.yaml").parse() == {
-        "stages": {
-            "train": {
-                "cmd": "python script.py",
-                "deps": ["data", "script.py"],
-                "live": {"dvclive": {"html": True, "summary": True}},
-                "outs": [{"models": {"checkpoint": True}}],
-                "params": ["foo"],
+        },
+    )
+    with pytest.raises(OverlappingOutputPathsError):
+        dvc.reproduce(":run-copy")
+
+
+def test_repro_when_new_deps_added_does_not_exist(tmp_dir, dvc, copy_script):
+    tmp_dir.gen("foo", "foo")
+    (tmp_dir / PROJECT_FILE).dump(
+        {
+            "stages": {
+                "run-copy": {
+                    "cmd": "python copy.py {} {}".format("foo", "foobar"),
+                    "deps": ["foo", "bar"],
+                    "outs": ["foobar"],
+                }
             }
-        }
-    }
-    assert not (tmp_dir / "dvc.lock").exists()
-    assert (tmp_dir / "script.py").read_text() == ""
-    assert (tmp_dir / "data").is_dir()
-    scm._reset()
-    assert scm.is_tracked("dvc.yaml")
-    assert scm.is_tracked("params.yaml")
-    assert scm.is_tracked(".gitignore")
-    assert scm.is_ignored("models")
-
-    out, err = capsys.readouterr()
-
-    if interactive:
-        assert "'script.py' does not exist, the file will be created." in err
-        assert "'data' does not exist, the directory will be created." in err
-    else:
-        assert "Using experiment project structure: " in out
-    assert "Created script.py and data" in out
-
-
-@pytest.mark.parametrize(
-    "interactive, inp",
-    [
-        (False, None),
-        (True, io.StringIO()),
-    ],
-)
-def test_init_with_type_live_and_models_plots_provided(
-    tmp_dir, dvc, interactive, inp
-):
-    (tmp_dir / "params.yaml").dump({"foo": 1})
-    init(
-        dvc,
-        type="dl",
-        interactive=interactive,
-        stream=inp,
-        defaults=CmdExperimentsInit.DEFAULTS,
-        overrides={"cmd": "cmd", "metrics": "m", "plots": "p"},
-    )
-    assert (tmp_dir / "dvc.yaml").parse() == {
-        "stages": {
-            "train": {
-                "cmd": "cmd",
-                "deps": ["data", "src"],
-                "live": {"dvclive": {"html": True, "summary": True}},
-                "metrics": [{"m": {"cache": False}}],
-                "outs": [{"models": {"checkpoint": True}}],
-                "params": ["foo"],
-                "plots": [{"p": {"cache": False}}],
+        },
+    )
+    with pytest.raises(ReproductionError):
+        dvc.reproduce(":run-copy")
+
+
+def test_repro_when_new_outs_added_does_not_exist(tmp_dir, dvc, copy_script):
+    tmp_dir.gen("foo", "foo")
+    (tmp_dir / PROJECT_FILE).dump(
+        {
+            "stages": {
+                "run-copy": {
+                    "cmd": "python copy.py {} {}".format("foo", "foobar"),
+                    "deps": ["foo"],
+                    "outs": ["foobar", "bar"],
+                }
             }
-        }
-    }
-    assert (tmp_dir / "src").is_dir()
-    assert (tmp_dir / "data").is_dir()
+        },
+    )
+    with pytest.raises(ReproductionError):
+        dvc.reproduce(":run-copy")
 
 
-@pytest.mark.parametrize(
-    "interactive, inp",
-    [
-        (False, None),
-        (True, io.StringIO()),
-    ],
-)
-def test_init_with_type_default_and_live_provided(
-    tmp_dir, dvc, interactive, inp
-):
-    (tmp_dir / "params.yaml").dump({"foo": 1})
-    init(
-        dvc,
-        interactive=interactive,
-        stream=inp,
-        defaults=CmdExperimentsInit.DEFAULTS,
-        overrides={"cmd": "cmd", "live": "live"},
-    )
-    assert (tmp_dir / "dvc.yaml").parse() == {
-        "stages": {
-            "train": {
-                "cmd": "cmd",
-                "deps": ["data", "src"],
-                "live": {"live": {"html": True, "summary": True}},
-                "metrics": [{"metrics.json": {"cache": False}}],
-                "outs": [{"models": {"checkpoint": True}}],
-                "params": ["foo"],
-                "plots": [{"plots": {"cache": False}}],
+def test_repro_when_lockfile_gets_deleted(tmp_dir, dvc, copy_script):
+    tmp_dir.gen("foo", "foo")
+    (tmp_dir / PROJECT_FILE).dump(
+        {
+            "stages": {
+                "run-copy": {
+                    "cmd": "python copy.py {} {}".format("foo", "foobar"),
+                    "deps": ["foo"],
+                    "outs": ["foobar"],
+                }
             }
-        }
-    }
-    assert (tmp_dir / "src").is_dir()
-    assert (tmp_dir / "data").is_dir()
+        },
+    )
+    assert dvc.reproduce(":run-copy")
+    assert os.path.exists(LOCK_FILE)
+
+    assert not dvc.reproduce(":run-copy")
+    os.unlink(LOCK_FILE)
+    stages = dvc.reproduce(":run-copy")
+    assert stages
+    assert stages[0].relpath == PROJECT_FILE
+    assert stages[0].name == "run-copy"
+
+
+def test_cyclic_graph_error(tmp_dir, dvc, run_copy):
+    tmp_dir.gen("foo", "foo")
+    run_copy("foo", "bar", name="copy-foo-bar")
+    run_copy("bar", "baz", name="copy-bar-baz")
+    run_copy("baz", "foobar", name="copy-baz-foobar")
+
+    data = (tmp_dir / PROJECT_FILE).parse()
+    data["stages"]["copy-baz-foo"] = {
+        "cmd": "echo baz > foo",
+        "deps": ["baz"],
+        "outs": ["foo"],
+    }
+    (tmp_dir / PROJECT_FILE).dump(data)
+    with pytest.raises(CyclicGraphError):
+        dvc.reproduce(":copy-baz-foo")
+
+
+def test_repro_multiple_params(tmp_dir, dvc):
+    from dvc.stage.utils import split_params_deps
+    from tests.func.test_run_multistage import supported_params
+
+    (tmp_dir / "params2.yaml").dump(supported_params)
+    (tmp_dir / "params.yaml").dump(supported_params)
+
+    (tmp_dir / "foo").write_text("foo")
+    stage = dvc.run(
+        name="read_params",
+        deps=["foo"],
+        outs=["bar"],
+        params=[
+            "params2.yaml:lists,floats,name",
+            "answer,floats,nested.nested1",
+        ],
+        cmd="cat params2.yaml params.yaml > bar",
+    )
+
+    params, deps = split_params_deps(stage)
+    assert len(params) == 2
+    assert len(deps) == 1
+    assert len(stage.outs) == 1
+
+    lockfile = stage.dvcfile._lockfile
+    assert lockfile.load()["stages"]["read_params"]["params"] == {
+        "params2.yaml": {
+            "lists": [42, 42.0, "42"],
+            "floats": 42.0,
+            "name": "Answer",
+        },
+        "params.yaml": {
+            "answer": 42,
+            "floats": 42.0,
+            "nested.nested1": {"nested2": "42", "nested2-2": 41.99999},
+        },
+    }
+    data, _ = stage.dvcfile._load()
+    params = data["stages"]["read_params"]["params"]
+
+    custom, defaults = lsplit(lambda v: isinstance(v, dict), params)
+    assert set(custom[0]["params2.yaml"]) == {"name", "lists", "floats"}
+    assert set(defaults) == {"answer", "floats", "nested.nested1"}
+
+    assert not dvc.reproduce(stage.addressing)
+    params = deepcopy(supported_params)
+    params["answer"] = 43
+    (tmp_dir / "params.yaml").dump(params)
+
+    assert dvc.reproduce(stage.addressing) == [stage]
+
+
+@pytest.mark.parametrize("multiline", [True, False])
+def test_repro_list_of_commands_in_order(tmp_dir, dvc, multiline):
+    cmd = ["echo foo>foo", "echo bar>bar"]
+    if multiline:
+        cmd = "\n".join(cmd)
+
+    (tmp_dir / "dvc.yaml").dump({"stages": {"multi": {"cmd": cmd}}})
+
+    (tmp_dir / "dvc.yaml").write_text(
+        dedent(
+            """\
+            stages:
+              multi:
+                cmd:
+                - echo foo>foo
+                - echo bar>bar
+        """
+        )
+    )
+    dvc.reproduce(targets=["multi"])
+    assert (tmp_dir / "foo").read_text() == "foo\n"
+    assert (tmp_dir / "bar").read_text() == "bar\n"
+
+
+@pytest.mark.parametrize("multiline", [True, False])
+def test_repro_list_of_commands_raise_and_stops_after_failure(tmp_dir, dvc, multiline):
+    cmd = ["echo foo>foo", "failed_command", "echo baz>bar"]
+    if multiline:
+        cmd = "\n".join(cmd)
+
+    (tmp_dir / "dvc.yaml").dump({"stages": {"multi": {"cmd": cmd}}})
+
+    with pytest.raises(ReproductionError):
+        dvc.reproduce(targets=["multi"])
+    assert (tmp_dir / "foo").read_text() == "foo\n"
+    assert not (tmp_dir / "bar").exists()
+
+
+def test_repro_pulls_mising_data_source(tmp_dir, dvc, mocker, local_remote):
+    (foo,) = tmp_dir.dvc_gen("foo", "foo")
+
+    dvc.push()
+
+    dvc.stage.add(name="copy-foo", cmd="cp foo bar", deps=["foo"], outs=["bar"])
+    remove("foo")
+    remove(foo.outs[0].cache_path)
+
+    assert dvc.reproduce(pull=True)
+
+
+def test_repro_pulls_mising_import(tmp_dir, dvc, mocker, erepo_dir, local_remote):
+    with erepo_dir.chdir():
+        erepo_dir.dvc_gen("foo", "foo", commit="first")
+
+    foo_import = dvc.imp(os.fspath(erepo_dir), "foo")
+
+    dvc.push()
+
+    dvc.stage.add(name="copy-foo", cmd="cp foo bar", deps=["foo"], outs=["bar"])
+    remove("foo")
+    remove(foo_import.outs[0].cache_path)
+
+    assert dvc.reproduce(pull=True)
+
+
+def test_repro_allow_missing(tmp_dir, dvc):
+    tmp_dir.gen("fixed", "fixed")
+    dvc.stage.add(name="create-foo", cmd="echo foo > foo", deps=["fixed"], outs=["foo"])
+    dvc.stage.add(name="copy-foo", cmd="cp foo bar", deps=["foo"], outs=["bar"])
+    (create_foo, copy_foo) = dvc.reproduce()
+
+    remove("foo")
+    remove(create_foo.outs[0].cache_path)
+    remove(dvc.stage_cache.cache_dir)
+
+    ret = dvc.reproduce(allow_missing=True)
+    # both stages are skipped
+    assert not ret
+
+
+def test_repro_allow_missing_and_pull(tmp_dir, dvc, mocker, local_remote):
+    tmp_dir.gen("fixed", "fixed")
+    dvc.stage.add(name="create-foo", cmd="echo foo > foo", deps=["fixed"], outs=["foo"])
+    dvc.stage.add(name="copy-foo", cmd="cp foo bar", deps=["foo"], outs=["bar"])
+    (create_foo,) = dvc.reproduce("create-foo")
+
+    dvc.push()
+
+    remove("foo")
+    remove(create_foo.outs[0].cache_path)
+    remove(dvc.stage_cache.cache_dir)
+
+    ret = dvc.reproduce(pull=True, allow_missing=True)
+    # create-foo is skipped ; copy-foo pulls missing dep
+    assert len(ret) == 1
```

### Comparing `dvc-2.9.5/tests/func/experiments/test_remote.py` & `dvc-3.0.0a0/tests/func/experiments/test_remote.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 import os
 
 import pytest
 from funcy import first
 
-from dvc.exceptions import DvcException
 from dvc.repo.experiments.utils import exp_refs_by_rev
 
 
 @pytest.mark.parametrize("use_url", [True, False])
 def test_push(tmp_dir, scm, dvc, git_upstream, exp_stage, use_url):
     from dvc.exceptions import InvalidArgumentError
 
@@ -35,69 +34,115 @@
     git_upstream.tmp_dir.scm.remove_ref(str(ref_info1))
     assert git_upstream.tmp_dir.scm.get_ref(str(ref_info1)) is None
 
     dvc.experiments.push(remote, [ref_info1.name])
     assert git_upstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1
 
 
+@pytest.mark.parametrize("all_,rev,result3", [(True, False, True), (False, True, None)])
+def test_push_args(tmp_dir, scm, dvc, git_upstream, exp_stage, all_, rev, result3):
+    remote = git_upstream.url
+    baseline = scm.get_rev()
+
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
+    exp1 = first(results)
+    ref_info1 = first(exp_refs_by_rev(scm, exp1))
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
+    exp2 = first(results)
+    ref_info2 = first(exp_refs_by_rev(scm, exp2))
+
+    scm.commit("new_baseline")
+
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
+    exp3 = first(results)
+    ref_info3 = first(exp_refs_by_rev(scm, exp3))
+
+    if rev:
+        rev = baseline
+    dvc.experiments.push(remote, [], all_commits=all_, rev=rev)
+    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1
+    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info2)) == exp2
+    if result3:
+        result3 = exp3
+    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info3)) == result3
+
+
+def test_push_multi_rev(tmp_dir, scm, dvc, git_upstream, exp_stage):
+    remote = git_upstream.url
+    baseline = scm.get_rev()
+
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
+    exp1 = first(results)
+    ref_info1 = first(exp_refs_by_rev(scm, exp1))
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
+    exp2 = first(results)
+    ref_info2 = first(exp_refs_by_rev(scm, exp2))
+
+    scm.commit("new_baseline")
+
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
+    exp3 = first(results)
+    ref_info3 = first(exp_refs_by_rev(scm, exp3))
+
+    dvc.experiments.push(remote, [], rev=[baseline, scm.get_rev()])
+    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1
+    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info2)) == exp2
+    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info3)) == exp3
+
+
 def test_push_diverged(tmp_dir, scm, dvc, git_upstream, exp_stage):
     git_upstream.tmp_dir.scm_gen("foo", "foo", commit="init")
     remote_rev = git_upstream.tmp_dir.scm.get_rev()
 
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
     exp = first(results)
     ref_info = first(exp_refs_by_rev(scm, exp))
 
     git_upstream.tmp_dir.scm.set_ref(str(ref_info), remote_rev)
 
-    with pytest.raises(DvcException):
-        dvc.experiments.push(git_upstream.remote, [ref_info.name])
+    assert dvc.experiments.push(git_upstream.remote, [ref_info.name]) == {
+        "diverged": [ref_info.name],
+        "url": None,
+        "uploaded": 0,
+    }
     assert git_upstream.tmp_dir.scm.get_ref(str(ref_info)) == remote_rev
 
     dvc.experiments.push(git_upstream.remote, [ref_info.name], force=True)
     assert git_upstream.tmp_dir.scm.get_ref(str(ref_info)) == exp
 
 
 def test_push_checkpoint(tmp_dir, scm, dvc, git_upstream, checkpoint_stage):
-    results = dvc.experiments.run(
-        checkpoint_stage.addressing, params=["foo=2"]
-    )
+    results = dvc.experiments.run(checkpoint_stage.addressing, params=["foo=2"])
     exp_a = first(results)
     ref_info_a = first(exp_refs_by_rev(scm, exp_a))
 
     dvc.experiments.push(git_upstream.remote, [ref_info_a.name], force=True)
     assert git_upstream.tmp_dir.scm.get_ref(str(ref_info_a)) == exp_a
 
-    results = dvc.experiments.run(
-        checkpoint_stage.addressing, checkpoint_resume=exp_a
-    )
+    results = dvc.experiments.run(checkpoint_stage.addressing, checkpoint_resume=exp_a)
     exp_b = first(results)
     ref_info_b = first(exp_refs_by_rev(scm, exp_b))
 
     tmp_dir.scm_gen("new", "new", commit="new")
 
     dvc.experiments.push(git_upstream.remote, [ref_info_b.name], force=True)
     assert git_upstream.tmp_dir.scm.get_ref(str(ref_info_b)) == exp_b
 
 
 def test_push_ambiguous_name(tmp_dir, scm, dvc, git_upstream, exp_stage):
     from dvc.exceptions import InvalidArgumentError
 
     remote = git_upstream.remote
 
-    results = dvc.experiments.run(
-        exp_stage.addressing, params=["foo=2"], name="foo"
-    )
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"], name="foo")
     exp_a = first(results)
     ref_info_a = first(exp_refs_by_rev(scm, exp_a))
 
     tmp_dir.scm_gen("new", "new", commit="new")
-    results = dvc.experiments.run(
-        exp_stage.addressing, params=["foo=3"], name="foo"
-    )
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"], name="foo")
     exp_b = first(results)
     ref_info_b = first(exp_refs_by_rev(scm, exp_b))
 
     dvc.experiments.push(remote, ["foo"])
     assert git_upstream.tmp_dir.scm.get_ref(str(ref_info_b)) == exp_b
 
     tmp_dir.scm_gen("new", "new 2", commit="new 2")
@@ -117,34 +162,34 @@
     ref_info_a = first(exp_refs_by_rev(scm, exp_a))
 
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
     exp_b = first(results)
     ref_info_b = first(exp_refs_by_rev(scm, exp_b))
 
     tmp_dir.scm_gen("new", "new", commit="new")
-    baseline_c = scm.get_rev()
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=4"])
     exp_c = first(results)
     ref_info_c = first(exp_refs_by_rev(scm, exp_c))
 
     remote = git_downstream.url if use_url else git_downstream.remote
 
     assert git_downstream.tmp_dir.scm.get_ref("HEAD") != scm.get_ref("HEAD")
     downstream_exp = git_downstream.tmp_dir.dvc.experiments
     assert downstream_exp.ls(git_remote=remote) == {}
 
+    git_downstream.tmp_dir.scm.fetch_refspecs(remote, ["master:master"])
     exp_list = downstream_exp.ls(rev=baseline_a, git_remote=remote)
     assert {key: set(val) for key, val in exp_list.items()} == {
-        baseline_a: {ref_info_a.name, ref_info_b.name}
+        baseline_a[:7]: {ref_info_a.name, ref_info_b.name}
     }
 
-    exp_list = downstream_exp.ls(all_=True, git_remote=remote)
+    exp_list = downstream_exp.ls(all_commits=True, git_remote=remote)
     assert {key: set(val) for key, val in exp_list.items()} == {
-        baseline_a: {ref_info_a.name, ref_info_b.name},
-        baseline_c: {ref_info_c.name},
+        baseline_a[:7]: {ref_info_a.name, ref_info_b.name},
+        "master": {ref_info_c.name},
     }
 
 
 @pytest.mark.parametrize("use_url", [True, False])
 def test_pull(tmp_dir, scm, dvc, git_downstream, exp_stage, use_url):
     from dvc.exceptions import InvalidArgumentError
 
@@ -172,67 +217,112 @@
 
     git_downstream.tmp_dir.scm.remove_ref(str(ref_info1))
 
     downstream_exp.pull(remote, [str(ref_info1)])
     assert git_downstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1
 
 
+@pytest.mark.parametrize("all_,rev,result3", [(True, False, True), (False, True, None)])
+def test_pull_args(tmp_dir, scm, dvc, git_downstream, exp_stage, all_, rev, result3):
+    baseline = scm.get_rev()
+
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
+    exp1 = first(results)
+    ref_info1 = first(exp_refs_by_rev(scm, exp1))
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
+    exp2 = first(results)
+    ref_info2 = first(exp_refs_by_rev(scm, exp2))
+
+    scm.commit("new_baseline")
+
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
+    exp3 = first(results)
+    ref_info3 = first(exp_refs_by_rev(scm, exp3))
+
+    if rev:
+        rev = baseline
+
+    downstream_exp = git_downstream.tmp_dir.dvc.experiments
+    git_downstream.tmp_dir.scm.fetch_refspecs(str(tmp_dir), ["master:master"])
+    downstream_exp.pull(git_downstream.remote, [], all_commits=all_, rev=rev)
+    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1
+    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info2)) == exp2
+    if result3:
+        result3 = exp3
+    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info3)) == result3
+
+
+def test_pull_multi_rev(tmp_dir, scm, dvc, git_downstream, exp_stage):
+    baseline = scm.get_rev()
+
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
+    exp1 = first(results)
+    ref_info1 = first(exp_refs_by_rev(scm, exp1))
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
+    exp2 = first(results)
+    ref_info2 = first(exp_refs_by_rev(scm, exp2))
+
+    scm.commit("new_baseline")
+
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
+    exp3 = first(results)
+    ref_info3 = first(exp_refs_by_rev(scm, exp3))
+
+    downstream_exp = git_downstream.tmp_dir.dvc.experiments
+    git_downstream.tmp_dir.scm.fetch_refspecs(str(tmp_dir), ["master:master"])
+    downstream_exp.pull(git_downstream.remote, [], rev=[baseline, scm.get_rev()])
+    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1
+    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info2)) == exp2
+    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info3)) == exp3
+
+
 def test_pull_diverged(tmp_dir, scm, dvc, git_downstream, exp_stage):
     git_downstream.tmp_dir.scm_gen("foo", "foo", commit="init")
     remote_rev = git_downstream.tmp_dir.scm.get_rev()
 
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
     exp = first(results)
     ref_info = first(exp_refs_by_rev(scm, exp))
 
     git_downstream.tmp_dir.scm.set_ref(str(ref_info), remote_rev)
 
     downstream_exp = git_downstream.tmp_dir.dvc.experiments
-    with pytest.raises(DvcException):
-        downstream_exp.pull(git_downstream.remote, ref_info.name)
+    assert downstream_exp.pull(git_downstream.remote, ref_info.name) == []
     assert git_downstream.tmp_dir.scm.get_ref(str(ref_info)) == remote_rev
 
     downstream_exp.pull(git_downstream.remote, ref_info.name, force=True)
     assert git_downstream.tmp_dir.scm.get_ref(str(ref_info)) == exp
 
 
 def test_pull_checkpoint(tmp_dir, scm, dvc, git_downstream, checkpoint_stage):
-    results = dvc.experiments.run(
-        checkpoint_stage.addressing, params=["foo=2"]
-    )
+    results = dvc.experiments.run(checkpoint_stage.addressing, params=["foo=2"])
     exp_a = first(results)
     ref_info_a = first(exp_refs_by_rev(scm, exp_a))
 
     downstream_exp = git_downstream.tmp_dir.dvc.experiments
     downstream_exp.pull(git_downstream.remote, [ref_info_a.name], force=True)
     assert git_downstream.tmp_dir.scm.get_ref(str(ref_info_a)) == exp_a
 
-    results = dvc.experiments.run(
-        checkpoint_stage.addressing, checkpoint_resume=exp_a
-    )
+    results = dvc.experiments.run(checkpoint_stage.addressing, checkpoint_resume=exp_a)
     exp_b = first(results)
     ref_info_b = first(exp_refs_by_rev(scm, exp_b))
 
     downstream_exp.pull(git_downstream.remote, [ref_info_b.name], force=True)
     assert git_downstream.tmp_dir.scm.get_ref(str(ref_info_b)) == exp_b
 
 
 def test_pull_ambiguous_name(tmp_dir, scm, dvc, git_downstream, exp_stage):
     from dvc.exceptions import InvalidArgumentError
 
-    results = dvc.experiments.run(
-        exp_stage.addressing, params=["foo=2"], name="foo"
-    )
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"], name="foo")
     exp_a = first(results)
     ref_info_a = first(exp_refs_by_rev(scm, exp_a))
 
     tmp_dir.scm_gen("new", "new", commit="new")
-    results = dvc.experiments.run(
-        exp_stage.addressing, params=["foo=3"], name="foo"
-    )
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"], name="foo")
     exp_b = first(results)
     ref_info_b = first(exp_refs_by_rev(scm, exp_b))
 
     remote = git_downstream.remote
     downstream_exp = git_downstream.tmp_dir.dvc.experiments
     with pytest.raises(InvalidArgumentError):
         downstream_exp.pull(remote, ["foo"])
@@ -248,62 +338,79 @@
 def test_push_pull_cache(
     tmp_dir, scm, dvc, git_upstream, checkpoint_stage, local_remote
 ):
     from dvc.utils.fs import remove
     from tests.func.test_diff import digest
 
     remote = git_upstream.remote
-    results = dvc.experiments.run(
-        checkpoint_stage.addressing, params=["foo=2"]
-    )
+    results = dvc.experiments.run(checkpoint_stage.addressing, params=["foo=2"])
     exp = first(results)
     ref_info = first(exp_refs_by_rev(scm, exp))
 
     dvc.experiments.push(remote, [ref_info.name], push_cache=True)
     for x in range(2, checkpoint_stage.iterations + 1):
         hash_ = digest(str(x))
         path = os.path.join(local_remote.url, hash_[:2], hash_[2:])
         assert os.path.exists(path)
-        assert open(path, encoding="utf-8").read() == str(x)
+        with open(path, encoding="utf-8") as f:
+            assert f.read() == str(x)
 
-    remove(dvc.odb.local.cache_dir)
+    remove(dvc.cache.local.path)
 
     dvc.experiments.pull(remote, [ref_info.name], pull_cache=True)
     for x in range(2, checkpoint_stage.iterations + 1):
         hash_ = digest(str(x))
-        path = os.path.join(dvc.odb.local.cache_dir, hash_[:2], hash_[2:])
+        path = os.path.join(dvc.cache.local.path, hash_[:2], hash_[2:])
         assert os.path.exists(path)
-        assert open(path, encoding="utf-8").read() == str(x)
+        with open(path, encoding="utf-8") as f:
+            assert f.read() == str(x)
 
 
 def test_auth_error_list(tmp_dir, scm, dvc, http_auth_patch):
     from dvc.scm import GitAuthError
 
     with pytest.raises(
         GitAuthError,
-        match=f"HTTP Git authentication is not supported: '{http_auth_patch}'",
+        match=f"Authentication failed for: '{http_auth_patch}'",
     ):
         dvc.experiments.ls(git_remote=http_auth_patch)
 
 
 def test_auth_error_pull(tmp_dir, scm, dvc, http_auth_patch):
     from dvc.scm import GitAuthError
 
     with pytest.raises(
         GitAuthError,
-        match=f"HTTP Git authentication is not supported: '{http_auth_patch}'",
+        match=f"Authentication failed for: '{http_auth_patch}'",
     ):
         dvc.experiments.pull(http_auth_patch, ["foo"])
 
 
 def test_auth_error_push(tmp_dir, scm, dvc, exp_stage, http_auth_patch):
     from dvc.scm import GitAuthError
 
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
     exp = first(results)
     ref_info = first(exp_refs_by_rev(scm, exp))
 
     with pytest.raises(
         GitAuthError,
-        match=f"HTTP Git authentication is not supported: '{http_auth_patch}'",
+        match=f"Authentication failed for: '{http_auth_patch}'",
     ):
         dvc.experiments.push(http_auth_patch, [ref_info.name])
+
+
+@pytest.mark.parametrize("use_ref", [True, False])
+def test_get(tmp_dir, scm, dvc, exp_stage, erepo_dir, use_ref):
+    from dvc.repo import Repo
+
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
+    exp_rev = first(results)
+    exp_ref = first(exp_refs_by_rev(scm, exp_rev))
+
+    with erepo_dir.chdir():
+        Repo.get(
+            str(tmp_dir),
+            "params.yaml",
+            rev=exp_ref.name if use_ref else exp_rev,
+        )
+        assert (erepo_dir / "params.yaml").read_text().strip() == "foo: 2"
```

### Comparing `dvc-2.9.5/tests/func/experiments/test_remove.py` & `dvc-3.0.0a0/tests/func/experiments/test_remove.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,79 +1,83 @@
 import pytest
 from funcy import first
 
 from dvc.exceptions import InvalidArgumentError
+from dvc.repo.experiments.exceptions import UnresolvedExpNamesError
 from dvc.repo.experiments.utils import exp_refs_by_rev
 
 
 def test_remove_experiments_by_ref(tmp_dir, scm, dvc, exp_stage, caplog):
     queue_length = 3
-    ref_list = []
+    ref_info_list = []
+    ref_name_list = []
 
     for i in range(queue_length):
-        results = dvc.experiments.run(
-            exp_stage.addressing, params=[f"foo={i}"]
-        )
+        results = dvc.experiments.run(exp_stage.addressing, params=[f"foo={i}"])
         ref_info = first(exp_refs_by_rev(scm, first(results)))
-        ref_list.append(str(ref_info))
+        ref_info_list.append(ref_info)
+        ref_name_list.append(str(ref_info))
 
     with pytest.raises(InvalidArgumentError):
-        assert dvc.experiments.remove(ref_list[:2] + ["non-exist"])
-    assert scm.get_ref(str(ref_list[0])) is None
-    assert scm.get_ref(str(ref_list[1])) is None
-    assert scm.get_ref(str(ref_list[2])) is not None
+        dvc.experiments.remove(ref_name_list[:2] + ["non-exist"])
+    assert scm.get_ref(ref_name_list[0]) is not None
+    assert scm.get_ref(ref_name_list[1]) is not None
+    assert scm.get_ref(ref_name_list[2]) is not None
+
+    assert set(dvc.experiments.remove(ref_name_list[:2])) == set(ref_name_list[:2])
+    assert scm.get_ref(ref_name_list[0]) is None
+    assert scm.get_ref(ref_name_list[1]) is None
+    assert scm.get_ref(ref_name_list[2]) is not None
 
 
 def test_remove_all_queued_experiments(tmp_dir, scm, dvc, exp_stage):
     queue_length = 3
-
     for i in range(queue_length):
-        dvc.experiments.run(
-            exp_stage.addressing, params=[f"foo={i}"], queue=True
-        )
+        dvc.experiments.run(exp_stage.addressing, params=[f"foo={i}"], queue=True)
 
-    results = dvc.experiments.run(
-        exp_stage.addressing, params=[f"foo={queue_length}"]
-    )
+    results = dvc.experiments.run(exp_stage.addressing, params=[f"foo={queue_length}"])
     ref_info = first(exp_refs_by_rev(scm, first(results)))
 
-    assert len(dvc.experiments.stash) == queue_length
-    assert dvc.experiments.remove(queue=True) == queue_length
-    assert len(dvc.experiments.stash) == 0
+    assert len(dvc.experiments.stash_revs) == queue_length
+    assert len(dvc.experiments.remove(queue=True)) == queue_length
+    assert len(dvc.experiments.stash_revs) == 0
     assert scm.get_ref(str(ref_info)) is not None
 
 
 def test_remove_special_queued_experiments(tmp_dir, scm, dvc, exp_stage):
-    results = dvc.experiments.run(
+    dvc.experiments.run(
         exp_stage.addressing, params=["foo=1"], queue=True, name="queue1"
     )
-    rev1 = first(results)
-    results = dvc.experiments.run(
+    dvc.experiments.run(
         exp_stage.addressing, params=["foo=2"], queue=True, name="queue2"
     )
-    rev2 = first(results)
-    results = dvc.experiments.run(
+    dvc.experiments.run(
         exp_stage.addressing, params=["foo=3"], queue=True, name="queue3"
     )
-    rev3 = first(results)
+    queue_revs = {
+        entry.name: entry.stash_rev
+        for entry in dvc.experiments.celery_queue.iter_queued()
+    }
+    assert len(queue_revs) == 3
+
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=4"])
     ref_info1 = first(exp_refs_by_rev(scm, first(results)))
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=5"])
     ref_info2 = first(exp_refs_by_rev(scm, first(results)))
 
-    assert rev1 in dvc.experiments.stash_revs
-    assert rev2 in dvc.experiments.stash_revs
-    assert rev3 in dvc.experiments.stash_revs
     assert scm.get_ref(str(ref_info1)) is not None
     assert scm.get_ref(str(ref_info2)) is not None
 
-    assert dvc.experiments.remove(["queue1", rev2[:5], str(ref_info1)]) == 3
-    assert rev1 not in dvc.experiments.stash_revs
-    assert rev2 not in dvc.experiments.stash_revs
-    assert rev3 in dvc.experiments.stash_revs
+    rev2 = queue_revs["queue2"]
+    assert set(dvc.experiments.remove(["queue1", rev2[:5], str(ref_info1)])) == {
+        "queue1",
+        rev2[:5],
+        str(ref_info1),
+    }
+    assert len(list(dvc.experiments.celery_queue.iter_queued())) == 1
     assert scm.get_ref(str(ref_info1)) is None
     assert scm.get_ref(str(ref_info2)) is not None
 
 
 def test_remove_all(tmp_dir, scm, dvc, exp_stage):
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
     ref_info1 = first(exp_refs_by_rev(scm, first(results)))
@@ -81,40 +85,97 @@
     scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
     scm.commit("update baseline")
 
     results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
     ref_info2 = first(exp_refs_by_rev(scm, first(results)))
     dvc.experiments.run(exp_stage.addressing, params=["foo=4"], queue=True)
 
-    removed = dvc.experiments.remove(clear_all=True)
-    assert removed == 2
-    assert len(dvc.experiments.stash) == 2
+    assert set(dvc.experiments.remove(all_commits=True)) == {
+        ref_info1.name,
+        ref_info2.name,
+    }
+    assert len(dvc.experiments.stash_revs) == 2
     assert scm.get_ref(str(ref_info2)) is None
     assert scm.get_ref(str(ref_info1)) is None
 
 
 @pytest.mark.parametrize("use_url", [True, False])
 def test_remove_remote(tmp_dir, scm, dvc, exp_stage, git_upstream, use_url):
     remote = git_upstream.url if use_url else git_upstream.remote
 
     ref_info_list = []
     exp_list = []
     for i in range(3):
-        results = dvc.experiments.run(
-            exp_stage.addressing, params=[f"foo={i}"]
-        )
+        results = dvc.experiments.run(exp_stage.addressing, params=[f"foo={i}"])
         exp = first(results)
         exp_list.append(exp)
         ref_info = first(exp_refs_by_rev(scm, exp))
         ref_info_list.append(ref_info)
         dvc.experiments.push(remote, [ref_info.name])
         assert git_upstream.tmp_dir.scm.get_ref(str(ref_info)) == exp
 
     dvc.experiments.remove(
-        remote=remote, exp_names=[str(ref_info_list[0]), ref_info_list[1].name]
+        git_remote=remote,
+        exp_names=[str(ref_info_list[0]), ref_info_list[1].name],
     )
 
     assert git_upstream.tmp_dir.scm.get_ref(str(ref_info_list[0])) is None
     assert git_upstream.tmp_dir.scm.get_ref(str(ref_info_list[1])) is None
-    assert (
-        git_upstream.tmp_dir.scm.get_ref(str(ref_info_list[2])) == exp_list[2]
+    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info_list[2])) == exp_list[2]
+
+    with pytest.raises(
+        UnresolvedExpNamesError, match=f"Experiment 'foo' does not exist in '{remote}'"
+    ):
+        dvc.experiments.remove(git_remote=remote, exp_names=["foo"])
+
+
+def test_remove_experiments_by_rev(tmp_dir, scm, dvc, exp_stage):
+    baseline = scm.get_rev()
+
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
+    baseline_exp_ref = first(exp_refs_by_rev(scm, first(results)))
+
+    dvc.experiments.run(
+        exp_stage.addressing, params=["foo=2"], queue=True, name="queue2"
     )
+    scm.commit("new_baseline")
+
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
+    ref_info = first(exp_refs_by_rev(scm, first(results)))
+    new_exp_ref = str(ref_info)
+
+    dvc.experiments.run(
+        exp_stage.addressing, params=["foo=4"], queue=True, name="queue4"
+    )
+
+    assert dvc.experiments.remove(rev=baseline) == [baseline_exp_ref.name]
+    queue_revs = {
+        entry.name: entry.stash_rev
+        for entry in dvc.experiments.celery_queue.iter_queued()
+    }
+    assert scm.get_ref(str(baseline_exp_ref)) is None
+    assert "queue2" in queue_revs
+    assert scm.get_ref(new_exp_ref) is not None
+    assert "queue4" in queue_revs
+
+
+def test_remove_multi_rev(tmp_dir, scm, dvc, exp_stage):
+    baseline = scm.get_rev()
+
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
+    baseline_exp_ref = first(exp_refs_by_rev(scm, first(results)))
+
+    dvc.experiments.run(
+        exp_stage.addressing, params=["foo=2"], queue=True, name="queue2"
+    )
+    scm.commit("new_baseline")
+
+    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
+    new_exp_ref = first(exp_refs_by_rev(scm, first(results)))
+
+    assert set(dvc.experiments.remove(rev=[baseline, scm.get_rev()])) == {
+        baseline_exp_ref.name,
+        new_exp_ref.name,
+    }
+
+    assert scm.get_ref(str(baseline_exp_ref)) is None
+    assert scm.get_ref(str(new_exp_ref)) is None
```

### Comparing `dvc-2.9.5/tests/func/experiments/test_show.py` & `dvc-3.0.0a0/tests/func/experiments/test_show.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,28 +1,30 @@
 import logging
 import os
 from datetime import datetime
 from unittest.mock import ANY
 
 import pytest
-from funcy import first, get_in
+from funcy import first
+from scmrepo.exceptions import SCMError
 
 from dvc.cli import main
-from dvc.exceptions import InvalidArgumentError
-from dvc.repo.experiments.base import EXPS_STASH, ExpRefInfo
-from dvc.repo.experiments.executor.base import (
-    EXEC_PID_DIR,
-    EXEC_TMP_DIR,
-    BaseExecutor,
-    ExecutorInfo,
-)
-from dvc.repo.experiments.utils import exp_refs_by_rev
-from dvc.utils.fs import makedirs
-from dvc.utils.serialize import YAMLFileCorruptedError
-from tests.func.test_repro_multistage import COPY_SCRIPT
+from dvc.repo.experiments.executor.base import BaseExecutor, ExecutorInfo, TaskStatus
+from dvc.repo.experiments.refs import CELERY_STASH
+from dvc.repo.experiments.utils import EXEC_PID_DIR, EXEC_TMP_DIR, exp_refs_by_rev
+from dvc.utils import relpath
+
+LOCK_CONTENTS = {
+    "read": {
+        "data/MNIST": [{"pid": 54062, "cmd": "dvc exp run"}],
+    },
+    "write": {
+        "data/MNIST": {"pid": 54062, "cmd": "dvc exp run"},
+    },
+}
 
 
 def make_executor_info(**kwargs):
     # set default values for required info fields
     for key in (
         "git_url",
         "baseline_rev",
@@ -31,197 +33,263 @@
         "dvc_dir",
     ):
         if key not in kwargs:
             kwargs[key] = ""
     return ExecutorInfo(**kwargs)
 
 
-def test_show_simple(tmp_dir, scm, dvc, exp_stage):
-    assert dvc.experiments.show()["workspace"] == {
-        "baseline": {
-            "data": {
-                "deps": {
-                    "copy.py": {
-                        "hash": ANY,
-                        "size": ANY,
-                        "nfiles": None,
-                    }
-                },
-                "metrics": {"metrics.yaml": {"data": {"foo": 1}}},
-                "outs": {},
-                "params": {"params.yaml": {"data": {"foo": 1}}},
-                "queued": False,
-                "running": False,
-                "executor": None,
-                "timestamp": None,
-            }
+def make_executor(local=None, **kwargs):
+    if local:
+        local_executor = {
+            "root": ANY,
+            "log": ANY,
+            "pid": ANY,
+            "returncode": ANY,
+            "task_id": ANY,
         }
+        local_executor.update(local)
+    else:
+        local_executor = ANY
+    data = {
+        "state": ANY,
+        "local": local_executor,
+        "name": ANY,
+    }
+    data.update(kwargs)
+    return data
+
+
+def make_data(params=None, **kwargs):
+    params = {"data": params or {"foo": 1}}
+    data = {
+        "rev": ANY,
+        "deps": {
+            "copy.py": {
+                "hash": ANY,
+                "size": ANY,
+                "nfiles": None,
+            }
+        },
+        "metrics": {"metrics.yaml": params},
+        "outs": {},
+        "params": {"params.yaml": params},
+        "timestamp": ANY,
+        "meta": ANY,
+    }
+    data.update(kwargs)
+    return data
+
+
+@pytest.mark.vscode
+def test_show_branch_and_tag_name(tmp_dir, scm, dvc, exp_stage):
+    with tmp_dir.branch("new/branch", new=True):
+        tmp_dir.scm_gen("branch", "branch", "commit")
+
+    result = dvc.experiments.show(all_branches=True)
+    expected = [None, "master", "new/branch"]
+    assert [exp.name for exp in result] == expected
+
+    scm.tag("new/tag")
+    tag_rev = scm.get_rev()
+    with scm.detach_head(tag_rev):
+        result = dvc.experiments.show(all_tags=True)
+    expected = [None, "new/tag"]
+    assert [exp.name for exp in result] == expected
+
+
+@pytest.mark.vscode
+def test_show_simple(tmp_dir, scm, dvc, exp_stage):
+    assert dvc.experiments.show()[0].dumpd() == {
+        "rev": "workspace",
+        "name": None,
+        "data": make_data(rev="workspace"),
+        "error": None,
+        "experiments": None,
     }
 
 
+@pytest.mark.vscode
 @pytest.mark.parametrize("workspace", [True, False])
 def test_show_experiment(tmp_dir, scm, dvc, exp_stage, workspace):
     baseline_rev = scm.get_rev()
     timestamp = datetime.fromtimestamp(
         scm.gitpython.repo.rev_parse(baseline_rev).committed_date
     )
 
-    dvc.experiments.run(
-        exp_stage.addressing, params=["foo=2"], tmp_dir=not workspace
+    exp_rev = first(
+        dvc.experiments.run(
+            exp_stage.addressing, params=["foo=2"], tmp_dir=not workspace
+        )
     )
     results = dvc.experiments.show()
-
-    expected_baseline = {
-        "data": {
-            "deps": {
-                "copy.py": {
-                    "hash": ANY,
-                    "size": ANY,
-                    "nfiles": None,
-                }
-            },
-            "metrics": {"metrics.yaml": {"data": {"foo": 1}}},
-            "outs": {},
-            "params": {"params.yaml": {"data": {"foo": 1}}},
-            "queued": False,
-            "running": False,
-            "executor": None,
-            "timestamp": timestamp,
-            "name": "master",
-        }
+    assert results[1].dumpd() == {
+        "rev": baseline_rev,
+        "name": "master",
+        "data": make_data(rev=baseline_rev, timestamp=timestamp),
+        "error": None,
+        "experiments": [
+            {
+                "revs": [
+                    {
+                        "rev": exp_rev,
+                        "name": ANY,
+                        "data": make_data(rev=exp_rev, params={"foo": 2}),
+                        "error": None,
+                        "experiments": None,
+                    }
+                ],
+                "executor": None,
+                "name": ANY,
+            }
+        ],
     }
-    expected_params = {"data": {"foo": 2}}
-
-    assert set(results.keys()) == {"workspace", baseline_rev}
-    experiments = results[baseline_rev]
-    assert len(experiments) == 2
-    for rev, exp in experiments.items():
-        if rev == "baseline":
-            assert exp == expected_baseline
-        else:
-            assert exp["data"]["metrics"]["metrics.yaml"] == expected_params
-            assert exp["data"]["params"]["params.yaml"] == expected_params
 
 
+@pytest.mark.vscode
 def test_show_queued(tmp_dir, scm, dvc, exp_stage):
     baseline_rev = scm.get_rev()
 
     dvc.experiments.run(
         exp_stage.addressing, params=["foo=2"], queue=True, name="test_name"
     )
-    exp_rev = dvc.experiments.scm.resolve_rev(f"{EXPS_STASH}@{{0}}")
+    exp_rev = dvc.experiments.scm.resolve_rev(f"{CELERY_STASH}@{{0}}")
 
-    results = dvc.experiments.show()[baseline_rev]
-    assert len(results) == 2
-    exp = results[exp_rev]["data"]
-    assert exp["name"] == "test_name"
-    assert exp["queued"]
-    assert exp["params"]["params.yaml"] == {"data": {"foo": 2}}
+    results = dvc.experiments.show()
+    assert results[1].dumpd() == {
+        "rev": baseline_rev,
+        "name": "master",
+        "data": make_data(rev=baseline_rev),
+        "error": None,
+        "experiments": [
+            {
+                "revs": [
+                    {
+                        "rev": exp_rev,
+                        "name": "test_name",
+                        "data": make_data(rev=exp_rev, params={"foo": 2}, metrics=ANY),
+                        "error": None,
+                        "experiments": None,
+                    }
+                ],
+                "executor": make_executor(state="queued"),
+                "name": "test_name",
+            }
+        ],
+    }
 
     # test that only queued experiments for the current baseline are returned
     tmp_dir.gen("foo", "foo")
     scm.add(["foo"])
     scm.commit("new commit")
     new_rev = scm.get_rev()
 
     dvc.experiments.run(exp_stage.addressing, params=["foo=3"], queue=True)
-    exp_rev = dvc.experiments.scm.resolve_rev(f"{EXPS_STASH}@{{0}}")
+    exp_rev = dvc.experiments.scm.resolve_rev(f"{CELERY_STASH}@{{0}}")
 
-    results = dvc.experiments.show()[new_rev]
-    assert len(results) == 2
-    exp = results[exp_rev]["data"]
-    assert exp["queued"]
-    assert exp["params"]["params.yaml"] == {"data": {"foo": 3}}
+    results = dvc.experiments.show()
+    assert results[1].dumpd() == {
+        "rev": new_rev,
+        "name": "master",
+        "data": make_data(rev=new_rev),
+        "error": None,
+        "experiments": [
+            {
+                "revs": [
+                    {
+                        "rev": exp_rev,
+                        "name": ANY,
+                        "data": make_data(rev=exp_rev, params={"foo": 3}, metrics=ANY),
+                        "error": None,
+                        "experiments": None,
+                    }
+                ],
+                "executor": make_executor(state="queued"),
+                "name": ANY,
+            }
+        ],
+    }
 
 
-@pytest.mark.parametrize("workspace", [True, False])
-def test_show_checkpoint(
-    tmp_dir, scm, dvc, checkpoint_stage, capsys, workspace
-):
+@pytest.mark.vscode
+def test_show_failed_experiment(tmp_dir, scm, dvc, failed_exp_stage, test_queue):
     baseline_rev = scm.get_rev()
+    dvc.experiments.run(failed_exp_stage.addressing, params=["foo=2"], queue=True)
+    exp_rev = dvc.experiments.scm.resolve_rev(f"{CELERY_STASH}@{{0}}")
+    dvc.experiments.run(run_all=True)
+
+    results = dvc.experiments.show()
+    assert results[1].dumpd() == {
+        "rev": baseline_rev,
+        "name": "master",
+        "data": make_data(rev=baseline_rev, metrics=ANY),
+        "error": None,
+        "experiments": [
+            {
+                "revs": [
+                    {
+                        "rev": exp_rev,
+                        "name": ANY,
+                        "data": make_data(rev=exp_rev, params={"foo": 2}, metrics=ANY),
+                        "error": {"msg": "Experiment run failed", "type": ANY},
+                        "experiments": None,
+                    }
+                ],
+                "executor": make_executor(
+                    state="failed",
+                    local={"returncode": 255},
+                ),
+                "name": ANY,
+            }
+        ],
+    }
+
+
+@pytest.mark.vscode
+@pytest.mark.parametrize("workspace", [True, False])
+def test_show_checkpoint(tmp_dir, scm, dvc, checkpoint_stage, capsys, workspace):
     results = dvc.experiments.run(
         checkpoint_stage.addressing, params=["foo=2"], tmp_dir=not workspace
     )
     exp_rev = first(results)
 
-    results = dvc.experiments.show()[baseline_rev]
-    assert len(results) == checkpoint_stage.iterations + 1
-
-    checkpoints = []
-    for rev, exp in results.items():
-        if rev != "baseline":
-            checkpoints.append(rev)
-            assert exp["data"]["checkpoint_tip"] == exp_rev
+    baseline = dvc.experiments.show()[1]
+    assert baseline.data.meta.get("has_checkpoints")
+    checkpoints = first(baseline.experiments)
+    # 2 checkpoints + final commit
+    assert len(checkpoints) == checkpoint_stage.iterations + 1
+    assert first(checkpoints).rev == exp_rev
 
     capsys.readouterr()
     assert main(["exp", "show", "--no-pager"]) == 0
     cap = capsys.readouterr()
 
-    for i, rev in enumerate(checkpoints):
+    for i, exp in enumerate(checkpoints):
+        rev = exp.rev
         if i == 0:
-            name = dvc.experiments.get_exact_name(rev)
+            name = dvc.experiments.get_exact_name([rev])[rev]
             name = f"{rev[:7]} [{name}]"
             fs = "╓"
         elif i == len(checkpoints) - 1:
             name = rev[:7]
             fs = "╨"
         else:
             name = rev[:7]
             fs = "╟"
         assert f"{fs} {name}" in cap.out
 
 
-@pytest.mark.parametrize("workspace", [True, False])
-def test_show_checkpoint_branch(
-    tmp_dir, scm, dvc, checkpoint_stage, capsys, workspace
-):
-    results = dvc.experiments.run(
-        checkpoint_stage.addressing, params=["foo=2"], tmp_dir=not workspace
-    )
-    branch_rev = first(results)
-    if not workspace:
-        dvc.experiments.apply(branch_rev)
-
-    results = dvc.experiments.run(
-        checkpoint_stage.addressing,
-        checkpoint_resume=branch_rev,
-        tmp_dir=not workspace,
-    )
-    checkpoint_a = first(results)
-
-    dvc.experiments.apply(branch_rev)
-    results = dvc.experiments.run(
-        checkpoint_stage.addressing,
-        checkpoint_resume=branch_rev,
-        params=["foo=100"],
-        tmp_dir=not workspace,
-    )
-    checkpoint_b = first(results)
-
-    capsys.readouterr()
-    assert main(["exp", "show", "--no-pager"]) == 0
-    cap = capsys.readouterr()
-
-    for rev in (checkpoint_a, checkpoint_b):
-        ref = dvc.experiments.get_branch_by_rev(rev)
-        ref_info = ExpRefInfo.from_ref(ref)
-        name = f"{rev[:7]} [{ref_info.name}]"
-        assert f"╓ {name}" in cap.out
-    assert f"({branch_rev[:7]})" in cap.out
-
-
 def test_show_filter(
     tmp_dir,
     scm,
     dvc,
     capsys,
+    copy_script,
 ):
     capsys.readouterr()
 
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
     params_file = tmp_dir / "params.yaml"
     params_data = {
         "foo": 1,
         "bar": 1,
         "train/foo": 1,
         "train/bar": 1,
         "nested": {"foo": 1, "bar": 1},
@@ -278,186 +346,118 @@
     capsys.readouterr()
     assert main(["exp", "show", "--drop=Created|Experiment"]) == 0
     cap = capsys.readouterr()
     assert "Created" not in cap.out
     assert "Experiment" not in cap.out
 
 
+@pytest.mark.vscode
 def test_show_multiple_commits(tmp_dir, scm, dvc, exp_stage):
     init_rev = scm.get_rev()
     tmp_dir.scm_gen("file", "file", "commit")
     next_rev = scm.get_rev()
 
-    dvc.experiments.show(num=-1)
-    with pytest.raises(InvalidArgumentError):
-        dvc.experiments.show(num=-2)
+    dvc.experiments.show(num=-2)
 
-    expected = {"workspace", init_rev, next_rev}
+    expected = ["workspace", next_rev, init_rev]
     results = dvc.experiments.show(num=2)
-    assert set(results.keys()) == expected
+    assert [exp.rev for exp in results] == expected
 
-    expected = {"workspace"} | set(scm.branch_revs("master"))
+    expected = ["workspace", *scm.branch_revs("master")]
     results = dvc.experiments.show(all_commits=True)
-    assert set(results.keys()) == expected
+    assert [exp.rev for exp in results] == expected
 
     results = dvc.experiments.show(num=100)
-    assert set(results.keys()) == expected
+    assert [exp.rev for exp in results] == expected
 
 
 def test_show_sort(tmp_dir, scm, dvc, exp_stage, caplog):
+    dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
+
     with caplog.at_level(logging.ERROR):
         assert main(["exp", "show", "--no-pager", "--sort-by=bar"]) != 0
         assert "Unknown sort column" in caplog.text
 
     with caplog.at_level(logging.ERROR):
         assert main(["exp", "show", "--no-pager", "--sort-by=foo"]) != 0
         assert "Ambiguous sort column" in caplog.text
 
-    assert (
-        main(["exp", "show", "--no-pager", "--sort-by=params.yaml:foo"]) == 0
-    )
-
-    assert (
-        main(["exp", "show", "--no-pager", "--sort-by=metrics.yaml:foo"]) == 0
-    )
+    assert main(["exp", "show", "--no-pager", "--sort-by=params.yaml:foo"]) == 0
 
+    assert main(["exp", "show", "--no-pager", "--sort-by=metrics.yaml:foo"]) == 0
 
-def test_show_running_workspace(tmp_dir, scm, dvc, exp_stage, capsys):
-    pid_dir = os.path.join(dvc.tmp_dir, EXEC_TMP_DIR, EXEC_PID_DIR)
-    info = make_executor_info(location=BaseExecutor.DEFAULT_LOCATION)
-    pidfile = os.path.join(
-        pid_dir,
-        "workspace",
-        f"workspace{BaseExecutor.INFOFILE_EXT}",
-    )
-    makedirs(os.path.dirname(pidfile), True)
-    (tmp_dir / pidfile).dump_json(info.asdict())
 
-    assert dvc.experiments.show()["workspace"] == {
-        "baseline": {
-            "data": {
-                "deps": {
-                    "copy.py": {
-                        "hash": ANY,
-                        "size": ANY,
-                        "nfiles": None,
-                    }
-                },
-                "metrics": {"metrics.yaml": {"data": {"foo": 1}}},
-                "params": {"params.yaml": {"data": {"foo": 1}}},
-                "outs": {},
-                "queued": False,
-                "running": True,
-                "executor": info.location,
-                "timestamp": None,
-            }
-        }
-    }
-
-    capsys.readouterr()
-    assert main(["exp", "show", "--csv"]) == 0
-    cap = capsys.readouterr()
-    assert "Running" in cap.out
-    assert info.location in cap.out
-
-
-def test_show_running_executor(tmp_dir, scm, dvc, exp_stage):
-    baseline_rev = scm.get_rev()
-    dvc.experiments.run(exp_stage.addressing, params=["foo=2"], queue=True)
-    exp_rev = dvc.experiments.scm.resolve_rev(f"{EXPS_STASH}@{{0}}")
-
-    pid_dir = os.path.join(dvc.tmp_dir, EXEC_TMP_DIR, EXEC_PID_DIR)
-    info = make_executor_info(location=BaseExecutor.DEFAULT_LOCATION)
-    pidfile = os.path.join(
-        pid_dir,
-        exp_rev,
-        f"{exp_rev}{BaseExecutor.INFOFILE_EXT}",
-    )
-    makedirs(os.path.dirname(pidfile), True)
-    (tmp_dir / pidfile).dump_json(info.asdict())
-
-    results = dvc.experiments.show()
-    exp_data = get_in(results, [baseline_rev, exp_rev, "data"])
-    assert not exp_data["queued"]
-    assert exp_data["running"]
-    assert exp_data["executor"] == info.location
-
-    assert not results["workspace"]["baseline"]["data"]["running"]
-
-
-@pytest.mark.parametrize("workspace", [True, False])
-def test_show_running_checkpoint(
-    tmp_dir, scm, dvc, checkpoint_stage, workspace, mocker
+@pytest.mark.vscode
+@pytest.mark.parametrize(
+    "status, pid_exists",
+    [
+        (TaskStatus.RUNNING, True),
+        (TaskStatus.RUNNING, False),
+        (TaskStatus.FAILED, False),
+    ],
+)
+def test_show_running(
+    tmp_dir, scm, dvc, exp_stage, capsys, caplog, status, pid_exists, mocker
 ):
-    from dvc.repo.experiments.base import EXEC_BASELINE, EXEC_BRANCH
-    from dvc.repo.experiments.executor.local import TempDirExecutor
+    from dvc.rwlock import RWLOCK_FILE
+    from dvc_task.proc.process import ProcessInfo
 
     baseline_rev = scm.get_rev()
-    dvc.experiments.run(
-        checkpoint_stage.addressing, params=["foo=2"], queue=True
-    )
-    stash_rev = dvc.experiments.scm.resolve_rev(f"{EXPS_STASH}@{{0}}")
-
-    run_results = dvc.experiments.run(run_all=True)
-    checkpoint_rev = first(run_results)
-    exp_ref = first(exp_refs_by_rev(scm, checkpoint_rev))
-
     pid_dir = os.path.join(dvc.tmp_dir, EXEC_TMP_DIR, EXEC_PID_DIR)
-    executor = (
-        BaseExecutor.DEFAULT_LOCATION
-        if workspace
-        else TempDirExecutor.DEFAULT_LOCATION
-    )
+    lock_file = relpath(os.path.join(dvc.tmp_dir, RWLOCK_FILE), str(tmp_dir))
     info = make_executor_info(
-        git_url="foo.git",
+        location=BaseExecutor.DEFAULT_LOCATION,
+        status=status,
         baseline_rev=baseline_rev,
-        location=executor,
     )
-    rev = "workspace" if workspace else stash_rev
-    pidfile = os.path.join(pid_dir, f"{rev}{BaseExecutor.INFOFILE_EXT}")
-    makedirs(os.path.dirname(pidfile), True)
+    pidfile = os.path.join(
+        pid_dir,
+        "workspace",
+        f"workspace{BaseExecutor.INFOFILE_EXT}",
+    )
+    os.makedirs(os.path.dirname(pidfile), exist_ok=True)
     (tmp_dir / pidfile).dump_json(info.asdict())
+    (tmp_dir / lock_file).dump_json(LOCK_CONTENTS)
 
-    mocker.patch.object(
-        BaseExecutor, "fetch_exps", return_value=[str(exp_ref)]
-    )
-    if workspace:
-        scm.set_ref(EXEC_BRANCH, str(exp_ref), symbolic=True)
-        scm.set_ref(EXEC_BASELINE, str(baseline_rev))
-        scm.checkout(str(exp_ref))
+    mocker.patch.object(ProcessInfo, "load", return_value=mocker.Mock(pid=123))
+    mocker.patch("psutil.pid_exists", return_value=pid_exists)
 
+    tempdir_active = mocker.spy(dvc.experiments.tempdir_queue, "collect_active_data")
+    celery_active = mocker.spy(dvc.experiments.celery_queue, "collect_active_data")
     results = dvc.experiments.show()
-
-    checkpoint_res = get_in(results, [baseline_rev, checkpoint_rev, "data"])
-    assert checkpoint_res["running"]
-    assert checkpoint_res["executor"] == info.location
-
-    assert not results["workspace"]["baseline"]["data"]["running"]
+    assert results[1].dumpd() == {
+        "rev": ANY,
+        "name": "master",
+        "data": make_data(),
+        "error": None,
+        "experiments": [
+            {
+                "revs": ANY,
+                "executor": make_executor(state="running"),
+                "name": ANY,
+            }
+        ]
+        if pid_exists
+        else None,
+    }
+    tempdir_active.assert_called_once()
+    celery_active.assert_called_once()
 
 
 def test_show_with_broken_repo(tmp_dir, scm, dvc, exp_stage, caplog):
-    baseline_rev = scm.get_rev()
-    exp1 = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
-    exp2 = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
-
+    dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
     with open("dvc.yaml", "a", encoding="utf-8") as fd:
         fd.write("breaking the yaml!")
 
-    result = dvc.experiments.show()
-    rev1 = first(exp1)
-    rev2 = first(exp2)
-
-    baseline = result[baseline_rev]
-
-    paths = ["data", "params", "params.yaml"]
-    assert get_in(baseline[rev1], paths) == {"data": {"foo": 2}}
-    assert get_in(baseline[rev2], paths) == {"data": {"foo": 3}}
+    results = dvc.experiments.show()
+    assert results[0].error
+    assert results[0].error.type == "YAMLSyntaxError"
 
-    paths = ["workspace", "baseline", "error"]
-    assert isinstance(get_in(result, paths), YAMLFileCorruptedError)
+    for exp_range in results[1].experiments:
+        assert not any(exp.error for exp in exp_range)
 
 
 def test_show_csv(tmp_dir, scm, dvc, exp_stage, capsys):
     import time
 
     baseline_rev = scm.get_rev()
 
@@ -465,30 +465,33 @@
         return datetime.fromtimestamp(
             scm.gitpython.repo.rev_parse(rev).committed_date
         ).isoformat()
 
     result1 = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
     rev1 = first(result1)
     ref_info1 = first(exp_refs_by_rev(scm, rev1))
+
+    # at least 1 second gap between these experiments to make sure
+    # the previous experiment to be regarded as branch_base
     time.sleep(1)
     result2 = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
     rev2 = first(result2)
     ref_info2 = first(exp_refs_by_rev(scm, rev2))
 
     capsys.readouterr()
     assert main(["exp", "show", "--csv"]) == 0
     cap = capsys.readouterr()
     data_dep = first(x for x in dvc.index.deps if "copy.py" in x.fspath)
     data_hash = data_dep.hash_info.value[:7]
     assert "Experiment,rev,typ,Created,parent" in cap.out
     assert "metrics.yaml:foo,params.yaml:foo,copy.py" in cap.out
     assert f",workspace,baseline,,,3,3,{data_hash}" in cap.out
     assert (
-        "master,{},baseline,{},,1,1,{}".format(
-            baseline_rev[:7], _get_rev_isotimestamp(baseline_rev), data_hash
+        ",master,baseline,{},,1,1,{}".format(
+            _get_rev_isotimestamp(baseline_rev), data_hash
         )
         in cap.out
     )
     assert (
         "{},{},branch_base,{},,2,2,{}".format(
             ref_info1.name, rev1[:7], _get_rev_isotimestamp(rev1), data_hash
         )
@@ -498,27 +501,26 @@
         "{},{},branch_commit,{},,3,3,{}".format(
             ref_info2.name, rev2[:7], _get_rev_isotimestamp(rev2), data_hash
         )
         in cap.out
     )
 
 
-def test_show_only_changed(tmp_dir, dvc, scm, capsys):
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
+def test_show_only_changed(tmp_dir, dvc, scm, capsys, copy_script):
     params_file = tmp_dir / "params.yaml"
     params_data = {
         "foo": 1,
-        "bar": 1,
+        "goobar": 1,
     }
     (tmp_dir / params_file).dump(params_data)
 
     dvc.run(
         cmd="python copy.py params.yaml metrics.yaml",
         metrics_no_cache=["metrics.yaml"],
-        params=["foo", "bar"],
+        params=["foo", "goobar"],
         name="copy-file",
         deps=["copy.py"],
     )
     scm.add(
         [
             "dvc.yaml",
             "dvc.lock",
@@ -531,136 +533,137 @@
     scm.commit("init")
 
     dvc.experiments.run(params=["foo=2"])
 
     capsys.readouterr()
     assert main(["exp", "show"]) == 0
     cap = capsys.readouterr()
-    assert "bar" in cap.out
+    assert "goobar" in cap.out
 
     capsys.readouterr()
     assert main(["exp", "show", "--only-changed"]) == 0
     cap = capsys.readouterr()
-    assert "bar" not in cap.out
+    assert "goobar" not in cap.out
 
     capsys.readouterr()
     assert main(["exp", "show", "--only-changed", "--keep=.*bar"]) == 0
     cap = capsys.readouterr()
-    assert "params.yaml:bar" in cap.out
-    assert "metrics.yaml:bar" in cap.out
-
+    assert "params.yaml:goobar" in cap.out
+    assert "metrics.yaml:goobar" in cap.out
 
-def test_show_parallel_coordinates(tmp_dir, dvc, scm, mocker, capsys):
-    from dvc.commands.experiments import show
 
-    webbroser_open = mocker.patch("webbrowser.open")
-    show_experiments = mocker.spy(show, "show_experiments")
-
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
+@pytest.mark.vscode
+def test_show_outs(tmp_dir, dvc, scm, erepo_dir, copy_script):
     params_file = tmp_dir / "params.yaml"
     params_data = {
         "foo": 1,
         "bar": 1,
     }
     (tmp_dir / params_file).dump(params_data)
 
     dvc.run(
-        cmd="python copy.py params.yaml metrics.yaml",
+        cmd="python copy.py params.yaml metrics.yaml && echo out > out",
         metrics_no_cache=["metrics.yaml"],
         params=["foo", "bar"],
         name="copy-file",
         deps=["copy.py"],
+        outs=["out"],
     )
-    scm.add(
-        [
-            "dvc.yaml",
-            "dvc.lock",
-            "copy.py",
-            "params.yaml",
-            "metrics.yaml",
-            ".gitignore",
-        ]
-    )
-    scm.commit("init")
 
-    dvc.experiments.run(params=["foo=2"])
-
-    assert main(["exp", "show", "--pcp"]) == 0
-    kwargs = show_experiments.call_args[1]
-
-    html_text = (tmp_dir / "dvc_plots" / "index.html").read_text()
-    assert all(rev in html_text for rev in ["workspace", "master"])
-    assert "[exp-" not in html_text
-
-    assert '{"label": "metrics.yaml:foo", "values": [2.0, 1.0]}' in html_text
-    assert '{"label": "params.yaml:foo", "values": [2.0, 1.0]}' in html_text
-    assert '"line": {"color": [1, 0]' in html_text
-    assert '"label": "metrics.yaml:bar"' not in html_text
-    assert '"label": "Created"' not in html_text
-
-    assert main(["exp", "show", "--pcp", "--sort-by", "metrics.yaml:foo"]) == 0
-    kwargs = show_experiments.call_args[1]
-
-    html_text = (tmp_dir / "dvc_plots" / "index.html").read_text()
-    assert '"line": {"color": [2.0, 1.0]' in html_text
-
-    assert main(["exp", "show", "--pcp", "--out", "experiments"]) == 0
-    kwargs = show_experiments.call_args[1]
-
-    assert kwargs["out"] == "experiments"
-    assert (tmp_dir / "experiments" / "index.html").exists()
-
-    assert main(["exp", "show", "--pcp", "--open"]) == 0
-
-    webbroser_open.assert_called()
-
-    params_data = {"foo": 1, "bar": 1, "foobar": 2}
-    (tmp_dir / params_file).dump(params_data)
-    assert main(["exp", "show", "--pcp"]) == 0
-    html_text = (tmp_dir / "dvc_plots" / "index.html").read_text()
-    assert '{"label": "foobar", "values": [2.0, null, null]}' in html_text
-
-    assert main(["exp", "show", "--pcp", "--drop", "foobar"]) == 0
-    html_text = (tmp_dir / "dvc_plots" / "index.html").read_text()
-    assert '"label": "Created"' not in html_text
-    assert '"label": "foobar"' not in html_text
+    scm.commit("init")
 
+    results = dvc.experiments.show()
+    assert results[0].dumpd() == {
+        "rev": "workspace",
+        "name": None,
+        "data": make_data(
+            params=ANY,
+            outs={
+                "out": {
+                    "hash": ANY,
+                    "size": ANY,
+                    "nfiles": None,
+                    "use_cache": True,
+                    "is_data_source": False,
+                }
+            },
+        ),
+        "error": None,
+        "experiments": None,
+    }
 
-def test_show_outs(tmp_dir, dvc, scm):
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
-    params_file = tmp_dir / "params.yaml"
-    params_data = {
-        "foo": 1,
-        "bar": 1,
+    tmp_dir.dvc_gen("out_add", "foo", commit="dvc add output")
+    results = dvc.experiments.show()
+    assert results[0].dumpd() == {
+        "rev": "workspace",
+        "name": None,
+        "data": make_data(
+            params=ANY,
+            outs={
+                "out": {
+                    "hash": ANY,
+                    "size": ANY,
+                    "nfiles": None,
+                    "use_cache": True,
+                    "is_data_source": False,
+                },
+                "out_add": {
+                    "hash": ANY,
+                    "size": ANY,
+                    "nfiles": None,
+                    "use_cache": True,
+                    "is_data_source": True,
+                },
+            },
+        ),
+        "error": None,
+        "experiments": None,
     }
-    (tmp_dir / params_file).dump(params_data)
 
-    dvc.run(
-        cmd="python copy.py params.yaml metrics.yaml && echo out > out",
-        metrics_no_cache=["metrics.yaml"],
-        params=["foo", "bar"],
-        name="copy-file",
-        deps=["copy.py"],
-        outs=["out"],
-    )
+    with erepo_dir.chdir():
+        erepo_dir.dvc_gen("out", "out content", commit="create out")
 
-    scm.commit("init")
+    dvc.imp(os.fspath(erepo_dir), "out", "out_imported")
 
-    outs = dvc.experiments.show()["workspace"]["baseline"]["data"]["outs"]
-    assert outs == {
-        "out": {
-            "hash": ANY,
-            "size": ANY,
-            "nfiles": None,
-        }
+    results = dvc.experiments.show()
+    assert results[0].dumpd() == {
+        "rev": "workspace",
+        "name": None,
+        "data": make_data(
+            params=ANY,
+            outs={
+                "out": {
+                    "hash": ANY,
+                    "size": ANY,
+                    "nfiles": None,
+                    "use_cache": True,
+                    "is_data_source": False,
+                },
+                "out_add": {
+                    "hash": ANY,
+                    "size": ANY,
+                    "nfiles": None,
+                    "use_cache": True,
+                    "is_data_source": True,
+                },
+                "out_imported": {
+                    "hash": ANY,
+                    "size": ANY,
+                    "nfiles": None,
+                    "use_cache": True,
+                    "is_data_source": True,
+                },
+            },
+        ),
+        "error": None,
+        "experiments": None,
     }
 
 
-def test_metrics_renaming(tmp_dir, dvc, scm, capsys):
-    tmp_dir.gen("copy.py", COPY_SCRIPT)
+def test_metrics_renaming(tmp_dir, dvc, scm, capsys, copy_script):
     params_file = tmp_dir / "params.yaml"
     params_data = {
         "foo": 1,
     }
     (tmp_dir / params_file).dump(params_data)
 
     dvc.run(
@@ -707,20 +710,15 @@
     cap = capsys.readouterr()
 
     def _get_rev_isotimestamp(rev):
         return datetime.fromtimestamp(
             scm.gitpython.repo.rev_parse(rev).committed_date
         ).isoformat()
 
-    assert (
-        "master,{},baseline,{},,1,,1".format(
-            scores_rev[:7], _get_rev_isotimestamp(scores_rev)
-        )
-        in cap.out
-    )
+    assert f",master,baseline,{_get_rev_isotimestamp(scores_rev)},,1,,1" in cap.out
     assert (
         ",{},baseline,{},,,1,1".format(
             metrics_rev[:7], _get_rev_isotimestamp(metrics_rev)
         )
         in cap.out
     )
 
@@ -737,7 +735,84 @@
         deps=["a", "b", "z", "c"],
     )
 
     capsys.readouterr()
     assert main(["exp", "show", "--csv"]) == 0
     cap = capsys.readouterr()
     assert "a,b,c,z" in cap.out
+
+
+@pytest.mark.vscode
+def test_show_queued_error(tmp_dir, scm, dvc, exp_stage, mocker):
+    dvc.experiments.run(
+        exp_stage.addressing, params=["foo=2"], queue=True, name="test_name"
+    )
+    exp_rev_2 = dvc.experiments.scm.resolve_rev(f"{CELERY_STASH}@{{0}}")
+    commit_2 = scm.resolve_commit(exp_rev_2)
+
+    dvc.experiments.run(exp_stage.addressing, params=["foo=3"], queue=True)
+    exp_rev_3 = dvc.experiments.scm.resolve_rev(f"{CELERY_STASH}@{{0}}")
+
+    def resolve_commit(rev):
+        if rev == exp_rev_3:
+            raise SCMError
+        return commit_2
+
+    mocker.patch.object(
+        scm,
+        "resolve_commit",
+        side_effect=mocker.MagicMock(side_effect=resolve_commit),
+    )
+
+    results = dvc.experiments.show()[1].experiments
+    assert len(results) == 2
+    queued = results[0]
+    assert queued.executor.state == "queued"
+    errored = results[1]
+    assert errored.revs[0].error
+
+
+@pytest.mark.vscode
+def test_show_completed_error(tmp_dir, scm, dvc, exp_stage, mocker):
+    result_2 = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
+    exp_rev_2 = first(result_2)
+    commit_2 = scm.resolve_commit(exp_rev_2)
+    result_3 = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
+    exp_rev_3 = first(result_3)
+
+    def resolve_commit(rev):
+        if rev == exp_rev_3:
+            raise SCMError
+        return commit_2
+
+    mocker.patch.object(
+        scm,
+        "resolve_commit",
+        side_effect=mocker.MagicMock(side_effect=resolve_commit),
+    )
+    results = dvc.experiments.show()[1].experiments
+    assert len(results) == 1
+    assert not results[0].revs[0].error
+
+
+@pytest.mark.vscode
+def test_show_baseline_error(tmp_dir, scm, dvc, exp_stage, mocker):
+    baseline_rev = scm.get_rev()
+
+    result_2 = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
+    exp_rev_2 = first(result_2)
+    commit_2 = scm.resolve_commit(exp_rev_2)
+
+    def resolve_commit(rev):
+        if rev == baseline_rev:
+            raise SCMError
+        return commit_2
+
+    mocker.patch.object(
+        scm,
+        "resolve_commit",
+        side_effect=mocker.MagicMock(side_effect=resolve_commit),
+    )
+
+    results = dvc.experiments.show()
+    assert results[1].error
+    assert len(results[1].experiments) == 1
```

### Comparing `dvc-2.9.5/tests/func/machine/conftest.py` & `dvc-3.0.0a0/tests/func/machine/conftest.py`

 * *Files 4% similar despite different names*

```diff
@@ -31,15 +31,15 @@
     "timeouts": None,
 }
 
 
 @pytest.fixture
 def machine_config(tmp_dir):
     (tmp_dir / ".dvc" / "config").write_text(BASIC_CONFIG)
-    yield BASIC_CONFIG
+    return BASIC_CONFIG
 
 
 @pytest.fixture
 def machine_instance(tmp_dir, dvc, mocker):
     with dvc.config.edit() as conf:
         conf["machine"]["foo"] = {"cloud": "aws"}
 
@@ -48,8 +48,8 @@
             return iter([TEST_INSTANCE])
         return iter([])
 
     mocker.patch(
         "tpi.terraform.TerraformBackend.instances",
         mocker.MagicMock(side_effect=mock_instances),
     )
-    yield TEST_INSTANCE
+    return TEST_INSTANCE
```

### Comparing `dvc-2.9.5/tests/func/machine/test_machine_config.py` & `dvc-3.0.0a0/tests/func/machine/test_machine_config.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,11 @@
 import os
 import textwrap
 
 import pytest
-import tpi
 
 from dvc.cli import main
 from dvc.ui import ui
 from tests.utils import console_width
 
 from .conftest import BASIC_CONFIG
 
@@ -52,17 +51,15 @@
             "other-west",
             "expected one of us-west, us-east, eu-west, eu-north",
         ),
         ("spot_price", "NUM", "expected float"),
         ("instance_hdd_size", "BIG", "expected int"),
     ],
 )
-def test_machine_modify_fail(
-    tmp_dir, dvc, machine_config, caplog, slot, value, msg
-):
+def test_machine_modify_fail(tmp_dir, dvc, machine_config, caplog, slot, value, msg):
     assert main(["machine", "modify", "foo", slot, value]) == 251
     assert (tmp_dir / ".dvc" / "config").read_text() == machine_config
     assert msg in caplog.text
 
 
 FULL_CONFIG_TEXT = textwrap.dedent(
     """\
@@ -106,38 +103,34 @@
         assert main(["machine", "list", "bar"]) == 0
     out, _ = capsys.readouterr()
     assert "foo" not in out
     assert "name    cloud" in out
     assert "bar     azure" in out
 
 
-def test_machine_rename_success(
-    tmp_dir, scm, dvc, machine_config, capsys, mocker
-):
+def test_machine_rename_success(tmp_dir, scm, dvc, machine_config, capsys, mocker):
+    import tpi
+
     config_file = tmp_dir / ".dvc" / "config"
 
     mocker.patch.object(
         tpi.terraform.TerraformBackend,
         "state_mv",
         autospec=True,
         return_value=True,
     )
 
-    os.makedirs((tmp_dir / ".dvc" / "tmp" / "machine" / "terraform" / "foo"))
+    os.makedirs(tmp_dir / ".dvc" / "tmp" / "machine" / "terraform" / "foo")
 
     assert main(["machine", "rename", "foo", "bar"]) == 0
     cap = capsys.readouterr()
     assert "Rename machine 'foo' to 'bar'." in cap.out
     assert config_file.read_text() == machine_config.replace("foo", "bar")
-    assert not (
-        tmp_dir / ".dvc" / "tmp" / "machine" / "terraform" / "foo"
-    ).exists()
-    assert (
-        tmp_dir / ".dvc" / "tmp" / "machine" / "terraform" / "bar"
-    ).exists()
+    assert not (tmp_dir / ".dvc" / "tmp" / "machine" / "terraform" / "foo").exists()
+    assert (tmp_dir / ".dvc" / "tmp" / "machine" / "terraform" / "bar").exists()
 
 
 def test_machine_rename_none_exist(tmp_dir, scm, dvc, caplog):
     config_alice = BASIC_CONFIG.replace("foo", "alice")
     config_file = tmp_dir / ".dvc" / "config"
     config_file.write_text(config_alice)
     assert main(["machine", "rename", "foo", "bar"]) == 251
@@ -150,19 +143,19 @@
     config_file = tmp_dir / ".dvc" / "config"
     config_file.write_text(config_bar)
     assert main(["machine", "rename", "foo", "bar"]) == 251
     assert config_file.read_text() == config_bar
     assert "Machine 'bar' already exists." in caplog.text
 
 
-def test_machine_rename_error(
-    tmp_dir, scm, dvc, machine_config, caplog, mocker
-):
+def test_machine_rename_error(tmp_dir, scm, dvc, machine_config, caplog, mocker):
+    import tpi
+
     config_file = tmp_dir / ".dvc" / "config"
-    os.makedirs((tmp_dir / ".dvc" / "tmp" / "machine" / "terraform" / "foo"))
+    os.makedirs(tmp_dir / ".dvc" / "tmp" / "machine" / "terraform" / "foo")
 
     def cmd_error(self, source, destination, **kwargs):
         raise tpi.TPIError("test error")
 
     mocker.patch.object(tpi.terraform.TerraformBackend, "state_mv", cmd_error)
 
     assert main(["machine", "rename", "foo", "bar"]) == 251
```

### Comparing `dvc-2.9.5/tests/func/machine/test_machine_status.py` & `dvc-3.0.0a0/tests/func/machine/test_machine_status.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,23 +1,22 @@
 from dvc.cli import main
 from dvc.ui import ui
 from tests.utils import console_width
 
 
 def test_status(tmp_dir, scm, dvc, machine_config, machine_instance, capsys):
-
     assert main(["machine", "add", "bar", "aws"]) == 0
     with console_width(ui.rich_console, 255):
         assert main(["machine", "status"]) == 0
     cap = capsys.readouterr()
     assert (
         "name    instance    status    cloud    instance_ip      "
-        "instance_type    instance_hdd_size    instance_gpu"
-    ) in cap.out
+        "instance_type    instance_hdd_size    instance_gpu" in cap.out
+    )
     assert (
         "bar     -           offline   -        -                "
-        "-                -                    -"
-    ) in cap.out
+        "-                -                    -" in cap.out
+    )
     assert (
         "foo     num_1       running   aws      123.123.123.123  "
-        "m                35                   None"
-    ) in cap.out
+        "m                35                   None" in cap.out
+    )
```

### Comparing `dvc-2.9.5/tests/func/metrics/test_diff.py` & `dvc-3.0.0a0/tests/func/metrics/test_diff.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,14 @@
 import json
+from os.path import join
+
+import pytest
 
 from dvc.cli import main
+from dvc.utils import relpath
 
 
 def test_metrics_diff_simple(tmp_dir, scm, dvc, run_copy_metrics):
     def _gen(val):
         tmp_dir.gen({"m_temp.yaml": str(val)})
         run_copy_metrics("m_temp.yaml", "m.yaml", metrics=["m.yaml"])
         dvc.scm.commit(str(val))
@@ -18,50 +22,54 @@
     assert dvc.metrics.diff(a_rev="HEAD~2") == expected
 
 
 def test_metrics_diff_yaml(tmp_dir, scm, dvc, run_copy_metrics):
     def _gen(val):
         metrics = {"a": {"b": {"c": val, "d": 1, "e": str(val)}}}
         (tmp_dir / "m_temp.yaml").dump(metrics)
-        run_copy_metrics(
-            "m_temp.yaml", "m.yaml", metrics=["m.yaml"], commit=str(val)
-        )
+        run_copy_metrics("m_temp.yaml", "m.yaml", metrics=["m.yaml"], commit=str(val))
 
     _gen(1)
     _gen(2)
     _gen(3)
 
-    expected = {"m.yaml": {"a.b.c": {"old": 1, "new": 3, "diff": 2}}}
+    expected = {
+        "m.yaml": {
+            "a.b.e": {"old": "1", "new": "3"},
+            "a.b.c": {"old": 1, "new": 3, "diff": 2},
+        }
+    }
 
     assert dvc.metrics.diff(a_rev="HEAD~2") == expected
 
 
 def test_metrics_diff_json(tmp_dir, scm, dvc, run_copy_metrics):
     def _gen(val):
         metrics = {"a": {"b": {"c": val, "d": 1, "e": str(val)}}}
         (tmp_dir / "m_temp.json").dump(metrics)
-        run_copy_metrics(
-            "m_temp.json", "m.json", metrics=["m.json"], commit=str(val)
-        )
+        run_copy_metrics("m_temp.json", "m.json", metrics=["m.json"], commit=str(val))
 
     _gen(1)
     _gen(2)
     _gen(3)
 
-    expected = {"m.json": {"a.b.c": {"old": 1, "new": 3, "diff": 2}}}
+    expected = {
+        "m.json": {
+            "a.b.e": {"old": "1", "new": "3"},
+            "a.b.c": {"old": 1, "new": 3, "diff": 2},
+        }
+    }
     assert dvc.metrics.diff(a_rev="HEAD~2") == expected
 
 
 def test_metrics_diff_json_unchanged(tmp_dir, scm, dvc, run_copy_metrics):
     def _gen(val):
         metrics = {"a": {"b": {"c": val, "d": 1, "e": str(val)}}}
         (tmp_dir / "m_temp.json").dump(metrics)
-        run_copy_metrics(
-            "m_temp.json", "m.json", metrics=["m.json"], commit=str(val)
-        )
+        run_copy_metrics("m_temp.json", "m.json", metrics=["m.json"], commit=str(val))
 
     _gen(1)
     _gen(2)
     _gen(1)
 
     assert dvc.metrics.diff(a_rev="HEAD~2") == {}
 
@@ -76,14 +84,15 @@
         commit="add metrics",
     )
 
     (tmp_dir / "m.json").write_text(json.dumps(metrics) + "ma\nlformed\n")
 
     assert dvc.metrics.diff() == {
         "m.json": {
+            "a.b.e": {"old": "3", "new": None},
             "a.b.c": {"old": 1, "new": None},
             "a.b.d": {"old": 1, "new": None},
         }
     }
 
 
 def test_metrics_diff_no_metrics(tmp_dir, scm, dvc):
@@ -94,14 +103,15 @@
 def test_metrics_diff_new_metric(tmp_dir, scm, dvc, run_copy_metrics):
     metrics = {"a": {"b": {"c": 1, "d": 1, "e": "3"}}}
     (tmp_dir / "m_temp.json").dump(metrics)
     run_copy_metrics("m_temp.json", "m.json", metrics_no_cache=["m.json"])
 
     assert dvc.metrics.diff() == {
         "m.json": {
+            "a.b.e": {"old": None, "new": "3"},
             "a.b.c": {"old": None, "new": 1},
             "a.b.d": {"old": None, "new": 1},
         }
     }
 
 
 def test_metrics_diff_deleted_metric(tmp_dir, scm, dvc, run_copy_metrics):
@@ -114,14 +124,15 @@
         commit="add metrics",
     )
 
     (tmp_dir / "m.json").unlink()
 
     assert dvc.metrics.diff() == {
         "m.json": {
+            "a.b.e": {"old": "3", "new": None},
             "a.b.c": {"old": 1, "new": None},
             "a.b.d": {"old": 1, "new": None},
         }
     }
 
 
 def test_metrics_diff_with_unchanged(tmp_dir, scm, dvc, run_copy_metrics):
@@ -141,17 +152,16 @@
             "foo": {"old": 1, "new": 3, "diff": 2},
             "xyz": {"old": 10, "new": 10, "diff": 0},
         }
     }
 
 
 def test_no_commits(tmp_dir):
-    from scmrepo.git import Git
-
     from dvc.repo import Repo
+    from dvc.scm import Git
 
     git = Git.init(tmp_dir.fs_path)
     assert git.no_commits
 
     assert Repo.init().metrics.diff() == {}
 
 
@@ -184,25 +194,59 @@
 
     caplog.clear()
     capsys.readouterr()  # clearing the buffer
     assert main(["metrics", "diff", "HEAD~2"]) == 0
 
     captured = capsys.readouterr()
 
-    assert captured.out == (
-        "Path    Metric    HEAD~2    workspace    Change\n"
+    assert (
+        captured.out == "Path    Metric    HEAD~2    workspace    Change\n"
         "m.yaml  foo       1.23457   3.45679      2.22222\n"
     )
 
 
 def test_metrics_diff_non_metrics(tmp_dir, scm, dvc):
     def _gen(val):
         tmp_dir.scm_gen({"some_file.yaml": f"foo: {val}"}, commit=str(val))
 
     _gen(1)
     _gen(2)
     _gen(3)
 
     result = dvc.metrics.diff(targets=["some_file.yaml"], a_rev="HEAD~2")
-    assert result == {
-        "some_file.yaml": {"foo": {"old": 1, "new": 3, "diff": 2}}
+    assert result == {"some_file.yaml": {"foo": {"old": 1, "new": 3, "diff": 2}}}
+
+
+@pytest.mark.parametrize(
+    "dvcfile, metrics_file",
+    [
+        ("dvc.yaml", "my_metrics.yaml"),
+        ("dir/dvc.yaml", "my_metrics.yaml"),
+        ("dir/dvc.yaml", join("..", "my_metrics.yaml")),
+    ],
+)
+def test_diff_top_level_metrics(tmp_dir, dvc, scm, dvcfile, metrics_file):
+    directory = (tmp_dir / dvcfile).parent
+    directory.mkdir(exist_ok=True)
+    (tmp_dir / dvcfile).dump({"metrics": [metrics_file]})
+
+    metrics_file = directory / metrics_file
+    metrics_file.dump({"foo": 3})
+    scm.add_commit([metrics_file, tmp_dir / dvcfile], message="add metrics")
+
+    metrics_file.dump({"foo": 5})
+    assert dvc.metrics.diff() == {
+        relpath(directory / metrics_file): {"foo": {"diff": 2, "new": 5, "old": 3}}
     }
+
+
+def test_metrics_diff_active_branch_unchanged(tmp_dir, scm, dvc, run_copy_metrics):
+    def _gen(val):
+        metrics = {"a": {"b": {"c": val, "d": 1, "e": str(val)}}}
+        (tmp_dir / "m_temp.yaml").dump(metrics)
+        run_copy_metrics("m_temp.yaml", "m.yaml", metrics=["m.yaml"], commit=str(val))
+
+    _gen(1)
+    _gen(2)
+    _gen(1)
+
+    assert dvc.metrics.diff(a_rev=tmp_dir.scm.active_branch()) == {}
```

### Comparing `dvc-2.9.5/tests/func/metrics/test_show.py` & `dvc-3.0.0a0/tests/func/metrics/test_show.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,49 +1,78 @@
-import logging
 import os
 
 import pytest
 from funcy import get_in
 
-from dvc.dvcfile import PIPELINE_FILE
+from dvc.dvcfile import PROJECT_FILE
 from dvc.exceptions import OverlappingOutputPathsError
 from dvc.repo import Repo
 from dvc.utils.fs import remove
-from dvc.utils.serialize import YAMLFileCorruptedError
+from dvc.utils.serialize import JSONFileCorruptedError, YAMLFileCorruptedError
 
 
 def test_show_simple(tmp_dir, dvc, run_copy_metrics):
     tmp_dir.gen("metrics_t.yaml", "1.1")
+    run_copy_metrics("metrics_t.yaml", "metrics.yaml", metrics=["metrics.yaml"])
+    assert dvc.metrics.show() == {"": {"data": {"metrics.yaml": {"data": 1.1}}}}
+
+
+def test_show_simple_from_subdir(tmp_dir, dvc, run_copy_metrics):
+    subdir = tmp_dir / "subdir"
+    subdir.mkdir()
+    tmp_dir.gen("metrics_t.yaml", "1.1")
     run_copy_metrics(
-        "metrics_t.yaml", "metrics.yaml", metrics=["metrics.yaml"]
+        "metrics_t.yaml",
+        "subdir/metrics.yaml",
+        metrics=["subdir/metrics.yaml"],
     )
-    assert dvc.metrics.show() == {
-        "": {"data": {"metrics.yaml": {"data": 1.1}}}
-    }
+
+    expected_path = os.path.join("subdir", "metrics.yaml")
+    assert dvc.metrics.show() == {"": {"data": {expected_path: {"data": 1.1}}}}
+
+    expected_path = os.path.join("..", "subdir", "metrics.yaml")
+    with subdir.chdir():
+        assert dvc.metrics.show() == {"": {"data": {expected_path: {"data": 1.1}}}}
+    subdir2 = tmp_dir / "subdir2"
+    subdir2.mkdir()
+    with subdir2.chdir():
+        assert dvc.metrics.show() == {"": {"data": {expected_path: {"data": 1.1}}}}
 
 
 def test_show(tmp_dir, dvc, run_copy_metrics):
     tmp_dir.gen("metrics_t.yaml", "foo: 1.1")
-    run_copy_metrics(
-        "metrics_t.yaml", "metrics.yaml", metrics=["metrics.yaml"]
-    )
+    run_copy_metrics("metrics_t.yaml", "metrics.yaml", metrics=["metrics.yaml"])
     assert dvc.metrics.show() == {
         "": {"data": {"metrics.yaml": {"data": {"foo": 1.1}}}}
     }
 
 
+def test_show_toml(tmp_dir, dvc, run_copy_metrics):
+    tmp_dir.gen("metrics_t.toml", "[foo]\nbar = 1.2")
+    run_copy_metrics("metrics_t.toml", "metrics.toml", metrics=["metrics.toml"])
+    assert dvc.metrics.show() == {
+        "": {"data": {"metrics.toml": {"data": {"foo": {"bar": 1.2}}}}}
+    }
+
+
+def test_show_targets(tmp_dir, dvc, run_copy_metrics):
+    tmp_dir.gen("metrics_t.yaml", "foo: 1.1")
+    run_copy_metrics("metrics_t.yaml", "metrics.yaml", metrics=["metrics.yaml"])
+    expected = {"": {"data": {"metrics.yaml": {"data": {"foo": 1.1}}}}}
+    assert dvc.metrics.show(targets=["metrics.yaml"]) == expected
+    assert dvc.metrics.show(targets=(tmp_dir / "metrics.yaml").fs_path) == expected
+
+
 def test_show_multiple(tmp_dir, dvc, run_copy_metrics):
     tmp_dir.gen("foo_temp", "foo: 1\n")
     tmp_dir.gen("baz_temp", "baz: 2\n")
     run_copy_metrics("foo_temp", "foo", fname="foo.dvc", metrics=["foo"])
     run_copy_metrics("baz_temp", "baz", fname="baz.dvc", metrics=["baz"])
     assert dvc.metrics.show() == {
-        "": {
-            "data": {"foo": {"data": {"foo": 1}}, "baz": {"data": {"baz": 2}}}
-        }
+        "": {"data": {"foo": {"data": {"foo": 1}}, "baz": {"data": {"baz": 2}}}}
     }
 
 
 def test_show_branch(tmp_dir, scm, dvc, run_copy_metrics):
     tmp_dir.gen("metrics_temp.yaml", "foo: 1")
     run_copy_metrics(
         "metrics_temp.yaml", "metrics.yaml", metrics_no_cache=["metrics.yaml"]
@@ -92,17 +121,15 @@
         "workspace": {"data": {expected_path: {"data": {"foo": 1}}}},
         "v1": {"data": {expected_path: {"data": {"foo": 1}}}},
     }
 
 
 def test_missing_cache(tmp_dir, dvc, run_copy_metrics):
     tmp_dir.gen("metrics_t.yaml", "1.1")
-    run_copy_metrics(
-        "metrics_t.yaml", "metrics.yaml", metrics=["metrics.yaml"]
-    )
+    run_copy_metrics("metrics_t.yaml", "metrics.yaml", metrics=["metrics.yaml"])
 
     # This one should be skipped
     stage = run_copy_metrics(
         "metrics_t.yaml", "metrics2.yaml", metrics=["metrics2.yaml"]
     )
     remove(stage.outs[0].fspath)
     remove(stage.outs[0].cache_path)
@@ -153,32 +180,24 @@
     }
 
     if not use_dvc:
         assert not (tmp_dir / ".dvc").exists()
 
 
 def test_non_metric_and_recurisve_show(tmp_dir, dvc, run_copy_metrics):
-    tmp_dir.gen(
-        {"metrics_t.yaml": "foo: 1.1", "metrics": {"metric1.yaml": "bar: 1.2"}}
-    )
+    tmp_dir.gen({"metrics_t.yaml": "foo: 1.1", "metrics": {"metric1.yaml": "bar: 1.2"}})
 
     metric2 = os.fspath(tmp_dir / "metrics" / "metric2.yaml")
     run_copy_metrics("metrics_t.yaml", metric2, metrics=[metric2])
 
-    assert dvc.metrics.show(
-        targets=["metrics_t.yaml", "metrics"], recursive=True
-    ) == {
+    assert dvc.metrics.show(targets=["metrics_t.yaml", "metrics"], recursive=True) == {
         "": {
             "data": {
-                os.path.join("metrics", "metric1.yaml"): {
-                    "data": {"bar": 1.2}
-                },
-                os.path.join("metrics", "metric2.yaml"): {
-                    "data": {"foo": 1.1}
-                },
+                os.path.join("metrics", "metric1.yaml"): {"data": {"bar": 1.2}},
+                os.path.join("metrics", "metric2.yaml"): {"data": {"foo": 1.1}},
                 "metrics_t.yaml": {"data": {"foo": 1.1}},
             }
         }
     }
 
 
 def test_show_falsey(tmp_dir, dvc):
@@ -198,38 +217,29 @@
     }
 
 
 def test_show_malformed_metric(tmp_dir, scm, dvc, caplog):
     tmp_dir.gen("metric.json", '{"m":1')
 
     assert isinstance(
-        dvc.metrics.show(targets=["metric.json"])[""]["data"]["metric.json"][
-            "error"
-        ],
-        YAMLFileCorruptedError,
+        dvc.metrics.show(targets=["metric.json"])[""]["data"]["metric.json"]["error"],
+        JSONFileCorruptedError,
     )
 
 
-def test_metrics_show_no_target(tmp_dir, dvc, caplog):
-    with caplog.at_level(logging.WARNING):
-        assert dvc.metrics.show(targets=["metrics.json"]) == {"": {}}
-
-    assert (
-        "'metrics.json' was not found in current workspace." in caplog.messages
-    )
+def test_metrics_show_no_target(tmp_dir, dvc, capsys):
+    assert dvc.metrics.show(targets=["metrics.json"]) == {"": {}}
 
 
 def test_show_no_metrics_files(tmp_dir, dvc, caplog):
     assert dvc.metrics.show() == {"": {}}
 
 
 @pytest.mark.parametrize("clear_before_run", [True, False])
-def test_metrics_show_overlap(
-    tmp_dir, dvc, run_copy_metrics, clear_before_run
-):
+def test_metrics_show_overlap(tmp_dir, dvc, run_copy_metrics, clear_before_run):
     data_dir = tmp_dir / "data"
     data_dir.mkdir()
 
     (data_dir / "m1_temp.yaml").dump({"a": {"b": {"c": 2, "d": 1}}})
     run_copy_metrics(
         str(data_dir / "m1_temp.yaml"),
         str(data_dir / "m1.yaml"),
@@ -245,32 +255,30 @@
             "outs": ["data"],
         }
 
     # running by clearing and not clearing stuffs
     # so as it works even for optimized cases
     if clear_before_run:
         remove(data_dir)
-        remove(dvc.odb.local.cache_dir)
+        remove(dvc.cache.local.path)
 
     dvc._reset()
 
     res = dvc.metrics.show()
     assert isinstance(res[""]["error"], OverlappingOutputPathsError)
 
 
 @pytest.mark.parametrize(
     "file,error_path",
     (
-        (PIPELINE_FILE, ["workspace", "error"]),
+        (PROJECT_FILE, ["workspace", "error"]),
         ("metrics.yaml", ["workspace", "data", "metrics.yaml", "error"]),
     ),
 )
-def test_log_errors(
-    tmp_dir, scm, dvc, capsys, run_copy_metrics, file, error_path
-):
+def test_log_errors(tmp_dir, scm, dvc, capsys, run_copy_metrics, file, error_path):
     tmp_dir.gen("metrics_t.yaml", "m: 1.1")
     run_copy_metrics(
         "metrics_t.yaml",
         "metrics.yaml",
         metrics=["metrics.yaml"],
         single_stage=False,
         name="train",
@@ -282,10 +290,9 @@
 
     result = dvc.metrics.show(revs=["v1"])
 
     _, error = capsys.readouterr()
 
     assert isinstance(get_in(result, error_path), YAMLFileCorruptedError)
     assert (
-        "DVC failed to load some metrics for following revisions: 'workspace'."
-        in error
+        "DVC failed to load some metrics for following revisions: 'workspace'." in error
     )
```

### Comparing `dvc-2.9.5/tests/func/objects/db/test_index.py` & `dvc-3.0.0a0/tests/func/data/db/test_index.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,41 +1,39 @@
+import os
+
 import pytest
 
-from dvc.data.db import get_index
 from dvc.exceptions import DownloadError, UploadError
-from dvc.fs.local import LocalFileSystem
 from dvc.utils.fs import remove
-from tests.utils import clean_staging
+from dvc_data.hashfile.db import get_index
 
 
 @pytest.fixture
 def index(dvc, local_remote, mocker):
     odb = dvc.cloud.get_remote_odb("upstream")
     return get_index(odb)
 
 
 def test_indexed_on_status(tmp_dir, dvc, index):
     foo = tmp_dir.dvc_gen({"foo": "foo content"})[0].outs[0]
     bar = tmp_dir.dvc_gen({"bar": {"baz": "baz content"}})[0].outs[0]
     baz_hash = bar.obj._trie.get(("baz",))[1]
-    clean_staging()
     dvc.push()
     index.clear()
 
     dvc.status(cloud=True)
     assert {bar.hash_info.value, baz_hash.value} == set(index.hashes())
     assert [bar.hash_info.value] == list(index.dir_hashes())
     assert foo.hash_info.value not in index.hashes()
 
 
 def test_indexed_on_push(tmp_dir, dvc, index):
     foo = tmp_dir.dvc_gen({"foo": "foo content"})[0].outs[0]
     bar = tmp_dir.dvc_gen({"bar": {"baz": "baz content"}})[0].outs[0]
     baz_hash = bar.obj._trie.get(("baz",))[1]
-    clean_staging()
 
     dvc.push()
     assert {bar.hash_info.value, baz_hash.value} == set(index.hashes())
     assert [bar.hash_info.value] == list(index.dir_hashes())
     assert foo.hash_info.value not in index.hashes()
 
 
@@ -56,34 +54,66 @@
     assert not list(index.hashes())
 
 
 def test_clear_on_download_err(tmp_dir, dvc, index, mocker):
     out = tmp_dir.dvc_gen({"dir": {"foo": "foo content"}})[0].outs[0]
     dvc.push()
 
-    for _, _, oid in out.obj:
-        remove(dvc.odb.local.get(oid).fs_path)
+    for _, _, hi in out.obj:
+        remove(dvc.cache.local.get(hi.value).path)
     remove(out.fs_path)
 
     assert list(index.hashes())
 
-    mocker.patch("dvc.fs.utils.transfer", side_effect=Exception)
+    def unreliable_download(_from_fs, from_info, _to_fs, to_info, **kwargs):
+        on_error = kwargs["on_error"]
+        assert on_error
+        if isinstance(from_info, str):
+            from_info = [from_info]
+        if isinstance(to_info, str):
+            to_info = [to_info]
+        for from_i, to_i in zip(from_info, to_info):
+            on_error(from_i, to_i, Exception())
+
+    mocker.patch("dvc_objects.fs.generic.transfer", unreliable_download)
     with pytest.raises(DownloadError):
         dvc.pull()
     assert not list(index.hashes())
 
 
 def test_partial_upload(tmp_dir, dvc, index, mocker):
+    from dvc_objects.fs import generic
+
     tmp_dir.dvc_gen({"foo": "foo content"})
-    tmp_dir.dvc_gen({"bar": {"baz": "baz content"}})
+    baz = tmp_dir.dvc_gen({"bar": {"baz": "baz content"}})[0].outs[0]
+
+    original = generic.transfer
+    odb = dvc.cloud.get_remote_odb("upstream")
 
-    original = LocalFileSystem.upload
+    def unreliable_upload(from_fs, from_info, to_fs, to_info, **kwargs):
+        on_error = kwargs["on_error"]
+        assert on_error
+        if isinstance(from_info, str):
+            from_info = [from_info]
+        else:
+            from_info = list(from_info)
+        if isinstance(to_info, str):
+            to_info = [to_info]
+        else:
+            to_info = list(to_info)
+        for i in range(len(from_info) - 1, -1, -1):
+            from_i = from_info[i]
+            to_i = to_info[i]
+            if os.path.abspath(to_i) == os.path.abspath(
+                odb.get(baz.hash_info.value).path
+            ):
+                if on_error:
+                    on_error(from_i, to_i, Exception("stop baz"))
+                del from_info[i]
+                del to_info[i]
 
-    def unreliable_upload(self, from_file, to_info, name=None, **kwargs):
-        if "baz" in name:
-            raise Exception("stop baz")
-        return original(self, from_file, to_info, name, **kwargs)
+        return original(from_fs, from_info, to_fs, to_info, **kwargs)
 
-    mocker.patch("dvc.fs.utils.transfer", unreliable_upload)
+    mocker.patch("dvc_objects.fs.generic.transfer", unreliable_upload)
     with pytest.raises(UploadError):
         dvc.push()
     assert not list(index.hashes())
```

### Comparing `dvc-2.9.5/tests/func/params/test_diff.py` & `dvc-3.0.0a0/tests/func/params/test_diff.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,7 +1,11 @@
+from os.path import join
+
+import pytest
+
 from dvc.utils import relpath
 
 
 def test_diff_no_params(tmp_dir, scm, dvc):
     assert dvc.params.diff() == {}
 
 
@@ -32,39 +36,33 @@
     dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
     scm.add(["params.yaml", "Dvcfile"])
     scm.commit("bar")
 
     tmp_dir.scm_gen("params.yaml", "foo: baz", commit="baz")
     tmp_dir.gen("params.yaml", "foo: qux")
 
-    assert dvc.params.diff() == {
-        "params.yaml": {"foo": {"old": "baz", "new": "qux"}}
-    }
+    assert dvc.params.diff() == {"params.yaml": {"foo": {"old": "baz", "new": "qux"}}}
 
 
 def test_diff_new(tmp_dir, scm, dvc):
     tmp_dir.gen("params.yaml", "foo: bar")
     dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
 
-    assert dvc.params.diff() == {
-        "params.yaml": {"foo": {"old": None, "new": "bar"}}
-    }
+    assert dvc.params.diff() == {"params.yaml": {"foo": {"old": None, "new": "bar"}}}
 
 
 def test_diff_deleted(tmp_dir, scm, dvc):
     tmp_dir.gen("params.yaml", "foo: bar")
     dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
     scm.add(["params.yaml", "Dvcfile"])
     scm.commit("bar")
 
     (tmp_dir / "params.yaml").unlink()
 
-    assert dvc.params.diff() == {
-        "params.yaml": {"foo": {"old": "bar", "new": None}}
-    }
+    assert dvc.params.diff() == {"params.yaml": {"foo": {"old": "bar", "new": None}}}
 
 
 def test_diff_list(tmp_dir, scm, dvc):
     tmp_dir.gen("params.yaml", "foo:\n- bar\n- baz")
     dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
     scm.add(["params.yaml", "Dvcfile"])
     scm.commit("foo")
@@ -105,34 +103,33 @@
             "foo": {"old": "bar", "new": "qux"},
             "xyz": {"old": "val", "new": "val"},
         }
     }
 
 
 def test_pipeline_tracked_params(tmp_dir, scm, dvc, run_copy):
-    from dvc.dvcfile import PIPELINE_FILE
+    from dvc.dvcfile import PROJECT_FILE
 
     tmp_dir.gen({"foo": "foo", "params.yaml": "foo: bar\nxyz: val"})
     run_copy("foo", "bar", name="copy-foo-bar", params=["foo,xyz"])
 
-    scm.add(["params.yaml", PIPELINE_FILE])
+    scm.add(["params.yaml", PROJECT_FILE])
     scm.commit("add stage")
 
     tmp_dir.scm_gen("params.yaml", "foo: baz\nxyz: val", commit="baz")
     tmp_dir.scm_gen("params.yaml", "foo: qux\nxyz: val", commit="qux")
 
     assert dvc.params.diff(a_rev="HEAD~2") == {
         "params.yaml": {"foo": {"old": "bar", "new": "qux"}}
     }
 
 
 def test_no_commits(tmp_dir):
-    from scmrepo.git import Git
-
     from dvc.repo import Repo
+    from dvc.scm import Git
 
     git = Git.init(tmp_dir.fs_path)
     assert git.no_commits
 
     assert Repo.init().params.diff() == {}
 
 
@@ -158,31 +155,29 @@
     }
     scm.add(["dvc.yaml", "test_params.yaml"])
     scm.commit("added stages")
 
     param_data["vars"]["model1"]["epoch"] = 20
     (tmp_dir / params_file).dump(param_data)
     assert dvc.params.diff() == {
-        "test_params.yaml": {
-            "vars.model1.epoch": {"new": 20, "old": 15, "diff": 5}
-        }
+        "test_params.yaml": {"vars.model1.epoch": {"new": 20, "old": 15, "diff": 5}}
     }
 
     data_dir = tmp_dir / "data"
     data_dir.mkdir()
     with data_dir.chdir():
         assert dvc.params.diff() == {
             relpath(params_file): {
                 "vars.model1.epoch": {"new": 20, "old": 15, "diff": 5}
             }
         }
 
 
 def test_diff_targeted(tmp_dir, scm, dvc, run_copy):
-    from dvc.dvcfile import PIPELINE_FILE
+    from dvc.dvcfile import PROJECT_FILE
 
     tmp_dir.gen(
         {
             "foo": "foo",
             "params.yaml": "foo: bar",
             "other_params.yaml": "xyz: val",
         }
@@ -190,15 +185,15 @@
     run_copy(
         "foo",
         "bar",
         name="copy-foo-bar",
         params=["foo", "other_params.yaml:xyz"],
     )
 
-    scm.add(["params.yaml", "other_params.yaml", PIPELINE_FILE])
+    scm.add(["params.yaml", "other_params.yaml", PROJECT_FILE])
     scm.commit("add stage")
 
     tmp_dir.scm_gen(
         {"params.yaml": "foo: baz", "other_params.yaml": "xyz: val2"},
         commit="baz",
     )
     tmp_dir.scm_gen(
@@ -214,7 +209,59 @@
     assert dvc.params.diff(a_rev="HEAD~2", targets=["params.yaml"]) == {
         "params.yaml": {"foo": {"old": "bar", "new": "qux"}}
     }
 
     assert dvc.params.diff(a_rev="HEAD~2", targets=["other_params.yaml"]) == {
         "other_params.yaml": {"xyz": {"old": "val", "new": "val3"}}
     }
+
+
+@pytest.mark.parametrize("file", ["params.yaml", "other_params.yaml"])
+def test_diff_without_targets_specified(tmp_dir, dvc, scm, file):
+    params_file = tmp_dir / file
+    params_file.dump({"foo": {"bar": "bar"}, "x": "0"})
+    dvc.stage.add(
+        name="test",
+        cmd=f"echo {file}",
+        params=[{file: None}],
+    )
+    scm.add_commit([params_file, "dvc.yaml"], message="foo")
+
+    params_file.dump({"foo": {"bar": "baz"}, "y": "100"})
+    assert dvc.params.diff() == {
+        file: {
+            "foo.bar": {"new": "baz", "old": "bar"},
+            "x": {"new": None, "old": "0"},
+            "y": {"new": "100", "old": None},
+        }
+    }
+
+
+@pytest.mark.parametrize(
+    "dvcfile, params_file",
+    [
+        ("dvc.yaml", "my_params.yaml"),
+        ("dir/dvc.yaml", "my_params.yaml"),
+        ("dir/dvc.yaml", join("..", "my_params.yaml")),
+    ],
+)
+def test_diff_top_level_params(tmp_dir, dvc, scm, dvcfile, params_file):
+    directory = (tmp_dir / dvcfile).parent
+    directory.mkdir(exist_ok=True)
+    (tmp_dir / dvcfile).dump({"params": [params_file]})
+
+    params_file = directory / params_file
+    params_file.dump({"foo": 3})
+    scm.add_commit([params_file, tmp_dir / dvcfile], message="add params")
+
+    params_file.dump({"foo": 5})
+    assert dvc.params.diff() == {
+        relpath(directory / params_file): {"foo": {"diff": 2, "new": 5, "old": 3}}
+    }
+
+
+def test_diff_active_branch_no_changes(tmp_dir, scm, dvc):
+    tmp_dir.gen("params.yaml", "foo: bar")
+    dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
+    scm.add(["params.yaml", "Dvcfile"])
+    scm.commit("bar")
+    assert dvc.params.diff(a_rev=tmp_dir.scm.active_branch()) == {}
```

### Comparing `dvc-2.9.5/tests/func/parsing/__init__.py` & `dvc-3.0.0a0/tests/func/parsing/__init__.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/func/parsing/test_errors.py` & `dvc-3.0.0a0/tests/func/parsing/test_errors.py`

 * *Files 3% similar despite different names*

```diff
@@ -25,31 +25,31 @@
 @pytest.mark.parametrize("vars_", ["${file}_params.yaml", {"foo": "${foo}"}])
 def test_vars_interpolation_errors(tmp_dir, dvc, vars_):
     definition = make_entry_definition(tmp_dir, "build", {"vars": [vars_]})
     with pytest.raises(ResolveError) as exc_info:
         definition.resolve()
 
     assert (
-        str(exc_info.value)
-        == "failed to parse 'stages.build.vars' in 'dvc.yaml': "
+        str(exc_info.value) == "failed to parse 'stages.build.vars' in 'dvc.yaml': "
         "interpolating is not allowed"
     )
 
 
 def test_failed_to_interpolate(tmp_dir, dvc):
     context = Context(models={"foo": "bar"})
     definition = make_entry_definition(
         tmp_dir, "build", {"cmd": "echo ${models.foo.}"}, context
     )
 
     with pytest.raises(ResolveError) as exc_info:
         definition.resolve()
 
-    assert escape_ansi(str(exc_info.value)) == (
-        "failed to parse 'stages.build.cmd' in 'dvc.yaml':\n"
+    assert (
+        escape_ansi(str(exc_info.value))
+        == "failed to parse 'stages.build.cmd' in 'dvc.yaml':\n"
         "${models.foo.}\n"
         "            ^\n"
         "ParseException: Expected end of text, found '.'"
         "  (at char 12), (line:1, col:13)"
     )
     assert definition.context == {"models": {"foo": "bar"}}
 
@@ -60,16 +60,16 @@
         "build",
         {"vars": ["not_existing_params.yaml"], "cmd": "echo ${models.foo}"},
     )
 
     with pytest.raises(ResolveError) as exc_info:
         definition.resolve()
 
-    assert str(exc_info.value) == (
-        "failed to parse stage 'build' in 'dvc.yaml': "
+    assert (
+        str(exc_info.value) == "failed to parse stage 'build' in 'dvc.yaml': "
         "'not_existing_params.yaml' does not exist"
     )
     assert not definition.context
 
 
 def test_specified_key_does_not_exist(tmp_dir, dvc):
     definition = make_entry_definition(
@@ -77,31 +77,33 @@
         "build",
         {"cmd": "echo ${models.foobar}"},
         Context(models={"foo": "foo"}),
     )
     with pytest.raises(ResolveError) as exc_info:
         definition.resolve()
 
-    assert str(exc_info.value) == (
-        "failed to parse 'stages.build.cmd' in 'dvc.yaml': "
+    assert (
+        str(exc_info.value) == "failed to parse 'stages.build.cmd' in 'dvc.yaml': "
         "Could not find 'models.foobar'"
     )
     assert definition.context == {"models": {"foo": "foo"}}
 
 
 @pytest.mark.parametrize(
     "wdir, expected_msg",
     [
         ("${models[foobar]}", " Could not find 'models.foobar'"),
         (
             "${models.foo]}",
-            "\n${models.foo]}\n"
-            "            ^\n"
-            "ParseException: Expected end of text, found ']'"
-            "  (at char 12), (line:1, col:13)",
+            (
+                "\n${models.foo]}\n"
+                "            ^\n"
+                "ParseException: Expected end of text, found ']'"
+                "  (at char 12), (line:1, col:13)"
+            ),
         ),
     ],
 )
 def test_wdir_failed_to_interpolate(tmp_dir, dvc, wdir, expected_msg):
     definition = make_entry_definition(
         tmp_dir,
         "build",
@@ -115,100 +117,114 @@
         "failed to parse 'stages.build.wdir' in 'dvc.yaml':" + expected_msg
     )
     assert definition.context == {"models": {"bar": "bar"}}
 
 
 def test_interpolate_non_string(tmp_dir, dvc):
     definition = make_entry_definition(
-        tmp_dir, "build", {"cmd": "echo ${models}"}, Context(models={})
+        tmp_dir, "build", {"outs": "${models}"}, Context(models={})
     )
     with pytest.raises(ResolveError) as exc_info:
         definition.resolve()
 
-    assert str(exc_info.value) == (
-        "failed to parse 'stages.build.cmd' in 'dvc.yaml':\n"
+    assert (
+        str(exc_info.value) == "failed to parse 'stages.build.outs' in 'dvc.yaml':\n"
         "Cannot interpolate data of type 'dict'"
     )
     assert definition.context == {"models": {}}
 
 
+def test_interpolate_nested_iterable(tmp_dir, dvc):
+    definition = make_entry_definition(
+        tmp_dir,
+        "build",
+        {"cmd": "echo ${models}"},
+        Context(models={"list": [1, [2, 3]]}),
+    )
+    with pytest.raises(ResolveError) as exc_info:
+        definition.resolve()
+
+    assert (
+        str(exc_info.value) == "failed to parse 'stages.build.cmd' in 'dvc.yaml':\n"
+        "Cannot interpolate nested iterable in 'list'"
+    )
+
+
 def test_partial_vars_doesnot_exist(tmp_dir, dvc):
     (tmp_dir / "test_params.yaml").dump({"sub1": "sub1", "sub2": "sub2"})
 
     definition = make_entry_definition(
         tmp_dir,
         "build",
         {"vars": ["test_params.yaml:sub3"], "cmd": "echo ${sub1} ${sub2}"},
     )
 
     with pytest.raises(ResolveError) as exc_info:
         definition.resolve()
 
-    assert str(exc_info.value) == (
-        "failed to parse stage 'build' in 'dvc.yaml': "
+    assert (
+        str(exc_info.value) == "failed to parse stage 'build' in 'dvc.yaml': "
         "could not find 'sub3' in 'test_params.yaml'"
     )
     assert not definition.context
 
 
 # Tests foreach generated stages and their error messages
 
 
 def test_foreach_data_syntax_error(tmp_dir, dvc):
     definition = make_foreach_def(tmp_dir, "build", "${syntax.[error}", {})
     with pytest.raises(ResolveError) as exc_info:
         definition.resolve_all()
 
-    assert escape_ansi(str(exc_info.value)) == (
-        "failed to parse 'stages.build.foreach' in 'dvc.yaml':\n"
+    assert (
+        escape_ansi(str(exc_info.value))
+        == "failed to parse 'stages.build.foreach' in 'dvc.yaml':\n"
         "${syntax.[error}\n"
         "        ^\n"
         "ParseException: Expected end of text, found '.'"
         "  (at char 8), (line:1, col:9)"
     )
 
 
 @pytest.mark.parametrize("key", ["modelss", "modelss.123"])
 def test_foreach_data_key_does_not_exists(tmp_dir, dvc, key):
     definition = make_foreach_def(tmp_dir, "build", embrace(key), {})
     with pytest.raises(ResolveError) as exc_info:
         definition.resolve_all()
-    assert str(exc_info.value) == (
-        "failed to parse 'stages.build.foreach' in 'dvc.yaml': "
+    assert (
+        str(exc_info.value) == "failed to parse 'stages.build.foreach' in 'dvc.yaml': "
         f"Could not find '{key}'"
     )
 
 
 @pytest.mark.parametrize(
     "foreach_data", ["${foo}", "${dct.model1}", "${lst.0}", "foobar"]
 )
 def test_foreach_data_expects_list_or_dict(tmp_dir, dvc, foreach_data):
-    context = Context(
-        {"foo": "bar", "dct": {"model1": "a-out"}, "lst": ["foo", "bar"]}
-    )
+    context = Context({"foo": "bar", "dct": {"model1": "a-out"}, "lst": ["foo", "bar"]})
     definition = make_foreach_def(tmp_dir, "build", foreach_data, {}, context)
     with pytest.raises(ResolveError) as exc_info:
         definition.resolve_all()
-    assert str(exc_info.value) == (
-        "failed to resolve 'stages.build.foreach' in 'dvc.yaml': "
+    assert (
+        str(exc_info.value)
+        == "failed to resolve 'stages.build.foreach' in 'dvc.yaml': "
         "expected list/dictionary, got str"
     )
 
 
 @pytest.mark.parametrize(
     "global_data, where",
     [
         ({"item": 10, "key": 10}, "item and key are"),
         ({"item": 10}, "item is"),
         ({"key": 5}, "key is"),
     ],
 )
-def test_foreach_overwriting_item_in_list(
-    tmp_dir, dvc, caplog, global_data, where
-):
+def test_foreach_overwriting_item_in_list(tmp_dir, dvc, caplog, global_data, where):
     context = Context(global_data)
     definition = make_foreach_def(
         tmp_dir, "build", {"model1": 10, "model2": 5}, {}, context
     )
     with caplog.at_level(logging.WARNING, logger="dvc.parsing"):
         definition.resolve_all()
 
@@ -222,16 +238,17 @@
     definition = make_foreach_def(
         tmp_dir, "build", ["foo", "bar"], {"cmd": "echo ${syntax.[error}"}
     )
 
     with pytest.raises(ResolveError) as exc_info:
         definition.resolve_all()
 
-    assert escape_ansi(str(exc_info.value)) == (
-        "failed to parse 'stages.build.cmd' in 'dvc.yaml':\n"
+    assert (
+        escape_ansi(str(exc_info.value))
+        == "failed to parse 'stages.build.cmd' in 'dvc.yaml':\n"
         "${syntax.[error}\n"
         "        ^\n"
         "ParseException: Expected end of text, found '.'"
         "  (at char 8), (line:1, col:9)"
     )
 
 
@@ -254,16 +271,17 @@
         {"cmd": embrace(key)},
         context,
     )
 
     with pytest.raises(ResolveError) as exc_info:
         definition.resolve_all()
 
-    assert str(exc_info.value) == (
-        f"failed to parse '{loc}' in 'dvc.yaml': Could not find '{key}'"
+    assert (
+        str(exc_info.value)
+        == f"failed to parse '{loc}' in 'dvc.yaml': Could not find '{key}'"
     )
 
     # should have no `item` and `key` even though it failed to resolve.
     assert context == {"foo": "bar"}
 
 
 @pytest.mark.parametrize(
@@ -293,16 +311,16 @@
 
     with pytest.raises(ResolveError) as exc_info:
         definition.resolve_all()
 
     message = str(exc_info.value)
     assert (
         "failed to parse stage 'build@model1' in 'dvc.yaml': "
-        "attempted to modify reserved"
-    ) in message
+        "attempted to modify reserved" in message
+    )
 
     key_or_keys = "keys" if len(redefine) > 1 else "key"
     assert f"{key_or_keys} {join(redefine)}" in message
     if from_file:
         assert "in 'test_params.yaml'" in message
     assert context == {"foo": "bar"}
 
@@ -315,11 +333,10 @@
         {"wdir": "${ite}", "cmd": "echo ${item}"},
         Context(models=["foo", "bar"]),
     )
     with pytest.raises(ResolveError) as exc_info:
         definition.resolve_all()
     assert (
         str(exc_info.value)
-        == "failed to parse 'stages.build@foo.wdir' in 'dvc.yaml': "
-        "Could not find 'ite'"
+        == "failed to parse 'stages.build@foo.wdir' in 'dvc.yaml': Could not find 'ite'"
     )
     assert definition.context == {"models": ["foo", "bar"]}
```

### Comparing `dvc-2.9.5/tests/func/parsing/test_foreach.py` & `dvc-3.0.0a0/tests/func/parsing/test_foreach.py`

 * *Files 4% similar despite different names*

```diff
@@ -40,14 +40,31 @@
 
     # check that `foreach` item-key replacement didnot leave any leftovers.
     assert not context
     assert not resolver.tracked_vars["build@model1"]
     assert not resolver.tracked_vars["build@model2"]
 
 
+def test_with_dict_with_non_str_keys(tmp_dir, dvc):
+    resolver = DataResolver(dvc, tmp_dir.fs_path, {})
+    context = Context()
+
+    foreach_data = {2021: {"thresh": "foo"}, 2022: {"thresh": "bar"}}
+    data = {"foreach": foreach_data, "do": {"cmd": "echo ${key} ${item.thresh}"}}
+    definition = ForeachDefinition(resolver, context, "build", data)
+
+    assert definition.resolve_one("2021") == {"build@2021": {"cmd": "echo 2021 foo"}}
+    assert definition.resolve_one("2022") == {"build@2022": {"cmd": "echo 2022 bar"}}
+
+    # check that `foreach` item-key replacement didnot leave any leftovers.
+    assert not context
+    assert not resolver.tracked_vars["build@2021"]
+    assert not resolver.tracked_vars["build@2022"]
+
+
 def test_with_composite_list(tmp_dir, dvc):
     resolver = DataResolver(dvc, tmp_dir.fs_path, {})
 
     context = Context()
     foreach_data = [{"thresh": "foo"}, {"thresh": "bar"}]
     data = {"foreach": foreach_data, "do": {"cmd": "echo ${item.thresh}"}}
     definition = ForeachDefinition(resolver, context, "build", data)
@@ -176,17 +193,15 @@
     assert not any(item for item in resolver.tracked_vars.values())
 
 
 def test_mixed_vars_for_foreach_data(tmp_dir, dvc):
     (tmp_dir / "params.yaml").dump({"models": {"model1": "foo"}})
     (tmp_dir / "test_params.yaml").dump({"models": {"model2": "bar"}})
 
-    resolver = DataResolver(
-        dvc, tmp_dir.fs_path, {"vars": ["test_params.yaml"]}
-    )
+    resolver = DataResolver(dvc, tmp_dir.fs_path, {"vars": ["test_params.yaml"]})
     data = {"foreach": "${models}", "do": {"cmd": "echo ${item}"}}
     definition = ForeachDefinition(resolver, resolver.context, "build", data)
 
     assert definition.resolve_all() == {
         "build@model1": {"cmd": "echo foo"},
         "build@model2": {"cmd": "echo bar"},
     }
@@ -201,17 +216,15 @@
     (tmp_dir / "params.yaml").dump(
         {"models": {"model1": {"thresh": 10}, "model2": {"thresh": 15}}},
     )
     (tmp_dir / "test_params.yaml").dump(
         {"models": {"model1": {"epochs": 5}, "model2": {"epochs": 10}}},
     )
 
-    resolver = DataResolver(
-        dvc, tmp_dir.fs_path, {"vars": ["test_params.yaml"]}
-    )
+    resolver = DataResolver(dvc, tmp_dir.fs_path, {"vars": ["test_params.yaml"]})
     data = {
         "foreach": "${models}",
         "do": {"cmd": "echo ${item.thresh} ${item.epochs}"},
     }
     definition = ForeachDefinition(resolver, resolver.context, "build", data)
 
     assert definition.resolve_all() == {
@@ -312,20 +325,16 @@
     assert resolver.context == {"models": {"model1": {"thresh": "foo"}}}
     assert resolver.tracked_vars == {
         "build@foo": {"test_params.yaml": {"train.epochs": 10}},
         "build@bar": {"test_params.yaml": {"train.epochs": 10}},
     }
 
 
-@pytest.mark.parametrize(
-    "local_import", ["params.yaml", "params.yaml:train,prepare"]
-)
-def test_foreach_with_interpolated_wdir_and_local_vars(
-    tmp_dir, dvc, local_import
-):
+@pytest.mark.parametrize("local_import", ["params.yaml", "params.yaml:train,prepare"])
+def test_foreach_with_interpolated_wdir_and_local_vars(tmp_dir, dvc, local_import):
     (tmp_dir / "params.yaml").dump({"models": {"model1": {"thresh": "foo"}}})
 
     for i in range(5):
         build_dir = tmp_dir / ("model-" + str(i))
         build_dir.mkdir()
         (build_dir / "params.yaml").dump(
             {"train": {"epochs": 1 + i}, "prepare": {"nums": 10 * i}},
@@ -382,17 +391,15 @@
         "build@4": {
             os.path.join("model-4", "params.yaml"): {
                 "train.epochs": 5,
                 "prepare.nums": 40,
             }
         },
     }
-    assert resolver.context.imports == {
-        str(tmp_dir / DEFAULT_PARAMS_FILE): None
-    }
+    assert resolver.context.imports == {DEFAULT_PARAMS_FILE: None}
 
 
 def test_foreach_do_syntax_is_checked_once(tmp_dir, dvc, mocker):
     do_def = {"cmd": "python script.py --epochs ${item}"}
     data = {"foreach": [0, 1, 2, 3, 4], "do": do_def}
     definition = ForeachDefinition(
         DataResolver(dvc, tmp_dir.fs_path, {}), Context(), "build", data
```

### Comparing `dvc-2.9.5/tests/func/parsing/test_interpolated_entry.py` & `dvc-3.0.0a0/tests/func/parsing/test_interpolated_entry.py`

 * *Files 16% similar despite different names*

```diff
@@ -2,21 +2,17 @@
 from copy import deepcopy
 
 import pytest
 
 from dvc.dependency import _merge_params
 from dvc.parsing import DEFAULT_PARAMS_FILE, DataResolver
 from dvc.parsing.context import recurse_not_a_node
+from dvc.parsing.interpolate import escape_str
 
-from . import (
-    CONTEXT_DATA,
-    RESOLVED_DVC_YAML_DATA,
-    TEMPLATED_DVC_YAML_DATA,
-    USED_VARS,
-)
+from . import CONTEXT_DATA, RESOLVED_DVC_YAML_DATA, TEMPLATED_DVC_YAML_DATA, USED_VARS
 
 
 def assert_stage_equal(d1, d2):
     """Keeps the params section in order, and then checks for equality."""
     for d in [d1, d2]:
         assert recurse_not_a_node(d)
         for _, stage_d in d.get("stages", {}).items():
@@ -26,17 +22,15 @@
             if params:
                 stage_d["params"] = params
     assert d1 == d2
 
 
 def test_simple(tmp_dir, dvc):
     (tmp_dir / DEFAULT_PARAMS_FILE).dump(CONTEXT_DATA)
-    resolver = DataResolver(
-        dvc, tmp_dir.fs_path, deepcopy(TEMPLATED_DVC_YAML_DATA)
-    )
+    resolver = DataResolver(dvc, tmp_dir.fs_path, deepcopy(TEMPLATED_DVC_YAML_DATA))
     assert_stage_equal(resolver.resolve(), deepcopy(RESOLVED_DVC_YAML_DATA))
     assert resolver.tracked_vars == {
         "stage1": {DEFAULT_PARAMS_FILE: USED_VARS["stage1"]},
         "stage2": {DEFAULT_PARAMS_FILE: USED_VARS["stage2"]},
     }
 
 
@@ -130,17 +124,15 @@
                 "cmd": "echo ${dict.foo} ${dict.bar}",
                 "params": ["value1"],
                 "wdir": "${dict.ws}",
                 "vars": [DEFAULT_PARAMS_FILE],
             }
         }
     }
-    (tmp_dir / DEFAULT_PARAMS_FILE).dump(
-        {"dict": {"bar": "bar", "ws": "data"}}
-    )
+    (tmp_dir / DEFAULT_PARAMS_FILE).dump({"dict": {"bar": "bar", "ws": "data"}})
     data_dir = tmp_dir / "data"
     data_dir.mkdir()
     (data_dir / DEFAULT_PARAMS_FILE).dump({"dict": {"foo": "foo"}})
     resolver = DataResolver(dvc, tmp_dir.fs_path, d)
 
     assert_stage_equal(
         resolver.resolve(),
@@ -156,15 +148,15 @@
     )
     assert resolver.tracked_vars == {
         "stage1": {
             os.path.join("data", DEFAULT_PARAMS_FILE): {"dict.foo": "foo"},
             DEFAULT_PARAMS_FILE: {"dict.bar": "bar", "dict.ws": "data"},
         }
     }
-    assert resolver.context.imports == {str(tmp_dir / "params.yaml"): None}
+    assert resolver.context.imports == {"params.yaml": None}
     assert resolver.context == {"dict": {"bar": "bar", "ws": "data"}}
 
 
 def test_resolve_local_tries_to_load_globally_used_files(tmp_dir, dvc):
     iterable = {"bar": "bar", "foo": "foo"}
     (tmp_dir / "params.json").dump(iterable)
 
@@ -232,15 +224,15 @@
                 "cmd": "echo ${bar}",
                 "vars": ["../params.yaml"],
             }
         },
     }
     resolver = DataResolver(dvc, tmp_dir.fs_path, d)
     resolver.resolve()
-    assert resolver.context.imports == {str(tmp_dir / "params.yaml"): None}
+    assert resolver.context.imports == {"params.yaml": None}
 
 
 @pytest.mark.parametrize("local", [True, False])
 @pytest.mark.parametrize(
     "vars_",
     [
         ["test_params.yaml:bar", "test_params.yaml:foo"],
@@ -255,7 +247,71 @@
     d = {"stages": {"build": {"cmd": "echo ${bar}"}}}
     if local:
         d["stages"]["build"]["vars"] = vars_
     else:
         d["vars"] = vars_
     resolver = DataResolver(dvc, tmp_dir.fs_path, d)
     resolver.resolve()
+
+
+@pytest.mark.parametrize(
+    "bool_config, list_config",
+    [(None, None), ("store_true", "nargs"), ("boolean_optional", "append")],
+)
+def test_cmd_dict(tmp_dir, dvc, bool_config, list_config):
+    with dvc.config.edit() as conf:
+        if bool_config:
+            conf["parsing"]["bool"] = bool_config
+        if list_config:
+            conf["parsing"]["list"] = list_config
+
+    string = "spaced string"
+    mixed_quote_string = "quote\"'d"
+    data = {
+        "dict": {
+            "foo": "foo",
+            "bar": 2,
+            "string": string,
+            "mixed_quote_string": mixed_quote_string,
+            "bool": True,
+            "bool-false": False,
+            "list": [1, 2, "foo", mixed_quote_string],
+            "nested": {"foo": "foo"},
+        }
+    }
+    (tmp_dir / DEFAULT_PARAMS_FILE).dump(data)
+    resolver = DataResolver(
+        dvc,
+        tmp_dir.fs_path,
+        {"stages": {"stage1": {"cmd": "python script.py ${dict}"}}},
+    )
+
+    if bool_config is None or bool_config == "store_true":
+        bool_resolved = " --bool"
+    else:
+        bool_resolved = " --bool --no-bool-false"
+
+    if list_config is None or list_config == "nargs":
+        list_resolved = f" --list 1 2 foo {escape_str(mixed_quote_string)}"
+    else:
+        list_resolved = " --list 1 --list 2 --list foo"
+        list_resolved += f" --list {escape_str(mixed_quote_string)}"
+
+    assert_stage_equal(
+        resolver.resolve(),
+        {
+            "stages": {
+                "stage1": {
+                    "cmd": (
+                        "python script.py"
+                        " --foo foo --bar 2"
+                        f" --string {escape_str(string)}"
+                        " --mixed_quote_string"
+                        f" {escape_str(mixed_quote_string)}"
+                        f"{bool_resolved}"
+                        f"{list_resolved}"
+                        " --nested.foo foo"
+                    )
+                }
+            }
+        },
+    )
```

### Comparing `dvc-2.9.5/tests/func/parsing/test_resolver.py` & `dvc-3.0.0a0/tests/func/parsing/test_resolver.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-import os
 from copy import deepcopy
 
 import pytest
 
 from dvc.parsing import DEFAULT_PARAMS_FILE, DataResolver, ResolveError
 from dvc.parsing.context import Context
 from dvc.utils.serialize import dumps_yaml
@@ -42,17 +41,15 @@
         DataResolver(dvc, tmp_dir.fs_path, {"vars": [vars_, {"bar": "foo"}]})
     assert (
         str(exc_info.value)
         == "failed to parse 'vars' in 'dvc.yaml': interpolating is not allowed"
     )
 
 
-@pytest.mark.parametrize(
-    "vars_", [{}, {"vars": []}, {"vars": [DEFAULT_PARAMS_FILE]}]
-)
+@pytest.mark.parametrize("vars_", [{}, {"vars": []}, {"vars": [DEFAULT_PARAMS_FILE]}])
 def test_default_params_file(tmp_dir, dvc, vars_):
     (tmp_dir / DEFAULT_PARAMS_FILE).dump(DATA)
     resolver = DataResolver(dvc, tmp_dir.fs_path, vars_)
     assert resolver.context == DATA
 
 
 def test_load_vars_from_file(tmp_dir, dvc):
@@ -67,21 +64,21 @@
     expected.update(datasets)
     assert resolver.context == expected
 
 
 def test_load_vars_with_relpath(tmp_dir, scm, dvc):
     tmp_dir.scm_gen(DEFAULT_PARAMS_FILE, dumps_yaml(DATA), commit="add params")
 
-    subdir = tmp_dir / "subdir"
-    d = {"vars": [os.path.relpath(tmp_dir / DEFAULT_PARAMS_FILE, subdir)]}
-
     revisions = ["HEAD", "workspace"]
     for rev in dvc.brancher(revs=["HEAD"]):
         assert rev == revisions.pop()
-        resolver = DataResolver(dvc, subdir.fs_path, d)
+        d = {
+            "vars": [f"../{DEFAULT_PARAMS_FILE}"],
+        }
+        resolver = DataResolver(dvc, "subdir", d)
         assert resolver.context == deepcopy(DATA)
 
 
 def test_partial_vars_doesnot_exist(tmp_dir, dvc):
     (tmp_dir / "test_params.yaml").dump({"sub1": "sub1"})
 
     with pytest.raises(ResolveError) as exc_info:
@@ -97,56 +94,56 @@
     (tmp_dir / DEFAULT_PARAMS_FILE).dump(DATA)
     (tmp_dir / "params.json").dump(DATA)
 
     d = {"vars": [DEFAULT_PARAMS_FILE, "params.json"]}
     with pytest.raises(ResolveError) as exc_info:
         DataResolver(dvc, tmp_dir.fs_path, d)
 
-    assert str(exc_info.value) == (
-        "failed to parse 'vars' in 'dvc.yaml':\n"
+    assert (
+        str(exc_info.value) == "failed to parse 'vars' in 'dvc.yaml':\n"
         "cannot redefine 'models.bar' from 'params.json' "
         "as it already exists in 'params.yaml'"
     )
 
 
 def test_global_overwrite_vars(tmp_dir, dvc):
     (tmp_dir / DEFAULT_PARAMS_FILE).dump(DATA)
     d = {"vars": [DATA]}
 
     with pytest.raises(ResolveError) as exc_info:
         DataResolver(dvc, tmp_dir.fs_path, d)
 
-    assert str(exc_info.value) == (
-        "failed to parse 'vars' in 'dvc.yaml':\n"
+    assert (
+        str(exc_info.value) == "failed to parse 'vars' in 'dvc.yaml':\n"
         "cannot redefine 'models.bar' from 'vars[0]' "
         "as it already exists in 'params.yaml'"
     )
 
 
 def test_local_declared_vars_overwrite(tmp_dir, dvc):
     (tmp_dir / DEFAULT_PARAMS_FILE).dump(DATA)
 
     d = {"vars": [DATA["models"], DATA["models"]]}
     with pytest.raises(ResolveError) as exc_info:
         DataResolver(dvc, tmp_dir.fs_path, d)
 
-    assert str(exc_info.value) == (
-        "failed to parse 'vars' in 'dvc.yaml':\n"
+    assert (
+        str(exc_info.value) == "failed to parse 'vars' in 'dvc.yaml':\n"
         "cannot redefine 'bar' from 'vars[1]' "
         "as it already exists in 'vars[0]'"
     )
 
 
 def test_specified_params_file_not_exist(tmp_dir, dvc):
     d = {"vars": ["not_existing_params.yaml"]}
     with pytest.raises(ResolveError) as exc_info:
         DataResolver(dvc, tmp_dir.fs_path, d)
 
-    assert str(exc_info.value) == (
-        "failed to parse 'vars' in 'dvc.yaml': "
+    assert (
+        str(exc_info.value) == "failed to parse 'vars' in 'dvc.yaml': "
         "'not_existing_params.yaml' does not exist"
     )
 
 
 @pytest.mark.parametrize("local", [True, False])
 @pytest.mark.parametrize(
     "vars_",
@@ -160,18 +157,17 @@
     d = {"stages": {"build": {"cmd": "echo ${sub1} ${sub2}"}}}
     (tmp_dir / "test_params.yaml").dump({"sub1": "sub1", "sub2": "sub2"})
     if not local:
         d["vars"] = vars_
     else:
         d["stages"]["build"]["vars"] = vars_
 
-    with pytest.raises(ResolveError) as exc_info:
+    with pytest.raises(ResolveError) as exc_info:  # noqa: PT012
         resolver = DataResolver(dvc, tmp_dir.fs_path, d)
         resolver.resolve()
-
     assert "partially" in str(exc_info.value)
 
 
 @pytest.mark.parametrize(
     "vars_, loc", [(DATA, "build.vars[0]"), ("params.json", "params.json")]
 )
 def test_local_overwrite_error(tmp_dir, dvc, vars_, loc):
@@ -180,12 +176,12 @@
 
     d = {"stages": {"build": {"cmd": "echo ${models.foo}", "vars": [vars_]}}}
 
     resolver = DataResolver(dvc, tmp_dir.fs_path, d)
     with pytest.raises(ResolveError) as exc_info:
         resolver.resolve()
 
-    assert str(exc_info.value) == (
-        "failed to parse stage 'build' in 'dvc.yaml':\n"
+    assert (
+        str(exc_info.value) == "failed to parse stage 'build' in 'dvc.yaml':\n"
         f"cannot redefine 'models.bar' from '{loc}' "
         "as it already exists in 'params.yaml'"
     )
```

### Comparing `dvc-2.9.5/tests/func/plots/test_modify.py` & `dvc-3.0.0a0/tests/func/plots/test_modify.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,32 +1,28 @@
-import os
-
 import pytest
 
-from dvc.dvcfile import PIPELINE_LOCK
+from dvc.dvcfile import LOCK_FILE
 from dvc.repo.plots import PropsNotFoundError
-from dvc.repo.plots.template import TemplateNotFoundError
 from dvc.utils import relpath
+from tests.utils.plots import get_plot
 
 
 def test_plots_modify_existing_template(
     tmp_dir, dvc, run_copy_metrics, custom_template
 ):
     metric = [{"a": 1, "b": 2}, {"a": 2, "b": 3}]
     (tmp_dir / "metric_t.json").dump_json(metric, sort_keys=True)
     stage = run_copy_metrics(
         "metric_t.json",
         "metric.json",
         plots_no_cache=["metric.json"],
         name="copy-metrics",
         single_stage=False,
     )
-    dvc.plots.modify(
-        "metric.json", props={"template": relpath(custom_template)}
-    )
+    dvc.plots.modify("metric.json", props={"template": relpath(custom_template)})
     stage = stage.reload()
     assert stage.outs[0].plot == {"template": relpath(custom_template)}
 
 
 def test_plots_modify_should_not_change_lockfile(
     tmp_dir, dvc, run_copy_metrics, custom_template
 ):
@@ -35,22 +31,22 @@
         "metric_t.json",
         "metric.json",
         plots_no_cache=["metric.json"],
         name="copy-metrics",
         single_stage=False,
     )
 
-    (tmp_dir / PIPELINE_LOCK).unlink()
-    dvc.plots.modify(
-        "metric.json", props={"template": relpath(custom_template)}
-    )
-    assert not (tmp_dir / PIPELINE_LOCK).exists()
+    (tmp_dir / LOCK_FILE).unlink()
+    dvc.plots.modify("metric.json", props={"template": relpath(custom_template)})
+    assert not (tmp_dir / LOCK_FILE).exists()
 
 
 def test_plots_modify_not_existing_template(dvc):
+    from dvc_render.vega_templates import TemplateNotFoundError
+
     with pytest.raises(TemplateNotFoundError):
         dvc.plots.modify(
             "metric.json", props={"template": "not-existing-template.json"}
         )
 
 
 def test_unset_nonexistent(tmp_dir, dvc, run_copy_metrics, custom_template):
@@ -73,24 +69,26 @@
     subdir.mkdir()
 
     metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]
 
     fname = "file.json"
     (tmp_dir / fname).dump_json(metric, sort_keys=True)
 
-    p1 = os.path.join("subdir", "p1.json")
-    p2 = os.path.join("subdir", "p2.json")
+    p1 = "subdir/p1.json"
+    p2 = "subdir/p2.json"
     tmp_dir.dvc.run(
         cmd=(
             f"mkdir subdir && python copy.py {fname} {p1} && "
             f"python copy.py {fname} {p2}"
         ),
         deps=[fname],
         single_stage=False,
         plots=["subdir"],
         name="copy_double",
     )
     dvc.plots.modify("subdir", {"title": "TITLE"})
 
     result = dvc.plots.show()
-    assert result["workspace"]["data"][p1]["props"]["title"] == "TITLE"
-    assert result["workspace"]["data"][p2]["props"]["title"] == "TITLE"
+    assert get_plot(result, "workspace", typ="definitions", file="") == {
+        p1: {"title": "TITLE"},
+        p2: {"title": "TITLE"},
+    }
```

### Comparing `dvc-2.9.5/tests/func/plots/test_show.py` & `dvc-3.0.0a0/tests/func/plots/test_show.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,20 +1,32 @@
+import json
 import os
 
 import pytest
-from funcy import get_in
 
 from dvc.cli import main
-from dvc.dvcfile import PIPELINE_FILE
+from dvc.dvcfile import PROJECT_FILE
 from dvc.exceptions import OverlappingOutputPathsError
 from dvc.repo import Repo
 from dvc.repo.plots import PlotMetricTypeError
 from dvc.utils import onerror_collect
 from dvc.utils.fs import remove
-from dvc.utils.serialize import EncodingError, YAMLFileCorruptedError
+from dvc.utils.serialize import EncodingError, YAMLFileCorruptedError, modify_yaml
+from tests.utils.plots import get_plot
+
+
+def test_show_targets(tmp_dir, dvc):
+    metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]
+    (tmp_dir / "metric.json").dump_json(metric, sort_keys=True)
+
+    plots = dvc.plots.show(targets=["metric.json"])
+    assert get_plot(plots, "workspace", file="metric.json") == metric
+
+    plots = dvc.plots.show(targets=(tmp_dir / "metric.json").fs_path)
+    assert get_plot(plots, "workspace", file="metric.json") == metric
 
 
 def test_plot_cache_missing(tmp_dir, scm, dvc, caplog, run_copy_metrics):
     metric1 = [{"y": 2}, {"y": 3}]
     (tmp_dir / "metric_t.json").dump_json(metric1, sort_keys=True)
     run_copy_metrics(
         "metric_t.json",
@@ -34,33 +46,34 @@
         commit="there is an another metric",
     )
     scm.tag("v2")
     remove(stage.outs[0].fspath)
     remove(stage.outs[0].cache_path)
 
     plots_data = dvc.plots.show(revs=["v1", "v2"], targets=["metric.json"])
-    assert plots_data["v1"]["data"]["metric.json"]["data"] == metric1
+
+    assert get_plot(plots_data, "v1", file="metric.json") == metric1
     assert isinstance(
-        plots_data["v2"]["data"]["metric.json"]["error"], FileNotFoundError
+        get_plot(plots_data, "v2", file="metric.json", endkey="error"),
+        FileNotFoundError,
     )
 
 
 def test_plot_wrong_metric_type(tmp_dir, scm, dvc, run_copy_metrics):
     tmp_dir.gen("metric_t.txt", "some text")
     run_copy_metrics(
         "metric_t.txt",
         "metric.txt",
         plots_no_cache=["metric.txt"],
         commit="add text metric",
     )
 
+    result = dvc.plots.show(targets=["metric.txt"], onerror=onerror_collect)
     assert isinstance(
-        dvc.plots.show(targets=["metric.txt"], onerror=onerror_collect)[
-            "workspace"
-        ]["data"]["metric.txt"]["error"],
+        get_plot(result, "workspace", file="metric.txt", endkey="error"),
         PlotMetricTypeError,
     )
 
 
 @pytest.mark.parametrize("use_dvc", [True, False])
 def test_show_non_plot(tmp_dir, scm, use_dvc):
     metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]
@@ -69,32 +82,34 @@
     if use_dvc:
         dvc = Repo.init()
     else:
         dvc = Repo(uninitialized=True)
 
     plots = dvc.plots.show(targets=["metric.json"])
 
-    assert plots["workspace"]["data"]["metric.json"]["data"] == metric
+    assert get_plot(plots, "workspace", file="metric.json") == metric
 
 
-def test_show_non_plot_and_plot_with_params(
-    tmp_dir, scm, dvc, run_copy_metrics
-):
+def test_show_non_plot_and_plot_with_params(tmp_dir, scm, dvc, run_copy_metrics):
     metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]
     (tmp_dir / "metric.json").dump_json(metric, sort_keys=True)
     run_copy_metrics(
-        "metric.json", "metric2.json", plots_no_cache=["metric2.json"]
+        "metric.json",
+        "metric2.json",
+        plots_no_cache=["metric2.json"],
+        name="train",
     )
     props = {"title": "TITLE"}
     dvc.plots.modify("metric2.json", props=props)
 
     result = dvc.plots.show(targets=["metric.json", "metric2.json"])
-    assert "metric.json" in result["workspace"]["data"]
-    assert "metric2.json" in result["workspace"]["data"]
-    assert result["workspace"]["data"]["metric2.json"]["props"] == props
+
+    assert get_plot(result, "workspace", file="metric.json") == metric
+    assert get_plot(result, "workspace", file="metric2.json") == metric
+    assert get_plot(result, "workspace", file="metric2.json", endkey="props") == props
 
 
 def test_show_from_subdir(tmp_dir, dvc, capsys):
     subdir = tmp_dir / "subdir"
 
     subdir.mkdir()
     metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]
@@ -105,21 +120,25 @@
 
     out, _ = capsys.readouterr()
     assert subdir.as_uri() in out
     assert (subdir / "dvc_plots").is_dir()
     assert (subdir / "dvc_plots" / "index.html").is_file()
 
 
-def test_plots_show_non_existing(tmp_dir, dvc, caplog):
+def test_plots_show_non_existing(tmp_dir, dvc, capsys):
     result = dvc.plots.show(targets=["plot.json"])
     assert isinstance(
-        result["workspace"]["data"]["plot.json"]["error"], FileNotFoundError
+        get_plot(result, "workspace", file="plot.json", endkey="error"),
+        FileNotFoundError,
     )
 
-    assert "'plot.json' was not found in current workspace." in caplog.text
+    cap = capsys.readouterr()
+    assert (
+        "DVC failed to load some plots for following revisions: 'workspace'" in cap.err
+    )
 
 
 @pytest.mark.parametrize("clear_before_run", [True, False])
 def test_plots_show_overlap(tmp_dir, dvc, run_copy_metrics, clear_before_run):
     data_dir = tmp_dir / "data"
     data_dir.mkdir()
 
@@ -139,76 +158,82 @@
             "outs": ["data"],
         }
 
     # running by clearing and not clearing stuffs
     # so as it works even for optimized cases
     if clear_before_run:
         remove(data_dir)
-        remove(dvc.odb.local.cache_dir)
+        remove(dvc.cache.local.path)
 
     dvc._reset()
 
+    result = dvc.plots.show(onerror=onerror_collect)
     assert isinstance(
-        dvc.plots.show(onerror=onerror_collect)["workspace"]["error"],
+        get_plot(result, "workspace", endkey="error"),
         OverlappingOutputPathsError,
     )
 
 
 def test_dir_plots(tmp_dir, dvc, run_copy_metrics):
     subdir = tmp_dir / "subdir"
     subdir.mkdir()
 
     metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]
 
     fname = "file.json"
     (tmp_dir / fname).dump_json(metric, sort_keys=True)
 
-    p1 = os.path.join("subdir", "p1.json")
-    p2 = os.path.join("subdir", "p2.json")
+    p1 = "subdir/p1.json"
+    p2 = "subdir/p2.json"
     tmp_dir.dvc.run(
         cmd=(
             f"mkdir subdir && python copy.py {fname} {p1} && "
             f"python copy.py {fname} {p2}"
         ),
         deps=[fname],
         single_stage=False,
         plots=["subdir"],
         name="copy_double",
     )
     props = {"title": "TITLE"}
     dvc.plots.modify("subdir", props)
 
     result = dvc.plots.show()
-    assert set(result["workspace"]["data"]) == {p1, p2}
-    assert result["workspace"]["data"][p1]["props"] == props
-    assert result["workspace"]["data"][p2]["props"] == props
+
+    assert set(get_plot(result, "workspace")) == {p1, p2}
+    assert get_plot(result, "workspace", typ="definitions", file="") == {
+        p1: props,
+        p2: props,
+    }
 
 
 def test_ignore_parsing_error(tmp_dir, dvc, run_copy_metrics):
     with open("file", "wb", encoding=None) as fobj:
         fobj.write(b"\xc1")
 
     run_copy_metrics("file", "plot_file.json", plots=["plot_file.json"])
     result = dvc.plots.show(onerror=onerror_collect)
 
     assert isinstance(
-        result["workspace"]["data"]["plot_file.json"]["error"], EncodingError
+        get_plot(result, "workspace", file="plot_file.json", endkey="error"),
+        EncodingError,
     )
 
 
 @pytest.mark.parametrize(
-    "file,error_path",
+    "file,path_kwargs",
     (
-        (PIPELINE_FILE, ["workspace", "error"]),
-        ("plot.yaml", ["workspace", "data", "plot.yaml", "error"]),
+        (PROJECT_FILE, {"revision": "workspace", "endkey": "error"}),
+        (
+            "plot.yaml",
+            {"revision": "workspace", "file": "plot.yaml", "endkey": "error"},
+        ),
     ),
 )
-def test_log_errors(
-    tmp_dir, scm, dvc, run_copy_metrics, file, error_path, capsys
-):
+def test_log_errors(tmp_dir, scm, dvc, run_copy_metrics, file, path_kwargs, capsys):
     metric = [{"val": 2}, {"val": 3}]
     (tmp_dir / "metric_t.yaml").dump(metric)
     run_copy_metrics(
         "metric_t.yaml",
         "plot.yaml",
         plots=["plot.yaml"],
         single_stage=False,
@@ -218,46 +243,48 @@
 
     with open(file, "a", encoding="utf-8") as fd:
         fd.write("\nMALFORMED!")
 
     result = dvc.plots.show(onerror=onerror_collect)
     _, error = capsys.readouterr()
 
-    assert isinstance(get_in(result, error_path), YAMLFileCorruptedError)
+    assert isinstance(get_plot(result, **path_kwargs), YAMLFileCorruptedError)
     assert (
-        "DVC failed to load some plots for following revisions: 'workspace'."
-        in error
+        "DVC failed to load some plots for following revisions: 'workspace'." in error
     )
 
 
-def test_plots_binary(tmp_dir, scm, dvc, run_copy_metrics, custom_template):
-    with open("image.jpg", "wb") as fd:
+@pytest.mark.parametrize("ext", ["jpg", "svg"])
+def test_plots_binary(tmp_dir, scm, dvc, run_copy_metrics, custom_template, ext):
+    file1 = f"image.{ext}"
+    file2 = f"plot.{ext}"
+    with open(file1, "wb") as fd:
         fd.write(b"content")
 
-    dvc.add(["image.jpg"])
+    dvc.add([file1])
     run_copy_metrics(
-        "image.jpg",
-        "plot.jpg",
+        file1,
+        file2,
         commit="run training",
-        plots=["plot.jpg"],
+        plots=[file2],
         name="s2",
         single_stage=False,
     )
 
     scm.add(["dvc.yaml", "dvc.lock"])
     scm.commit("initial")
 
     scm.tag("v1")
 
-    with open("plot.jpg", "wb") as fd:
+    with open(file2, "wb") as fd:
         fd.write(b"content2")
 
     result = dvc.plots.show(revs=["v1", "workspace"])
-    assert result["v1"]["data"]["plot.jpg"]["data"] == b"content"
-    assert result["workspace"]["data"]["plot.jpg"]["data"] == b"content2"
+    assert get_plot(result, "v1", file=file2) == b"content"
+    assert get_plot(result, "workspace", file=file2) == b"content2"
 
 
 def test_collect_non_existing_dir(tmp_dir, dvc, run_copy_metrics):
     subdir = tmp_dir / "subdir"
     subdir.mkdir()
 
     metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]
@@ -289,10 +316,110 @@
         commit="there is metric",
     )
 
     remove(subdir_stage.outs[0].cache_path)
     remove(subdir_stage.outs[0].fs_path)
 
     result = dvc.plots.show()
-    assert "error" in result["workspace"]["data"]["subdir"]
+    assert get_plot(result, "workspace", typ="definitions", file="", endkey="error")
     # make sure others gets loaded
-    assert result["workspace"]["data"]["plot.json"]["data"] == metric
+    assert get_plot(result, "workspace", file="plot.json") == metric
+
+
+@pytest.mark.parametrize(
+    "plot_config,expected_datafiles",
+    [
+        (
+            {
+                "comparison": {
+                    "x": {"data1.json": "a"},
+                    "y": {"sub/dir/data2.json": "b"},
+                }
+            },
+            ["data1.json", os.path.join("sub", "dir", "data2.json")],
+        ),
+        (
+            {"data1.json": {"x": "c", "y": "a", "title": "File as key test"}},
+            ["data1.json"],
+        ),
+        (
+            {
+                "infer_data_from_y": {
+                    "x": "a",
+                    "y": {"data1.json": "b", "sub/dir/data2.json": "c"},
+                }
+            },
+            ["data1.json", os.path.join("sub", "dir", "data2.json")],
+        ),
+    ],
+)
+def test_top_level_plots(
+    tmp_dir,
+    dvc,
+    plot_config,
+    expected_datafiles,
+):
+    data = {
+        "data1.json": [
+            {"a": 1, "b": 0.1, "c": 0.01},
+            {"a": 2, "b": 0.2, "c": 0.02},
+        ],
+        os.path.join("sub", "dir", "data.json"): [
+            {"a": 6, "b": 0.6, "c": 0.06},
+            {"a": 7, "b": 0.7, "c": 0.07},
+        ],
+    }
+
+    for filename, content in data.items():
+        dirname = os.path.dirname(filename)
+        if dirname:
+            os.makedirs(dirname)
+        (tmp_dir / filename).dump_json(content, sort_keys=True)
+
+    config_file = "dvc.yaml"
+    with modify_yaml(config_file) as dvcfile_content:
+        dvcfile_content["plots"] = [plot_config]
+
+    result = dvc.plots.show()
+
+    assert plot_config == get_plot(
+        result, "workspace", typ="definitions", file=config_file
+    )
+
+    for filename, content in data.items():
+        if filename in expected_datafiles:
+            assert content == get_plot(result, "workspace", file=filename)
+        else:
+            assert filename not in get_plot(result, "workspace")
+
+
+def test_show_plots_defined_with_native_os_path(tmp_dir, dvc, scm, capsys):
+    """Regression test for #8689"""
+    top_level_plot = os.path.join("subdir", "top_level_plot.csv")
+    stage_plot = os.path.join("subdir", "stage_plot.csv")
+    (tmp_dir / "subdir").mkdir()
+    (tmp_dir / top_level_plot).write_text("foo,bar\n1,2")
+    (tmp_dir / stage_plot).write_text("foo,bar\n1,2")
+    (tmp_dir / "dvc.yaml").dump({"plots": [top_level_plot]})
+
+    dvc.stage.add(name="foo", plots=[stage_plot], cmd="echo foo")
+
+    plots = dvc.plots.show()
+
+    # sources are in posixpath format
+    sources = plots["workspace"]["sources"]["data"]
+    assert sources["subdir/top_level_plot.csv"]["data"] == [{"foo": "1", "bar": "2"}]
+    assert sources["subdir/stage_plot.csv"]["data"] == [{"foo": "1", "bar": "2"}]
+    # definitions are in native os format
+    definitions = plots["workspace"]["definitions"]["data"]
+    assert top_level_plot in definitions["dvc.yaml"]["data"]
+    assert stage_plot in definitions[""]["data"]
+
+    capsys.readouterr()
+    assert main(["plots", "show", "--json"]) == 0
+    out, _ = capsys.readouterr()
+    json_out = json.loads(out)
+    assert "errors" not in json_out
+
+    json_data = json_out["data"]
+    assert json_data[f"dvc.yaml::{top_level_plot}"]
+    assert json_data[stage_plot]
```

### Comparing `dvc-2.9.5/tests/func/test_add.py` & `dvc-3.0.0a0/tests/func/test_add.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,48 +1,44 @@
 import errno
 import filecmp
 import os
 import shutil
 import stat
 import textwrap
-import time
-from unittest.mock import call, patch
+from unittest.mock import call
 
 import colorama
 import pytest
 
 import dvc as dvc_module
+import dvc_data
+from dvc.cachemgr import CacheManager
 from dvc.cli import main
-from dvc.data.db import ODBManager
+from dvc.config import ConfigError
 from dvc.dvcfile import DVC_FILE_SUFFIX
 from dvc.exceptions import (
     DvcException,
     InvalidArgumentError,
-    OutputDuplicationError,
     OverlappingOutputPathsError,
     RecursiveAddingWhileUsingFilename,
 )
-from dvc.fs.local import LocalFileSystem
-from dvc.hash_info import HashInfo
+from dvc.fs import LocalFileSystem, system
 from dvc.output import (
     OutputAlreadyTrackedError,
     OutputDoesNotExistError,
     OutputIsStageFileError,
 )
 from dvc.stage import Stage
-from dvc.stage.exceptions import (
-    StageExternalOutputsError,
-    StagePathNotFoundError,
-)
-from dvc.system import System
-from dvc.testing.test_workspace import TestAdd
-from dvc.utils import LARGE_DIR_SIZE, file_md5, relpath
+from dvc.stage.exceptions import StageExternalOutputsError, StagePathNotFoundError
+from dvc.testing.workspace_tests import TestAdd
+from dvc.utils import LARGE_DIR_SIZE
 from dvc.utils.fs import path_isin
-from dvc.utils.serialize import YAMLFileCorruptedError, load_yaml
-from tests.basic_env import TestDvc
+from dvc.utils.serialize import YAMLFileCorruptedError
+from dvc_data.hashfile.hash import file_md5
+from dvc_data.hashfile.hash_info import HashInfo
 from tests.utils import get_gitignore_content
 
 
 def test_add(tmp_dir, dvc):
     (stage,) = tmp_dir.dvc_gen({"foo": "foo"})
     md5 = file_md5("foo", dvc.fs)
 
@@ -93,78 +89,65 @@
 
     (stage,) = dvc.add("\xe1")
 
     assert os.path.isfile(stage.path)
 
 
 def test_add_unsupported_file(dvc):
-    with pytest.raises(DvcException):
+    with pytest.raises(ConfigError, match="Unsupported URL type"):
         dvc.add("unsupported://unsupported")
 
 
 def test_add_directory(tmp_dir, dvc):
-    from dvc.data import load
+    from dvc_data.hashfile import load
 
     (stage,) = tmp_dir.dvc_gen({"dir": {"file": "file"}})
 
     assert stage is not None
     assert len(stage.deps) == 0
     assert len(stage.outs) == 1
 
     hash_info = stage.outs[0].hash_info
 
-    obj = load(dvc.odb.local, hash_info)
+    obj = load(dvc.cache.local, hash_info)
     for key, _, _ in obj:
         for part in key:
             assert "\\" not in part
 
 
-class TestAddDirectoryRecursive(TestDvc):
-    def test(self):
-        stages = self.dvc.add(self.DATA_DIR, recursive=True)
-        self.assertEqual(len(stages), 2)
-
-
-class TestAddCmdDirectoryRecursive(TestDvc):
-    def test(self):
-        ret = main(["add", "--recursive", self.DATA_DIR])
-        self.assertEqual(ret, 0)
-
-    def test_warn_about_large_directories(self):
-        warning = (
-            "You are adding a large directory 'large-dir' recursively."
-            "\nConsider tracking it as a whole instead with "
-            "`{cyan}dvc add large-dir{nc}`"
-        ).format(
-            cyan=colorama.Fore.CYAN,
-            nc=colorama.Style.RESET_ALL,
-        )
-
-        os.mkdir("large-dir")
-
-        # Create a lot of files
-        for iteration in range(LARGE_DIR_SIZE + 1):
-            path = os.path.join("large-dir", str(iteration))
-            with open(path, "w", encoding="utf-8") as fobj:
-                fobj.write(path)
-
-        assert main(["add", "--recursive", "large-dir"]) == 0
-        assert warning in self._capsys.readouterr()[1]
-
-
-class TestAddDirectoryWithForwardSlash(TestDvc):
-    def test(self):
-        dname = "directory/"
-        os.mkdir(dname)
-        self.create(os.path.join(dname, "file"), "file")
-        stages = self.dvc.add(dname)
-        self.assertEqual(len(stages), 1)
-        stage = stages[0]
-        self.assertTrue(stage is not None)
-        self.assertEqual(os.path.abspath("directory.dvc"), stage.path)
+def test_add_directory_recursive(tmp_dir, dvc):
+    tmp_dir.gen("data", {"file1": "file1", "sub": {"file2": "file2"}})
+    stages = dvc.add("data", recursive=True)
+    assert len(stages) == 2
+
+
+def test_add_cmd_directory_recursive(tmp_dir, dvc):
+    tmp_dir.gen("data", {"file1": "file1", "sub": {"file2": "file2"}})
+    assert main(["add", "--recursive", "data"]) == 0
+
+
+def test_warn_about_large_directories_recursive_add(tmp_dir, dvc, capsys):
+    warning = (
+        "You are adding a large directory 'large-dir' recursively."
+        "\nConsider tracking it as a whole instead with "
+        "`{cyan}dvc add large-dir{nc}`"
+    ).format(
+        cyan=colorama.Fore.CYAN,
+        nc=colorama.Style.RESET_ALL,
+    )
+
+    tmp_dir.gen("large-dir", {f"{i}": f"{i}" for i in range(LARGE_DIR_SIZE + 1)})
+    assert main(["add", "--recursive", "large-dir"]) == 0
+    assert warning in capsys.readouterr()[1]
+
+
+def test_add_directory_with_forward_slash(tmp_dir, dvc):
+    tmp_dir.gen("directory", {"file": "file"})
+    (stage,) = dvc.add("directory/")
+    assert stage.relpath == "directory.dvc"
 
 
 def test_add_tracked_file(tmp_dir, scm, dvc):
     path = "tracked_file"
     tmp_dir.scm_gen(path, "...", commit="add tracked file")
     msg = f""" output '{path}' is already tracked by SCM \\(e.g. Git\\).
     You can remove it from Git, then add to DVC.
@@ -172,41 +155,31 @@
             git rm -r --cached '{path}'
             git commit -m "stop tracking {path}" """
 
     with pytest.raises(OutputAlreadyTrackedError, match=msg):
         dvc.add(path)
 
 
-class TestAddDirWithExistingCache(TestDvc):
-    def test(self):
-        dname = "a"
-        fname = os.path.join(dname, "b")
-        os.mkdir(dname)
-        shutil.copyfile(self.FOO, fname)
-
-        stages = self.dvc.add(self.FOO)
-        self.assertEqual(len(stages), 1)
-        self.assertTrue(stages[0] is not None)
-        stages = self.dvc.add(dname)
-        self.assertEqual(len(stages), 1)
-        self.assertTrue(stages[0] is not None)
-
-
-class TestAddModifiedDir(TestDvc):
-    def test(self):
-        stages = self.dvc.add(self.DATA_DIR)
-        self.assertEqual(len(stages), 1)
-        self.assertTrue(stages[0] is not None)
-        os.unlink(self.DATA)
-
-        time.sleep(2)
-
-        stages = self.dvc.add(self.DATA_DIR)
-        self.assertEqual(len(stages), 1)
-        self.assertTrue(stages[0] is not None)
+def test_add_dir_with_existing_cache(tmp_dir, dvc):
+    tmp_dir.gen({"foo": "foo", "dir": {"file": "foo"}})
+
+    (stage,) = dvc.add("foo")
+    assert stage is not None
+    (stage,) = dvc.add("dir")
+    assert stage is not None
+
+
+def test_add_modified_dir(tmp_dir, dvc):
+    tmp_dir.gen("data", {"foo": "foo", "sub": {"bar": "bar"}})
+    (stage,) = dvc.add("data")
+    assert stage is not None
+
+    (tmp_dir / "data" / "foo").unlink()
+    (stage,) = dvc.add("data")
+    assert stage is not None
 
 
 def test_add_file_in_dir(tmp_dir, dvc):
     tmp_dir.gen({"dir": {"subdir": {"subdata": "subdata content"}}})
     subdir_path = os.path.join("dir", "subdir", "subdata")
 
     (stage,) = dvc.add(subdir_path)
@@ -320,69 +293,66 @@
         "  size: 4\n"
         f"  path: {rel}\n"
     )
     assert fpath.read_text() == "file"
     assert dvc.status() == {}
 
 
-class TestAddLocalRemoteFile(TestDvc):
-    def test(self):
-        """
-        Making sure that 'remote' syntax is handled properly for local outs.
-        """
-        cwd = os.getcwd()
-        remote = "myremote"
-
-        ret = main(["remote", "add", remote, cwd])
-        self.assertEqual(ret, 0)
+def test_add_local_remote_file(tmp_dir, dvc):
+    """
+    Making sure that 'remote' syntax is handled properly for local outs.
+    """
+    tmp_dir.gen({"foo": "foo", "bar": "bar"})
+    tmp_dir.add_remote(url=tmp_dir.fs_path, name="myremote")
 
-        self.dvc.config.load()
+    assert main(["add", "remote://myremote/foo"]) == 0
+    d = (tmp_dir / "foo.dvc").load_yaml()
+    assert d["outs"][0]["path"] == "remote://myremote/foo"
 
-        foo = f"remote://{remote}/{self.FOO}"
-        ret = main(["add", foo])
-        self.assertEqual(ret, 0)
+    assert main(["add", (tmp_dir / "bar").fs_path]) == 0
+    d = (tmp_dir / "bar.dvc").load_yaml()
+    assert d["outs"][0]["path"] == "bar"
 
-        d = load_yaml("foo.dvc")
-        self.assertEqual(d["outs"][0]["path"], foo)
 
-        bar = os.path.join(cwd, self.BAR)
-        ret = main(["add", bar])
-        self.assertEqual(ret, 0)
+def test_cmd_add(tmp_dir, dvc):
+    tmp_dir.gen("foo", "foo")
+    ret = main(["add", "foo"])
+    assert ret == 0
 
-        d = load_yaml("bar.dvc")
-        self.assertEqual(d["outs"][0]["path"], self.BAR)
+    ret = main(["add", "non-existing-file"])
+    assert ret != 0
 
 
-class TestCmdAdd(TestDvc):
-    def test(self):
-        ret = main(["add", self.FOO])
-        self.assertEqual(ret, 0)
+def test_double_add_unchanged_file(tmp_dir, dvc):
+    tmp_dir.gen("foo", "foo")
+    ret = main(["add", "foo"])
+    assert ret == 0
 
-        ret = main(["add", "non-existing-file"])
-        self.assertNotEqual(ret, 0)
+    ret = main(["add", "foo"])
+    assert ret == 0
 
 
-class TestDoubleAddUnchanged(TestDvc):
-    def test_file(self):
-        ret = main(["add", self.FOO])
-        self.assertEqual(ret, 0)
+def test_double_add_unchanged_dir(tmp_dir, dvc):
+    tmp_dir.gen("data", {"foo": "foo"})
+    ret = main(["add", "data"])
+    assert ret == 0
 
-        ret = main(["add", self.FOO])
-        self.assertEqual(ret, 0)
+    ret = main(["add", "data"])
+    assert ret == 0
 
-    def test_dir(self):
-        ret = main(["add", self.DATA_DIR])
-        self.assertEqual(ret, 0)
 
-        ret = main(["add", self.DATA_DIR])
-        self.assertEqual(ret, 0)
+@pytest.mark.skipif(os.name == "nt", reason="unsupported on Windows")
+def test_add_colon_in_filename(tmp_dir, dvc):
+    tmp_dir.gen("fo:o", "foo")
+    ret = main(["add", "fo:o"])
+    assert ret == 0
 
 
 def test_should_update_state_entry_for_file_after_add(mocker, dvc, tmp_dir):
-    file_md5_counter = mocker.spy(dvc_module.data.stage, "file_md5")
+    file_md5_counter = mocker.spy(dvc_data.hashfile.hash, "file_md5")
     tmp_dir.gen("foo", "foo")
 
     ret = main(["config", "cache.type", "copy"])
     assert ret == 0
 
     ret = main(["add", "foo"])
     assert ret == 0
@@ -402,152 +372,152 @@
     assert file_md5_counter.mock.call_count == 1
 
     ret = main(["status"])
     assert ret == 0
     assert file_md5_counter.mock.call_count == 1
 
 
-def test_should_update_state_entry_for_directory_after_add(
-    mocker, dvc, tmp_dir
-):
-    file_md5_counter = mocker.spy(dvc_module.data.stage, "file_md5")
+def test_should_update_state_entry_for_directory_after_add(mocker, dvc, tmp_dir):
+    file_md5_counter = mocker.spy(dvc_data.hashfile.hash, "file_md5")
 
     tmp_dir.gen({"data/data": "foo", "data/data_sub/sub_data": "foo"})
 
     ret = main(["config", "cache.type", "copy"])
     assert ret == 0
 
     ret = main(["add", "data"])
     assert ret == 0
-    assert file_md5_counter.mock.call_count == 3
+    assert file_md5_counter.mock.call_count == 4
 
     ret = main(["status"])
     assert ret == 0
-    assert file_md5_counter.mock.call_count == 3
+    assert file_md5_counter.mock.call_count == 5
 
     ls = "dir" if os.name == "nt" else "ls"
-    ret = main(
-        ["run", "--single-stage", "-d", "data", "{} {}".format(ls, "data")]
-    )
+    ret = main(["run", "--single-stage", "-d", "data", "{} {}".format(ls, "data")])
     assert ret == 0
-    assert file_md5_counter.mock.call_count == 3
+    assert file_md5_counter.mock.call_count == 7
 
-    os.rename("data", "data" + ".back")
+    os.rename("data", "data.back")
     ret = main(["checkout"])
     assert ret == 0
-    assert file_md5_counter.mock.call_count == 3
+    assert file_md5_counter.mock.call_count == 7
 
     ret = main(["status"])
     assert ret == 0
-    assert file_md5_counter.mock.call_count == 3
+    assert file_md5_counter.mock.call_count == 9
 
 
-class TestAddCommit(TestDvc):
-    def test(self):
-        ret = main(["add", self.FOO, "--no-commit"])
-        self.assertEqual(ret, 0)
-        self.assertTrue(os.path.isfile(self.FOO))
-        self.assertFalse(os.path.exists(self.dvc.odb.local.cache_dir))
-
-        ret = main(["commit", self.FOO + ".dvc"])
-        self.assertEqual(ret, 0)
-        self.assertTrue(os.path.isfile(self.FOO))
-        self.assertEqual(len(os.listdir(self.dvc.odb.local.cache_dir)), 1)
+def test_add_commit(tmp_dir, dvc):
+    tmp_dir.gen("foo", "foo")
+    ret = main(["add", "foo", "--no-commit"])
+    assert ret == 0
+    assert os.path.isfile("foo")
+    assert not os.path.exists(dvc.cache.local.path)
+
+    ret = main(["commit", "foo.dvc"])
+    assert ret == 0
+    assert os.path.isfile("foo")
+    assert dvc.cache.local.exists("acbd18db4cc2f85cedef654fccc4a4d8")
 
 
 def test_should_collect_dir_cache_only_once(mocker, tmp_dir, dvc):
     tmp_dir.gen({"data/data": "foo"})
-    counter = mocker.spy(dvc_module.data.stage, "_stage_tree")
+    counter = mocker.spy(dvc_data.hashfile.build, "_build_tree")
     ret = main(["add", "data"])
     assert ret == 0
-    assert counter.mock.call_count == 1
+    assert counter.mock.call_count == 2
 
     ret = main(["status"])
     assert ret == 0
-    assert counter.mock.call_count == 1
+    assert counter.mock.call_count == 3
 
     ret = main(["status"])
     assert ret == 0
-    assert counter.mock.call_count == 1
+    assert counter.mock.call_count == 4
+
 
+def test_should_place_stage_in_data_dir_if_repository_below_symlink(
+    mocker, tmp_dir, dvc
+):
+    def is_symlink_true_below_dvc_root(path):
+        return path == os.path.dirname(dvc.root_dir)
 
-class TestShouldPlaceStageInDataDirIfRepositoryBelowSymlink(TestDvc):
-    def test(self):
-        def is_symlink_true_below_dvc_root(path):
-            if path == os.path.dirname(self.dvc.root_dir):
-                return True
-            return False
+    tmp_dir.gen({"data": {"foo": "foo"}})
+    mocker.patch.object(
+        system, "is_symlink", side_effect=is_symlink_true_below_dvc_root
+    )
+    ret = main(["add", os.path.join("data", "foo")])
+    assert ret == 0
 
-        with patch.object(
-            System, "is_symlink", side_effect=is_symlink_true_below_dvc_root
-        ):
+    assert not (tmp_dir / "foo.dvc").exists()
+    assert (tmp_dir / "data" / "foo.dvc").exists()
 
-            ret = main(["add", self.DATA])
-            self.assertEqual(0, ret)
 
-            stage_file_path_on_data_below_symlink = (
-                os.path.basename(self.DATA) + DVC_FILE_SUFFIX
-            )
-            self.assertFalse(
-                os.path.exists(stage_file_path_on_data_below_symlink)
-            )
+def test_should_throw_proper_exception_on_corrupted_stage_file(caplog, tmp_dir, dvc):
+    tmp_dir.gen({"foo": "foo", "bar": " bar"})
+    assert main(["add", "foo"]) == 0
 
-            stage_file_path = self.DATA + DVC_FILE_SUFFIX
-            self.assertTrue(os.path.exists(stage_file_path))
+    with (tmp_dir / "foo.dvc").open("a+") as f:
+        f.write("this will break yaml file structure")
 
+    caplog.clear()
+    assert main(["add", "bar"]) == 1
+    expected_error = "unable to read: 'foo.dvc', YAML file structure is corrupted"
+    assert expected_error in caplog.text
 
-class TestShouldThrowProperExceptionOnCorruptedStageFile(TestDvc):
-    def test(self):
-        ret = main(["add", self.FOO])
-        assert 0 == ret
 
-        foo_stage = relpath(self.FOO + DVC_FILE_SUFFIX)
+def test_should_throw_proper_exception_on_existing_out(caplog, tmp_dir, dvc):
+    tmp_dir.gen({"foo": "foo"})
+    (tmp_dir / "out").write_text("old contents")
 
-        # corrupt stage file
-        with open(foo_stage, "a+", encoding="utf-8") as file:
-            file.write("this will break yaml file structure")
+    assert main(["add", "foo", "--out", "out"]) == 1
 
-        self._caplog.clear()
+    assert (tmp_dir / "out").read_text() == "old contents"
+    expected_error_lines = [
+        "Error: The file 'out' already exists locally.",
+        "To override it, re-run with '--force'.",
+    ]
+    assert all(line in caplog.text for line in expected_error_lines)
 
-        ret = main(["add", self.BAR])
-        assert 1 == ret
 
-        expected_error = (
-            f"unable to read: '{foo_stage}', YAML file structure is corrupted"
-        )
+def test_add_force_overwrite_out(caplog, tmp_dir, dvc):
+    tmp_dir.gen({"foo": "foo"})
+    (tmp_dir / "out").write_text("old contents")
 
-        assert expected_error in self._caplog.text
+    assert main(["add", "foo", "--out", "out", "--force"]) == 0
+    assert (tmp_dir / "foo").read_text() == "foo"
 
 
-class TestAddFilename(TestDvc):
-    def test(self):
-        ret = main(["add", self.FOO, self.BAR, "--file", "error.dvc"])
-        self.assertNotEqual(0, ret)
+def test_add_filename(tmp_dir, dvc):
+    tmp_dir.gen({"foo": "foo", "bar": "bar", "data": {"file": "file"}})
+    ret = main(["add", "foo", "bar", "--file", "error.dvc"])
+    assert ret != 0
 
-        ret = main(["add", "-R", self.DATA_DIR, "--file", "error.dvc"])
-        self.assertNotEqual(0, ret)
+    ret = main(["add", "-R", "data", "--file", "error.dvc"])
+    assert ret != 0
 
-        with self.assertRaises(RecursiveAddingWhileUsingFilename):
-            self.dvc.add(self.DATA_DIR, recursive=True, fname="error.dvc")
+    with pytest.raises(RecursiveAddingWhileUsingFilename):
+        dvc.add("data", recursive=True, fname="error.dvc")
 
-        ret = main(["add", self.DATA_DIR, "--file", "data_directory.dvc"])
-        self.assertEqual(0, ret)
-        self.assertTrue(os.path.exists("data_directory.dvc"))
+    ret = main(["add", "data", "--file", "data_directory.dvc"])
+    assert ret == 0
+    assert (tmp_dir / "data_directory.dvc").exists()
 
-        ret = main(["add", self.FOO, "--file", "bar.dvc"])
-        self.assertEqual(0, ret)
-        self.assertTrue(os.path.exists("bar.dvc"))
-        self.assertFalse(os.path.exists("foo.dvc"))
+    ret = main(["add", "foo", "--file", "bar.dvc"])
+    assert ret == 0
+    assert (tmp_dir / "bar.dvc").exists()
+    assert not (tmp_dir / "foo.dvc").exists()
 
-        os.remove("bar.dvc")
+    (tmp_dir / "bar.dvc").unlink()
 
-        ret = main(["add", self.FOO, "--file", "bar.dvc"])
-        self.assertEqual(0, ret)
-        self.assertTrue(os.path.exists("bar.dvc"))
-        self.assertFalse(os.path.exists("foo.dvc"))
+    ret = main(["add", "foo", "--file", "bar.dvc"])
+    assert ret == 0
+    assert (tmp_dir / "bar.dvc").exists()
+    assert not (tmp_dir / "foo.dvc").exists()
 
 
 def test_failed_add_cleanup(tmp_dir, scm, dvc):
     tmp_dir.gen({"foo": "foo", "bar": "bar"})
 
     # Add and corrupt a stage file
     dvc.add("foo")
@@ -569,42 +539,41 @@
     assert ret == 0
 
     created_stages_filenames = stage_creator_spy.mock.call_args[0][1]
     for fname in created_stages_filenames:
         assert ".git" not in fname
 
 
-class TestAddUnprotected(TestDvc):
-    def test(self):
-        ret = main(["config", "cache.type", "hardlink"])
-        self.assertEqual(ret, 0)
+def test_add_unprotected(tmp_dir, dvc):
+    tmp_dir.gen("foo", "foo")
+    ret = main(["config", "cache.type", "hardlink"])
+    assert ret == 0
 
-        ret = main(["add", self.FOO])
-        self.assertEqual(ret, 0)
+    ret = main(["add", "foo"])
+    assert ret == 0
 
-        self.assertFalse(os.access(self.FOO, os.W_OK))
-        self.assertTrue(System.is_hardlink(self.FOO))
+    assert not os.access("foo", os.W_OK)
+    assert system.is_hardlink("foo")
 
-        ret = main(["unprotect", self.FOO])
-        self.assertEqual(ret, 0)
+    ret = main(["unprotect", "foo"])
+    assert ret == 0
 
-        ret = main(["add", self.FOO])
-        self.assertEqual(ret, 0)
+    ret = main(["add", "foo"])
+    assert ret == 0
 
-        self.assertFalse(os.access(self.FOO, os.W_OK))
-        self.assertTrue(System.is_hardlink(self.FOO))
+    assert not os.access("foo", os.W_OK)
+    assert system.is_hardlink("foo")
 
 
 @pytest.fixture
 def temporary_windows_drive(tmp_path_factory):
     import string
     from ctypes import windll
 
     try:
-        # pylint: disable=import-error
         import win32api
         from win32con import DDD_REMOVE_DEFINITION
     except ImportError:
         pytest.skip("pywin32 not installed")
 
     drives = [
         s[0].upper()
@@ -637,84 +606,82 @@
 
 
 @pytest.mark.skipif(os.name != "nt", reason="Windows specific")
 def test_windows_should_add_when_cache_on_different_drive(
     tmp_dir, dvc, temporary_windows_drive
 ):
     dvc.config["cache"]["dir"] = temporary_windows_drive
-    dvc.odb = ODBManager(dvc)
+    dvc.cache = CacheManager(dvc)
 
     (stage,) = tmp_dir.dvc_gen({"file": "file"})
     cache_path = stage.outs[0].cache_path
 
     assert path_isin(cache_path, temporary_windows_drive)
     assert os.path.isfile(cache_path)
     filecmp.cmp("file", cache_path)
 
 
 def test_readding_dir_should_not_unprotect_all(tmp_dir, dvc, mocker):
     tmp_dir.gen("dir/data", "data")
 
-    dvc.odb.local.cache_types = ["symlink"]
+    dvc.cache.local.cache_types = ["symlink"]
 
     dvc.add("dir")
     tmp_dir.gen("dir/new_file", "new_file_content")
 
-    unprotect_spy = mocker.spy(dvc.odb.local, "unprotect")
+    unprotect_spy = mocker.spy(dvc.cache.local, "unprotect")
     dvc.add("dir")
 
     assert not unprotect_spy.mock.called
-    assert System.is_symlink(os.path.join("dir", "new_file"))
+    assert system.is_symlink(os.path.join("dir", "new_file"))
 
 
 def test_should_not_checkout_when_adding_cached_copy(tmp_dir, dvc, mocker):
-    dvc.odb.local.cache_types = ["copy"]
+    dvc.cache.local.cache_types = ["copy"]
 
     tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"})
 
     shutil.copy("bar", "foo")
 
-    copy_spy = mocker.spy(dvc.odb.local.fs, "copy")
+    copy_spy = mocker.spy(dvc.cache.local.fs, "copy")
 
     dvc.add("foo")
 
     assert copy_spy.mock.call_count == 0
 
 
 @pytest.mark.parametrize(
     "link,new_link,link_test_func",
     [
-        ("hardlink", "copy", lambda path: not System.is_hardlink(path)),
-        ("symlink", "copy", lambda path: not System.is_symlink(path)),
-        ("copy", "hardlink", System.is_hardlink),
-        ("copy", "symlink", System.is_symlink),
+        ("hardlink", "copy", lambda path: not system.is_hardlink(path)),
+        ("symlink", "copy", lambda path: not system.is_symlink(path)),
+        ("copy", "hardlink", system.is_hardlink),
+        ("copy", "symlink", system.is_symlink),
     ],
 )
-def test_should_relink_on_repeated_add(
-    link, new_link, link_test_func, tmp_dir, dvc
-):
+def test_should_relink_on_repeated_add(link, new_link, link_test_func, tmp_dir, dvc):
     dvc.config["cache"]["type"] = link
 
     tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"})
 
     os.remove("foo")
-    getattr(dvc.odb.local.fs, link)(
+    getattr(dvc.cache.local.fs, link)(
         (tmp_dir / "bar").fs_path, (tmp_dir / "foo").fs_path
     )
 
-    dvc.odb.local.cache_types = [new_link]
+    dvc.cache.local.cache_types = [new_link]
 
     dvc.add("foo")
 
     assert link_test_func("foo")
 
 
 @pytest.mark.parametrize("link", ["hardlink", "symlink", "copy"])
 def test_should_protect_on_repeated_add(link, tmp_dir, dvc):
-    dvc.odb.local.cache_types = [link]
+    dvc.cache.local.cache_types = [link]
 
     tmp_dir.dvc_gen({"foo": "foo"})
 
     dvc.unprotect("foo")
 
     dvc.add("foo")
 
@@ -740,84 +707,92 @@
         fname = "file!with_weird#naming_[1].txt"
         ignored_fname = r"/file\!with_weird\#naming_\[1\].txt"
 
     tmp_dir.dvc_gen(fname, "...")
     assert ignored_fname in get_gitignore_content()
 
 
-@pytest.mark.xfail(reason="error message relpath")
 def test_add_from_data_dir(tmp_dir, scm, dvc):
     tmp_dir.dvc_gen({"dir": {"file1": "file1 content"}})
 
     tmp_dir.gen({"dir": {"file2": "file2 content"}})
 
+    dvc.add(os.path.join("dir", "file2"))
+
+
+def test_add_parent_dir(tmp_dir, scm, dvc):
+    tmp_dir.gen({"dir": {"file1": "file1 content"}})
+    out_path = os.path.join("dir", "file1")
+    dvc.add(out_path, fname=out_path + ".dvc")
+
     with pytest.raises(OverlappingOutputPathsError) as e:
-        dvc.add(os.path.join("dir", "file2"), fname="file2.dvc")
+        dvc.add("dir", fname="dir.dvc")
     assert str(e.value) == (
-        "Cannot add '{out}', because it is overlapping with other DVC "
-        "tracked output: 'dir'.\n"
-        "To include '{out}' in 'dir', run 'dvc commit dir.dvc'"
-    ).format(out=os.path.join("dir", "file2"))
+        "Cannot add 'dir', because it is overlapping with other DVC "
+        "tracked output: '{out}'.\n"
+        "To include '{out}' in 'dir', run 'dvc remove {out}.dvc' "
+        "and then 'dvc add dir'"
+    ).format(out=os.path.join("dir", "file1"))
 
 
 def test_not_raises_on_re_add(tmp_dir, dvc):
     tmp_dir.dvc_gen("file", "file content")
 
     tmp_dir.gen({"file2": "file2 content", "file": "modified file"})
     dvc.add(["file2", "file"])
 
 
 @pytest.mark.parametrize("link", ["hardlink", "symlink", "copy"])
 def test_add_empty_files(tmp_dir, dvc, link):
     file = "foo"
-    dvc.odb.local.cache_types = [link]
+    dvc.cache.local.cache_types = [link]
     stages = tmp_dir.dvc_gen(file, "")
 
     assert (tmp_dir / file).exists()
     assert (tmp_dir / (file + DVC_FILE_SUFFIX)).exists()
     assert os.path.exists(stages[0].outs[0].cache_path)
 
 
 def test_add_optimization_for_hardlink_on_empty_files(tmp_dir, dvc, mocker):
-    dvc.odb.local.cache_types = ["hardlink"]
+    dvc.cache.local.cache_types = ["hardlink"]
     tmp_dir.gen({"foo": "", "bar": "", "lorem": "lorem", "ipsum": "ipsum"})
     m = mocker.spy(LocalFileSystem, "is_hardlink")
     stages = dvc.add(["foo", "bar", "lorem", "ipsum"])
 
-    assert m.call_count == 4
+    assert m.call_count == 8
     assert m.call_args != call(tmp_dir / "foo")
     assert m.call_args != call(tmp_dir / "bar")
 
     for stage in stages[:2]:
         # hardlinks are not created for empty files
-        assert not System.is_hardlink(stage.outs[0].fs_path)
+        assert not system.is_hardlink(stage.outs[0].fs_path)
 
     for stage in stages[2:]:
-        assert System.is_hardlink(stage.outs[0].fs_path)
+        assert system.is_hardlink(stage.outs[0].fs_path)
 
     for stage in stages:
         assert os.path.exists(stage.path)
         assert os.path.exists(stage.outs[0].cache_path)
 
 
-def test_output_duplication_for_pipeline_tracked(tmp_dir, dvc, run_copy):
+def test_try_adding_pipeline_tracked_output(tmp_dir, dvc, run_copy):
     tmp_dir.dvc_gen("foo", "foo")
     run_copy("foo", "bar", name="copy-foo-bar")
-    with pytest.raises(OutputDuplicationError):
+    with pytest.raises(DvcException, match="cannot update 'bar': not a data source"):
         dvc.add("bar")
 
 
 def test_add_pipeline_file(tmp_dir, dvc, run_copy):
-    from dvc.dvcfile import PIPELINE_FILE
+    from dvc.dvcfile import PROJECT_FILE
 
     tmp_dir.dvc_gen("foo", "foo")
     run_copy("foo", "bar", name="copy-foo-bar")
 
     with pytest.raises(OutputIsStageFileError):
-        dvc.add(PIPELINE_FILE)
+        dvc.add(PROJECT_FILE)
 
 
 def test_add_symlink_file(tmp_dir, dvc):
     tmp_dir.gen({"dir": {"bar": "bar"}})
 
     (tmp_dir / "dir" / "foo").symlink_to(os.path.join(".", "bar"))
 
@@ -826,17 +801,17 @@
     assert not (tmp_dir / "foo.dvc").exists()
     assert (tmp_dir / "dir" / "foo.dvc").exists()
     assert not (tmp_dir / "dir" / "foo").is_symlink()
     assert not (tmp_dir / "dir" / "bar").is_symlink()
     assert (tmp_dir / "dir" / "foo").read_text() == "bar"
     assert (tmp_dir / "dir" / "bar").read_text() == "bar"
 
-    assert (tmp_dir / ".dvc" / "cache").read_text() == {
-        "37": {"b51d194a7513e45b56f6524f2d51f2": "bar"}
-    }
+    assert (
+        tmp_dir / ".dvc" / "cache" / "37" / "b51d194a7513e45b56f6524f2d51f2"
+    ).read_text() == "bar"
     assert not (
         tmp_dir / ".dvc" / "cache" / "37" / "b51d194a7513e45b56f6524f2d51f2"
     ).is_symlink()
 
     # Test that subsequent add succeeds
     # See https://github.com/iterative/dvc/issues/4654
     dvc.add(os.path.join("dir", "foo"))
@@ -852,15 +827,16 @@
         tmp_dir.gen({"data": {"foo": "foo"}})
         target = os.path.join(".", "data")
 
     tmp_dir.gen({"data": {"foo": "foo"}})
 
     (tmp_dir / "dir").symlink_to(target)
 
-    with pytest.raises(DvcException):
+    msg = "Cannot add files inside symlinked directories to DVC"
+    with pytest.raises(DvcException, match=msg):
         dvc.add("dir")
 
 
 @pytest.mark.parametrize("external", [True, False])
 def test_add_file_in_symlink_dir(make_tmp_dir, tmp_dir, dvc, external):
     if external:
         data_dir = make_tmp_dir("data")
@@ -868,104 +844,92 @@
         target = os.fspath(data_dir / "dir")
     else:
         tmp_dir.gen({"data": {"foo": "foo"}})
         target = os.path.join(".", "data")
 
     (tmp_dir / "dir").symlink_to(target)
 
-    with pytest.raises(DvcException):
+    msg = "Cannot add files inside symlinked directories to DVC"
+    with pytest.raises(DvcException, match=msg):
         dvc.add(os.path.join("dir", "foo"))
 
 
 def test_add_with_cache_link_error(tmp_dir, dvc, mocker, capsys):
     tmp_dir.gen("foo", "foo")
 
     mocker.patch(
-        "dvc.data.checkout.test_links",
+        "dvc_data.hashfile.checkout.test_links",
         return_value=[],
     )
     dvc.add("foo")
     err = capsys.readouterr()[1]
     assert "reconfigure cache types" in err
 
     assert (tmp_dir / "foo").exists()
     assert (tmp_dir / "foo.dvc").exists()
-    assert (tmp_dir / ".dvc" / "cache").read_text() == {
-        "ac": {"bd18db4cc2f85cedef654fccc4a4d8": "foo"}
-    }
+    assert (
+        tmp_dir / ".dvc" / "cache" / "ac" / "bd18db4cc2f85cedef654fccc4a4d8"
+    ).read_text() == "foo"
 
 
-def test_add_preserve_meta(tmp_dir, dvc):
+def test_add_preserve_fields(tmp_dir, dvc):
     text = textwrap.dedent(
         """\
         # top comment
         desc: top desc
         outs:
         - path: foo # out comment
           desc: out desc
+          type: mytype
+          labels:
+          - label1
+          - label2
+          remote: testremote
         meta: some metadata
     """
     )
     tmp_dir.gen("foo.dvc", text)
-
     tmp_dir.dvc_gen("foo", "foo")
     assert (tmp_dir / "foo.dvc").read_text() == textwrap.dedent(
         """\
         # top comment
         desc: top desc
         outs:
         - path: foo # out comment
           desc: out desc
+          type: mytype
+          labels:
+          - label1
+          - label2
+          remote: testremote
           md5: acbd18db4cc2f85cedef654fccc4a4d8
           size: 3
         meta: some metadata
     """
     )
 
 
 # NOTE: unless long paths are enabled on Windows, PATH_MAX and NAME_MAX
 # are the same 260 chars, which makes the test unnecessarily complex
 @pytest.mark.skipif(os.name == "nt", reason="unsupported on Windows")
 def test_add_long_fname(tmp_dir, dvc):
-    name_max = os.pathconf(tmp_dir, "PC_NAME_MAX")  # pylint: disable=no-member
+    name_max = os.pathconf(tmp_dir, "PC_NAME_MAX")
     name = "a" * name_max
     tmp_dir.gen({"data": {name: "foo"}})
 
     # nothing we can do in this case, as the resulting dvcfile
     # will definitely exceed NAME_MAX
-    with pytest.raises(OSError) as info:
+    with pytest.raises(OSError, match=f"File name too long: .*{name}") as info:
         dvc.add(os.path.join("data", name))
     assert info.value.errno == errno.ENAMETOOLONG
 
     dvc.add("data")
     assert (tmp_dir / "data").read_text() == {name: "foo"}
 
 
-def test_add_to_remote(tmp_dir, dvc, remote, workspace):
-    workspace.gen("foo", "foo")
-
-    url = "remote://workspace/foo"
-    [stage] = dvc.add(url, to_remote=True)
-
-    assert not (tmp_dir / "foo").exists()
-    assert (tmp_dir / "foo.dvc").exists()
-
-    assert len(stage.deps) == 0
-    assert len(stage.outs) == 1
-
-    hash_info = stage.outs[0].hash_info
-    meta = stage.outs[0].meta
-    with open(
-        remote.hash_to_path(hash_info.value), encoding="utf-8"
-    ) as stream:
-        assert stream.read() == "foo"
-
-    assert meta.size == len("foo")
-
-
 def test_add_to_remote_absolute(tmp_dir, make_tmp_dir, dvc, remote):
     tmp_abs_dir = make_tmp_dir("abs")
     tmp_foo = tmp_abs_dir / "foo"
     tmp_foo.write_text("foo")
 
     dvc.add(str(tmp_foo), to_remote=True)
     tmp_foo.unlink()
@@ -974,16 +938,16 @@
     assert foo.with_suffix(".dvc").exists()
     assert not os.path.exists(tmp_foo)
 
     dvc.pull("foo")
     assert not os.path.exists(tmp_foo)
     assert foo.read_text() == "foo"
 
+    tmp_bar = tmp_abs_dir / "bar"
     with pytest.raises(StageExternalOutputsError):
-        tmp_bar = tmp_abs_dir / "bar"
         dvc.add(str(tmp_foo), out=str(tmp_bar), to_remote=True)
 
 
 @pytest.mark.parametrize(
     "invalid_opt, kwargs",
     [
         ("multiple targets", {"targets": ["foo", "bar", "baz"]}),
@@ -1029,14 +993,21 @@
 
     foo.unlink()
     status = dvc.checkout(str(foo))
     assert status["added"] == ["foo"]
     assert foo.read_text() == "foo"
 
 
+def test_add_with_out(tmp_dir, scm, dvc):
+    tmp_dir.gen({"foo": "foo"})
+    dvc.add("foo", out="out_foo")
+    gitignore_content = get_gitignore_content()
+    assert "/out_foo" in gitignore_content
+
+
 def test_add_to_cache_different_name(tmp_dir, dvc, local_cloud):
     local_cloud.gen({"data": {"foo": "foo", "bar": "bar"}})
 
     dvc.add(str(local_cloud / "data"), out="not_data")
 
     not_data = tmp_dir / "not_data"
     assert not_data.read_text() == {"foo": "foo", "bar": "bar"}
@@ -1119,38 +1090,92 @@
     assert (tmp_dir / stage.path).read_text() == dvcfile_contents
 
 
 @pytest.mark.parametrize(
     "target",
     [
         "dvc.repo.index.Index.check_graph",
-        "dvc.stage.Stage.save",
-        "dvc.stage.Stage.commit",
+        "dvc.stage.Stage.add_outs",
     ],
 )
-def test_add_does_not_remove_stage_file_on_failure(
-    tmp_dir, dvc, mocker, target
-):
+def test_add_does_not_remove_stage_file_on_failure(tmp_dir, dvc, mocker, target):
     (stage,) = tmp_dir.dvc_gen("foo", "foo")
     tmp_dir.gen("foo", "foobar")  # update file
     dvcfile_contents = (tmp_dir / stage.path).read_text()
 
     exc_msg = f"raising error from mocked '{target}'"
     mocker.patch(
         target,
         side_effect=DvcException(exc_msg),
     )
 
-    with pytest.raises(DvcException) as exc_info:
+    with pytest.raises(DvcException, match=exc_msg):
         dvc.add("foo")
-    assert str(exc_info.value) == exc_msg
     assert (tmp_dir / "foo.dvc").exists()
     assert (tmp_dir / stage.path).read_text() == dvcfile_contents
 
 
 def test_add_ignore_duplicated_targets(tmp_dir, dvc, capsys):
     tmp_dir.gen({"foo": "foo", "bar": "bar", "foobar": "foobar"})
     stages = dvc.add(["foo", "bar", "foobar", "bar", "foo"])
 
     _, err = capsys.readouterr()
     assert len(stages) == 3
     assert "ignoring duplicated targets: foo, bar" in err
+
+
+def test_add_updates_to_cloud_versioning_dir(tmp_dir, dvc):
+    data_dvc = tmp_dir / "data.dvc"
+    data_dvc.dump(
+        {
+            "outs": [
+                {
+                    "path": "data",
+                    "files": [
+                        {
+                            "size": 3,
+                            "version_id": "WYRG4BglP7pD.gEoJP6a4AqOhl.FRA.h",
+                            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
+                            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
+                            "relpath": "bar",
+                        },
+                        {
+                            "size": 3,
+                            "version_id": "0vL53tFVY5vVAoJ4HG2jCS1mEcohDPE0",
+                            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
+                            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
+                            "relpath": "foo",
+                        },
+                    ],
+                }
+            ]
+        }
+    )
+
+    data = tmp_dir / "data"
+    data.mkdir()
+    (data / "foo").write_text("foo")
+    (data / "bar").write_text("bar2")
+
+    dvc.add("data")
+
+    assert (tmp_dir / "data.dvc").parse() == {
+        "outs": [
+            {
+                "path": "data",
+                "files": [
+                    {
+                        "size": 4,
+                        "md5": "224e2539f52203eb33728acd228b4432",
+                        "relpath": "bar",
+                    },
+                    {
+                        "size": 3,
+                        "version_id": "0vL53tFVY5vVAoJ4HG2jCS1mEcohDPE0",
+                        "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
+                        "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
+                        "relpath": "foo",
+                    },
+                ],
+            }
+        ]
+    }
```

### Comparing `dvc-2.9.5/tests/func/test_analytics.py` & `dvc-3.0.0a0/tests/func/test_analytics.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,35 +8,34 @@
 from dvc.repo import Repo
 from tests.utils import ANY
 
 
 def test_daemon_analytics(mocker, tmp_path):
     mock_send = mocker.patch("dvc.analytics.send")
     report = os.fspath(tmp_path)
-    assert 0 == main(["daemon", "analytics", report])
+    assert main(["daemon", "analytics", report]) == 0
 
     mock_send.assert_called_with(report)
 
 
 def test_main_analytics(mocker, tmp_dir, dvc):
     mock_is_enabled = mocker.patch("dvc.analytics.collect_and_send_report")
     mock_report = mocker.patch("dvc.analytics.is_enabled", return_value=True)
     tmp_dir.gen("foo", "text")
-    assert 0 == main(["add", "foo"])
+    assert main(["add", "foo"]) == 0
     assert mock_is_enabled.called
     assert mock_report.called
 
 
 @pytest.fixture
 def mock_daemon(mocker):
     def func(argv):
         return main(["daemon", *argv])
 
-    m = mocker.patch("dvc.daemon.daemon", mocker.MagicMock(side_effect=func))
-    yield m
+    return mocker.patch("dvc.daemon.daemon", mocker.MagicMock(side_effect=func))
 
 
 def test_collect_and_send_report(mocker, dvc, mock_daemon):
     mock_post = mocker.patch("requests.post")
     collect_and_send_report()
 
     assert mock_daemon.call_count == 1
```

### Comparing `dvc-2.9.5/tests/func/test_api.py` & `dvc-3.0.0a0/tests/func/api/test_data.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,38 +1,38 @@
 import os
 
 import pytest
 from funcy import first, get_in
 
 from dvc import api
-from dvc.exceptions import FileMissingError, OutputNotFoundError
-from dvc.testing.test_api import TestAPI  # noqa, pylint: disable=unused-import
+from dvc.exceptions import OutputNotFoundError, PathMissingError
+from dvc.testing.api_tests import TestAPI  # noqa, pylint: disable=unused-import
+from dvc.testing.tmp_dir import make_subrepo
 from dvc.utils.fs import remove
-from tests.unit.fs.test_repo import make_subrepo
 
 
 def test_get_url_external(tmp_dir, erepo_dir, cloud):
     erepo_dir.add_remote(config=cloud.config)
     with erepo_dir.chdir():
         erepo_dir.dvc_gen("foo", "foo", commit="add foo")
 
     # Using file url to force clone to tmp repo
-    repo_url = f"file://{erepo_dir}"
+    repo_url = f"file://{erepo_dir.as_posix()}"
     expected_url = (cloud / "ac/bd18db4cc2f85cedef654fccc4a4d8").url
     assert api.get_url("foo", repo=repo_url) == expected_url
 
 
 def test_get_url_requires_dvc(tmp_dir, scm):
     tmp_dir.scm_gen({"foo": "foo"}, commit="initial")
 
     with pytest.raises(OutputNotFoundError, match="output 'foo'"):
         api.get_url("foo", repo=os.fspath(tmp_dir))
 
     with pytest.raises(OutputNotFoundError, match="output 'foo'"):
-        api.get_url("foo", repo=f"file://{tmp_dir}")
+        api.get_url("foo", repo=f"file://{tmp_dir.as_posix()}")
 
 
 def test_open_external(tmp_dir, erepo_dir, cloud):
     erepo_dir.add_remote(config=cloud.config)
 
     with erepo_dir.chdir():
         erepo_dir.dvc_gen("version", "master", commit="add version")
@@ -40,73 +40,71 @@
         with erepo_dir.branch("branch", new="True"):
             # NOTE: need file to be other size for Mac
             erepo_dir.dvc_gen("version", "branchver", commit="add version")
 
     erepo_dir.dvc.push(all_branches=True)
 
     # Remove cache to force download
-    remove(erepo_dir.dvc.odb.local.cache_dir)
+    remove(erepo_dir.dvc.cache.local.path)
 
     # Using file url to force clone to tmp repo
-    repo_url = f"file://{erepo_dir}"
+    repo_url = f"file://{erepo_dir.as_posix()}"
     with api.open("version", repo=repo_url) as fd:
         assert fd.read() == "master"
 
     assert api.read("version", repo=repo_url, rev="branch") == "branchver"
 
 
 def test_open_granular(tmp_dir, dvc, remote):
     tmp_dir.dvc_gen({"dir": {"foo": "foo-text"}})
     dvc.push()
 
     # Remove cache to force download
-    remove(dvc.odb.local.cache_dir)
+    remove(dvc.cache.local.path)
 
     with api.open("dir/foo") as fd:
         assert fd.read() == "foo-text"
 
 
 def test_missing(tmp_dir, dvc, remote):
     tmp_dir.dvc_gen("foo", "foo")
 
     # Remove cache to make foo missing
-    remove(dvc.odb.local.cache_dir)
+    remove(dvc.cache.local.path)
 
     api.read("foo")
 
     remove("foo")
 
-    with pytest.raises(FileMissingError):
+    with pytest.raises(PathMissingError):
         api.read("foo")
 
 
 def test_open_scm_controlled(tmp_dir, erepo_dir):
     erepo_dir.scm_gen({"scm_controlled": "file content"}, commit="create file")
 
     with api.open("scm_controlled", repo=os.fspath(erepo_dir)) as fd:
         assert fd.read() == "file content"
 
 
 def test_open_not_cached(dvc):
     metric_file = "metric.txt"
     metric_content = "0.6"
-    metric_code = "open('{}', 'w').write('{}')".format(
-        metric_file, metric_content
-    )
+    metric_code = f"open('{metric_file}', 'w').write('{metric_content}')"
     dvc.run(
         single_stage=True,
         metrics_no_cache=[metric_file],
-        cmd=(f'python -c "{metric_code}"'),
+        cmd=f'python -c "{metric_code}"',
     )
 
     with api.open(metric_file) as fd:
         assert fd.read() == metric_content
 
     os.remove(metric_file)
-    with pytest.raises(FileMissingError):
+    with pytest.raises(PathMissingError):
         api.read(metric_file)
 
 
 def test_open_rev(tmp_dir, scm, dvc):
     tmp_dir.scm_gen("foo", "foo", commit="foo")
 
     (tmp_dir / "foo").write_text("bar")
@@ -127,18 +125,18 @@
 def test_api_missing_local_cache_exists_on_remote(
     tmp_dir, scm, dvc, as_external, remote, files, to_read
 ):
     tmp_dir.dvc_gen(files, commit="DVC track files")
     dvc.push()
 
     # Remove cache to make foo missing
-    remove(dvc.odb.local.cache_dir)
+    remove(dvc.cache.local.path)
     remove(first(files))
 
-    repo_url = f"file://{tmp_dir}" if as_external else None
+    repo_url = f"file://{tmp_dir.as_posix()}" if as_external else None
     file_content = get_in(files, to_read.split(os.sep))
     assert api.read(to_read, repo=repo_url) == file_content
 
 
 @pytest.mark.parametrize("local_repo", [False, True])
 def test_read_with_subrepos(tmp_dir, scm, local_cloud, local_repo):
     tmp_dir.scm_gen("foo.txt", "foo.txt", commit="add foo.txt")
@@ -146,37 +144,31 @@
     make_subrepo(subrepo, scm, config=local_cloud.config)
     with subrepo.chdir():
         subrepo.scm_gen({"lorem": "lorem"}, commit="add lorem")
         subrepo.dvc_gen({"dir": {"file.txt": "file.txt"}}, commit="add dir")
         subrepo.dvc_gen("dvc-file", "dvc-file", commit="add dir")
         subrepo.dvc.push()
 
-    repo_path = None if local_repo else f"file:///{tmp_dir}"
+    repo_path = None if local_repo else f"file://{tmp_dir.as_posix()}"
     subrepo_path = os.path.join("dir", "subrepo")
 
     assert api.read("foo.txt", repo=repo_path) == "foo.txt"
+    assert api.read(os.path.join(subrepo_path, "lorem"), repo=repo_path) == "lorem"
     assert (
-        api.read(os.path.join(subrepo_path, "lorem"), repo=repo_path)
-        == "lorem"
-    )
-    assert (
-        api.read(os.path.join(subrepo_path, "dvc-file"), repo=repo_path)
-        == "dvc-file"
+        api.read(os.path.join(subrepo_path, "dvc-file"), repo=repo_path) == "dvc-file"
     )
     assert (
         api.read(os.path.join(subrepo_path, "dir", "file.txt"), repo=repo_path)
         == "file.txt"
     )
 
 
 def test_get_url_granular(tmp_dir, dvc, cloud):
     tmp_dir.add_remote(config=cloud.config)
-    tmp_dir.dvc_gen(
-        {"dir": {"foo": "foo", "bar": "bar", "nested": {"file": "file"}}}
-    )
+    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar", "nested": {"file": "file"}}})
 
     expected_url = (cloud / "5f" / "c28ea78987408341668eba6525ebd1.dir").url
     assert api.get_url("dir") == expected_url
 
     expected_url = (cloud / "ac" / "bd18db4cc2f85cedef654fccc4a4d8").url
     assert api.get_url("dir/foo") == expected_url
 
@@ -187,21 +179,30 @@
     assert api.get_url(os.path.join("dir", "nested", "file")) == expected_url
 
 
 def test_get_url_subrepos(tmp_dir, scm, local_cloud):
     subrepo = tmp_dir / "subrepo"
     make_subrepo(subrepo, scm, config=local_cloud.config)
     with subrepo.chdir():
-        subrepo.dvc_gen(
-            {"dir": {"foo": "foo"}, "bar": "bar"}, commit="add files"
-        )
+        subrepo.dvc_gen({"dir": {"foo": "foo"}, "bar": "bar"}, commit="add files")
         subrepo.dvc.push()
 
-    expected_url = os.fspath(
-        local_cloud / "ac" / "bd18db4cc2f85cedef654fccc4a4d8"
-    )
+    expected_url = os.fspath(local_cloud / "ac" / "bd18db4cc2f85cedef654fccc4a4d8")
     assert api.get_url(os.path.join("subrepo", "dir", "foo")) == expected_url
 
-    expected_url = os.fspath(
-        local_cloud / "37" / "b51d194a7513e45b56f6524f2d51f2"
-    )
+    expected_url = os.fspath(local_cloud / "37" / "b51d194a7513e45b56f6524f2d51f2")
     assert api.get_url("subrepo/bar") == expected_url
+
+
+def test_open_from_remote(tmp_dir, erepo_dir, cloud, local_cloud):
+    erepo_dir.add_remote(config=cloud.config, name="other")
+    erepo_dir.add_remote(config=local_cloud.config, default=True)
+    erepo_dir.dvc_gen({"dir": {"foo": "foo content"}}, commit="create file")
+    erepo_dir.dvc.push(remote="other")
+    remove(erepo_dir.dvc.cache.local.path)
+
+    with api.open(
+        os.path.join("dir", "foo"),
+        repo=f"file://{erepo_dir.as_posix()}",
+        remote="other",
+    ) as fd:
+        assert fd.read() == "foo content"
```

### Comparing `dvc-2.9.5/tests/func/test_check_ignore.py` & `dvc-3.0.0a0/tests/func/test_check_ignore.py`

 * *Files 5% similar despite different names*

```diff
@@ -61,15 +61,15 @@
         ["-a", "file"],
         ["-q", "-d", "file"],
         ["--stdin", "file"],
         [],
     ],
 )
 def test_check_ignore_error_args_cases(tmp_dir, dvc, args, caplog):
-    assert main(["check-ignore"] + args) == 255
+    assert main(["check-ignore", *args]) == 255
     assert "Having any troubles?" not in caplog.text
 
 
 @pytest.mark.parametrize("path,ret", [({"dir": {}}, 0), ({"dir": "files"}, 1)])
 def test_check_ignore_dir(tmp_dir, dvc, path, ret):
     tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "dir/")
     tmp_dir.gen(path)
@@ -83,23 +83,19 @@
 
 def test_check_ignore_out_side_repo(tmp_dir, dvc):
     tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "file")
     assert main(["check-ignore", "-q", "../file"]) == 1
 
 
 def test_check_ignore_sub_repo(tmp_dir, dvc, capsys):
-    tmp_dir.gen(
-        {DvcIgnore.DVCIGNORE_FILE: "other", "dir": {".dvc": {}, "foo": "bar"}}
-    )
+    tmp_dir.gen({DvcIgnore.DVCIGNORE_FILE: "other", "dir": {".dvc": {}, "foo": "bar"}})
 
     assert main(["check-ignore", "-d", os.path.join("dir", "foo")]) == 0
     out, _ = capsys.readouterr()
-    assert (
-        "in sub_repo:{}\t{}".format("dir", os.path.join("dir", "foo")) in out
-    )
+    assert "in sub_repo:{}\t{}".format("dir", os.path.join("dir", "foo")) in out
 
 
 def test_check_sub_dir_ignore_file(tmp_dir, dvc, capsys):
     tmp_dir.gen(
         {
             DvcIgnore.DVCIGNORE_FILE: "other",
             "dir": {DvcIgnore.DVCIGNORE_FILE: "bar\nfoo", "foo": "bar"},
@@ -132,16 +128,14 @@
     assert f"{DvcIgnore.DVCIGNORE_FILE}:1:f*\tfoo\n" in out
     assert f"{DvcIgnore.DVCIGNORE_FILE}:2:!foo\tfoo\n" in out
 
 
 @pytest.mark.parametrize(
     "file,ret,output", [("ignored", 0, True), ("not_ignored", 1, False)]
 )
-def test_check_ignore_stdin_mode(
-    tmp_dir, dvc, file, ret, output, capsys, mocker
-):
+def test_check_ignore_stdin_mode(tmp_dir, dvc, file, ret, output, capsys, mocker):
     tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "ignored")
     mocker.patch("builtins.input", side_effect=[file, ""])
 
     assert main(["check-ignore", "--stdin"]) == ret
     out, _ = capsys.readouterr()
     assert (file in out) is output
```

### Comparing `dvc-2.9.5/tests/func/test_checkout.py` & `dvc-3.0.0a0/tests/func/test_checkout.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,532 +1,418 @@
-import collections
-import filecmp
 import logging
 import os
 import shutil
 import stat
 import textwrap
-from unittest.mock import patch
 
 import pytest
 
 from dvc.cli import main
-from dvc.dvcfile import DVC_FILE_SUFFIX, PIPELINE_FILE, Dvcfile
+from dvc.dvcfile import PROJECT_FILE, load_file
 from dvc.exceptions import (
     CheckoutError,
     CheckoutErrorSuggestGit,
     ConfirmRemoveError,
-    DvcException,
     NoOutputOrStageError,
 )
-from dvc.fs.local import LocalFileSystem
+from dvc.fs import LocalFileSystem, system
 from dvc.stage import Stage
 from dvc.stage.exceptions import StageFileDoesNotExistError
-from dvc.system import System
 from dvc.utils import relpath
-from dvc.utils.fs import remove, walk_files
+from dvc.utils.fs import remove
 from dvc.utils.serialize import dump_yaml, load_yaml
-from tests.basic_env import TestDvc, TestDvcGit
-from tests.func.test_repro import TestRepro
+from tests.utils import get_gitignore_content
 
 logger = logging.getLogger("dvc")
 
 
-class TestCheckout(TestRepro):
-    def setUp(self):
-        super().setUp()
+def walk_files(directory):
+    for root, _, files in os.walk(directory):
+        for f in files:
+            yield os.path.join(root, f)
 
-        stages = self.dvc.add(self.DATA_DIR)
-        self.assertEqual(len(stages), 1)
-        self.data_dir_stage = stages[0]
-        self.assertTrue(self.data_dir_stage is not None)
 
-        self.orig = "orig"
-        shutil.copy(self.FOO, self.orig)
-        os.unlink(self.FOO)
-
-        self.orig_dir = "orig_dir"
-        shutil.copytree(self.DATA_DIR, self.orig_dir)
-        shutil.rmtree(self.DATA_DIR)
-
-    def test(self):
-        self.dvc.checkout(force=True)
-        self._test_checkout()
-
-    def _test_checkout(self):
-        self.assertTrue(os.path.isfile(self.FOO))
-        self.assertTrue(filecmp.cmp(self.FOO, self.orig, shallow=False))
-
-
-class TestCheckoutSingleStage(TestCheckout):
-    def test(self):
-        ret = main(["checkout", "--force", self.foo_stage.path])
-        self.assertEqual(ret, 0)
-
-        ret = main(["checkout", "--force", self.data_dir_stage.path])
-        self.assertEqual(ret, 0)
-
-        self._test_checkout()
-
-
-class TestCheckoutCorruptedCacheFile(TestRepro):
-    def test(self):
-        cache = self.foo_stage.outs[0].cache_path
-
-        os.chmod(cache, 0o644)
-        with open(cache, "a", encoding="utf-8") as fd:
-            fd.write("1")
-
-        with pytest.raises(CheckoutError):
-            self.dvc.checkout(force=True)
-
-        self.assertFalse(os.path.isfile(self.FOO))
-        self.assertFalse(os.path.isfile(cache))
-
-
-class TestCheckoutCorruptedCacheDir(TestDvc):
-    def test(self):
-        from dvc.data import load
-
-        # NOTE: using 'copy' so that cache and link don't have same inode
-        ret = main(["config", "cache.type", "copy"])
-        self.assertEqual(ret, 0)
-
-        self.dvc.config.load()
-
-        stages = self.dvc.add(self.DATA_DIR)
-        self.assertEqual(len(stages), 1)
-        self.assertEqual(len(stages[0].outs), 1)
-        out = stages[0].outs[0]
-
-        # NOTE: modifying cache file for one of the files inside the directory
-        # to check if dvc will detect that the cache is corrupted.
-        obj = load(self.dvc.odb.local, out.hash_info)
-        _, _, entry_oid = list(obj)[0]
-        cache = self.dvc.odb.local.hash_to_path(entry_oid.value)
-
-        os.chmod(cache, 0o644)
-        with open(cache, "w+", encoding="utf-8") as fobj:
-            fobj.write("1")
-
-        with pytest.raises(CheckoutError):
-            self.dvc.checkout(force=True)
-
-        self.assertFalse(os.path.exists(cache))
-
-
-class TestCmdCheckout(TestCheckout):
-    def test(self):
-        ret = main(["checkout", "--force"])
-        self.assertEqual(ret, 0)
-        self._test_checkout()
-
-
-class CheckoutBase(TestDvcGit):
-    GIT_IGNORE = ".gitignore"
-
-    def commit_data_file(self, fname, content="random text"):
-        with open(fname, "w", encoding="utf-8") as fd:
-            fd.write(content)
-        stages = self.dvc.add(fname)
-        self.assertEqual(len(stages), 1)
-        self.assertTrue(stages[0] is not None)
-        self.dvc.scm.add([fname + ".dvc", ".gitignore"])
-        self.dvc.scm.commit("adding " + fname)
-
-    def read_ignored(self):
-        with open(self.GIT_IGNORE, encoding="utf-8") as f:
-            return [s.strip("\n") for s in f.readlines()]
+def test_checkout(tmp_dir, dvc, copy_script):
+    tmp_dir.dvc_gen({"foo": "foo", "data": {"file": "file"}})
+    dvc.run(
+        fname="file1.dvc",
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        single_stage=True,
+    )
+    remove(tmp_dir / "foo")
+    remove("data")
 
-    def outs_info(self, stage):
-        FileInfo = collections.namedtuple("FileInfo", "path inode")
+    dvc.checkout(force=True)
+    assert (tmp_dir / "foo").read_text() == "foo"
+    assert (tmp_dir / "data").read_text() == {"file": "file"}
 
-        paths = [
-            path
-            for output in stage["outs"]
-            for path in self.dvc.fs.find(output["path"])
-        ]
 
-        return [
-            FileInfo(path=path, inode=System.inode(path)) for path in paths
-        ]
+def test_checkout_cli(tmp_dir, dvc, copy_script):
+    tmp_dir.dvc_gen({"foo": "foo", "data": {"file": "file"}})
+    dvc.run(
+        fname="file1.dvc",
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        single_stage=True,
+    )
+    remove(tmp_dir / "foo")
+    remove("data")
 
+    assert main(["checkout", "--force"]) == 0
+    assert (tmp_dir / "foo").read_text() == "foo"
+    assert (tmp_dir / "data").read_text() == {"file": "file"}
 
-class TestRemoveFilesWhenCheckout(CheckoutBase):
-    def test(self):
-        fname = "file_in_a_branch"
-        branch_master = "master"
-        branch_1 = "b1"
+    remove(tmp_dir / "foo")
+    remove("data")
 
-        self.dvc.scm.add(self.dvc.scm.untracked_files())
-        self.dvc.scm.commit("add all files")
+    assert main(["checkout", "--force", "foo.dvc"]) == 0
+    assert main(["checkout", "--force", "data.dvc"]) == 0
+    assert (tmp_dir / "foo").read_text() == "foo"
+    assert (tmp_dir / "data").read_text() == {"file": "file"}
 
-        # add the file into a separate branch
-        self.dvc.scm.checkout(branch_1, True)
-        ret = main(["checkout", "--force"])
-        self.assertEqual(ret, 0)
-        self.commit_data_file(fname)
 
-        # Checkout back in master
-        self.dvc.scm.checkout(branch_master)
-        self.assertTrue(os.path.exists(fname))
+def test_checkout_corrupted_cache_file(tmp_dir, dvc):
+    (foo_stage,) = tmp_dir.dvc_gen("foo", "foo")
+    cache = foo_stage.outs[0].cache_path
 
-        # Make sure `dvc checkout` removes the file
-        # self.dvc.checkout()
-        ret = main(["checkout", "--force"])
-        self.assertEqual(ret, 0)
-        self.assertFalse(os.path.exists(fname))
+    os.chmod(cache, 0o644)
+    with open(cache, "a", encoding="utf-8") as fd:
+        fd.write("1")
 
+    with pytest.raises(CheckoutError):
+        dvc.checkout(force=True)
 
-class TestCheckoutCleanWorkingDir(CheckoutBase):
-    @patch("dvc.prompt.confirm", return_value=True)
-    def test(self, mock_prompt):
-        mock_prompt.return_value = True
+    assert not os.path.isfile("foo")
+    assert not os.path.isfile(cache)
 
-        stages = self.dvc.add(self.DATA_DIR)
-        stage = stages[0]
 
-        working_dir_change = os.path.join(self.DATA_DIR, "not_cached.txt")
-        with open(working_dir_change, "w", encoding="utf-8") as f:
-            f.write("not_cached")
+def test_checkout_corrupted_cache_dir(tmp_dir, dvc):
+    from dvc_data.hashfile import load
 
-        ret = main(["checkout", stage.relpath])
-        self.assertEqual(ret, 0)
-        self.assertFalse(os.path.exists(working_dir_change))
+    tmp_dir.gen("data", {"foo": "foo", "bar": "bar"})
+    # NOTE: using 'copy' so that cache and link don't have same inode
+    ret = main(["config", "cache.type", "copy"])
+    assert ret == 0
+
+    dvc.config.load()
+
+    stages = dvc.add("data")
+    assert len(stages) == 1
+    assert len(stages[0].outs) == 1
+    out = stages[0].outs[0]
+
+    # NOTE: modifying cache file for one of the files inside the directory
+    # to check if dvc will detect that the cache is corrupted.
+    obj = load(dvc.cache.local, out.hash_info)
+    _, _, entry_oid = list(obj)[0]
+    cache = dvc.cache.local.oid_to_path(entry_oid.value)
 
-    @patch("dvc.prompt.confirm", return_value=False)
-    def test_force(self, mock_prompt):
-        mock_prompt.return_value = False
+    os.chmod(cache, 0o644)
+    with open(cache, "w+", encoding="utf-8") as fobj:
+        fobj.write("1")
 
-        stages = self.dvc.add(self.DATA_DIR)
-        self.assertEqual(len(stages), 1)
-        stage = stages[0]
+    with pytest.raises(CheckoutError):
+        dvc.checkout(force=True)
 
-        working_dir_change = os.path.join(self.DATA_DIR, "not_cached.txt")
-        with open(working_dir_change, "w", encoding="utf-8") as f:
-            f.write("not_cached")
+    assert not os.path.exists(cache)
 
-        ret = main(["checkout", stage.relpath])
-        self.assertNotEqual(ret, 0)
 
+def test_remove_files_when_checkout(tmp_dir, dvc, scm):
+    # add the file into a separate branch
+    scm.checkout("branch", True)
+    ret = main(["checkout", "--force"])
+    assert ret == 0
+    tmp_dir.dvc_gen("file_in_a_branch", "random text", commit="add file")
+
+    # Checkout back in master
+    scm.checkout("master")
+    assert os.path.exists("file_in_a_branch")
+
+    # Make sure `dvc checkout` removes the file
+    # self.dvc.checkout()
+    ret = main(["checkout", "--force"])
+    assert ret == 0
+    assert not os.path.exists("file_in_a_branch")
+
+
+class TestCheckoutCleanWorkingDir:
+    def test(self, mocker, tmp_dir, dvc):
+        mock_prompt = mocker.patch("dvc.prompt.confirm", return_value=True)
+        (stage,) = tmp_dir.dvc_gen("data", {"foo": "foo"})
+
+        # change working directory
+        (tmp_dir / "data").gen("not_cached.txt", "not_cached")
+        assert main(["checkout", stage.relpath]) == 0
         mock_prompt.assert_called()
-        self.assertNotEqual(ret, 0)
-        self.assertRaises(DvcException)
-
-
-class TestCheckoutSelectiveRemove(CheckoutBase):
-    def test(self):
-        # Use copy to test for changes in the inodes
-        ret = main(["config", "cache.type", "copy"])
-        self.assertEqual(ret, 0)
-
-        ret = main(["add", self.DATA_DIR])
-        self.assertEqual(0, ret)
-
-        stage_path = self.DATA_DIR + DVC_FILE_SUFFIX
-        stage = load_yaml(stage_path)
-        staged_files = self.outs_info(stage)
+        assert not (tmp_dir / "data" / "not_cached.txt").exists()
 
-        # move instead of remove, to lock inode assigned to stage_files[0].path
-        # if we were to use remove, we might end up with same inode assigned to
-        # newly checked out file
-        shutil.move(staged_files[0].path, "random_name")
-
-        ret = main(["checkout", "--force", stage_path])
-        self.assertEqual(ret, 0)
-
-        checkedout_files = self.outs_info(stage)
-
-        self.assertEqual(len(staged_files), len(checkedout_files))
-        self.assertEqual(staged_files[0].path, checkedout_files[0].path)
-        self.assertNotEqual(staged_files[0].inode, checkedout_files[0].inode)
-        self.assertEqual(staged_files[1].inode, checkedout_files[1].inode)
-
-
-class TestGitIgnoreBasic(CheckoutBase):
-    def test(self):
-        fname1 = "file_1"
-        fname2 = "file_2"
-        fname3 = "file_3"
-
-        self.dvc.scm.add(self.dvc.scm.untracked_files())
-        self.dvc.scm.commit("add all files")
-
-        self.assertFalse(os.path.exists(self.GIT_IGNORE))
-
-        self.commit_data_file(fname1)
-        self.commit_data_file(fname2)
-        self.dvc.run(
-            single_stage=True,
-            cmd=f"python {self.CODE} {self.FOO} {fname3}",
-            deps=[self.CODE, self.FOO],
-            outs_no_cache=[fname3],
-        )
-
-        self.assertTrue(os.path.exists(self.GIT_IGNORE))
-
-        ignored = self.read_ignored()
-
-        self.assertEqual(len(ignored), 2)
-
-        self.assertIn("/" + fname1, ignored)
-        self.assertIn("/" + fname2, ignored)
-
-
-class TestGitIgnoreWhenCheckout(CheckoutBase):
-    def test(self):
-        fname_master = "file_in_a_master"
-        branch_master = "master"
-        fname_branch = "file_in_a_branch"
-        branch_1 = "b1"
+    def test_force(self, mocker, tmp_dir, dvc):
+        mock_prompt = mocker.patch("dvc.prompt.confirm", return_value=False)
+        (stage,) = tmp_dir.dvc_gen("data", {"foo": "foo"})
+
+        # change working directory
+        (tmp_dir / "data").gen("not_cached.txt", "not_cached")
+        assert main(["checkout", stage.relpath]) != 0
+        mock_prompt.assert_called()
+        assert (tmp_dir / "data" / "not_cached.txt").exists()
 
-        self.dvc.scm.add(self.dvc.scm.untracked_files())
-        self.dvc.scm.commit("add all files")
-        self.commit_data_file(fname_master)
 
-        self.dvc.scm.checkout(branch_1, True)
-        ret = main(["checkout", "--force"])
-        self.assertEqual(ret, 0)
-        self.commit_data_file(fname_branch)
+def test_checkout_selective_remove(tmp_dir, dvc):
+    # Use copy to test for changes in the inodes
+    dvc.cache.local.cache_types = ["copy"]
+    tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar"}})
 
-        self.dvc.scm.checkout(branch_master)
-        ret = main(["checkout", "--force"])
-        self.assertEqual(ret, 0)
+    foo_inode = system.inode(os.path.join("data", "foo"))
+    bar_inode = system.inode(os.path.join("data", "bar"))
+    # move instead of remove, to lock inode assigned to stage_files[0].path
+    # if we were to use remove, we might end up with same inode assigned to
+    # newly checked out file
+    shutil.move(os.path.join("data", "foo"), "random_name")
 
-        ignored = self.read_ignored()
+    assert main(["checkout", "--force", "data.dvc"]) == 0
+    assert (tmp_dir / "data").read_text() == {"foo": "foo", "bar": "bar"}
+    assert system.inode(os.path.join("data", "foo")) != foo_inode
+    assert system.inode(os.path.join("data", "bar")) == bar_inode
 
-        self.assertEqual(len(ignored), 1)
-        self.assertIn("/" + fname_master, ignored)
 
-        self.dvc.scm.checkout(branch_1)
-        ret = main(["checkout", "--force"])
-        self.assertEqual(ret, 0)
-        ignored = self.read_ignored()
-        self.assertIn("/" + fname_branch, ignored)
+def test_gitignore_basic(tmp_dir, dvc, scm):
+    tmp_dir.gen("foo", "foo")
+    assert not os.path.exists(scm.GITIGNORE)
 
+    tmp_dir.dvc_gen("file1", "random text1", commit="add file1")
+    tmp_dir.dvc_gen("file2", "random text2", commit="add file2")
+    dvc.run(
+        single_stage=True,
+        cmd="cp foo file3",
+        deps=["foo"],
+        outs_no_cache=["file3"],
+    )
+    assert get_gitignore_content() == ["/file1", "/file2"]
 
-class TestCheckoutMissingMd5InStageFile(TestRepro):
-    def test(self):
-        d = load_yaml(self.file1_stage)
-        del d[Stage.PARAM_OUTS][0][LocalFileSystem.PARAM_CHECKSUM]
-        del d[Stage.PARAM_DEPS][0][LocalFileSystem.PARAM_CHECKSUM]
-        dump_yaml(self.file1_stage, d)
 
-        with pytest.raises(CheckoutError):
-            self.dvc.checkout(force=True)
+def test_gitignore_when_checkout(tmp_dir, dvc, scm):
+    tmp_dir.dvc_gen("file_in_a_master", "master", commit="master")
 
+    scm.checkout("branch", True)
+    ret = main(["checkout", "--force"])
+    assert ret == 0
+    tmp_dir.dvc_gen("file_in_a_branch", "branch", commit="branch")
 
-class TestCheckoutEmptyDir(TestDvc):
-    def test(self):
-        dname = "empty_dir"
-        os.mkdir(dname)
+    scm.checkout("master")
+    ret = main(["checkout", "--force"])
+    assert ret == 0
 
-        stages = self.dvc.add(dname)
-        self.assertEqual(len(stages), 1)
-        stage = stages[0]
-        self.assertTrue(stage is not None)
-        self.assertEqual(len(stage.outs), 1)
+    ignored = get_gitignore_content()
 
-        stage.outs[0].remove()
-        self.assertFalse(os.path.exists(dname))
+    assert len(ignored) == 1
+    assert "/file_in_a_master" in ignored
 
-        stats = self.dvc.checkout(force=True)
-        assert stats["added"] == [dname + os.sep]
+    scm.checkout("branch")
+    ret = main(["checkout", "--force"])
+    assert ret == 0
+    ignored = get_gitignore_content()
+    assert "/file_in_a_branch" in ignored
 
-        self.assertTrue(os.path.isdir(dname))
-        self.assertEqual(len(os.listdir(dname)), 0)
 
+def test_checkout_missing_md5_in_stage_file(tmp_dir, dvc, copy_script):
+    tmp_dir.dvc_gen({"foo": "foo", "data": {"file": "file"}})
+    stage = dvc.run(
+        fname="file1.dvc",
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        single_stage=True,
+    )
+    d = load_yaml(stage.relpath)
+    del d[Stage.PARAM_OUTS][0][LocalFileSystem.PARAM_CHECKSUM]
+    del d[Stage.PARAM_DEPS][0][LocalFileSystem.PARAM_CHECKSUM]
+    dump_yaml(stage.relpath, d)
 
-class TestCheckoutNotCachedFile(TestDvc):
-    def test(self):
-        cmd = "python {} {} {}".format(self.CODE, self.FOO, "out")
+    with pytest.raises(CheckoutError):
+        dvc.checkout(force=True)
 
-        self.dvc.add(self.FOO)
-        stage = self.dvc.run(
-            cmd=cmd,
-            deps=[self.FOO, self.CODE],
-            outs_no_cache=["out"],
-            single_stage=True,
-        )
-        self.assertTrue(stage is not None)
 
-        stats = self.dvc.checkout(force=True)
-        assert not any(stats.values())
+def test_checkout_empty_dir(tmp_dir, dvc):
+    empty_dir = tmp_dir / "empty_dir"
+    empty_dir.mkdir()
+    (stage,) = dvc.add("empty_dir")
 
+    stage.outs[0].remove()
+    assert not empty_dir.exists()
 
-class TestCheckoutWithDeps(TestRepro):
-    def test(self):
-        os.unlink(self.FOO)
-        os.unlink(self.file1)
+    stats = dvc.checkout(force=True)
+    assert stats["added"] == [os.path.join("empty_dir", "")]
+    assert empty_dir.is_dir()
+    assert not list(empty_dir.iterdir())
 
-        self.assertFalse(os.path.exists(self.FOO))
-        self.assertFalse(os.path.exists(self.file1))
 
-        ret = main(["checkout", "--force", self.file1_stage, "--with-deps"])
-        self.assertEqual(ret, 0)
+def test_checkout_not_cached_file(tmp_dir, dvc):
+    tmp_dir.dvc_gen("foo", "foo")
+    dvc.run(
+        cmd="cp foo bar",
+        deps=["foo"],
+        outs_no_cache=["bar"],
+        single_stage=True,
+    )
+    stats = dvc.checkout(force=True)
+    assert not any(stats.values())
 
-        self.assertTrue(os.path.exists(self.FOO))
-        self.assertTrue(os.path.exists(self.file1))
 
+def test_checkout_with_deps_cli(tmp_dir, dvc, copy_script):
+    tmp_dir.dvc_gen({"foo": "foo", "data": {"file": "file"}})
+    dvc.run(
+        fname="file1.dvc",
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        single_stage=True,
+    )
+    remove("foo")
+    remove("file1")
 
-class TestCheckoutDirectory(TestRepro):
-    def test(self):
-        stage = self.dvc.add(self.DATA_DIR)[0]
+    assert not os.path.exists("foo")
+    assert not os.path.exists("file1")
 
-        shutil.rmtree(self.DATA_DIR)
-        self.assertFalse(os.path.exists(self.DATA_DIR))
+    ret = main(["checkout", "--force", "file1.dvc", "--with-deps"])
+    assert ret == 0
 
-        ret = main(["checkout", stage.path])
-        self.assertEqual(ret, 0)
+    assert os.path.exists("foo")
+    assert os.path.exists("file1")
 
-        self.assertTrue(os.path.exists(self.DATA_DIR))
 
+def test_checkout_directory(tmp_dir, dvc):
+    (stage,) = tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar"}})
 
-class TestCheckoutHook(TestDvc):
-    @patch("sys.stdout.isatty", return_value=True)
-    @patch("dvc.prompt.input", side_effect=EOFError)
-    def test(self, _mock_input, _mock_isatty):
-        """Test that dvc checkout handles EOFError gracefully, which is what
-        it will experience when running in a git hook.
-        """
-        stages = self.dvc.add(self.DATA_DIR)
-        self.assertEqual(len(stages), 1)
-        stage = stages[0]
-        self.assertNotEqual(stage, None)
+    remove("data")
+    assert not os.path.exists("data")
 
-        self.create(os.path.join(self.DATA_DIR, "test"), "test")
+    ret = main(["checkout", stage.path])
+    assert ret == 0
 
-        with self.assertRaises(ConfirmRemoveError):
-            self.dvc.checkout()
+    assert os.path.exists("data")
 
 
-class TestCheckoutSuggestGit(TestRepro):
-    def test(self):
-        # pylint: disable=no-member
+def test_checkout_hook(mocker, tmp_dir, dvc):
+    """Test that dvc checkout handles EOFError gracefully, which is what
+    it will experience when running in a git hook.
+    """
+    tmp_dir.dvc_gen({"data": {"foo": "foo"}})
+    mocker.patch("sys.stdout.isatty", return_value=True)
+    mocker.patch("dvc.prompt.input", side_effect=EOFError)
 
-        try:
-            self.dvc.checkout(targets="gitbranch")
-        except DvcException as exc:
-            self.assertIsInstance(exc, CheckoutErrorSuggestGit)
-            self.assertIsInstance(exc.__cause__, NoOutputOrStageError)
-            self.assertIsInstance(
-                exc.__cause__.__cause__, StageFileDoesNotExistError
-            )
+    (tmp_dir / "data").gen("test", "test")
+    with pytest.raises(ConfirmRemoveError):
+        dvc.checkout()
 
-        try:
-            self.dvc.checkout(targets=self.FOO)
-        except DvcException as exc:
-            self.assertIsInstance(exc, CheckoutErrorSuggestGit)
-            self.assertIsInstance(exc.__cause__, NoOutputOrStageError)
-            self.assertIsNone(exc.__cause__.__cause__)
 
-        try:
-            self.dvc.checkout(targets="looks-like-dvcfile.dvc")
-        except DvcException as exc:
-            self.assertIsInstance(exc, CheckoutErrorSuggestGit)
-            self.assertIsInstance(exc.__cause__, StageFileDoesNotExistError)
-            self.assertIsNone(exc.__cause__.__cause__)
+def test_checkout_suggest_git(tmp_dir, dvc, scm):
+    with pytest.raises(CheckoutErrorSuggestGit) as e:
+        dvc.checkout(targets="gitbranch")
+    assert isinstance(e.value.__cause__, NoOutputOrStageError)
+    assert isinstance(e.value.__cause__.__cause__, StageFileDoesNotExistError)
 
+    with pytest.raises(CheckoutErrorSuggestGit) as e:
+        dvc.checkout(targets="foobar")
+    assert isinstance(e.value.__cause__, NoOutputOrStageError)
+    assert isinstance(e.value.__cause__.__cause__, StageFileDoesNotExistError)
 
-class TestCheckoutTargetRecursiveShouldNotRemoveOtherUsedFiles(TestDvc):
-    def test(self):
-        ret = main(["add", self.DATA_DIR, self.FOO, self.BAR])
-        self.assertEqual(0, ret)
+    with pytest.raises(CheckoutErrorSuggestGit) as e:
+        dvc.checkout(targets="looks-like-dvcfile.dvc")
+    assert isinstance(e.value.__cause__, StageFileDoesNotExistError)
+    assert e.value.__cause__.__cause__ is None
 
-        ret = main(["checkout", "-R", self.DATA_DIR])
-        self.assertEqual(0, ret)
 
-        self.assertTrue(os.path.exists(self.FOO))
-        self.assertTrue(os.path.exists(self.BAR))
+def test_checkout_target_recursive_should_not_remove_other_used_files(tmp_dir, dvc):
+    tmp_dir.dvc_gen({"foo": "foo", "bar": "bar", "data": {"file": "file"}})
+    assert main(["checkout", "-R", "data"]) == 0
+    assert (tmp_dir / "foo").exists()
+    assert (tmp_dir / "bar").exists()
 
 
-class TestCheckoutRecursiveNotDirectory(TestDvc):
-    def test(self):
-        ret = main(["add", self.FOO])
-        self.assertEqual(0, ret)
+def test_checkout_recursive_not_directory(tmp_dir, dvc):
+    tmp_dir.gen("foo", "foo")
+    ret = main(["add", "foo"])
+    assert ret == 0
 
-        stats = self.dvc.checkout(targets=[self.FOO + ".dvc"], recursive=True)
-        assert stats == {"added": [], "modified": [], "deleted": []}
+    stats = dvc.checkout(targets=["foo.dvc"], recursive=True)
+    assert stats == {"added": [], "modified": [], "deleted": []}
 
 
-class TestCheckoutMovedCacheDirWithSymlinks(TestDvc):
-    def test(self):
-        ret = main(["config", "cache.type", "symlink"])
-        self.assertEqual(ret, 0)
+def test_checkout_moved_cache_dir_with_symlinks(tmp_dir, dvc):
+    tmp_dir.gen({"foo": "foo", "data": {"file": "file"}})
+    ret = main(["config", "cache.type", "symlink"])
+    assert ret == 0
 
-        ret = main(["add", self.FOO])
-        self.assertEqual(ret, 0)
+    ret = main(["add", "foo"])
+    assert ret == 0
 
-        ret = main(["add", self.DATA_DIR])
-        self.assertEqual(ret, 0)
+    ret = main(["add", "data"])
+    assert ret == 0
 
-        self.assertTrue(System.is_symlink(self.FOO))
-        old_foo_link = os.path.realpath(self.FOO)
+    assert system.is_symlink("foo")
+    old_foo_link = os.path.realpath("foo")
 
-        self.assertTrue(System.is_symlink(self.DATA))
-        old_data_link = os.path.realpath(self.DATA)
+    assert system.is_symlink(os.path.join("data", "file"))
+    old_data_link = os.path.realpath(os.path.join("data", "file"))
 
-        old_cache_dir = self.dvc.odb.local.cache_dir
-        new_cache_dir = old_cache_dir + "_new"
-        os.rename(old_cache_dir, new_cache_dir)
+    old_cache_dir = dvc.cache.local.path
+    new_cache_dir = old_cache_dir + "_new"
+    os.rename(old_cache_dir, new_cache_dir)
 
-        ret = main(["cache", "dir", new_cache_dir])
-        self.assertEqual(ret, 0)
+    ret = main(["cache", "dir", new_cache_dir])
+    assert ret == 0
 
-        ret = main(["checkout", "-f"])
-        self.assertEqual(ret, 0)
+    ret = main(["checkout", "-f"])
+    assert ret == 0
 
-        self.assertTrue(System.is_symlink(self.FOO))
-        new_foo_link = os.path.realpath(self.FOO)
+    assert system.is_symlink("foo")
+    new_foo_link = os.path.realpath("foo")
 
-        self.assertTrue(System.is_symlink(self.DATA))
-        new_data_link = os.path.realpath(self.DATA)
+    assert system.is_symlink(os.path.join("data", "file"))
+    new_data_link = os.path.realpath(os.path.join("data", "file"))
 
-        self.assertEqual(
-            relpath(old_foo_link, old_cache_dir),
-            relpath(new_foo_link, new_cache_dir),
-        )
+    assert relpath(old_foo_link, old_cache_dir) == relpath(new_foo_link, new_cache_dir)
 
-        self.assertEqual(
-            relpath(old_data_link, old_cache_dir),
-            relpath(new_data_link, new_cache_dir),
-        )
+    assert relpath(old_data_link, old_cache_dir) == relpath(
+        new_data_link, new_cache_dir
+    )
 
 
 def test_checkout_no_checksum(tmp_dir, dvc):
     tmp_dir.gen("file", "file content")
-    stage = dvc.run(
-        outs=["file"], no_exec=True, cmd="somecmd", single_stage=True
-    )
+    stage = dvc.run(outs=["file"], no_exec=True, cmd="somecmd", single_stage=True)
 
     with pytest.raises(CheckoutError):
         dvc.checkout([stage.path], force=True)
 
     assert not os.path.exists("file")
 
 
 @pytest.mark.parametrize(
     "link, link_test_func",
-    [("hardlink", System.is_hardlink), ("symlink", System.is_symlink)],
+    [("hardlink", system.is_hardlink), ("symlink", system.is_symlink)],
 )
 def test_checkout_relink(tmp_dir, dvc, link, link_test_func):
-    dvc.odb.local.cache_types = [link]
+    dvc.cache.local.cache_types = [link]
 
     tmp_dir.dvc_gen({"dir": {"data": "text"}})
     dvc.unprotect("dir/data")
     assert not link_test_func("dir/data")
 
     stats = dvc.checkout(["dir.dvc"], relink=True)
     assert stats == empty_checkout
     assert link_test_func("dir/data")
 
 
 @pytest.mark.parametrize("link", ["hardlink", "symlink", "copy"])
 def test_checkout_relink_protected(tmp_dir, dvc, link):
-    dvc.odb.local.cache_types = [link]
+    dvc.cache.local.cache_types = [link]
 
     tmp_dir.dvc_gen("foo", "foo")
     dvc.unprotect("foo")
     assert os.access("foo", os.W_OK)
 
     stats = dvc.checkout(["foo.dvc"], relink=True)
     assert stats == empty_checkout
@@ -599,15 +485,15 @@
 
 @pytest.mark.xfail(reason="values relpath")
 def test_checkout_stats_on_failure(tmp_dir, dvc, scm):
     tmp_dir.dvc_gen(
         {"foo": "foo", "dir": {"subdir": {"file": "file"}}, "other": "other"},
         commit="initial",
     )
-    stage = Dvcfile(dvc, "foo.dvc").stage
+    stage = load_file(dvc, "foo.dvc").stage
     tmp_dir.dvc_gen({"foo": "foobar", "other": "other other"}, commit="second")
 
     # corrupt cache
     cache = stage.outs[0].cache_path
     os.chmod(cache, 0o644)
     with open(cache, "a", encoding="utf-8") as fd:
         fd.write("destroy cache")
@@ -670,17 +556,15 @@
     assert dvc.checkout() == empty_checkout
 
     scm.checkout("-")
     assert dvc.checkout() == {**empty_checkout, "modified": ["dir" + os.sep]}
     assert dvc.checkout() == empty_checkout
 
 
-def test_stats_on_show_changes_does_not_show_summary(
-    tmp_dir, dvc, scm, capsys
-):
+def test_stats_on_show_changes_does_not_show_summary(tmp_dir, dvc, scm, capsys):
     tmp_dir.dvc_gen(
         {"dir": {"subdir": {"file": "file"}}, "other": "other"},
         commit="initial",
     )
     scm.checkout("HEAD~")
 
     assert main(["checkout"]) == 0
@@ -698,24 +582,24 @@
         commit="initial",
     )
     scm.checkout("HEAD~")
 
     assert main(["checkout", "--summary"]) == 0
 
     out, _ = capsys.readouterr()
-    assert "2 files deleted" == out.rstrip()
+    assert out.rstrip() == "2 files deleted"
 
 
 @pytest.mark.parametrize("link", ["hardlink", "symlink", "copy"])
 def test_checkout_with_relink_existing(tmp_dir, dvc, link):
     tmp_dir.dvc_gen("foo", "foo")
     (tmp_dir / "foo").unlink()
 
     tmp_dir.dvc_gen("bar", "bar")
-    dvc.odb.local.cache_types = [link]
+    dvc.cache.local.cache_types = [link]
 
     stats = dvc.checkout(relink=True)
     assert stats == {**empty_checkout, "added": ["foo"]}
 
 
 def test_checkout_with_deps(tmp_dir, dvc):
     tmp_dir.dvc_gen({"foo": "foo"})
@@ -776,15 +660,15 @@
 def test_checkouts_with_different_addressing(tmp_dir, dvc, run_copy):
     tmp_dir.gen({"foo": "foo", "lorem": "lorem"})
     run_copy("foo", "bar", name="copy-foo-bar")
     run_copy("lorem", "ipsum", name="copy-lorem-ipsum")
 
     (tmp_dir / "bar").unlink()
     (tmp_dir / "ipsum").unlink()
-    assert set(dvc.checkout(PIPELINE_FILE)["added"]) == {"bar", "ipsum"}
+    assert set(dvc.checkout(PROJECT_FILE)["added"]) == {"bar", "ipsum"}
 
     (tmp_dir / "bar").unlink()
     (tmp_dir / "ipsum").unlink()
     assert set(dvc.checkout(":")["added"]) == {"bar", "ipsum"}
 
     (tmp_dir / "bar").unlink()
     assert dvc.checkout("copy-foo-bar")["added"] == ["bar"]
@@ -824,15 +708,15 @@
     stage2 = run_copy("lorem", "ipsum", name="copy-lorem-ipsum")
 
     for out in ["bar", "ipsum"]:
         (tmp_dir / out).unlink()
     assert dvc.checkout(["bar"])["added"] == ["bar"]
 
     (tmp_dir / "bar").unlink()
-    assert set(dvc.checkout([PIPELINE_FILE])["added"]) == {"bar", "ipsum"}
+    assert set(dvc.checkout([PROJECT_FILE])["added"]) == {"bar", "ipsum"}
 
     for out in ["bar", "ipsum"]:
         (tmp_dir / out).unlink()
     assert set(dvc.checkout([stage1.addressing])["added"]) == {"bar"}
 
     (tmp_dir / "bar").unlink()
     assert set(dvc.checkout([stage2.addressing])["added"]) == {"ipsum"}
@@ -871,17 +755,15 @@
         # NOTE: you can't set exec bits on Windows
         assert not isexec
     else:
         assert isexec
 
 
 def test_checkout_partial(tmp_dir, dvc):
-    tmp_dir.dvc_gen(
-        {"data": {"foo": "foo", "bar": "bar", "sub_dir": {"baz": "baz"}}}
-    )
+    tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar", "sub_dir": {"baz": "baz"}}})
 
     data_dir = tmp_dir / "data"
     shutil.rmtree(data_dir)
 
     dvc.checkout(str(data_dir / "foo"))
     assert data_dir.read_text() == {"foo": "foo"}
 
@@ -941,17 +823,15 @@
 
     # Everything is in place, no action taken
     stats = dvc.checkout(str(data_dir))
     assert not any(stats.values())
 
 
 def test_checkout_partial_subdir(tmp_dir, dvc):
-    tmp_dir.dvc_gen(
-        {"data": {"foo": "foo", "sub_dir": {"bar": "bar", "baz": "baz"}}}
-    )
+    tmp_dir.dvc_gen({"data": {"foo": "foo", "sub_dir": {"bar": "bar", "baz": "baz"}}})
 
     data_dir = tmp_dir / "data"
     sub_dir = data_dir / "sub_dir"
     sub_dir_bar = sub_dir / "baz"
 
     shutil.rmtree(sub_dir)
     dvc.checkout(str(sub_dir))
```

### Comparing `dvc-2.9.5/tests/func/test_cli.py` & `dvc-3.0.0a0/tests/func/test_cli.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,237 +1,205 @@
 import os
 
+import pytest
+
 from dvc.cli import DvcParserError, parse_args
 from dvc.cli.command import CmdBase
 from dvc.commands.add import CmdAdd
 from dvc.commands.checkout import CmdCheckout
 from dvc.commands.config import CmdConfig
 from dvc.commands.data_sync import CmdDataPull, CmdDataPush
-from dvc.commands.gc import CmdGC
 from dvc.commands.init import CmdInit
 from dvc.commands.remove import CmdRemove
 from dvc.commands.repro import CmdRepro
 from dvc.commands.run import CmdRun
 from dvc.commands.status import CmdDataStatus
-from dvc.exceptions import DvcException
-from tests.basic_env import TestDvc
-
-
-class TestArgParse(TestDvc):
-    def test(self):
-        args = parse_args(["init"])
-        self.assertIsInstance(args.func(args), CmdInit)
-
-
-class TestRun(TestDvc):
-    def test(self):
-        dep1 = "dep1"
-        dep2 = "dep2"
-
-        out1 = "out1"
-        out2 = "out2"
-
-        out_no_cache1 = "out_no_cache1"
-        out_no_cache2 = "out_no_cache2"
-
-        fname = "dvc.dvc"
-        cmd = "cmd"
-        arg1 = "arg1"
-        arg2 = "arg2"
+from dvc.exceptions import NotDvcRepoError
 
-        args = parse_args(
-            [
-                "run",
-                "-d",
-                dep1,
-                "--deps",
-                dep2,
-                "-o",
-                out1,
-                "--outs",
-                out2,
-                "-O",
-                out_no_cache1,
-                "--outs-no-cache",
-                out_no_cache2,
-                "--file",
-                fname,
-                cmd,
-                arg1,
-                arg2,
-            ]
-        )
-        cmd_cls = args.func(args)
-        self.assertIsInstance(cmd_cls, CmdRun)
-        self.assertEqual(args.deps, [dep1, dep2])
-        self.assertEqual(args.outs, [out1, out2])
-        self.assertEqual(args.outs_no_cache, [out_no_cache1, out_no_cache2])
-        self.assertEqual(args.file, fname)
-        self.assertEqual(args.command, [cmd, arg1, arg2])
 
-        cmd_cls.repo.close()
+def test_argparse(dvc):
+    args = parse_args(["init"])
+    assert isinstance(args.func(args), CmdInit)
 
 
-class TestPull(TestDvc):
-    def test(self):
-        args = parse_args(["pull"])
-        cmd = args.func(args)
-        self.assertIsInstance(cmd, CmdDataPull)
+def test_run(dvc):
+    dep1 = "dep1"
+    dep2 = "dep2"
 
-        cmd.repo.close()
+    out1 = "out1"
+    out2 = "out2"
 
+    out_no_cache1 = "out_no_cache1"
+    out_no_cache2 = "out_no_cache2"
 
-class TestPush(TestDvc):
-    def test(self):
-        args = parse_args(["push"])
-        cmd = args.func(args)
-        self.assertIsInstance(cmd, CmdDataPush)
+    fname = "dvc.dvc"
+    cmd = "cmd"
+    arg1 = "arg1"
+    arg2 = "arg2"
 
-        cmd.repo.close()
+    args = parse_args(
+        [
+            "run",
+            "-d",
+            dep1,
+            "--deps",
+            dep2,
+            "-o",
+            out1,
+            "--outs",
+            out2,
+            "-O",
+            out_no_cache1,
+            "--outs-no-cache",
+            out_no_cache2,
+            "--file",
+            fname,
+            cmd,
+            arg1,
+            arg2,
+        ]
+    )
+    cmd_cls = args.func(args)
+    assert isinstance(cmd_cls, CmdRun)
+    assert args.deps == [dep1, dep2]
+    assert args.outs == [out1, out2]
+    assert args.outs_no_cache == [out_no_cache1, out_no_cache2]
+    assert args.file == fname
+    assert args.command == [cmd, arg1, arg2]
 
+    cmd_cls.repo.close()
 
-class TestStatus(TestDvc):
-    def test(self):
-        args = parse_args(["status"])
-        cmd = args.func(args)
-        self.assertIsInstance(cmd, CmdDataStatus)
 
-        cmd.repo.close()
+def test_pull(dvc):
+    args = parse_args(["pull"])
+    cmd = args.func(args)
+    assert isinstance(cmd, CmdDataPull)
 
+    cmd.repo.close()
 
-class TestRepro(TestDvc):
-    def test(self):
-        target1 = "1"
-        target2 = "2"
 
-        args = parse_args(
-            ["repro", target1, target2, "-f", "--force", "-s", "--single-item"]
-        )
+def test_push(dvc):
+    args = parse_args(["push"])
+    cmd = args.func(args)
+    assert isinstance(cmd, CmdDataPush)
 
-        cmd = args.func(args)
-        self.assertIsInstance(cmd, CmdRepro)
-        self.assertEqual(args.targets, [target1, target2])
-        self.assertEqual(args.force, True)
-        self.assertEqual(args.single_item, True)
+    cmd.repo.close()
 
-        cmd.repo.close()
 
+def test_status(dvc):
+    args = parse_args(["status"])
+    cmd = args.func(args)
+    assert isinstance(cmd, CmdDataStatus)
 
-class TestRemove(TestDvc):
-    def test(self):
-        target1 = "1"
-        target2 = "2"
+    cmd.repo.close()
 
-        args = parse_args(["remove", target1, target2])
 
-        cmd = args.func(args)
-        self.assertIsInstance(cmd, CmdRemove)
-        self.assertEqual(args.targets, [target1, target2])
+def test_repro(dvc):
+    target1 = "1"
+    target2 = "2"
 
-        cmd.repo.close()
+    args = parse_args(
+        ["repro", target1, target2, "-f", "--force", "-s", "--single-item"]
+    )
 
+    cmd = args.func(args)
+    assert isinstance(cmd, CmdRepro)
+    assert args.targets == [target1, target2]
+    assert args.force
+    assert args.single_item
 
-class TestAdd(TestDvc):
-    def test(self):
-        target1 = "1"
-        target2 = "2"
+    cmd.repo.close()
 
-        args = parse_args(["add", target1, target2])
 
-        cmd = args.func(args)
-        self.assertIsInstance(cmd, CmdAdd)
-        self.assertEqual(args.targets, [target1, target2])
+def test_remove(dvc):
+    target1 = "1"
+    target2 = "2"
 
-        cmd.repo.close()
+    args = parse_args(["remove", target1, target2])
 
+    cmd = args.func(args)
+    assert isinstance(cmd, CmdRemove)
+    assert args.targets == [target1, target2]
 
-class TestGC(TestDvc):
-    def test(self):
-        args = parse_args(["gc"])
-        cmd = args.func(args)
-        self.assertIsInstance(cmd, CmdGC)
+    cmd.repo.close()
 
-        cmd.repo.close()
 
+def test_add(dvc):
+    target1 = "1"
+    target2 = "2"
 
-class TestGCMultipleDvcRepos(TestDvc):
-    def test(self):
-        args = parse_args(["gc", "-p", "/tmp/asdf", "/tmp/xyz"])
+    args = parse_args(["add", target1, target2])
 
-        cmd = args.func(args)
-        self.assertIsInstance(cmd, CmdGC)
+    cmd = args.func(args)
+    assert isinstance(cmd, CmdAdd)
+    assert args.targets == [target1, target2]
 
-        self.assertEqual(args.repos, ["/tmp/asdf", "/tmp/xyz"])
+    cmd.repo.close()
 
-        cmd.repo.close()
 
+def test_config_unset(dvc):
+    name = "section.option"
+    value = "1"
 
-class TestConfig(TestDvc):
-    def test(self):
-        name = "section.option"
-        value = "1"
+    args = parse_args(["config", "-u", "--unset", name, value])
 
-        args = parse_args(["config", "-u", "--unset", name, value])
+    cmd = args.func(args)
+    assert isinstance(cmd, CmdConfig)
+    assert args.unset
+    assert args.name == (False, "section", "option")
+    assert args.value == value
 
-        cmd = args.func(args)
-        self.assertIsInstance(cmd, CmdConfig)
-        self.assertEqual(args.unset, True)
-        self.assertEqual(args.name, (False, "section", "option"))
-        self.assertEqual(args.value, value)
 
-    def test_config_list(self):
-        args = parse_args(["config", "--list"])
+def test_config_list():
+    args = parse_args(["config", "--list"])
 
-        self.assertTrue(args.list)
-        self.assertIsNone(args.name)
-        self.assertIsNone(args.value)
+    assert args.list
+    assert args.name is None
+    assert args.value is None
 
 
-class TestCheckout(TestDvc):
-    def test(self):
-        args = parse_args(["checkout"])
-        cmd = args.func(args)
-        self.assertIsInstance(cmd, CmdCheckout)
+def test_checkout(dvc):
+    args = parse_args(["checkout"])
+    cmd = args.func(args)
+    assert isinstance(cmd, CmdCheckout)
 
-        cmd.repo.close()
+    cmd.repo.close()
 
 
-class TestFindRoot(TestDvc):
-    def test(self):
-        class Cmd(CmdBase):
-            def run(self):
-                pass
+def test_find_root(dvc):
+    class Cmd(CmdBase):
+        def run(self):
+            pass
 
-        class A:
-            quiet = False
-            verbose = True
-            cd = os.path.pardir
+    class A:
+        quiet = False
+        verbose = True
+        cd = os.path.pardir
 
-        args = A()
-        with self.assertRaises(DvcException):
-            Cmd(args)
+    args = A()
+    with pytest.raises(NotDvcRepoError):
+        Cmd(args)
 
 
-class TestCd(TestDvc):
-    def test(self):
-        class Cmd(CmdBase):
-            def run(self):
-                pass
+def test_cd(dvc):
+    class Cmd(CmdBase):
+        def run(self):
+            pass
 
-        class A:
-            quiet = False
-            verbose = True
-            cd = os.path.pardir
+    class A:
+        quiet = False
+        verbose = True
+        cd = os.path.pardir
 
-        parent_dir = os.path.realpath(os.path.pardir)
-        args = A()
-        with self.assertRaises(DvcException):
-            Cmd(args)
-        current_dir = os.path.realpath(os.path.curdir)
-        self.assertEqual(parent_dir, current_dir)
+    parent_dir = os.path.realpath(os.path.pardir)
+    args = A()
+    with pytest.raises(NotDvcRepoError):
+        Cmd(args)
+    current_dir = os.path.realpath(os.path.curdir)
+    assert parent_dir == current_dir
 
 
 def test_unknown_command_help(capsys):
     try:
         _ = parse_args(["unknown"])
     except DvcParserError:
         pass
```

### Comparing `dvc-2.9.5/tests/func/test_config.py` & `dvc-3.0.0a0/tests/func/test_config.py`

 * *Files 8% similar despite different names*

```diff
@@ -125,29 +125,29 @@
             profile = iterative
         ['remote "other"']
             url = gs://bucket/path
         """
         )
     )
 
-    assert main(["config"] + args) == ret
+    assert main(["config", *args]) == ret
     text = caplog.text if ret else capsys.readouterr()[0]
     assert msg in text
 
 
 @pytest.mark.parametrize(
     "args, ret",
     [
         (["--local", "core.remote"], 251),
         (["--project", "core.remote"], 251),
         (["core.remote"], 0),
     ],
 )
 def test_config_get_in_non_dvc_repo(tmp_dir, caplog, args, ret):
-    assert main(["config"] + args) == ret
+    assert main(["config", *args]) == ret
     if ret != 0:
         out = caplog.text
         assert "Not inside a DVC repo" in out
 
 
 def test_config_list(tmp_dir, dvc, capsys):
     (tmp_dir / ".dvc" / "config").write_text(
@@ -195,43 +195,43 @@
     [
         (["--list", "--local"], 251),
         (["--list", "--project"], 251),
         (["--list"], 0),
     ],
 )
 def test_config_list_in_non_dvc_repo(tmp_dir, caplog, args, ret):
-    assert main(["config"] + args) == ret
+    assert main(["config", *args]) == ret
     if ret != 0:
         out = caplog.text
         assert "Not inside a DVC repo" in out
 
 
 @pytest.mark.parametrize(
     "args", [["core.analytics"], ["core.analytics", "false"], ["--unset"]]
 )
 def test_list_bad_args(tmp_dir, dvc, caplog, args):
     caplog.clear()
-    assert main(["config", "--list"] + args) == 1
+    assert main(["config", "--list", *args]) == 1
     assert (
         "-l/--list can't be used together with any of these options: "
-        "-u/--unset, name, value"
-    ) in caplog.text
+        "-u/--unset, name, value" in caplog.text
+    )
 
 
 def test_set_invalid_key(dvc):
-    with pytest.raises(ConfigError, match=r"extra keys not allowed"):
+    with pytest.raises(ConfigError, match=r"extra keys not allowed"):  # noqa: PT012
         with dvc.config.edit() as conf:
             conf["core"]["invalid_key"] = "value"
 
 
 def test_merging_two_levels(dvc):
     with dvc.config.edit() as conf:
         conf["remote"]["test"] = {"url": "ssh://example.com"}
 
-    with pytest.raises(
+    with pytest.raises(  # noqa: PT012
         ConfigError, match=r"expected 'url' for dictionary value"
     ):
         with dvc.config.edit("global") as conf:
             conf["remote"]["test"] = {"password": "1"}
 
     with dvc.config.edit("local") as conf:
         conf["remote"]["test"] = {"password": "1"}
@@ -241,15 +241,15 @@
         "password": "1",
         "verify": False,
     }
 
 
 def test_config_loads_without_error_for_non_dvc_repo(tmp_dir):
     # regression testing for https://github.com/iterative/dvc/issues/3328
-    Config(validate=True)
+    Config.from_cwd(validate=True)
 
 
 @pytest.mark.parametrize(
     "field, remote_url",
     [
         ("credentialpath", "s3://mybucket/my/path"),
         ("credentialpath", "gs://my-bucket/path"),
@@ -270,73 +270,66 @@
     assert dvc.config["remote"]["test"][field] == os.path.join(
         dvc_dir, "..", "file.txt"
     )
 
     # load config and check that it contains what we expect
     # (relative paths are evaluated correctly)
     cfg = Config(dvc_dir)
-    assert cfg["remote"]["test"][field] == os.path.join(
-        dvc_dir, "..", "file.txt"
-    )
+    assert cfg["remote"]["test"][field] == os.path.join(dvc_dir, "..", "file.txt")
+
+
+def test_config_gdrive_fields(tmp_dir, dvc):
+    with dvc.config.edit() as conf:
+        conf["remote"]["test"] = {
+            "url": "gdrive://root/test",
+            "profile": "myprofile",
+        }
+
+    Config.from_cwd(validate=True)
 
 
 def test_config_remote(tmp_dir, dvc, capsys):
     (tmp_dir / ".dvc" / "config").write_text(
-        "['remote \"myremote\"']\n"
-        "  url = s3://bucket/path\n"
-        "  region = myregion\n"
+        "['remote \"myremote\"']\n  url = s3://bucket/path\n  region = myregion\n"
     )
 
     assert main(["config", "remote.myremote.url"]) == 0
     out, _ = capsys.readouterr()
     assert "s3://bucket/path" in out
 
     assert main(["config", "remote.myremote.region"]) == 0
     out, _ = capsys.readouterr()
     assert "myregion" in out
 
 
 def test_config_show_origin_single(tmp_dir, dvc, capsys):
     (tmp_dir / ".dvc" / "config").write_text(
-        "['remote \"myremote\"']\n"
-        "  url = s3://bucket/path\n"
-        "  region = myregion\n"
+        "['remote \"myremote\"']\n  url = s3://bucket/path\n  region = myregion\n"
     )
 
-    assert (
-        main(["config", "--show-origin", "--project", "remote.myremote.url"])
-        == 0
-    )
+    assert main(["config", "--show-origin", "--project", "remote.myremote.url"]) == 0
     out, _ = capsys.readouterr()
-    assert (
-        "{}\t{}\n".format(os.path.join(".dvc", "config"), "s3://bucket/path")
-        in out
-    )
+    assert "{}\t{}\n".format(os.path.join(".dvc", "config"), "s3://bucket/path") in out
 
-    assert (
-        main(["config", "--show-origin", "--local", "remote.myremote.url"])
-        == 251
-    )
+    assert main(["config", "--show-origin", "--local", "remote.myremote.url"]) == 251
 
     assert main(["config", "--list", "--project", "--show-origin"]) == 0
     out, _ = capsys.readouterr()
     assert (
         "{}\t{}\n".format(
             os.path.join(".dvc", "config"),
             "remote.myremote.url=s3://bucket/path",
         )
         in out
     )
 
 
 def test_config_show_origin_merged(tmp_dir, dvc, capsys):
     (tmp_dir / ".dvc" / "config").write_text(
-        "['remote \"myremote\"']\n"
-        "  url = s3://bucket/path\n"
-        "  region = myregion\n"
+        "['remote \"myremote\"']\n  url = s3://bucket/path\n  region = myregion\n"
     )
 
     (tmp_dir / ".dvc" / "config.local").write_text(
         "['remote \"myremote\"']\n  timeout = 100\n"
     )
 
     assert main(["config", "--list", "--show-origin"]) == 0
```

### Comparing `dvc-2.9.5/tests/func/test_data_cloud.py` & `dvc-3.0.0a0/tests/func/test_data_cloud.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,27 +1,26 @@
 import logging
 import os
 import shutil
 
 import pytest
 from flaky.flaky_decorator import flaky
 
-import dvc as dvc_module
+import dvc_data
 from dvc.cli import main
-from dvc.data.db.local import LocalObjectDB
-from dvc.external_repo import clean_repos
-from dvc.objects.db import ObjectDB
+from dvc.exceptions import CheckoutError
+from dvc.repo.open_repo import clean_repos
 from dvc.stage.exceptions import StageNotFound
-from dvc.testing.test_remote import (  # noqa, pylint: disable=unused-import
-    TestRemote,
-)
+from dvc.testing.remote_tests import TestRemote  # noqa, pylint: disable=unused-import
 from dvc.utils.fs import remove
+from dvc_data.hashfile.db import HashFileDB
+from dvc_data.hashfile.db.local import LocalHashFileDB
 
 
-def test_cloud_cli(tmp_dir, dvc, remote, mocker):
+def test_cloud_cli(tmp_dir, dvc, remote, mocker):  # noqa: PLR0915
     jobs = 2
     args = ["-v", "-j", str(jobs)]
 
     (stage,) = tmp_dir.dvc_gen("foo", "foo")
     cache = stage.outs[0].cache_path
 
     (stage_dir,) = tmp_dir.dvc_gen(
@@ -33,92 +32,84 @@
             }
         }
     )
     assert stage_dir is not None
     cache_dir = stage_dir.outs[0].cache_path
 
     # FIXME check status output
-    hashes_exist = mocker.spy(LocalObjectDB, "hashes_exist")
+    oids_exist = mocker.spy(LocalHashFileDB, "oids_exist")
 
-    assert main(["push"] + args) == 0
+    assert main(["push", *args]) == 0
     assert os.path.exists(cache)
     assert os.path.isfile(cache)
     assert os.path.isfile(cache_dir)
-    assert hashes_exist.called
+    assert oids_exist.called
     assert all(
-        _kwargs["jobs"] == jobs
-        for (_args, _kwargs) in hashes_exist.call_args_list
+        _kwargs["jobs"] == jobs for (_args, _kwargs) in oids_exist.call_args_list
     )
 
-    remove(dvc.odb.local.cache_dir)
-    hashes_exist.reset_mock()
+    dvc.cache.local.clear()
+    oids_exist.reset_mock()
 
-    assert main(["fetch"] + args) == 0
+    assert main(["fetch", *args]) == 0
     assert os.path.exists(cache)
     assert os.path.isfile(cache)
     assert os.path.isfile(cache_dir)
-    assert hashes_exist.called
+    assert oids_exist.called
     assert all(
-        _kwargs["jobs"] == jobs
-        for (_args, _kwargs) in hashes_exist.call_args_list
+        _kwargs["jobs"] == jobs for (_args, _kwargs) in oids_exist.call_args_list
     )
 
-    hashes_exist.reset_mock()
+    oids_exist.reset_mock()
 
-    assert main(["pull"] + args) == 0
+    assert main(["pull", *args]) == 0
     assert os.path.exists(cache)
     assert os.path.isfile(cache)
     assert os.path.isfile(cache_dir)
     assert os.path.isfile("foo")
     assert os.path.isdir("data_dir")
-    assert hashes_exist.called
+    assert oids_exist.called
     assert all(
-        _kwargs["jobs"] == jobs
-        for (_args, _kwargs) in hashes_exist.call_args_list
+        _kwargs["jobs"] == jobs for (_args, _kwargs) in oids_exist.call_args_list
     )
 
     with open(cache, encoding="utf-8") as fd:
         assert fd.read() == "foo"
     assert os.path.isfile(cache_dir)
 
     # NOTE: http doesn't support gc yet
     if remote.url.startswith("http"):
         return
 
-    hashes_exist.reset_mock()
+    oids_exist.reset_mock()
 
-    _list_hashes_traverse = mocker.spy(ObjectDB, "_list_hashes_traverse")
+    _list_oids_traverse = mocker.spy(HashFileDB, "_list_oids_traverse")
     # NOTE: check if remote gc works correctly on directories
-    assert main(["gc", "-cw", "-f"] + args) == 0
-    assert _list_hashes_traverse.called
-    assert all(
-        _kwargs["jobs"] == 2
-        for (_args, _kwargs) in hashes_exist.call_args_list
-    )
-    shutil.move(dvc.odb.local.cache_dir, dvc.odb.local.cache_dir + ".back")
+    assert main(["gc", "-cw", "-f", *args]) == 0
+    assert _list_oids_traverse.called
+    assert all(_kwargs["jobs"] == 2 for (_args, _kwargs) in oids_exist.call_args_list)
+    shutil.move(dvc.cache.local.path, dvc.cache.local.path + ".back")
 
-    assert main(["fetch"] + args) == 0
+    assert main(["fetch", *args]) == 0
 
-    assert hashes_exist.called
+    assert oids_exist.called
     assert all(
-        _kwargs["jobs"] == jobs
-        for (_args, _kwargs) in hashes_exist.call_args_list
+        _kwargs["jobs"] == jobs for (_args, _kwargs) in oids_exist.call_args_list
     )
 
-    hashes_exist.reset_mock()
-    assert main(["pull", "-f"] + args) == 0
+    oids_exist.reset_mock()
+    assert main(["pull", "-f", *args]) == 0
     assert os.path.exists(cache)
     assert os.path.isfile(cache)
     assert os.path.isfile(cache_dir)
     assert os.path.isfile("foo")
     assert os.path.isdir("data_dir")
-    assert hashes_exist.called
+    assert oids_exist.called
     assert all(
-        _kwargs["jobs"] == jobs
-        for (_args, _kwargs) in hashes_exist.call_args_list
+        _kwargs["jobs"] == jobs for (_args, _kwargs) in oids_exist.call_args_list
     )
 
 
 def test_data_cloud_error_cli(dvc):
     f = "non-existing-file"
     assert main(["status", "-c", f])
     assert main(["push", f])
@@ -145,34 +136,31 @@
         )
 
         assert expected_warning in caplog.text
 
 
 def test_hash_recalculation(mocker, dvc, tmp_dir, local_remote):
     tmp_dir.gen({"foo": "foo"})
-    test_file_md5 = mocker.spy(dvc_module.data.stage, "file_md5")
+    test_file_md5 = mocker.spy(dvc_data.hashfile.hash, "file_md5")
     ret = main(["config", "cache.type", "hardlink"])
     assert ret == 0
     ret = main(["add", "foo"])
     assert ret == 0
     ret = main(["push"])
     assert ret == 0
     ret = main(["run", "--single-stage", "-d", "foo", "echo foo"])
     assert ret == 0
     assert test_file_md5.mock.call_count == 1
 
 
 def test_missing_cache(tmp_dir, dvc, local_remote, caplog):
-    from tests.utils import clean_staging
-
     tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"})
 
     # purge cache
-    remove(dvc.odb.local.cache_dir)
-    clean_staging()
+    dvc.cache.local.clear()
 
     header = (
         "Some of the cache files do not exist "
         "neither locally nor on remote. Missing cache files:\n"
     )
     foo = "name: bar, md5: 37b51d194a7513e45b56f6524f2d51f2\n"
     bar = "name: foo, md5: acbd18db4cc2f85cedef654fccc4a4d8\n"
@@ -195,38 +183,37 @@
         "foo": "missing",
     }
     assert header not in caplog.text
     assert foo not in caplog.text
     assert bar not in caplog.text
 
 
-def test_verify_hashes(
-    tmp_dir, scm, dvc, mocker, tmp_path_factory, local_remote
-):
+def test_verify_hashes(tmp_dir, scm, dvc, mocker, tmp_path_factory, local_remote):
     tmp_dir.dvc_gen({"file": "file1 content"}, commit="add file")
     tmp_dir.dvc_gen({"dir": {"subfile": "file2 content"}}, commit="add dir")
     dvc.push()
 
     # remove artifacts and cache to trigger fetching
     remove("file")
     remove("dir")
-    remove(dvc.odb.local.cache_dir)
+    dvc.cache.local.clear()
 
-    hash_spy = mocker.spy(dvc_module.data.stage, "file_md5")
+    hash_spy = mocker.spy(dvc_data.hashfile.hash, "file_md5")
 
     dvc.pull()
-    assert hash_spy.call_count == 0
+    # NOTE: 1 is for index.data_tree building
+    assert hash_spy.call_count == 1
 
     # Removing cache will invalidate existing state entries
-    remove(dvc.odb.local.cache_dir)
+    dvc.cache.local.clear()
 
     dvc.config["remote"]["upstream"]["verify"] = True
 
     dvc.pull()
-    assert hash_spy.call_count == 3
+    assert hash_spy.call_count == 6
 
 
 @flaky(max_runs=3, min_passes=1)
 @pytest.mark.parametrize(
     "erepo", [pytest.lazy_fixture("git_dir"), pytest.lazy_fixture("erepo_dir")]
 )
 def test_pull_git_imports(tmp_dir, dvc, scm, erepo):
@@ -235,20 +222,21 @@
         erepo.scm_gen("foo", "foo", commit="first")
 
     dvc.imp(os.fspath(erepo), "foo")
     dvc.imp(os.fspath(erepo), "dir", out="new_dir", rev="HEAD~")
 
     assert dvc.pull()["fetched"] == 0
 
-    for item in ["foo", "new_dir", dvc.odb.local.cache_dir]:
+    for item in ["foo", "new_dir"]:
         remove(item)
-    os.makedirs(dvc.odb.local.cache_dir, exist_ok=True)
+    dvc.cache.local.clear()
+    os.makedirs(dvc.cache.local.path, exist_ok=True)
     clean_repos()
 
-    assert dvc.pull(force=True)["fetched"] == 3
+    assert dvc.pull(force=True)["fetched"] == 2
 
     assert (tmp_dir / "foo").exists()
     assert (tmp_dir / "foo").read_text() == "foo"
 
     assert (tmp_dir / "new_dir").exists()
     assert (tmp_dir / "new_dir" / "bar").read_text() == "bar"
 
@@ -273,17 +261,27 @@
     assert (tmp_dir / "foo").exists()
     assert (tmp_dir / "foo").read_text() == "foo"
 
     assert (tmp_dir / "new_dir").exists()
     assert (tmp_dir / "new_dir" / "bar").read_text() == "bar"
 
 
-def test_pull_external_dvc_imports_mixed(
-    tmp_dir, dvc, scm, erepo_dir, local_remote
-):
+def test_pull_partial_import(tmp_dir, dvc, local_workspace):
+    local_workspace.gen("file", "file content")
+    dst = tmp_dir / "file"
+    stage = dvc.imp_url("remote://workspace/file", os.fspath(dst), no_download=True)
+
+    result = dvc.pull("file")
+    assert result["fetched"] == 1
+    assert dst.exists()
+
+    assert stage.outs[0].get_hash().value == "d10b4c3ff123b26dc068d43a8bef2d23"
+
+
+def test_pull_external_dvc_imports_mixed(tmp_dir, dvc, scm, erepo_dir, local_remote):
     with erepo_dir.chdir():
         erepo_dir.dvc_gen("foo", "foo", commit="first")
         os.remove("foo")
 
     # imported: foo
     dvc.imp(os.fspath(erepo_dir), "foo")
 
@@ -295,30 +293,25 @@
 
     assert dvc.pull()["fetched"] == 2
     assert (tmp_dir / "foo").read_text() == "foo"
     assert (tmp_dir / "bar").read_text() == "bar"
 
 
 def clean(outs, dvc=None):
-    from tests.utils import clean_staging
-
     if dvc:
-        outs = outs + [dvc.odb.local.cache_dir]
+        dvc.cache.local.clear()
     for path in outs:
-        print(path)
         remove(path)
     if dvc:
-        os.makedirs(dvc.odb.local.cache_dir, exist_ok=True)
         clean_repos()
-        clean_staging()
 
 
 def recurse_list_dir(d):
     return [
-        os.path.join(d, f) for _, _, filenames in os.walk(d) for f in filenames
+        os.path.join(root, f) for root, _, filenames in os.walk(d) for f in filenames
     ]
 
 
 def test_dvc_pull_pipeline_stages(tmp_dir, dvc, run_copy, local_remote):
     (stage0,) = tmp_dir.dvc_gen("foo", "foo")
     stage1 = run_copy("foo", "bar", single_stage=True)
     stage2 = run_copy("bar", "foobar", name="copy-bar-foobar")
@@ -466,30 +459,30 @@
 
     clean(["foo", "bar", "baz"], dvc)
     assert dvc.pull(**{key: True})["fetched"] == expected
 
 
 def test_push_pull_fetch_pipeline_stages(tmp_dir, dvc, run_copy, local_remote):
     tmp_dir.dvc_gen("foo", "foo")
-    run_copy("foo", "bar", no_commit=True, name="copy-foo-bar")
+    run_copy("foo", "bar", name="copy-foo-bar")
 
     dvc.push("copy-foo-bar")
     assert len(recurse_list_dir(local_remote.url)) == 1
     # pushing everything so as we can check pull/fetch only downloads
     # from specified targets
     dvc.push()
     clean(["foo", "bar"], dvc)
 
     dvc.pull("copy-foo-bar")
     assert (tmp_dir / "bar").exists()
-    assert len(recurse_list_dir(dvc.odb.local.cache_dir)) == 1
+    assert len(recurse_list_dir(dvc.cache.local.path)) == 2
     clean(["bar"], dvc)
 
     dvc.fetch("copy-foo-bar")
-    assert len(recurse_list_dir(dvc.odb.local.cache_dir)) == 1
+    assert len(recurse_list_dir(dvc.cache.local.path)) == 2
 
 
 def test_pull_partial(tmp_dir, dvc, local_remote):
     tmp_dir.dvc_gen({"foo": {"bar": {"baz": "baz"}, "spam": "spam"}})
     dvc.push()
     clean(["foo"], dvc)
 
@@ -527,15 +520,15 @@
         "b8a9f715dbb64fd5c56e7783c6820a61",
     }
 
     clean(["foo", "bar", "data"], dvc)
 
     dvc.pull()
 
-    assert set(dvc.odb.local.all()) == {
+    assert set(dvc.cache.local.all()) == {
         "37b51d194a7513e45b56f6524f2d51f2",
         "acbd18db4cc2f85cedef654fccc4a4d8",
         "f97c5d29941bfb1b2fdab0874906ab82",
         "6b18131dc289fd37006705affe961ef8.dir",
         "b8a9f715dbb64fd5c56e7783c6820a61",
     }
 
@@ -560,13 +553,27 @@
         "b8a9f715dbb64fd5c56e7783c6820a61",
     }
 
     clean(["foo", "data"], dvc)
 
     dvc.pull(remote="myremote")
 
-    assert set(dvc.odb.local.all()) == {
+    assert set(dvc.cache.local.all()) == {
         "acbd18db4cc2f85cedef654fccc4a4d8",
         "f97c5d29941bfb1b2fdab0874906ab82",
         "6b18131dc289fd37006705affe961ef8.dir",
         "b8a9f715dbb64fd5c56e7783c6820a61",
     }
+
+
+def test_pull_allow_missing(tmp_dir, dvc, local_remote):
+    dvc.stage.add(name="bar", outs=["bar"], cmd="echo bar > bar")
+
+    with pytest.raises(CheckoutError):
+        dvc.pull()
+
+    tmp_dir.dvc_gen("foo", "foo")
+    dvc.push()
+    clean(["foo"], dvc)
+
+    stats = dvc.pull(allow_missing=True)
+    assert stats["fetched"] == 1
```

### Comparing `dvc-2.9.5/tests/func/test_diff.py` & `dvc-3.0.0a0/tests/func/test_diff.py`

 * *Files 2% similar despite different names*

```diff
@@ -74,15 +74,20 @@
         "deleted": [],
         "modified": [
             {
                 "path": "file",
                 "hash": {"old": digest("first"), "new": digest("second")},
             }
         ],
-        "not in cache": [],
+        "not in cache": [
+            {
+                "path": "file",
+                "hash": digest("first"),
+            }
+        ],
         "renamed": [],
     }
 
 
 @pytest.mark.parametrize("delete_data", [True, False])
 def test_deleted(tmp_dir, scm, dvc, delete_data):
     tmp_dir.dvc_gen("file", "text", commit="add file")
@@ -113,22 +118,53 @@
             }
         ],
         "not in cache": [],
         "renamed": [],
     }
 
 
+def test_modified_subrepo(tmp_dir, scm, dvc):
+    from dvc.repo import Repo
+
+    tmp_dir.gen({"subdir": {"file": "first"}})
+    subrepo_dir = tmp_dir / "subdir"
+
+    with subrepo_dir.chdir():
+        subrepo = Repo.init(subdir=True)
+        subrepo.add("file")
+
+    scm.add(os.path.join("subdir", "file.dvc"))
+    scm.commit("init")
+
+    (subrepo_dir / "file").write_text("second")
+
+    with subrepo_dir.chdir():
+        subrepo = Repo()
+        assert subrepo.diff() == {
+            "added": [],
+            "deleted": [],
+            "modified": [
+                {
+                    "path": "file",
+                    "hash": {"old": digest("first"), "new": digest("second")},
+                }
+            ],
+            "not in cache": [],
+            "renamed": [],
+        }
+
+
 def test_refs(tmp_dir, scm, dvc):
     tmp_dir.dvc_gen("file", "first", commit="first version")
     tmp_dir.dvc_gen("file", "second", commit="second version")
     tmp_dir.dvc_gen("file", "third", commit="third version")
 
-    HEAD_2 = digest("first")
-    HEAD_1 = digest("second")
-    HEAD = digest("third")
+    HEAD_2 = digest("first")  # noqa: N806
+    HEAD_1 = digest("second")  # noqa: N806
+    HEAD = digest("third")  # noqa: N806
 
     assert dvc.diff("HEAD~1") == {
         "added": [],
         "deleted": [],
         "modified": [{"path": "file", "hash": {"old": HEAD_1, "new": HEAD}}],
         "not in cache": [],
         "renamed": [],
@@ -212,35 +248,38 @@
     }
 
 
 def test_diff_no_cache(tmp_dir, scm, dvc):
     tmp_dir.dvc_gen({"dir": {"file": "file content"}}, commit="first")
     scm.tag("v1")
 
-    tmp_dir.dvc_gen(
-        {"dir": {"file": "modified file content"}}, commit="second"
-    )
+    tmp_dir.dvc_gen({"dir": {"file": "modified file content"}}, commit="second")
     scm.tag("v2")
 
-    remove(dvc.odb.local.cache_dir)
+    remove(dvc.cache.local.path)
 
     # invalidate_dir_info to force cache loading
-    dvc.odb.local._dir_info = {}
+    dvc.cache.local._dir_info = {}
 
     diff = dvc.diff("v1", "v2")
     assert diff["added"] == []
     assert diff["deleted"] == []
     assert first(diff["modified"])["path"] == os.path.join("dir", "")
     assert diff["not in cache"] == []
 
     (tmp_dir / "dir" / "file").unlink()
     remove(str(tmp_dir / "dir"))
     diff = dvc.diff()
     assert diff["added"] == []
-    assert diff["deleted"] == []
+    assert diff["deleted"] == [
+        {
+            "path": os.path.join("dir", ""),
+            "hash": "f0f7a307d223921557c929f944bf5303.dir",
+        }
+    ]
     assert diff["renamed"] == []
     assert diff["modified"] == []
     assert diff["not in cache"] == [
         {
             "path": os.path.join("dir", ""),
             "hash": "f0f7a307d223921557c929f944bf5303.dir",
         }
@@ -263,17 +302,15 @@
         "added": [
             {
                 "hash": digest("dir file 2 content"),
                 "path": os.path.join("dir", "dir_file2"),
             },
             {"hash": "86d049de17c76ac44cdcac146042ec9b", "path": "new_file"},
         ],
-        "deleted": [
-            {"hash": "7f0b6bb0b7e951b7fd2b2a4a326297e1", "path": "file"}
-        ],
+        "deleted": [{"hash": "7f0b6bb0b7e951b7fd2b2a4a326297e1", "path": "file"}],
         "modified": [
             {
                 "hash": {
                     "new": "38175ad60f0e58ac94e0e2b7688afd81.dir",
                     "old": "92daf39af116ca2fb245acaeb2ae65f7.dir",
                 },
                 "path": os.path.join("dir", ""),
@@ -286,39 +323,48 @@
 
 def test_no_changes(tmp_dir, scm, dvc):
     tmp_dir.dvc_gen("file", "first", commit="add a file")
     assert dvc.diff() == {}
 
 
 def test_no_commits(tmp_dir):
-    from scmrepo.git import Git
-
     from dvc.repo import Repo
+    from dvc.scm import Git
 
     git = Git.init(tmp_dir.fs_path)
     assert git.no_commits
 
     assert Repo.init().diff() == {}
 
 
+def test_abs_target(tmp_dir, scm, dvc):
+    tmp_dir.dvc_gen("file", "text")
+
+    assert dvc.diff(targets=(tmp_dir / "file").fs_path) == {
+        "added": [{"path": "file", "hash": digest("text")}],
+        "deleted": [],
+        "modified": [],
+        "not in cache": [],
+        "renamed": [],
+    }
+
+
 def setup_targets_test(tmp_dir):
     tmp_dir.dvc_gen("file", "first", commit="add a file")
 
     tmp_dir.dvc_gen({"dir": {"1": "1", "2": "2"}})
     tmp_dir.dvc_gen("file", "second")
 
     tmp_dir.dvc_gen(os.path.join("dir_with", "file.txt"), "first")
 
 
 def test_targets_missing_path(tmp_dir, scm, dvc):
-    from dvc.exceptions import PathMissingError
-
     setup_targets_test(tmp_dir)
 
-    with pytest.raises(PathMissingError):
+    with pytest.raises(FileNotFoundError):
         dvc.diff(targets=["missing"])
 
 
 def test_targets_single_file(tmp_dir, scm, dvc):
     setup_targets_test(tmp_dir)
 
     assert dvc.diff(targets=["file"]) == {
@@ -367,17 +413,15 @@
         "renamed": [],
     }
 
 
 def test_targets_two_files_in_dir(tmp_dir, scm, dvc):
     setup_targets_test(tmp_dir)
 
-    assert dvc.diff(
-        targets=[os.path.join("dir", "1"), os.path.join("dir", "2")]
-    ) == {
+    assert dvc.diff(targets=[os.path.join("dir", "1"), os.path.join("dir", "2")]) == {
         "added": [
             {"path": os.path.join("dir", "1"), "hash": digest("1")},
             {"path": os.path.join("dir", "2"), "hash": digest("2")},
         ],
         "deleted": [],
         "modified": [],
         "not in cache": [],
@@ -420,16 +464,18 @@
         ],
         "deleted": [],
         "modified": [],
         "not in cache": [],
         "renamed": [],
     }
 
-    assert dvc.diff(targets=["dir_with"]) == expected_result
-    assert dvc.diff(targets=["dir_with" + os.path.sep]) == expected_result
+    assert dvc.diff(targets=["dir_with"], recursive=True) == expected_result
+    assert (
+        dvc.diff(targets=["dir_with" + os.path.sep], recursive=True) == expected_result
+    )
 
 
 def test_targets_single_file_in_dir_with_file(tmp_dir, scm, dvc):
     setup_targets_test(tmp_dir)
 
     assert dvc.diff(targets=[os.path.join("dir_with", "file.txt")]) == {
         "added": [
@@ -536,17 +582,15 @@
     if commit_last:
         last_commit_msg = "commit #2"
         a_rev = "HEAD~1"
     else:
         last_commit_msg = None
         a_rev = "HEAD"
 
-    paths = tmp_dir.gen(
-        {"dir": {"file": "text1", "subdir": {"file2": "text2"}}}
-    )
+    paths = tmp_dir.gen({"dir": {"file": "text1", "subdir": {"file2": "text2"}}})
     tmp_dir.dvc_add(paths, commit="commit #1")
     (tmp_dir / "dir" / "file").replace(tmp_dir / "dir" / "subdir" / "file3")
 
     tmp_dir.dvc_add(paths, commit=last_commit_msg)
     assert dvc.diff(a_rev) == {
         "added": [],
         "deleted": [],
```

### Comparing `dvc-2.9.5/tests/func/test_external_repo.py` & `dvc-3.0.0a0/tests/func/test_external_repo.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,40 +1,39 @@
 import os
-from unittest.mock import ANY, patch
+from unittest.mock import ANY
 
-from scmrepo.git import Git
-
-from dvc.data.stage import stage
-from dvc.data.transfer import transfer
-from dvc.external_repo import CLONES, external_repo
+from dvc.repo.open_repo import CLONES
+from dvc.repo.open_repo import _external_repo as external_repo
+from dvc.scm import Git
+from dvc.testing.tmp_dir import make_subrepo
 from dvc.utils import relpath
-from dvc.utils.fs import makedirs, remove
-from tests.unit.fs.test_repo import make_subrepo
-from tests.utils import clean_staging
+from dvc.utils.fs import remove
+from dvc_data.hashfile.build import build
+from dvc_data.hashfile.transfer import transfer
 
 
 def test_external_repo(erepo_dir, mocker):
     with erepo_dir.chdir():
         with erepo_dir.branch("branch", new=True):
             erepo_dir.dvc_gen("file", "branch", commit="create file on branch")
         erepo_dir.dvc_gen("file", "master", commit="create file on master")
 
     url = os.fspath(erepo_dir)
 
-    mock = mocker.patch.object(Git, "clone", wraps=Git.clone)
+    clone_spy = mocker.spy(Git, "clone")
 
     with external_repo(url) as repo:
-        with repo.open_by_relpath("file") as fd:
+        with repo.dvcfs.open("file") as fd:
             assert fd.read() == "master"
 
     with external_repo(url, rev="branch") as repo:
-        with repo.open_by_relpath("file") as fd:
+        with repo.dvcfs.open("file") as fd:
             assert fd.read() == "branch"
 
-    assert mock.call_count == 1
+    assert clone_spy.call_count == 1
 
 
 def test_source_change(erepo_dir):
     url = os.fspath(erepo_dir)
     with external_repo(url) as repo:
         old_rev = repo.scm.get_rev()
 
@@ -43,40 +42,40 @@
     with external_repo(url) as repo:
         new_rev = repo.scm.get_rev()
 
     assert old_rev != new_rev
 
 
 def test_cache_reused(erepo_dir, mocker, local_cloud):
-    import dvc.fs.utils
+    from dvc_objects.fs import generic
 
     erepo_dir.add_remote(config=local_cloud.config)
     with erepo_dir.chdir():
         erepo_dir.dvc_gen("file", "text", commit="add file")
     erepo_dir.dvc.push()
 
-    download_spy = mocker.spy(dvc.fs.utils, "transfer")
+    download_spy = mocker.spy(generic, "transfer")
 
     # Use URL to prevent any fishy optimizations
-    url = f"file://{erepo_dir}"
+    url = f"file://{erepo_dir.as_posix()}"
     with external_repo(url) as repo:
         repo.fetch()
         assert download_spy.mock.call_count == 1
 
     # Should not download second time
     erepo_dir.scm.branch("branch")
     with external_repo(url, "branch") as repo:
         repo.fetch()
         assert download_spy.mock.call_count == 1
 
 
 def test_known_sha(erepo_dir):
     erepo_dir.scm.commit("init")
 
-    url = f"file://{erepo_dir}"
+    url = f"file://{erepo_dir.as_posix()}"
     with external_repo(url) as repo:
         rev = repo.scm.get_rev()
         prev_rev = repo.scm.resolve_rev("HEAD^")
 
     # Hits cache
     with external_repo(url, rev) as repo:
         pass
@@ -91,16 +90,16 @@
         subdir = erepo_dir / "subdir"
         subdir.mkdir()
         (subdir / "file").write_text("contents")
         erepo_dir.dvc_add(subdir / "file", commit="create file")
 
     dest = tmp_dir / "file"
     with external_repo(os.fspath(erepo_dir)) as repo:
-        repo.repo_fs.download(
-            os.path.join(repo.root_dir, "subdir", "file"),
+        repo.dvcfs.get(
+            "subdir/file",
             os.fspath(dest),
         )
 
     assert dest.is_file()
     assert dest.read_text() == "contents"
 
 
@@ -113,115 +112,108 @@
     upstream_dir = tmp_dir
     upstream_url = relpath(upstream_dir, erepo_dir)
     erepo_dir.add_remote(url=upstream_url)
 
     erepo_dir.dvc.push()
 
     (erepo_dir / "file").unlink()
-    remove(erepo_dir.dvc.odb.local.cache_dir)
+    remove(erepo_dir.dvc.cache.local.path)
 
     url = os.fspath(erepo_dir)
 
     with external_repo(url) as repo:
         assert os.path.isabs(repo.config["remote"]["upstream"]["url"])
         assert os.path.isdir(repo.config["remote"]["upstream"]["url"])
-        with repo.open_by_relpath("file") as fd:
+        with repo.dvcfs.open("file") as fd:
             assert fd.read() == "contents"
 
 
-def test_shallow_clone_branch(erepo_dir):
+def test_shallow_clone_branch(erepo_dir, mocker):
     with erepo_dir.chdir():
         with erepo_dir.branch("branch", new=True):
             erepo_dir.dvc_gen("file", "branch", commit="create file on branch")
         erepo_dir.dvc_gen("file", "master", commit="create file on master")
 
     url = os.fspath(erepo_dir)
+    clone_spy = mocker.spy(Git, "clone")
 
-    with patch.object(Git, "clone", wraps=Git.clone) as mock_clone:
-        with external_repo(url, rev="branch") as repo:
-            with repo.open_by_relpath("file") as fd:
-                assert fd.read() == "branch"
+    with external_repo(url, rev="branch") as repo:
+        with repo.dvcfs.open("file") as fd:
+            assert fd.read() == "branch"
 
-        mock_clone.assert_called_with(
-            url, ANY, shallow_branch="branch", progress=ANY
-        )
-        _, shallow = CLONES[url]
-        assert shallow
+    clone_spy.assert_called_with(url, ANY, shallow_branch="branch", progress=ANY)
 
-        with external_repo(url) as repo:
-            with repo.open_by_relpath("file") as fd:
-                assert fd.read() == "master"
+    path, _ = CLONES[url]
+    CLONES[url] = (path, True)
 
-        assert mock_clone.call_count == 1
-        _, shallow = CLONES[url]
-        assert not shallow
+    mock_fetch = mocker.patch.object(Git, "fetch")
+    with external_repo(url) as repo:
+        with repo.dvcfs.open("file") as fd:
+            assert fd.read() == "master"
+    mock_fetch.assert_called_with(unshallow=True)
 
 
-def test_shallow_clone_tag(erepo_dir):
+def test_shallow_clone_tag(erepo_dir, mocker):
     with erepo_dir.chdir():
         erepo_dir.dvc_gen("file", "foo", commit="init")
         erepo_dir.scm.tag("v1")
         erepo_dir.dvc_gen("file", "bar", commit="update file")
 
     url = os.fspath(erepo_dir)
 
-    with patch.object(Git, "clone", wraps=Git.clone) as mock_clone:
-        with external_repo(url, rev="v1") as repo:
-            with repo.open_by_relpath("file") as fd:
-                assert fd.read() == "foo"
-
-        mock_clone.assert_called_with(
-            url, ANY, shallow_branch="v1", progress=ANY
-        )
-        _, shallow = CLONES[url]
-        assert shallow
-
-        with external_repo(url, rev="master") as repo:
-            with repo.open_by_relpath("file") as fd:
-                assert fd.read() == "bar"
-
-        assert mock_clone.call_count == 1
-        _, shallow = CLONES[url]
-        assert not shallow
+    clone_spy = mocker.spy(Git, "clone")
+    with external_repo(url, rev="v1") as repo:
+        with repo.dvcfs.open("file") as fd:
+            assert fd.read() == "foo"
+
+    clone_spy.assert_called_with(url, ANY, shallow_branch="v1", progress=ANY)
+
+    path, _ = CLONES[url]
+    CLONES[url] = (path, True)
+
+    mock_fetch = mocker.patch.object(Git, "fetch")
+    with external_repo(url, rev="master") as repo:
+        with repo.dvcfs.open("file") as fd:
+            assert fd.read() == "bar"
+    mock_fetch.assert_called_with(unshallow=True)
 
 
 def test_subrepos_are_ignored(tmp_dir, erepo_dir):
     subrepo = erepo_dir / "dir" / "subrepo"
     make_subrepo(subrepo, erepo_dir.scm)
     with erepo_dir.chdir():
         erepo_dir.dvc_gen("dir/foo", "foo", commit="foo")
         erepo_dir.scm_gen("dir/bar", "bar", commit="bar")
 
     with subrepo.chdir():
         subrepo.dvc_gen({"file": "file"}, commit="add files on subrepo")
 
     with external_repo(os.fspath(erepo_dir)) as repo:
-        repo.repo_fs.download(
-            os.path.join(repo.root_dir, "dir"),
+        repo.dvcfs.get(
+            "dir",
             os.fspath(tmp_dir / "out"),
         )
         expected_files = {"foo": "foo", "bar": "bar", ".gitignore": "/foo\n"}
         assert (tmp_dir / "out").read_text() == expected_files
 
         # clear cache to test saving to cache
-        cache_dir = tmp_dir / repo.odb.local.cache_dir
+        cache_dir = tmp_dir / repo.cache.local.path
         remove(cache_dir)
-        clean_staging()
-        makedirs(cache_dir)
+        os.makedirs(cache_dir)
 
-        staging, _, obj = stage(
-            repo.odb.local,
-            os.path.join(repo.root_dir, "dir"),
-            repo.repo_fs,
+        staging, _, obj = build(
+            repo.cache.local,
+            "dir",
+            repo.dvcfs,
             "md5",
-            dvcignore=repo.dvcignore,
+            ignore=repo.dvcignore,
         )
         transfer(
             staging,
-            repo.odb.local,
+            repo.cache.local,
             {obj.hash_info},
             shallow=False,
             hardlink=True,
         )
         assert set(cache_dir.glob("??/*")) == {
             cache_dir / "e1" / "d9e8eae5374860ae025ec84cfd85c7.dir",
             cache_dir / "37" / "b51d194a7513e45b56f6524f2d51f2",
@@ -237,13 +229,13 @@
         scm_files = {"foo": "foo", "bar": "bar", "subdir": {"lorem": "lorem"}}
         erepo_dir.scm_gen({"dir": scm_files}, commit="add scm dir")
 
     with subrepo.chdir():
         subrepo.dvc_gen({"file": "file"}, commit="add files on subrepo")
 
     with external_repo(os.fspath(erepo_dir)) as repo:
-        repo.repo_fs.download(
-            os.path.join(repo.root_dir, "dir"),
+        repo.dvcfs.get(
+            "dir",
             os.fspath(tmp_dir / "out"),
         )
         # subrepo files should not be here
         assert (tmp_dir / "out").read_text() == scm_files
```

### Comparing `dvc-2.9.5/tests/func/test_fs.py` & `dvc-3.0.0a0/tests/func/test_run_multistage.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,369 +1,397 @@
-import io
 import os
-from operator import itemgetter
-from os.path import join
+import textwrap
 
-import fsspec
 import pytest
 
-from dvc.fs import get_cloud_fs
-from dvc.fs.local import LocalFileSystem
-from dvc.repo import Repo
+from dvc.exceptions import InvalidArgumentError
+from dvc.stage.exceptions import DuplicateStageName, InvalidStageName
 
 
-def test_local_fs_open(tmp_dir):
-    tmp_dir.gen(
-        {
-            "foo": "foo",
-            "bar": "bar",
-            "тест": "проверка",
-            "code.py": "import sys\nimport shutil\n"
-            "shutil.copyfile(sys.argv[1], sys.argv[2])",
-            "data_dir": {
-                "data": "data",
-                "data_sub_dir": {"data_sub": "data_sub"},
-            },
-        }
-    )
-    fs = LocalFileSystem()
-
-    with fs.open("foo", encoding="utf-8") as fobj:
-        assert fobj.read() == "foo"
-    with fs.open("тест", encoding="utf-8") as fobj:
-        assert fobj.read() == "проверка"
-
-
-def test_local_fs_exists(tmp_dir):
-    tmp_dir.gen(
-        {
-            "foo": "foo",
-            "bar": "bar",
-            "тест": "проверка",
-            "code.py": "import sys\nimport shutil\n"
-            "shutil.copyfile(sys.argv[1], sys.argv[2])",
-            "data_dir": {
-                "data": "data",
-                "data_sub_dir": {"data_sub": "data_sub"},
-            },
-        }
-    )
-    fs = LocalFileSystem()
-
-    assert fs.exists("foo")
-    assert fs.exists("тест")
-    assert not fs.exists("not-existing-file")
-
-
-def test_local_fs_isdir(tmp_dir):
-    tmp_dir.gen(
-        {
-            "foo": "foo",
-            "bar": "bar",
-            "тест": "проверка",
-            "code.py": "import sys\nimport shutil\n"
-            "shutil.copyfile(sys.argv[1], sys.argv[2])",
-            "data_dir": {
-                "data": "data",
-                "data_sub_dir": {"data_sub": "data_sub"},
-            },
-        }
-    )
-    fs = LocalFileSystem()
-
-    assert fs.isdir("data_dir")
-    assert not fs.isdir("foo")
-    assert not fs.isdir("not-existing-file")
-
-
-def test_local_fs_isfile(tmp_dir):
-    tmp_dir.gen(
-        {
-            "foo": "foo",
-            "bar": "bar",
-            "тест": "проверка",
-            "code.py": "import sys\nimport shutil\n"
-            "shutil.copyfile(sys.argv[1], sys.argv[2])",
-            "data_dir": {
-                "data": "data",
-                "data_sub_dir": {"data_sub": "data_sub"},
-            },
-        }
-    )
-    fs = LocalFileSystem()
-
-    assert fs.isfile("foo")
-    assert not fs.isfile("data_dir")
-    assert not fs.isfile("not-existing-file")
-
-
-def convert_to_sets(walk_results):
-    return [
-        (root, set(dirs), set(nondirs)) for root, dirs, nondirs in walk_results
-    ]
-
-
-def test_walk_no_scm(tmp_dir):
-    tmp_dir.gen(
-        {
-            "foo": "foo",
-            "bar": "bar",
-            "тест": "проверка",
-            "code.py": "import sys\nimport shutil\n"
-            "shutil.copyfile(sys.argv[1], sys.argv[2])",
-            "data_dir": {
-                "data": "data",
-                "data_sub_dir": {"data_sub": "data_sub"},
-            },
-        }
-    )
-    fs = LocalFileSystem()
-    walk_results = fs.walk(str(tmp_dir))
-    assert convert_to_sets(walk_results) == [
-        (str(tmp_dir), {"data_dir"}, {"code.py", "bar", "тест", "foo"}),
-        (str(tmp_dir / "data_dir"), {"data_sub_dir"}, {"data"}),
-        (str(tmp_dir / "data_dir" / "data_sub_dir"), set(), {"data_sub"}),
-    ]
-
-    walk_results = fs.walk(join("data_dir", "data_sub_dir"))
-    assert convert_to_sets(walk_results) == [
-        (join("data_dir", "data_sub_dir"), set(), {"data_sub"}),
-    ]
-
-
-def test_walk_fs_with_git(tmp_dir, scm):
-    tmp_dir.gen(
-        {
-            "foo": "foo",
-            "bar": "bar",
-            "тест": "проверка",
-            "code.py": "import sys\nimport shutil\n"
-            "shutil.copyfile(sys.argv[1], sys.argv[2])",
-            "data_dir": {
-                "data": "data",
-                "data_sub_dir": {"data_sub": "data_sub"},
-            },
-        }
-    )
-    fs = LocalFileSystem(url=str(tmp_dir))
-    walk_result = []
-    for root, dirs, files in fs.walk("."):
-        dirs[:] = [i for i in dirs if i != ".git"]
-        walk_result.append((root, dirs, files))
-
-    assert convert_to_sets(walk_result) == [
-        (".", {"data_dir"}, {"bar", "тест", "code.py", "foo"}),
-        (join("data_dir"), {"data_sub_dir"}, {"data"}),
-        (join("data_dir", "data_sub_dir"), set(), {"data_sub"}),
-    ]
-
-    walk_result = fs.walk(join("data_dir", "data_sub_dir"))
-    assert convert_to_sets(walk_result) == [
-        (join("data_dir", "data_sub_dir"), set(), {"data_sub"})
-    ]
-
-
-def test_cleanfs_subrepo(tmp_dir, dvc, scm, monkeypatch):
-    tmp_dir.gen({"subdir": {}})
-    subrepo_dir = tmp_dir / "subdir"
-    with subrepo_dir.chdir():
-        subrepo = Repo.init(subdir=True)
-        subrepo_dir.gen({"foo": "foo", "dir": {"bar": "bar"}})
-
-    path = subrepo_dir.fs_path
-
-    assert dvc.fs.exists(dvc.fs.path.join(path, "foo"))
-    assert dvc.fs.isfile(dvc.fs.path.join(path, "foo"))
-    assert dvc.fs.exists(dvc.fs.path.join(path, "dir"))
-    assert dvc.fs.isdir(dvc.fs.path.join(path, "dir"))
-
-    assert subrepo.fs.exists(subrepo.fs.path.join(path, "foo"))
-    assert subrepo.fs.isfile(subrepo.fs.path.join(path, "foo"))
-    assert subrepo.fs.exists(subrepo.fs.path.join(path, "dir"))
-    assert subrepo.fs.isdir(subrepo.fs.path.join(path, "dir"))
+def test_run_with_name(tmp_dir, dvc, run_copy):
+    from dvc.dvcfile import LOCK_FILE, PROJECT_FILE
+    from dvc.stage import PipelineStage
 
+    tmp_dir.dvc_gen("foo", "foo")
+    assert not os.path.exists(PROJECT_FILE)
+    stage = run_copy("foo", "bar", name="copy-foo-to-bar")
+    assert isinstance(stage, PipelineStage)
+    assert stage.name == "copy-foo-to-bar"
+    assert os.path.exists(PROJECT_FILE)
+    assert os.path.exists(LOCK_FILE)
 
-def test_walk_dont_ignore_subrepos(tmp_dir, scm, dvc):
-    tmp_dir.dvc_gen({"foo": "foo"}, commit="add foo")
-    subrepo_dir = tmp_dir / "subdir"
-    subrepo_dir.mkdir()
-    with subrepo_dir.chdir():
-        Repo.init(subdir=True)
-    scm.add(["subdir"])
-    scm.commit("Add subrepo")
 
-    dvc_fs = dvc.fs
-    dvc._reset()
-    scm_fs = scm.get_fs("HEAD")
-    path = os.fspath(tmp_dir)
-    get_dirs = itemgetter(1)
+def test_run_no_exec(tmp_dir, dvc, run_copy):
+    from dvc.dvcfile import LOCK_FILE, PROJECT_FILE
+    from dvc.stage import PipelineStage
 
-    assert set(get_dirs(next(dvc_fs.walk(path)))) == {".dvc", "subdir", ".git"}
-    assert set(get_dirs(next(scm_fs.walk(path)))) == {".dvc", "subdir"}
+    tmp_dir.dvc_gen("foo", "foo")
+    assert not os.path.exists(PROJECT_FILE)
+    stage = run_copy("foo", "bar", name="copy-foo-to-bar", no_exec=True)
+    assert isinstance(stage, PipelineStage)
+    assert stage.name == "copy-foo-to-bar"
+    assert os.path.exists(PROJECT_FILE)
+    assert not os.path.exists(LOCK_FILE)
 
+    data, _ = stage.dvcfile._load()
+    assert data["stages"]["copy-foo-to-bar"] == {
+        "cmd": "python copy.py foo bar",
+        "deps": ["copy.py", "foo"],
+        "outs": ["bar"],
+    }
 
-def test_fs_getsize(dvc, cloud):
-    cloud.gen({"data": {"foo": "foo"}, "baz": "baz baz"})
-    cls, config, path = get_cloud_fs(dvc, **cloud.config)
-    fs = cls(**config)
 
-    assert fs.getsize(fs.path.join(path, "baz")) == 7
-    assert fs.getsize(fs.path.join(path, "data", "foo")) == 3
+def test_run_with_multistage_and_single_stage(tmp_dir, dvc, run_copy):
+    from dvc.stage import PipelineStage, Stage
 
+    tmp_dir.dvc_gen("foo", "foo")
+    stage1 = run_copy("foo", "foo1", single_stage=True)
+    stage2 = run_copy("foo1", "foo2", name="copy-foo1-foo2")
+    stage3 = run_copy("foo2", "foo3", single_stage=True)
 
-def test_fs_upload_fobj(dvc, tmp_dir, cloud):
-    tmp_dir.gen("foo", "foo")
-    cls, config, path = get_cloud_fs(dvc, **cloud.config)
-    fs = cls(**config)
+    assert isinstance(stage2, PipelineStage)
+    assert isinstance(stage1, Stage)
+    assert isinstance(stage3, Stage)
+    assert stage2.name == "copy-foo1-foo2"
 
-    from_info = tmp_dir / "foo"
-    to_info = fs.path.join(path, "foo")
 
-    with open(from_info, "rb") as stream:
-        fs.upload_fobj(stream, to_info)
+def test_run_multi_stage_repeat(tmp_dir, dvc, run_copy):
+    from dvc.dvcfile import PROJECT_FILE, load_file
+    from dvc.stage import PipelineStage
 
-    assert fs.exists(to_info)
-    with fs.open(to_info, "rb") as stream:
-        assert stream.read() == b"foo"
+    tmp_dir.dvc_gen("foo", "foo")
+    run_copy("foo", "foo1", name="copy-foo-foo1")
+    run_copy("foo1", "foo2", name="copy-foo1-foo2")
+    run_copy("foo2", "foo3", single_stage=True)
 
+    stages = list(load_file(dvc, PROJECT_FILE).stages.values())
+    assert len(stages) == 2
+    assert all(isinstance(stage, PipelineStage) for stage in stages)
+    assert {stage.name for stage in stages} == {
+        "copy-foo-foo1",
+        "copy-foo1-foo2",
+    }
 
-def test_fs_makedirs_on_upload_and_copy(dvc, cloud):
-    cls, config, _ = get_cloud_fs(dvc, **cloud.config)
-    fs = cls(**config)
 
-    with io.BytesIO(b"foo") as stream:
-        fs.upload(stream, (cloud / "dir" / "foo").fs_path)
+def test_multi_stage_run_cached(tmp_dir, dvc, run_copy, mocker):
+    from dvc.stage.run import subprocess
 
-    assert fs.isdir((cloud / "dir").fs_path)
-    assert fs.exists((cloud / "dir" / "foo").fs_path)
+    tmp_dir.dvc_gen("foo", "foo")
 
-    fs.makedirs((cloud / "dir2").fs_path)
-    fs.copy((cloud / "dir" / "foo").fs_path, (cloud / "dir2" / "foo").fs_path)
-    assert fs.isdir((cloud / "dir2").fs_path)
-    assert fs.exists((cloud / "dir2" / "foo").fs_path)
+    run_copy("foo", "foo2", name="copy-foo1-foo2")
+    spy = mocker.spy(subprocess, "Popen")
+    run_copy("foo", "foo2", name="copy-foo1-foo2")
+    assert not spy.called
 
 
-def test_upload_callback(tmp_dir, dvc, cloud):
-    tmp_dir.gen("foo", "foo")
-    cls, config, _ = get_cloud_fs(dvc, **cloud.config)
-    fs = cls(**config)
-    expected_size = os.path.getsize(tmp_dir / "foo")
-
-    callback = fsspec.Callback()
-    fs.upload(
-        (tmp_dir / "foo").fs_path,
-        (cloud / "foo").fs_path,
-        callback=callback,
+def test_multistage_dump_on_non_cached_outputs(tmp_dir, dvc):
+    tmp_dir.dvc_gen("foo")
+    dvc.run(
+        cmd="cp foo foo1",
+        deps=["foo"],
+        name="copy-foo1-foo2",
+        outs_no_cache=["foo1"],
     )
 
-    assert callback.size == expected_size
-    assert callback.value == expected_size
-
-
-def test_download_callback(tmp_dir, dvc, cloud, local_cloud):
-    cls, config, _ = get_cloud_fs(dvc, **cloud.config)
-    fs = cls(**config)
 
-    (tmp_dir / "to_upload").write_text("foo")
-    fs.upload((tmp_dir / "to_upload").fs_path, (cloud / "foo").fs_path)
-    expected_size = fs.getsize((cloud / "foo").fs_path)
+def test_multistage_with_wdir(tmp_dir, dvc):
+    from dvc.dvcfile import load_file
 
-    callback = fsspec.Callback()
-    fs.download_file(
-        (cloud / "foo").fs_path,
-        (tmp_dir / "foo").fs_path,
-        callback=callback,
+    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
+    stage = dvc.run(
+        cmd="cp foo foo1",
+        deps=["foo"],
+        name="copy-foo1-foo2",
+        outs=["foo1"],
+        wdir="dir",
     )
 
-    assert callback.size == expected_size
-    assert callback.value == expected_size
-    assert (tmp_dir / "foo").read_text() == "foo"
+    data, _ = load_file(dvc, stage.path)._load()
+    assert data["stages"]["copy-foo1-foo2"]["wdir"] == "dir"
 
 
-def test_download_dir_callback(tmp_dir, dvc, cloud):
-    cls, config, _ = get_cloud_fs(dvc, **cloud.config)
-    fs = cls(**config)
-    cloud.gen({"dir": {"foo": "foo", "bar": "bar"}})
+def test_multistage_always_changed(tmp_dir, dvc):
+    from dvc.dvcfile import load_file
 
-    callback = fsspec.Callback()
-    fs.download(
-        (cloud / "dir").fs_path, (tmp_dir / "dir").fs_path, callback=callback
+    tmp_dir.gen({"foo": "foo", "bar": "bar"})
+    stage = dvc.run(
+        cmd="cp foo foo1",
+        deps=["foo"],
+        name="copy-foo1-foo2",
+        outs=["foo1"],
+        always_changed=True,
     )
 
-    assert callback.size == 2
-    assert callback.value == 2
-    assert (tmp_dir / "dir").read_text() == {"foo": "foo", "bar": "bar"}
+    data, _ = load_file(dvc, stage.path)._load()
+    assert data["stages"]["copy-foo1-foo2"]["always_changed"]
 
 
-@pytest.mark.parametrize("fs_type", ["git", "dvc"])
-def test_download_callbacks_on_dvc_git_fs(tmp_dir, dvc, scm, fs_type):
-    from dvc.fs.git import GitFileSystem
+def test_graph(tmp_dir, dvc):
+    from dvc.exceptions import CyclicGraphError
 
-    gen = tmp_dir.scm_gen if fs_type == "git" else tmp_dir.dvc_gen
-    gen({"dir": {"foo": "foo", "bar": "bar"}, "file": "file"}, commit="gen")
+    tmp_dir.gen({"foo": "foo", "bar": "bar"})
 
-    fs = dvc.dvcfs if fs_type == "dvc" else GitFileSystem(scm=scm, rev="HEAD")
+    dvc.run(deps=["foo"], outs=["bar"], cmd="echo foo > bar", name="1")
 
-    callback = fsspec.Callback()
-    fs.download_file(
-        "file",
-        (tmp_dir / "file2").fs_path,
-        callback=callback,
-    )
+    dvc.run(deps=["bar"], outs=["baz"], cmd="echo bar > baz", name="2")
 
-    size = os.path.getsize(tmp_dir / "file")
-    assert (tmp_dir / "file2").read_text() == "file"
-    assert callback.size == size
-    assert callback.value == size
+    with pytest.raises(CyclicGraphError):
+        dvc.run(deps=["baz"], outs=["foo"], cmd="echo baz > foo", name="3")
 
-    callback = fsspec.Callback()
-    fs.download(
-        "dir",
-        (tmp_dir / "dir2").fs_path,
-        callback=callback,
-    )
 
-    assert (tmp_dir / "dir2").read_text() == {"foo": "foo", "bar": "bar"}
-    assert callback.size == 2
-    assert callback.value == 2
+def test_run_dump_on_multistage(tmp_dir, dvc, run_head):
+    from dvc.dvcfile import PROJECT_FILE, load_file
 
-
-def test_callback_on_repo_fs(tmp_dir, dvc, scm):
-    tmp_dir.dvc_gen({"dir": {"bar": "bar"}}, commit="dvc")
-    tmp_dir.scm_gen({"dir": {"foo": "foo"}}, commit="git")
-
-    fs = dvc.repo_fs
-
-    callback = fsspec.Callback()
-    fs.download(
-        (tmp_dir / "dir").fs_path,
-        (tmp_dir / "dir2").fs_path,
-        callback=callback,
+    tmp_dir.gen(
+        {
+            "dir": {
+                "foo": "foo\nfoo",
+                "bar": "bar\nbar",
+                "foobar": "foobar\foobar",
+            }
+        }
     )
 
-    assert (tmp_dir / "dir2").read_text() == {"foo": "foo", "bar": "bar"}
-    assert callback.size == 2
-    assert callback.value == 2
-
-    callback = fsspec.Callback()
-    fs.download(
-        (tmp_dir / "dir" / "foo").fs_path,
-        (tmp_dir / "foo").fs_path,
-        callback=callback,
-    )
+    dvc.run(
+        cmd="cp foo foo2",
+        deps=["foo"],
+        name="copy-foo-foo2",
+        wdir="dir",
+        outs_persist=["foo2"],
+        always_changed=True,
+    )
+    data = load_file(dvc, PROJECT_FILE)._load()[0]
+    assert data == {
+        "stages": {
+            "copy-foo-foo2": {
+                "cmd": "cp foo foo2",
+                "deps": ["foo"],
+                "outs": [{"foo2": {"persist": True}}],
+                "always_changed": True,
+                "wdir": "dir",
+            }
+        }
+    }
 
-    size = os.path.getsize(tmp_dir / "dir" / "foo")
-    assert (tmp_dir / "foo").read_text() == "foo"
-    assert callback.size == size
-    assert callback.value == size
+    run_head(
+        "foo",
+        "bar",
+        "foobar",
+        name="head-files",
+        outs=["bar-1"],
+        outs_persist=["foo-1"],
+        metrics_no_cache=["foobar-1"],
+        wdir="dir",
+    )
+    assert load_file(dvc, PROJECT_FILE)._load()[0] == {
+        "stages": {
+            "head-files": {
+                "cmd": "python {} foo bar foobar".format(
+                    (tmp_dir / "head.py").resolve()
+                ),
+                "wdir": "dir",
+                "deps": ["bar", "foo", "foobar"],
+                "outs": ["bar-1", {"foo-1": {"persist": True}}],
+                "metrics": [{"foobar-1": {"cache": False}}],
+            },
+            **data["stages"],
+        }
+    }
 
-    callback = fsspec.Callback()
-    fs.download(
-        (tmp_dir / "dir" / "bar").fs_path,
-        (tmp_dir / "bar").fs_path,
-        callback=callback,
-    )
 
-    size = os.path.getsize(tmp_dir / "dir" / "bar")
-    assert (tmp_dir / "bar").read_text() == "bar"
-    assert callback.size == size
-    assert callback.value == size
+@pytest.mark.parametrize("char", ["@:", "#", "$", ":", "/", "\\", ".", ";", ","])
+def test_run_with_invalid_stage_name(run_copy, char):
+    with pytest.raises(InvalidStageName):
+        run_copy("foo", "bar", name=f"copy_name-{char}")
+
+
+def test_run_with_name_having_hyphen_underscore(tmp_dir, dvc, run_copy):
+    tmp_dir.dvc_gen("foo", "foo")
+    run_copy("foo", "bar", name="copy-foo_bar")
+
+
+def test_run_already_exists(tmp_dir, dvc, run_copy):
+    tmp_dir.dvc_gen("foo", "foo")
+    run_copy("foo", "bar", name="copy")
+    with pytest.raises(DuplicateStageName):
+        run_copy("bar", "foobar", name="copy", force=False)
+    run_copy("bar", "foobar", name="copy", force=True)
+
+
+supported_params = {
+    "name": "Answer",
+    "answer": 42,
+    "floats": 42.0,
+    "lists": [42, 42.0, "42"],
+    "nested": {"nested1": {"nested2": "42", "nested2-2": 41.99999}},
+}
+
+
+def test_run_params_default(tmp_dir, dvc):
+    from dvc.dependency import ParamsDependency
+
+    (tmp_dir / "params.yaml").dump(supported_params)
+    stage = dvc.run(
+        name="read_params",
+        params=["nested.nested1.nested2"],
+        cmd="cat params.yaml",
+    )
+    assert isinstance(stage.deps[0], ParamsDependency)
+    assert stage.deps[0].params == ["nested.nested1.nested2"]
+
+    lockfile = stage.dvcfile._lockfile
+    assert lockfile.load()["stages"]["read_params"]["params"] == {
+        "params.yaml": {"nested.nested1.nested2": "42"}
+    }
+
+    data, _ = stage.dvcfile._load()
+    assert data["stages"]["read_params"]["params"] == ["nested.nested1.nested2"]
+
+
+def test_run_params_custom_file(tmp_dir, dvc):
+    from dvc.dependency import ParamsDependency
+
+    (tmp_dir / "params2.yaml").dump(supported_params)
+    stage = dvc.run(
+        name="read_params",
+        params=["params2.yaml:lists"],
+        cmd="cat params2.yaml",
+    )
+
+    isinstance(stage.deps[0], ParamsDependency)
+    assert stage.deps[0].params == ["lists"]
+    lockfile = stage.dvcfile._lockfile
+    assert lockfile.load()["stages"]["read_params"]["params"] == {
+        "params2.yaml": {"lists": [42, 42.0, "42"]}
+    }
+
+    data, _ = stage.dvcfile._load()
+    assert data["stages"]["read_params"]["params"] == [{"params2.yaml": ["lists"]}]
+
+
+def test_run_params_no_exec(tmp_dir, dvc):
+    from dvc.dependency import ParamsDependency
+
+    (tmp_dir / "params2.yaml").dump(supported_params)
+    stage = dvc.run(
+        name="read_params",
+        params=["params2.yaml:lists"],
+        cmd="cat params2.yaml",
+        no_exec=True,
+    )
+
+    isinstance(stage.deps[0], ParamsDependency)
+    assert stage.deps[0].params == ["lists"]
+    assert not stage.dvcfile._lockfile.exists()
+
+    data, _ = stage.dvcfile._load()
+    assert data["stages"]["read_params"]["params"] == [{"params2.yaml": ["lists"]}]
+
+
+@pytest.mark.parametrize(
+    "kwargs",
+    [
+        {"outs": ["foo"], "deps": ["bar"]},
+        {"outs": ["foo"], "deps": ["bar"], "name": "copy-foo-bar"},
+    ],
+)
+def test_run_without_cmd(tmp_dir, dvc, kwargs):
+    with pytest.raises(InvalidArgumentError) as exc:
+        dvc.run(**kwargs)
+    assert str(exc.value) == "command is not specified"
+
+
+def test_run_overwrite_order(tmp_dir, dvc, run_copy):
+    from dvc.dvcfile import PROJECT_FILE
+
+    tmp_dir.gen({"foo": "foo", "foo1": "foo1"})
+    run_copy("foo", "bar", name="copy-foo-bar")
+    run_copy("bar", "foobar", name="copy-bar-foobar")
+
+    run_copy("foo1", "bar1", name="copy-foo-bar", force=True)
+
+    data = (tmp_dir / PROJECT_FILE).parse()
+    assert list(data["stages"].keys()) == ["copy-foo-bar", "copy-bar-foobar"]
+
+
+def test_run_overwrite_preserves_meta_and_comment(tmp_dir, dvc, run_copy):
+    from dvc.dvcfile import PROJECT_FILE
+
+    tmp_dir.gen({"foo": "foo", "foo1": "foo1"})
+    text = textwrap.dedent(
+        """\
+        stages:
+          copy-foo-bar:
+            cmd: python copy.py {src} {dest}
+            deps:
+            - copy.py
+            - {src}
+            outs:
+            # comments are preserved
+            - {dest}
+            meta:
+              name: meta is preserved too
+    """
+    )
+    (tmp_dir / PROJECT_FILE).write_text(text.format(src="foo", dest="bar"))
+    assert dvc.reproduce(PROJECT_FILE)
+
+    assert run_copy("foo1", "bar1", name="copy-foo-bar", force=True)
+
+    assert (tmp_dir / PROJECT_FILE).read_text() == text.format(src="foo1", dest="bar1")
+
+
+def test_run_external_outputs(
+    tmp_dir,
+    dvc,
+    local_workspace,
+):
+    hash_name = "md5"
+    foo_hash = "acbd18db4cc2f85cedef654fccc4a4d8"
+    bar_hash = "37b51d194a7513e45b56f6524f2d51f2"
+
+    local_workspace.gen("foo", "foo")
+    dvc.run(
+        name="mystage",
+        cmd="mycmd",
+        deps=["remote://workspace/foo"],
+        outs=["remote://workspace/bar"],
+        no_exec=True,
+    )
+
+    dvc_yaml = (
+        "stages:\n"
+        "  mystage:\n"
+        "    cmd: mycmd\n"
+        "    deps:\n"
+        "    - remote://workspace/foo\n"
+        "    outs:\n"
+        "    - remote://workspace/bar\n"
+    )
+
+    assert (tmp_dir / "dvc.yaml").read_text() == dvc_yaml
+    assert not (tmp_dir / "dvc.lock").exists()
+
+    local_workspace.gen("bar", "bar")
+    dvc.commit("dvc.yaml", force=True)
+
+    assert (tmp_dir / "dvc.yaml").read_text() == dvc_yaml
+    assert (tmp_dir / "dvc.lock").read_text() == (
+        "schema: '2.0'\n"
+        "stages:\n"
+        "  mystage:\n"
+        "    cmd: mycmd\n"
+        "    deps:\n"
+        "    - path: remote://workspace/foo\n"
+        f"      {hash_name}: {foo_hash}\n"
+        "      size: 3\n"
+        "    outs:\n"
+        "    - path: remote://workspace/bar\n"
+        f"      {hash_name}: {bar_hash}\n"
+        "      size: 3\n"
+    )
+
+    assert (local_workspace / "foo").read_text() == "foo"
+    assert (local_workspace / "bar").read_text() == "bar"
+    assert (
+        local_workspace / "cache" / bar_hash[:2] / bar_hash[2:]
+    ).read_text() == "bar"
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `dvc-2.9.5/tests/func/test_get.py` & `dvc-3.0.0a0/tests/func/test_get.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,45 +1,62 @@
+import errno
 import logging
 import os
 
 import pytest
 
+from dvc.cachemgr import CacheManager
 from dvc.cli import main
-from dvc.data.db import ODBManager
+from dvc.exceptions import FileExistsLocallyError
+from dvc.fs import system
 from dvc.repo import Repo
 from dvc.repo.get import GetDVCFileError
-from dvc.system import System
-from dvc.utils.fs import makedirs
-from tests.unit.fs.test_repo import make_subrepo
+from dvc.testing.tmp_dir import make_subrepo
 
 
 def test_get_repo_file(tmp_dir, erepo_dir):
     with erepo_dir.chdir():
         erepo_dir.dvc_gen("file", "contents", commit="create file")
 
     Repo.get(os.fspath(erepo_dir), "file", "file_imported")
 
     assert os.path.isfile("file_imported")
     assert (tmp_dir / "file_imported").read_text() == "contents"
 
 
-def test_get_repo_file_replace_without_confirmation(tmp_dir, erepo_dir):
+def test_get_repo_file_no_override(tmp_dir, erepo_dir):
     with erepo_dir.chdir():
-        erepo_dir.dvc_gen("file", "contents", commit="create file")
-        erepo_dir.dvc_gen(
-            "file2", "something different", commit="create file2"
-        )
+        erepo_dir.dvc_gen("file1", "file1 contents", commit="create file")
+        erepo_dir.dvc_gen("file2", "file2 contents", commit="create file2")
 
-    Repo.get(os.fspath(erepo_dir), "file", "file_imported")
+    Repo.get(os.fspath(erepo_dir), "file1", "file_imported")
     # getting another file with a name that already exists in Repo.
-    with pytest.raises(FileExistsError):
+    with pytest.raises(FileExistsLocallyError) as exc_info:
         Repo.get(os.fspath(erepo_dir), "file2", "file_imported")
 
+    # Make sure it's a functional FileExistsError with errno
+    assert isinstance(exc_info.value, FileExistsError)
+    assert exc_info.value.errno == errno.EEXIST
+
     assert os.path.isfile("file_imported")
-    assert (tmp_dir / "file_imported").read_text() == "contents"
+    assert (tmp_dir / "file_imported").read_text() == "file1 contents"
+
+
+def test_get_repo_file_with_override(tmp_dir, erepo_dir):
+    with erepo_dir.chdir():
+        erepo_dir.dvc_gen("file1", "file1 contents", commit="create file")
+        erepo_dir.dvc_gen("file2", "file2 contents", commit="create file2")
+
+    Repo.get(os.fspath(erepo_dir), "file1", "file_imported")
+
+    # override with the 2nd file
+    Repo.get(os.fspath(erepo_dir), "file2", "file_imported", force=True)
+
+    assert os.path.isfile("file_imported")
+    assert (tmp_dir / "file_imported").read_text() == "file2 contents"
 
 
 def test_get_repo_dir(tmp_dir, erepo_dir):
     with erepo_dir.chdir():
         erepo_dir.dvc_gen({"dir": {"file": "contents"}}, commit="create dir")
 
     Repo.get(os.fspath(erepo_dir), "dir", "dir_imported")
@@ -64,37 +81,35 @@
 @pytest.mark.parametrize(
     "erepo", [pytest.lazy_fixture("git_dir"), pytest.lazy_fixture("erepo_dir")]
 )
 def test_get_git_dir(tmp_dir, erepo):
     src = "some_directory"
     dst = "some_directory_imported"
 
-    erepo.scm_gen(
-        {src: {"dir": {"file.txt": "hello"}}}, commit="add a regular dir"
-    )
+    erepo.scm_gen({src: {"dir": {"file.txt": "hello"}}}, commit="add a regular dir")
 
     Repo.get(os.fspath(erepo), src, dst)
 
     assert (tmp_dir / dst).read_text() == {"dir": {"file.txt": "hello"}}
 
 
 def test_cache_type_is_properly_overridden(tmp_dir, erepo_dir):
     with erepo_dir.chdir():
         with erepo_dir.dvc.config.edit() as conf:
             conf["cache"]["type"] = "symlink"
-        erepo_dir.dvc.odb = ODBManager(erepo_dir.dvc)
+        erepo_dir.dvc.cache = CacheManager(erepo_dir.dvc)
         erepo_dir.scm_add(
             [erepo_dir.dvc.config.files["repo"]], "set cache type to symlinks"
         )
         erepo_dir.dvc_gen("file", "contents", "create file")
-    assert System.is_symlink(erepo_dir / "file")
+    assert system.is_symlink(erepo_dir / "file")
 
     Repo.get(os.fspath(erepo_dir), "file", "file_imported")
 
-    assert not System.is_symlink("file_imported")
+    assert not system.is_symlink("file_imported")
     assert (tmp_dir / "file_imported").read_text() == "contents"
 
 
 def test_get_repo_rev(tmp_dir, erepo_dir):
     with erepo_dir.chdir(), erepo_dir.branch("branch", new=True):
         erepo_dir.dvc_gen("file", "contents", commit="create file on branch")
 
@@ -120,17 +135,15 @@
     external_data = path / "ext_data"
     external_data.write_text("ext_data")
 
     with erepo_dir.chdir():
         erepo_dir.dvc.add(os.fspath(external_data), external=True)
         erepo_dir.scm_add("ext_data.dvc", commit="add external data")
 
-    Repo.get(
-        os.fspath(erepo_dir), os.fspath(external_data), "ext_data_imported"
-    )
+    Repo.get(os.fspath(erepo_dir), os.fspath(external_data), "ext_data_imported")
     assert (tmp_dir / "ext_data_imported").read_text() == "ext_data"
 
 
 def test_non_cached_output(tmp_dir, erepo_dir):
     src = "non_cached_file"
     dst = src + "_imported"
 
@@ -166,15 +179,15 @@
 
 
 @pytest.mark.parametrize("dname", [".", "dir", "dir/subdir"])
 def test_get_to_dir(tmp_dir, erepo_dir, dname):
     with erepo_dir.chdir():
         erepo_dir.dvc_gen("file", "contents", commit="create file")
 
-    makedirs(dname, exist_ok=True)
+    os.makedirs(dname, exist_ok=True)
 
     Repo.get(os.fspath(erepo_dir), "file", dname)
 
     assert (tmp_dir / dname).is_dir()
     assert (tmp_dir / dname / "file").read_text() == "contents"
 
 
@@ -207,70 +220,64 @@
     Repo.get(os.fspath(erepo_dir), os.path.join("dir", "2"), out="file")
     assert (tmp_dir / "file").read_text() == "2"
 
     Repo.get(os.fspath(erepo_dir), os.path.join("dir", "subdir"))
     assert (tmp_dir / "subdir" / "foo").read_text() == "foo"
     assert (tmp_dir / "subdir" / "bar").read_text() == "bar"
 
-    Repo.get(
-        os.fspath(erepo_dir), os.path.join("dir", "subdir", "foo"), out="X"
-    )
+    Repo.get(os.fspath(erepo_dir), os.path.join("dir", "subdir", "foo"), out="X")
     assert (tmp_dir / "X").read_text() == "foo"
 
 
 def test_get_url_positive(tmp_dir, erepo_dir, caplog, local_cloud):
     erepo_dir.add_remote(config=local_cloud.config)
     with erepo_dir.chdir():
         erepo_dir.dvc_gen("foo", "foo")
     erepo_dir.dvc.push()
 
     caplog.clear()
     with caplog.at_level(logging.ERROR, logger="dvc"):
         assert main(["get", os.fspath(erepo_dir), "foo", "--show-url"]) == 0
-        assert caplog.text == ""
+        assert not caplog.text
 
 
 def test_get_url_not_existing(tmp_dir, erepo_dir, caplog):
     with caplog.at_level(logging.ERROR, logger="dvc"):
         assert (
             main(
                 [
                     "get",
                     os.fspath(erepo_dir),
                     "not-existing-file",
                     "--show-url",
                 ]
             )
-            == 1
+            != 0
         )
-        assert "failed to show URL" in caplog.text
 
 
 def test_get_url_git_only_repo(tmp_dir, scm, caplog):
     tmp_dir.scm_gen({"foo": "foo"}, commit="initial")
 
     with caplog.at_level(logging.ERROR):
-        assert main(["get", os.fspath(tmp_dir), "foo", "--show-url"]) == 1
-        assert "failed to show URL" in caplog.text
+        assert main(["get", os.fspath(tmp_dir), "foo", "--show-url"]) != 0
 
 
-def test_get_pipeline_tracked_outs(
-    tmp_dir, dvc, scm, git_dir, run_copy, local_remote
-):
-    from dvc.dvcfile import PIPELINE_FILE, PIPELINE_LOCK
+def test_get_pipeline_tracked_outs(tmp_dir, dvc, scm, git_dir, run_copy, local_remote):
+    from dvc.dvcfile import LOCK_FILE, PROJECT_FILE
 
     tmp_dir.gen("foo", "foo")
     run_copy("foo", "bar", name="copy-foo-bar")
     dvc.push()
 
-    dvc.scm.add([PIPELINE_FILE, PIPELINE_LOCK])
+    dvc.scm.add([PROJECT_FILE, LOCK_FILE])
     dvc.scm.commit("add pipeline stage")
 
     with git_dir.chdir():
-        Repo.get(f"file:///{os.fspath(tmp_dir)}", "bar", out="baz")
+        Repo.get(f"file://{tmp_dir.as_posix()}", "bar", out="baz")
         assert (git_dir / "baz").read_text() == "foo"
 
 
 def test_get_mixed_dir(tmp_dir, erepo_dir):
     with erepo_dir.chdir():
         erepo_dir.dvc_gen(os.path.join("dir", "foo"), "foo", commit="foo")
         erepo_dir.scm_gen(os.path.join("dir", "bar"), "bar", commit="bar")
```

### Comparing `dvc-2.9.5/tests/func/test_ignore.py` & `dvc-3.0.0a0/tests/func/test_ignore.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,97 +1,148 @@
 import os
 import shutil
 from pathlib import Path
+from typing import List
 
 import pytest
 
-from dvc.exceptions import DvcIgnoreInCollectedDirError
 from dvc.ignore import DvcIgnore, DvcIgnorePatterns
 from dvc.output import OutputIsIgnoredError
 from dvc.pathspec_math import PatternInfo, merge_patterns
 from dvc.repo import Repo
 from dvc.testing.tmp_dir import TmpDir
-from dvc.types import List
-from dvc.utils.fs import get_mtime_and_size
+from dvc_data.hashfile.build import IgnoreInCollectedDirError
+from dvc_data.hashfile.utils import get_mtime_and_size
 
 
 def _to_pattern_info_list(str_list: List):
     return [PatternInfo(a, "") for a in str_list]
 
 
-def walk_files(dvc, *args):
-    for fs_path in dvc.dvcignore.find(*args):
-        yield fs_path
-
-
 @pytest.mark.parametrize("filename", ["ignored", "тест"])
 def test_ignore(tmp_dir, dvc, filename):
     tmp_dir.gen({"dir": {filename: filename, "other": "text2"}})
     tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, f"dir/{filename}")
 
     dvc._reset()
 
-    result = walk_files(dvc, dvc.fs, tmp_dir)
+    result = dvc.dvcignore.find(dvc.fs, tmp_dir)
     assert set(result) == {
         (tmp_dir / DvcIgnore.DVCIGNORE_FILE).fs_path,
         (tmp_dir / "dir" / "other").fs_path,
     }
 
 
+def test_walk(tmp_dir, dvc):
+    tmp_dir.gen(
+        {
+            "foo": "foo",
+            "bar": "bar",
+            "dir": {
+                "foo": "foo",
+                "bar": "bar",
+                "baz": "baz",
+                "subdir": {"foo": "foo", "qux": "qux"},
+            },
+        }
+    )
+    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "dir/bar\nfoo")
+
+    dvc._reset()
+
+    result = list(dvc.dvcignore.walk(dvc.fs, tmp_dir))
+    assert result[0][0] == str(tmp_dir)
+    assert result[0][1] == ["dir"]
+    assert set(result[0][2]) == {"bar", ".dvcignore"}
+    assert result[1][0] == str(tmp_dir / "dir")
+    assert result[1][1] == ["subdir"]
+    assert result[1][2] == ["baz"]
+    assert result[2][0] == str(tmp_dir / "dir" / "subdir")
+    assert result[2][1] == []
+    assert result[2][2] == ["qux"]
+
+    result = list(dvc.dvcignore.walk(dvc.fs, tmp_dir, detail=True))
+    assert result == [
+        (
+            str(tmp_dir),
+            {"dir": dvc.fs.info(str(tmp_dir / "dir"))},
+            {
+                "bar": dvc.fs.info(str(tmp_dir / "bar")),
+                ".dvcignore": dvc.fs.info(str(tmp_dir / ".dvcignore")),
+            },
+        ),
+        (
+            str(tmp_dir / "dir"),
+            {
+                "subdir": dvc.fs.info(str(tmp_dir / "dir" / "subdir")),
+            },
+            {
+                "baz": dvc.fs.info(str(tmp_dir / "dir" / "baz")),
+            },
+        ),
+        (
+            str(tmp_dir / "dir" / "subdir"),
+            {},
+            {"qux": dvc.fs.info(str(tmp_dir / "dir" / "subdir" / "qux"))},
+        ),
+    ]
+
+
 def test_rename_ignored_file(tmp_dir, dvc):
     tmp_dir.gen({"dir": {"ignored": "...", "other": "text"}})
 
     tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "ignored*")
     dvc._reset()
 
     mtime, size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)
 
     shutil.move("dir/ignored", "dir/ignored_new")
     new_mtime, new_size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)
 
-    assert new_mtime == mtime and new_size == size
+    assert new_mtime == mtime
+    assert new_size == size
 
 
 def test_rename_file(tmp_dir, dvc):
     tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
     mtime, size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)
 
     shutil.move("dir/foo", "dir/foo_new")
     new_mtime, new_size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)
 
-    assert new_mtime != mtime and new_size == size
+    assert new_mtime != mtime
+    assert new_size == size
 
 
 def test_remove_ignored_file(tmp_dir, dvc):
     tmp_dir.gen({"dir": {"ignored": "...", "other": "text"}})
     tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "dir/ignored")
     dvc._reset()
 
     mtime, size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)
 
     os.remove("dir/ignored")
-    new_mtime, new_size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)
-
-    assert new_mtime == mtime and new_size == size
+    assert get_mtime_and_size("dir", dvc.fs, dvc.dvcignore) == (mtime, size)
 
 
 def test_remove_file(tmp_dir, dvc):
     tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
     mtime, size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)
 
     os.remove("dir/foo")
     new_mtime, new_size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)
 
-    assert new_mtime != mtime and new_size != size
+    assert new_mtime != mtime
+    assert new_size != size
 
 
 def test_dvcignore_in_out_dir(tmp_dir, dvc):
     tmp_dir.gen({"dir": {"foo": "foo", DvcIgnore.DVCIGNORE_FILE: ""}})
 
-    with pytest.raises(DvcIgnoreInCollectedDirError):
+    with pytest.raises(IgnoreInCollectedDirError):
         dvc.add("dir")
 
 
 @pytest.mark.parametrize("dname", ["dir", "dir/subdir"])
 def test_ignore_collecting_dvcignores(tmp_dir, dvc, dname):
     tmp_dir.gen({"dir": {"subdir": {}}})
 
@@ -107,97 +158,78 @@
     top_ignore_path = os.path.dirname(os.fspath(top_ignore_file))
 
     sub_dir_path = os.path.dirname(os.fspath(ignore_file))
 
     assert (
         DvcIgnorePatterns(
             *merge_patterns(
+                os.path,
                 _to_pattern_info_list([".hg/", ".git/", ".git", ".dvc/"]),
                 os.fspath(tmp_dir),
                 _to_pattern_info_list([os.path.basename(dname)]),
                 top_ignore_path,
-            )
+            ),
+            os.sep,
         )
         == dvcignore._get_trie_pattern(top_ignore_path)
         == dvcignore._get_trie_pattern(sub_dir_path)
     )
 
 
 def test_ignore_on_branch(tmp_dir, scm, dvc):
-    from dvc.fs.git import GitFileSystem
+    from dvc.fs import GitFileSystem
 
     tmp_dir.scm_gen({"foo": "foo", "bar": "bar"}, commit="add files")
 
     with tmp_dir.branch("branch", new=True):
         tmp_dir.scm_gen(DvcIgnore.DVCIGNORE_FILE, "foo", commit="add ignore")
 
     dvc._reset()
 
-    result = walk_files(dvc, dvc.fs, tmp_dir)
+    result = dvc.dvcignore.find(dvc.fs, tmp_dir)
     assert set(result) == {
         (tmp_dir / "foo").fs_path,
         (tmp_dir / "bar").fs_path,
         (tmp_dir / DvcIgnore.DVCIGNORE_FILE).fs_path,
     }
 
     dvc.fs = GitFileSystem(scm=scm, rev="branch")
-    assert dvc.dvcignore.is_ignored_file(tmp_dir / "foo")
+    dvc.root_dir = "/"
+    assert dvc.dvcignore.is_ignored_file("/foo")
 
 
 def test_match_nested(tmp_dir, dvc):
     tmp_dir.gen(
         {
             ".dvcignore": "*.backup\ntmp",
             "foo": "foo",
             "tmp": "...",
             "dir": {"x.backup": "x backup", "tmp": "content"},
         }
     )
     dvc._reset()
-    result = walk_files(dvc, dvc.fs, tmp_dir)
+    result = dvc.dvcignore.find(dvc.fs, tmp_dir)
     assert set(result) == {
         (tmp_dir / DvcIgnore.DVCIGNORE_FILE).fs_path,
         (tmp_dir / "foo").fs_path,
     }
 
 
 def test_ignore_external(tmp_dir, scm, dvc, tmp_path_factory):
     tmp_dir.gen(".dvcignore", "*.backup\ntmp")
     ext_dir = TmpDir(os.fspath(tmp_path_factory.mktemp("external_dir")))
     ext_dir.gen({"y.backup": "y", "tmp": {"file": "ext tmp"}})
 
-    result = walk_files(dvc, dvc.fs, ext_dir)
+    result = dvc.dvcignore.find(dvc.fs, ext_dir)
     assert set(result) == {
         (ext_dir / "y.backup").fs_path,
         (ext_dir / "tmp" / "file").fs_path,
     }
     assert dvc.dvcignore.is_ignored_dir(os.fspath(ext_dir / "tmp")) is False
-    assert (
-        dvc.dvcignore.is_ignored_file(os.fspath(ext_dir / "y.backup")) is False
-    )
-
-
-def test_ignore_subrepo(tmp_dir, scm, dvc):
-    tmp_dir.gen({".dvcignore": "foo", "subdir": {"foo": "foo"}})
-    scm.add([".dvcignore"])
-    scm.commit("init parent dvcignore")
-    dvc._reset()
-
-    subrepo_dir = tmp_dir / "subdir"
-
-    result = walk_files(dvc, dvc.fs, subrepo_dir)
-    assert set(result) == set()
-
-    with subrepo_dir.chdir():
-        subrepo = Repo.init(subdir=True)
-        scm.add(str(subrepo_dir / "foo"))
-        scm.commit("subrepo init")
-
-    for _ in subrepo.brancher(all_commits=True):
-        assert subrepo.fs.exists(subrepo_dir / "foo")
+    assert dvc.dvcignore.is_ignored_file(os.fspath(ext_dir / "y.backup")) is False
 
 
 def test_ignore_resurface_subrepo(tmp_dir, scm, dvc):
     tmp_dir.dvc_gen({"foo": "foo"}, commit="add foo")
     subrepo_dir = tmp_dir / "subdir"
     subrepo_dir.mkdir()
     with subrepo_dir.chdir():
@@ -221,21 +253,21 @@
     )
 
 
 def test_ignore_blank_line(tmp_dir, dvc):
     tmp_dir.gen({"dir": {"ignored": "text", "other": "text2"}})
     tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "foo\n\ndir/ignored")
     dvc._reset()
-    result = walk_files(dvc, dvc.fs, tmp_dir / "dir")
+    result = dvc.dvcignore.find(dvc.fs, tmp_dir / "dir")
     assert set(result) == {(tmp_dir / "dir" / "other").fs_path}
 
 
 # It is not possible to re-include a file if a parent directory of
 # that file is excluded.
-# Git doesn’t list excluded directories for performance reasons,
+# Git doesn't list excluded directories for performance reasons,
 # so any patterns on contained files have no effect,
 # no matter where they are defined.
 @pytest.mark.parametrize(
     "data_struct, pattern_list, result_set",
     [
         (
             {"dir": {"subdir": {"not_ignore": "121"}}},
@@ -256,18 +288,16 @@
 )
 def test_ignore_file_in_parent_path(
     tmp_dir, dvc, data_struct, pattern_list, result_set
 ):
     tmp_dir.gen(data_struct)
     tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "\n".join(pattern_list))
     dvc._reset()
-    result = walk_files(dvc, dvc.fs, tmp_dir / "dir")
-    assert set(result) == {
-        (tmp_dir / relpath).fs_path for relpath in result_set
-    }
+    result = dvc.dvcignore.find(dvc.fs, tmp_dir / "dir")
+    assert set(result) == {(tmp_dir / relpath).fs_path for relpath in result_set}
 
 
 # If there is a separator at the end of the pattern then the pattern
 # will only match directories,
 # otherwise the pattern can match both files and directories.
 # For example, a pattern doc/frotz/ matches doc/frotz directory,
 # but not a/doc/frotz directory;
@@ -279,38 +309,36 @@
                 "a": {"doc": {"fortz": {"a": "a"}}},
             }
         }
     )
     tmp_dir.gen({"dir": {DvcIgnore.DVCIGNORE_FILE: "doc/fortz"}})
 
     dvc._reset()
-    result = walk_files(dvc, dvc.fs, tmp_dir / "dir")
+    result = dvc.dvcignore.find(dvc.fs, tmp_dir / "dir")
     assert set(result) == {
         (tmp_dir / "dir" / "a" / "doc" / "fortz" / "a").fs_path,
         (tmp_dir / "dir" / DvcIgnore.DVCIGNORE_FILE).fs_path,
     }
 
 
 # however frotz/ matches frotz and a/frotz that is a directory
 def test_ignore_directory(tmp_dir, dvc):
     tmp_dir.gen({"dir": {"fortz": {}, "a": {"fortz": {}}}})
     tmp_dir.gen({"dir": {DvcIgnore.DVCIGNORE_FILE: "fortz"}})
     dvc._reset()
-    result = walk_files(dvc, dvc.fs, tmp_dir / "dir")
-    assert set(result) == {
-        (tmp_dir / "dir" / DvcIgnore.DVCIGNORE_FILE).fs_path
-    }
+    result = dvc.dvcignore.find(dvc.fs, tmp_dir / "dir")
+    assert set(result) == {(tmp_dir / "dir" / DvcIgnore.DVCIGNORE_FILE).fs_path}
 
 
 def test_multi_ignore_file(tmp_dir, dvc, monkeypatch):
     tmp_dir.gen({"dir": {"subdir": {"should_ignore": "1", "not_ignore": "1"}}})
     tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "dir/subdir/*_ignore")
     tmp_dir.gen({"dir": {DvcIgnore.DVCIGNORE_FILE: "!subdir/not_ignore"}})
     dvc._reset()
-    result = walk_files(dvc, dvc.fs, tmp_dir / "dir")
+    result = dvc.dvcignore.find(dvc.fs, tmp_dir / "dir")
     assert set(result) == {
         (tmp_dir / "dir" / "subdir" / "not_ignore").fs_path,
         (tmp_dir / "dir" / DvcIgnore.DVCIGNORE_FILE).fs_path,
     }
 
 
 def test_pattern_trie_fs(tmp_dir, dvc):
@@ -329,20 +357,16 @@
             },
             "other": {DvcIgnore.DVCIGNORE_FILE: "1\n2\n3"},
         }
     )
     dvc._reset()
     dvcignore = dvc.dvcignore
 
-    ignore_pattern_top = dvcignore._get_trie_pattern(
-        os.fspath(tmp_dir / "top")
-    )
-    ignore_pattern_other = dvcignore._get_trie_pattern(
-        os.fspath(tmp_dir / "other")
-    )
+    ignore_pattern_top = dvcignore._get_trie_pattern(os.fspath(tmp_dir / "top"))
+    ignore_pattern_other = dvcignore._get_trie_pattern(os.fspath(tmp_dir / "other"))
     ignore_pattern_first = dvcignore._get_trie_pattern(
         os.fspath(tmp_dir / "top" / "first")
     )
     ignore_pattern_middle = dvcignore._get_trie_pattern(
         os.fspath(tmp_dir / "top" / "first" / "middle")
     )
     ignore_pattern_second = dvcignore._get_trie_pattern(
@@ -353,38 +377,41 @@
     )
 
     base_pattern = (
         _to_pattern_info_list([".hg/", ".git/", ".git", ".dvc/"]),
         os.fspath(tmp_dir),
     )
     first_pattern = merge_patterns(
+        os.path,
         *base_pattern,
         _to_pattern_info_list(["a", "b", "c"]),
         os.fspath(tmp_dir / "top" / "first"),
     )
     second_pattern = merge_patterns(
+        os.path,
         *first_pattern,
         _to_pattern_info_list(["d", "e", "f"]),
         os.fspath(tmp_dir / "top" / "first" / "middle" / "second"),
     )
     other_pattern = merge_patterns(
+        os.path,
         *base_pattern,
         _to_pattern_info_list(["1", "2", "3"]),
         os.fspath(tmp_dir / "other"),
     )
 
-    assert DvcIgnorePatterns(*base_pattern) == ignore_pattern_top
-    assert DvcIgnorePatterns(*other_pattern) == ignore_pattern_other
+    assert DvcIgnorePatterns(*base_pattern, os.sep) == ignore_pattern_top
+    assert DvcIgnorePatterns(*other_pattern, os.sep) == ignore_pattern_other
     assert (
-        DvcIgnorePatterns(*first_pattern)
+        DvcIgnorePatterns(*first_pattern, os.sep)
         == ignore_pattern_first
         == ignore_pattern_middle
     )
     assert (
-        DvcIgnorePatterns(*second_pattern)
+        DvcIgnorePatterns(*second_pattern, os.sep)
         == ignore_pattern_second
         == ignore_pattern_bottom
     )
 
 
 def test_ignore_in_added_dir(tmp_dir, dvc):
     tmp_dir.gen(
@@ -397,15 +424,15 @@
             },
             ".dvcignore": "**/ignored",
         }
     )
     dvc._reset()
 
     ignored_path = tmp_dir / "dir" / "sub" / "ignored"
-    result = walk_files(dvc, dvc.fs, ignored_path)
+    result = dvc.dvcignore.find(dvc.fs, ignored_path)
     assert set(result) == set()
     assert ignored_path.exists()
 
     dvc.add("dir")
     shutil.rmtree(ignored_path)
     dvc.checkout()
 
@@ -429,7 +456,29 @@
     assert Path("copy/foo.log").exists()
 
 
 def test_run_dvcignored_dep(tmp_dir, dvc, run_copy):
     tmp_dir.gen({".dvcignore": "dir\n", "dir": {"foo": "foo"}})
     run_copy(os.path.join("dir", "foo"), "bar", name="copy-foo-to-bar")
     assert (tmp_dir / "bar").read_text() == "foo"
+
+
+def test_pull_ignore(tmp_dir, dvc, local_cloud):
+    tmp_dir.dvc_gen(
+        {
+            ".dvcignore": "data/processed/",
+            "data": {"foo": "foo", "processed": {"bar": "bar"}},
+        }
+    )
+    tmp_dir.add_remote(config=local_cloud.config)
+    dvc.add("data")
+    dvc.push()
+
+    foo_path = tmp_dir / "data" / "foo"
+    foo_path.unlink()
+    assert not foo_path.exists()
+
+    dvc.cache.local.clear()
+    dvc.pull()
+
+    assert foo_path.exists()
+    assert foo_path.read_text() == "foo"
```

### Comparing `dvc-2.9.5/tests/func/test_import.py` & `dvc-3.0.0a0/tests/func/test_import.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,24 +1,21 @@
 import filecmp
 import os
-from unittest.mock import patch
 
 import pytest
 from funcy import first
-from scmrepo.git import Git
 
+from dvc.cachemgr import CacheManager
 from dvc.config import NoRemoteError
-from dvc.data.db import ODBManager
-from dvc.dvcfile import Dvcfile
-from dvc.exceptions import DownloadError, PathMissingError
+from dvc.dvcfile import load_file
+from dvc.fs import system
+from dvc.scm import Git
 from dvc.stage.exceptions import StagePathNotFoundError
-from dvc.system import System
-from dvc.utils.fs import makedirs, remove
-from tests.unit.fs.test_repo import make_subrepo
-from tests.utils import clean_staging
+from dvc.testing.tmp_dir import make_subrepo
+from dvc.utils.fs import remove
 
 
 def test_import(tmp_dir, scm, dvc, erepo_dir):
     with erepo_dir.chdir():
         erepo_dir.dvc_gen("foo", "foo content", commit="create foo")
 
     stage = dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported")
@@ -26,14 +23,15 @@
     assert os.path.isfile("foo_imported")
     assert (tmp_dir / "foo_imported").read_text() == "foo content"
     assert scm.is_ignored("foo_imported")
     assert stage.deps[0].def_repo == {
         "url": os.fspath(erepo_dir),
         "rev_lock": erepo_dir.scm.get_rev(),
     }
+    assert stage.deps[0].fs.repo.cache.local.path == dvc.cache.local.path
 
 
 @pytest.mark.parametrize("src_is_dvc", [True, False])
 def test_import_git_file(tmp_dir, scm, dvc, git_dir, src_is_dvc):
     if src_is_dvc:
         git_dir.init(dvc=True)
 
@@ -45,29 +43,27 @@
     assert tmp_dir.scm.is_ignored(os.fspath(tmp_dir / "dst"))
     assert stage.deps[0].def_repo == {
         "url": os.fspath(git_dir),
         "rev_lock": git_dir.scm.get_rev(),
     }
 
 
-def test_import_cached_file(erepo_dir, tmp_dir, dvc, scm, monkeypatch):
+def test_import_cached_file(mocker, erepo_dir, tmp_dir, dvc, scm, monkeypatch):
     src = "some_file"
     dst = "some_file_imported"
 
     with erepo_dir.chdir():
         erepo_dir.dvc_gen({src: "hello"}, commit="add a regular file")
 
     tmp_dir.dvc_gen({dst: "hello"})
     (tmp_dir / dst).unlink()
 
     remote_exception = NoRemoteError("dvc import")
-    with patch.object(
-        dvc.cloud, "get_remote_odb", side_effect=remote_exception
-    ):
-        tmp_dir.dvc.imp(os.fspath(erepo_dir), src, dst)
+    mocker.patch.object(dvc.cloud, "get_remote_odb", side_effect=remote_exception)
+    tmp_dir.dvc.imp(os.fspath(erepo_dir), src, dst)
 
     assert (tmp_dir / dst).is_file()
     assert filecmp.cmp(erepo_dir / src, tmp_dir / dst, shallow=False)
 
 
 @pytest.mark.parametrize("src_is_dvc", [True, False])
 def test_import_git_dir(tmp_dir, scm, dvc, git_dir, src_is_dvc):
@@ -127,17 +123,15 @@
     assert (tmp_dir / "file.dvc").exists()
 
     dvc.imp(os.fspath(erepo_dir), os.path.join("dir", "subdir"))
     assert (tmp_dir / "subdir" / "foo").read_text() == "foo"
     assert (tmp_dir / "subdir" / "bar").read_text() == "bar"
     assert (tmp_dir / "subdir.dvc").exists()
 
-    dvc.imp(
-        os.fspath(erepo_dir), os.path.join("dir", "subdir", "foo"), out="X"
-    )
+    dvc.imp(os.fspath(erepo_dir), os.path.join("dir", "subdir", "foo"), out="X")
     assert (tmp_dir / "X").read_text() == "foo"
     assert (tmp_dir / "X.dvc").exists()
 
 
 def test_import_file_from_dir_to_dir(tmp_dir, scm, dvc, erepo_dir):
     with erepo_dir.chdir():
         erepo_dir.dvc_gen({"dir": {"foo": "foo"}}, commit="create dir")
@@ -165,17 +159,15 @@
     dst = src + "_imported"
 
     with erepo_dir.chdir():
         erepo_dir.dvc.run(
             cmd=f"echo hello > {src}", outs_no_cache=[src], single_stage=True
         )
 
-    erepo_dir.scm_add(
-        [os.fspath(erepo_dir / src)], commit="add a non-cached out"
-    )
+    erepo_dir.scm_add([os.fspath(erepo_dir / src)], commit="add a non-cached out")
 
     stage = tmp_dir.dvc.imp(os.fspath(erepo_dir), src, dst)
 
     assert (tmp_dir / dst).is_file()
     assert filecmp.cmp(erepo_dir / src, tmp_dir / dst, shallow=False)
     assert tmp_dir.scm.is_ignored(dst)
     assert stage.deps[0].def_repo == {
@@ -202,68 +194,126 @@
 
 
 def test_pull_imported_stage(tmp_dir, dvc, erepo_dir):
     with erepo_dir.chdir():
         erepo_dir.dvc_gen("foo", "foo content", commit="create foo")
     dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported")
 
-    dst_stage = Dvcfile(dvc, "foo_imported.dvc").stage
+    dst_stage = load_file(dvc, "foo_imported.dvc").stage
     dst_cache = dst_stage.outs[0].cache_path
 
     remove("foo_imported")
     remove(dst_cache)
     dvc.pull(["foo_imported.dvc"])
 
     assert os.path.isfile("foo_imported")
     assert os.path.isfile(dst_cache)
 
 
+def test_import_no_download(tmp_dir, scm, dvc, erepo_dir):
+    with erepo_dir.chdir():
+        erepo_dir.dvc_gen("foo", "foo content", commit="create foo")
+
+    dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported", no_download=True)
+
+    assert not os.path.exists("foo_imported")
+
+    dst_stage = load_file(dvc, "foo_imported.dvc").stage
+
+    assert dst_stage.deps[0].def_repo == {
+        "url": os.fspath(erepo_dir),
+        "rev_lock": erepo_dir.scm.get_rev(),
+    }
+    assert scm.is_ignored("foo_imported")
+
+
+def test_pull_import_no_download(tmp_dir, scm, dvc, erepo_dir):
+    with erepo_dir.chdir():
+        erepo_dir.scm_gen(os.path.join("foo", "bar"), b"bar", commit="add bar")
+        erepo_dir.dvc_gen(os.path.join("foo", "baz"), b"baz contents", commit="add baz")
+        size = (
+            len(b"bar")
+            + len(b"baz contents")
+            + len((erepo_dir / "foo" / ".gitignore").read_bytes())
+        )
+
+    dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported", no_download=True)
+
+    dvc.pull(["foo_imported.dvc"])
+    assert (tmp_dir / "foo_imported").exists
+    assert (tmp_dir / "foo_imported" / "bar").read_bytes() == b"bar"
+    assert (tmp_dir / "foo_imported" / "baz").read_bytes() == b"baz contents"
+
+    stage = load_file(dvc, "foo_imported.dvc").stage
+
+    assert stage.outs[0].hash_info.value == "bdb8641831d8fcb03939637e09011c21.dir"
+
+    assert stage.outs[0].meta.size == size
+    assert stage.outs[0].meta.nfiles == 3
+    assert stage.outs[0].meta.isdir
+
+
+def test_pull_import_no_download_rev_lock(
+    tmp_dir,
+    dvc,
+    erepo_dir,
+):
+    with erepo_dir.chdir():
+        erepo_dir.dvc_gen("foo", "foo content", commit="add")
+
+    dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported", no_download=True)
+
+    with erepo_dir.chdir():
+        erepo_dir.dvc_gen("foo", "modified foo content", commit="modify foo")
+
+    dvc.pull(["foo_imported.dvc"])
+    assert (tmp_dir / "foo_imported").read_text() == "foo content"
+
+
 def test_cache_type_is_properly_overridden(tmp_dir, scm, dvc, erepo_dir):
     with erepo_dir.chdir():
         with erepo_dir.dvc.config.edit() as conf:
             conf["cache"]["type"] = "symlink"
-        erepo_dir.dvc.odb = ODBManager(erepo_dir.dvc)
+        erepo_dir.dvc.cache = CacheManager(erepo_dir.dvc)
         erepo_dir.scm_add(
             [erepo_dir.dvc.config.files["repo"]],
             "set source repo cache type to symlink",
         )
         erepo_dir.dvc_gen("foo", "foo content", "create foo")
-    assert System.is_symlink(erepo_dir / "foo")
+    assert system.is_symlink(erepo_dir / "foo")
 
     dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported")
 
-    assert not System.is_symlink("foo_imported")
+    assert not system.is_symlink("foo_imported")
     assert (tmp_dir / "foo_imported").read_text() == "foo content"
     assert scm.is_ignored("foo_imported")
 
 
 def test_pull_imported_directory_stage(tmp_dir, dvc, erepo_dir):
     with erepo_dir.chdir():
         erepo_dir.dvc_gen({"dir": {"foo": "foo content"}}, commit="create dir")
 
     dvc.imp(os.fspath(erepo_dir), "dir", "dir_imported")
 
     remove("dir_imported")
-    remove(dvc.odb.local.cache_dir)
+    dvc.cache.local.clear()
 
     dvc.pull(["dir_imported.dvc"])
 
     assert (tmp_dir / "dir_imported").read_text() == {"foo": "foo content"}
 
 
 def test_pull_wildcard_imported_directory_stage(tmp_dir, dvc, erepo_dir):
     with erepo_dir.chdir():
-        erepo_dir.dvc_gen(
-            {"dir123": {"foo": "foo content"}}, commit="create dir"
-        )
+        erepo_dir.dvc_gen({"dir123": {"foo": "foo content"}}, commit="create dir")
 
     dvc.imp(os.fspath(erepo_dir), "dir123", "dir_imported123")
 
     remove("dir_imported123")
-    remove(dvc.odb.local.cache_dir)
+    dvc.cache.local.clear()
 
     dvc.pull(["dir_imported*.dvc"], glob=True)
 
     assert (tmp_dir / "dir_imported123").read_text() == {"foo": "foo content"}
 
 
 def test_push_wildcard_from_bare_git_repo(
@@ -276,50 +326,29 @@
         erepo_dir.dvc_gen(
             {
                 "dir123": {"foo": "foo content"},
                 "dirextra": {"extrafoo": "extra foo content"},
             },
             commit="initial",
         )
-    erepo_dir.dvc.push(
-        [os.path.join(os.fspath(erepo_dir), "dire*")], glob=True
-    )
+    erepo_dir.dvc.push([os.path.join(os.fspath(erepo_dir), "dire*")], glob=True)
 
     erepo_dir.scm.gitpython.repo.create_remote("origin", os.fspath(tmp_dir))
     erepo_dir.scm.gitpython.repo.remote("origin").push("master")
 
     dvc_repo = make_tmp_dir("dvc-repo", scm=True, dvc=True)
     with dvc_repo.chdir():
         dvc_repo.dvc.imp(os.fspath(tmp_dir), "dirextra")
-        clean_staging()
-
-        with pytest.raises(PathMissingError):
-            dvc_repo.dvc.imp(os.fspath(tmp_dir), "dir123")
-
 
-def test_download_error_pulling_imported_stage(tmp_dir, dvc, erepo_dir):
-    with erepo_dir.chdir():
-        erepo_dir.dvc_gen("foo", "foo content", commit="create foo")
-    dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported")
-
-    dst_stage = Dvcfile(dvc, "foo_imported.dvc").stage
-    dst_cache = dst_stage.outs[0].cache_path
-
-    remove("foo_imported")
-    remove(dst_cache)
-
-    with patch("dvc.fs.utils.transfer", side_effect=Exception), pytest.raises(
-        DownloadError
-    ):
-        dvc.pull(["foo_imported.dvc"])
+        dvc_repo.dvc.imp(os.fspath(tmp_dir), "dir123")
 
 
 @pytest.mark.parametrize("dname", [".", "dir", "dir/subdir"])
 def test_import_to_dir(dname, tmp_dir, dvc, erepo_dir):
-    makedirs(dname, exist_ok=True)
+    os.makedirs(dname, exist_ok=True)
 
     with erepo_dir.chdir():
         erepo_dir.dvc_gen("foo", "foo content", commit="create foo")
 
     stage = dvc.imp(os.fspath(erepo_dir), "foo", dname)
 
     dst = os.path.join(dname, "foo")
@@ -346,44 +375,42 @@
 
     remove(stage.outs[0].cache_path)
     dvc.fetch(all_tags=True)
     assert os.path.exists(stage.outs[0].cache_path)
 
 
 def test_import_non_existing(erepo_dir, tmp_dir, dvc):
-    with pytest.raises(PathMissingError):
+    with pytest.raises(FileNotFoundError):
         tmp_dir.dvc.imp(os.fspath(erepo_dir), "invalid_output")
 
     # https://github.com/iterative/dvc/pull/2837#discussion_r352123053
-    with pytest.raises(PathMissingError):
+    with pytest.raises(FileNotFoundError):
         tmp_dir.dvc.imp(os.fspath(erepo_dir), "/root/", "root")
 
 
 def test_pull_no_rev_lock(erepo_dir, tmp_dir, dvc):
     with erepo_dir.chdir():
         erepo_dir.dvc_gen("foo", "contents", commit="create foo")
 
     stage = dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported")
     assert "rev" not in stage.deps[0].def_repo
     stage.deps[0].def_repo.pop("rev_lock")
 
-    Dvcfile(dvc, stage.path).dump(stage)
+    load_file(dvc, stage.path).dump(stage)
 
     remove(stage.outs[0].cache_path)
     (tmp_dir / "foo_imported").unlink()
 
     dvc.pull([stage.path])
 
     assert (tmp_dir / "foo_imported").is_file()
     assert (tmp_dir / "foo_imported").read_text() == "contents"
 
 
-def test_import_from_bare_git_repo(
-    tmp_dir, make_tmp_dir, erepo_dir, local_cloud
-):
+def test_import_from_bare_git_repo(tmp_dir, make_tmp_dir, erepo_dir, local_cloud):
     Git.init(tmp_dir.fs_path, bare=True).close()
 
     erepo_dir.add_remote(config=local_cloud.config)
     with erepo_dir.chdir():
         erepo_dir.dvc_gen({"foo": "foo"}, commit="initial")
     erepo_dir.dvc.push()
 
@@ -394,25 +421,25 @@
     with dvc_repo.chdir():
         dvc_repo.dvc.imp(os.fspath(tmp_dir), "foo")
 
 
 def test_import_pipeline_tracked_outs(
     tmp_dir, dvc, scm, erepo_dir, run_copy, local_remote
 ):
-    from dvc.dvcfile import PIPELINE_FILE, PIPELINE_LOCK
+    from dvc.dvcfile import LOCK_FILE, PROJECT_FILE
 
     tmp_dir.gen("foo", "foo")
     run_copy("foo", "bar", name="copy-foo-bar")
     dvc.push()
 
-    dvc.scm.add([PIPELINE_FILE, PIPELINE_LOCK])
+    dvc.scm.add([PROJECT_FILE, LOCK_FILE])
     dvc.scm.commit("add pipeline stage")
 
     with erepo_dir.chdir():
-        erepo_dir.dvc.imp(f"file:///{os.fspath(tmp_dir)}", "bar", out="baz")
+        erepo_dir.dvc.imp(f"file://{tmp_dir.as_posix()}", "bar", out="baz")
         assert (erepo_dir / "baz").read_text() == "foo"
 
 
 def test_local_import(tmp_dir, dvc, scm):
     tmp_dir.dvc_gen("foo", "foo", commit="init")
     (tmp_dir / "outdir").mkdir()
     dvc.imp(".", "foo", out="outdir")
@@ -467,31 +494,28 @@
         "url": os.fspath(erepo_dir),
         "rev_lock": erepo_dir.scm.get_rev(),
     }
 
 
 @pytest.mark.parametrize("is_dvc", [True, False])
 @pytest.mark.parametrize("files", [{"foo": "foo"}, {"dir": {"bar": "bar"}}])
-def test_pull_imported_stage_from_subrepos(
-    tmp_dir, dvc, erepo_dir, is_dvc, files
-):
+def test_pull_imported_stage_from_subrepos(tmp_dir, dvc, erepo_dir, is_dvc, files):
     subrepo = erepo_dir / "subrepo"
     make_subrepo(subrepo, erepo_dir.scm)
     gen = subrepo.dvc_gen if is_dvc else subrepo.scm_gen
     with subrepo.chdir():
         gen(files, commit="files in subrepo")
 
     key = first(files)
     path = os.path.join("subrepo", key)
     dvc.imp(os.fspath(erepo_dir), path, out="out")
 
     # clean everything
-    remove(dvc.odb.local.cache_dir)
+    dvc.cache.local.clear()
     remove("out")
-    makedirs(dvc.odb.local.cache_dir)
 
     stats = dvc.pull(["out.dvc"])
 
     expected = [f"out{os.sep}"] if isinstance(files[key], dict) else ["out"]
     assert stats["added"] == expected
     assert (tmp_dir / "out").read_text() == files[key]
 
@@ -525,15 +549,15 @@
     dvc.imp(os.fspath(erepo_dir), "foo", out="foo_imported", no_exec=True)
 
     dst = tmp_dir / "foo_imported"
     assert not dst.exists()
 
 
 def test_import_with_jobs(mocker, dvc, erepo_dir):
-    import dvc.data.transfer as otransfer
+    import dvc_data.hashfile.transfer as otransfer
 
     with erepo_dir.chdir():
         erepo_dir.dvc_gen(
             {
                 "dir1": {
                     "file1": "file1",
                     "file2": "file2",
@@ -552,51 +576,63 @@
 
 
 def test_chained_import(tmp_dir, dvc, make_tmp_dir, erepo_dir, local_cloud):
     erepo_dir.add_remote(config=local_cloud.config)
     with erepo_dir.chdir():
         erepo_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}}, commit="init")
     erepo_dir.dvc.push()
-    remove(erepo_dir.dvc.odb.local.cache_dir)
+    remove(erepo_dir.dvc.cache.local.path)
     remove(os.fspath(erepo_dir / "dir"))
 
     erepo2 = make_tmp_dir("erepo2", scm=True, dvc=True)
     with erepo2.chdir():
         erepo2.dvc.imp(os.fspath(erepo_dir), "dir")
         erepo2.scm.add("dir.dvc")
         erepo2.scm.commit("import")
-    remove(erepo2.dvc.odb.local.cache_dir)
+    remove(erepo2.dvc.cache.local.path)
     remove(os.fspath(erepo2 / "dir"))
 
     dvc.imp(os.fspath(erepo2), "dir", "dir_imported")
     dst = tmp_dir / "dir_imported"
     assert (dst / "foo").read_text() == "foo"
     assert (dst / "bar").read_text() == "bar"
 
-    remove(dvc.odb.local.cache_dir)
+    dvc.cache.local.clear()
     remove("dir_imported")
 
     # pulled objects should come from the original upstream repo's remote,
     # no cache or remote should be needed from the intermediate repo
     dvc.pull(["dir_imported.dvc"])
-    assert not os.path.exists(erepo_dir.dvc.odb.local.cache_dir)
-    assert not os.path.exists(erepo2.dvc.odb.local.cache_dir)
+    assert not os.path.exists(erepo_dir.dvc.cache.local.path)
+    assert not os.path.exists(erepo2.dvc.cache.local.path)
     assert (dst / "foo").read_text() == "foo"
     assert (dst / "bar").read_text() == "bar"
 
 
-def test_circular_import(tmp_dir, dvc, scm, erepo_dir):
-    from dvc.exceptions import CircularImportError
-
-    with erepo_dir.chdir():
-        erepo_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}}, commit="init")
+@pytest.mark.parametrize("paths", ([], ["dir"]))
+def test_parameterized_repo(tmp_dir, dvc, scm, erepo_dir, paths):
+    path = erepo_dir.joinpath(*paths)
+    path.mkdir(parents=True, exist_ok=True)
+    (path / "params.yaml").dump({"out": "foo"})
+    (path / "dvc.yaml").dump(
+        {
+            "stages": {
+                "train": {"cmd": "echo ${out} > ${out}", "outs": ["${out}"]},
+            }
+        }
+    )
+    path.gen({"foo": "foo"})
+    with path.chdir():
+        erepo_dir.dvc.commit(None, force=True)
+        erepo_dir.scm.add_commit(
+            ["params.yaml", "dvc.yaml", "dvc.lock", ".gitignore"],
+            message="init",
+        )
 
-    dvc.imp(os.fspath(erepo_dir), "dir", "dir_imported")
-    scm.add("dir_imported.dvc")
-    scm.commit("import")
-    clean_staging()
+    to_import = os.path.join(*paths, "foo")
+    stage = dvc.imp(os.fspath(erepo_dir), to_import, "foo_imported")
 
-    with erepo_dir.chdir():
-        with pytest.raises(CircularImportError):
-            erepo_dir.dvc.imp(
-                os.fspath(tmp_dir), "dir_imported", "circular_import"
-            )
+    assert (tmp_dir / "foo_imported").read_text() == "foo"
+    assert stage.deps[0].def_repo == {
+        "url": os.fspath(erepo_dir),
+        "rev_lock": erepo_dir.scm.get_rev(),
+    }
```

### Comparing `dvc-2.9.5/tests/func/test_init.py` & `dvc-3.0.0a0/tests/func/test_init.py`

 * *Files 2% similar despite different names*

```diff
@@ -60,16 +60,16 @@
     assert Config(os.fspath(dvc_path))["core"]["no_scm"]
 
 
 def test_init_quiet_should_not_display_welcome_screen(tmp_dir, scm, caplog):
     with caplog.at_level(logging.INFO, logger="dvc"):
         ret = main(["init", "--quiet"])
 
-        assert 0 == ret
-        assert "" == caplog.text
+        assert ret == 0
+        assert not caplog.text
 
 
 def test_allow_init_dvc_subdir(tmp_dir, scm, monkeypatch):
     tmp_dir.gen({"subdir": {}})
 
     with monkeypatch.context() as m:
         m.chdir("subdir")
@@ -89,16 +89,16 @@
         with caplog.at_level(logging.ERROR, logger="dvc"):
             assert main(["init"]) == 1
 
     assert (
         "{} is not tracked by any supported SCM tool (e.g. Git). "
         "Use `--no-scm` if you don't want to use any SCM or "
         "`--subdir` if initializing inside a subdirectory of a parent SCM "
-        "repository.".format(os.fspath(tmp_dir / "subdir"))
-    ) in caplog.text
+        "repository.".format(os.fspath(tmp_dir / "subdir")) in caplog.text
+    )
 
 
 def test_gen_dvcignore(tmp_dir):
     DvcRepo.init(no_scm=True)
     text = (
         "# Add patterns of files dvc should ignore, which could improve\n"
         "# the performance. Learn more at\n"
@@ -114,8 +114,9 @@
         assert main(["init"]) == 1
     assert (
         "{dvc_dir} is ignored by your SCM tool. \n"
         "Make sure that it's tracked, "
         "for example, by adding '!.dvc' to .gitignore.".format(
             dvc_dir=tmp_dir / DvcRepo.DVC_DIR
         )
-    ) in caplog.text
+        in caplog.text
+    )
```

### Comparing `dvc-2.9.5/tests/func/test_install.py` & `dvc-3.0.0a0/tests/func/test_install.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 import pathlib
 import sys
 
 import pytest
 from git import GitCommandError
 
 from dvc.exceptions import DvcException
-from dvc.utils import file_md5
+from dvc_data.hashfile.hash import file_md5
 from tests.func.parsing.test_errors import escape_ansi
 
 
 @pytest.mark.skipif(
     sys.platform == "win32", reason="Git hooks aren't supported on Windows"
 )
 class TestInstall:
@@ -36,19 +36,19 @@
 
         precommit_path = pathlib.Path(".") / ".pre-commit-config.yaml"
         assert precommit_path.is_file()
 
     def test_fail_if_hook_exists(self, scm, dvc):
         self._hook("post-checkout").write_text("hook content")
 
-        with pytest.raises(DvcException) as exc_info:
+        with pytest.raises(DvcException) as exc_info:  # noqa: PT011
             dvc.install()
 
-        assert escape_ansi(str(exc_info.value)) == (
-            "Hook 'post-checkout' already exists. "
+        assert (
+            escape_ansi(str(exc_info.value)) == "Hook 'post-checkout' already exists. "
             "Please refer to <https://man.dvc.org/install> for more info."
         )
 
     def test_pre_commit_hook(self, tmp_dir, scm, dvc, caplog):
         tmp_dir.dvc_gen("file", "file content", commit="create foo")
         tmp_dir.gen("file", "file modified")
         dvc.install()
@@ -73,17 +73,15 @@
 
         with dvc.config.edit() as conf:
             conf["remote"]["store"] = {"url": os.fspath(storage_path)}
             conf["core"]["remote"] = "store"
         tmp_dir.dvc_gen("file", "file_content", "commit message")
 
         file_checksum = file_md5("file", dvc.fs)
-        expected_storage_path = (
-            storage_path / file_checksum[:2] / file_checksum[2:]
-        )
+        expected_storage_path = storage_path / file_checksum[:2] / file_checksum[2:]
 
         scm.gitpython.repo.clone(os.fspath(git_remote))
         scm.gitpython.repo.create_remote("origin", os.fspath(git_remote))
 
         dvc.install()
 
         assert not expected_storage_path.is_file()
@@ -105,17 +103,15 @@
     tmp_dir.dvc_gen({"data": {"bar": "bar"}}, commit="two: add data")
 
     # installing hook only before merge, as it runs `dvc` commands which makes
     # `checkouts` and `commits` above slower
     dvc.install()
     (tmp_dir / ".gitattributes").write_text("*.dvc merge=dvc")
 
-    scm.gitpython.repo.git.merge(
-        "one", m="merged", no_gpg_sign=True, no_signoff=True
-    )
+    scm.gitpython.repo.git.merge("one", m="merged", no_gpg_sign=True, no_signoff=True)
 
     # NOTE: dvc shouldn't checkout automatically as it might take a long time
     assert (tmp_dir / "data").read_text() == {"bar": "bar"}
     assert (tmp_dir / "data.dvc").read_text() == (
         "outs:\n"
         "- md5: 5ea40360f5b4ec688df672a4db9c17d1.dir\n"
         "  size: 6\n"
@@ -142,17 +138,15 @@
     tmp_dir.dvc_gen({"data": {"two": "two"}}, commit="two: add data")
 
     # installing hook only before merge, as it runs `dvc` commands on
     # `checkouts` and `commits` which slows tests down
     dvc.install()
     (tmp_dir / ".gitattributes").write_text("*.dvc merge=dvc")
 
-    scm.gitpython.repo.git.merge(
-        "one", m="merged", no_gpg_sign=True, no_signoff=True
-    )
+    scm.gitpython.repo.git.merge("one", m="merged", no_gpg_sign=True, no_signoff=True)
 
     # NOTE: dvc shouldn't checkout automatically as it might take a long time
     assert (tmp_dir / "data").read_text() == {"master": "master", "two": "two"}
     assert (tmp_dir / "data.dvc").read_text() == (
         "outs:\n"
         "- md5: 839ef9371606817569c1ee0e5f4ed233.dir\n"
         "  size: 12\n"
```

### Comparing `dvc-2.9.5/tests/func/test_lock.py` & `dvc-3.0.0a0/tests/func/test_lock.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,23 +1,48 @@
 import pytest
 
 from dvc.cli import main
+from dvc.exceptions import DvcException
 from dvc.lock import Lock, LockError
 
 
 def test_with(tmp_dir, dvc, mocker):
     # patching to speedup tests
     mocker.patch("dvc.lock.DEFAULT_TIMEOUT", 0.01)
 
     lockfile = tmp_dir / dvc.tmp_dir / "lock"
     with Lock(lockfile):
         with pytest.raises(LockError), Lock(lockfile):
             pass
 
 
+def test_unlock_lock_failed(tmp_dir, dvc, request, mocker):
+    # patching to speedup tests
+    mocker.patch("dvc.lock.DEFAULT_TIMEOUT", 0.01)
+
+    lockfile = tmp_dir / dvc.tmp_dir / "lock"
+    lock = Lock(lockfile)
+    lock_ext = Lock(lockfile)
+
+    # It's a common scenario now to have lock unlocked and locked back (e.g. in
+    # repro of a stage) in with. We should see LockError exception here.
+    with lock:
+        lock.unlock()
+        lock_ext.lock()  # imitate an external process had time to lock it
+        request.addfinalizer(lock_ext.unlock)
+        with pytest.raises(LockError):
+            lock.lock()
+
+
+def test_unlock_unlocked_raises():
+    lock = Lock("lock")
+    with pytest.raises(DvcException, match="Unlock called on an unlocked lock"):
+        lock.unlock()
+
+
 def test_cli(tmp_dir, dvc, mocker, caplog):
     # patching to speedup tests
     mocker.patch("dvc.lock.DEFAULT_TIMEOUT", 0.01)
 
     expected_error_msg = (
         "Unable to acquire lock. Most likely another DVC process is "
         "running or was terminated abruptly. Check the page "
```

### Comparing `dvc-2.9.5/tests/func/test_ls.py` & `dvc-3.0.0a0/tests/func/test_ls.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 import os
 import shutil
 import textwrap
 from operator import itemgetter
 
 import pytest
 
-from dvc.exceptions import PathMissingError
 from dvc.repo import Repo
 from dvc.scm import CloneError
 
 FS_STRUCTURE = {
     "README.md": "content",
     "model/script.py": "content",
     "model/train.py": "content",
@@ -38,21 +37,19 @@
         os.makedirs(os.path.dirname(f))
         open(f, "w+").close()
     """
     )
     tmp_dir.scm_gen({"script.py": script}, commit="init")
     tmp_dir.dvc_gen({"dep": "content"}, commit="init dvc")
     dvc.run(
-        **{
-            "cmd": "python script.py {}".format(os.path.join("out", "file")),
-            "outs": [os.path.join("out", "file")],
-            "deps": ["dep"],
-            "fname": "out.dvc",
-            "single_stage": True,
-        }
+        cmd="python script.py {}".format(os.path.join("out", "file")),
+        outs=[os.path.join("out", "file")],
+        deps=["dep"],
+        fname="out.dvc",
+        single_stage=True,
     )
     tmp_dir.scm_add(["out.dvc"], commit="run")
     shutil.rmtree("out")
 
 
 def test_ls_repo(tmp_dir, dvc, scm):
     tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
@@ -146,17 +143,25 @@
     )
 
 
 def test_ls_repo_with_path_dir_dvc_only_empty(tmp_dir, dvc, scm):
     tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
     tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")
     tmp_dir.scm_gen({"folder/.keep": "content"}, commit="add .keep")
+    tmp_dir.scm_gen({"empty_scm_folder/": {}}, commit="add scm empty")
+    tmp_dir.dvc_gen({"empty_dvc_folder": {}}, commit="empty dvc folder")
 
-    with pytest.raises(PathMissingError):
-        Repo.ls(os.fspath(tmp_dir), path="folder", dvc_only=True)
+    with pytest.raises(FileNotFoundError):
+        Repo.ls(os.fspath(tmp_dir), path="not_exist_folder")
+
+    assert Repo.ls(os.fspath(tmp_dir), path="empty_scm_folder") == []
+
+    assert Repo.ls(os.fspath(tmp_dir), path="folder", dvc_only=True) == []
+
+    assert Repo.ls(os.fspath(tmp_dir), path="empty_dvc_folder", dvc_only=True) == []
 
 
 def test_ls_repo_with_path_subdir(tmp_dir, dvc, scm):
     tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
     tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")
 
     path = os.path.join("data", "subcontent")
@@ -183,17 +188,15 @@
 
 def test_ls_repo_with_path_subdir_dvc_only_recursive(tmp_dir, dvc, scm):
     tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
     tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")
 
     path = os.path.join("data", "subcontent")
     files = Repo.ls(os.fspath(tmp_dir), path, dvc_only=True, recursive=True)
-    match_files(
-        files, ((("data.xml",), True), (("statistics", "data.csv"), True))
-    )
+    match_files(files, ((("data.xml",), True), (("statistics", "data.csv"), True)))
 
 
 def test_ls_repo_with_path_file_out(tmp_dir, dvc, scm):
     tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
     tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")
 
     path = os.path.join("data", "subcontent", "data.xml")
@@ -210,31 +213,29 @@
     match_files(files, ((("README.md",), False),))
 
 
 def test_ls_repo_with_missed_path(tmp_dir, dvc, scm):
     tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
     tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")
 
-    with pytest.raises(PathMissingError) as exc_info:
+    with pytest.raises(FileNotFoundError):
         Repo.ls(os.fspath(tmp_dir), path="missed_path")
-    assert not exc_info.value.dvc_only
 
 
 def test_ls_repo_with_missed_path_dvc_only(tmp_dir, dvc, scm):
     tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
     tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")
 
-    with pytest.raises(PathMissingError) as exc_info:
+    with pytest.raises(FileNotFoundError):
         Repo.ls(
             os.fspath(tmp_dir),
             path="missed_path",
             recursive=True,
             dvc_only=True,
         )
-    assert exc_info.value.dvc_only
 
 
 def test_ls_repo_with_removed_dvc_dir(tmp_dir, dvc, scm):
     create_dvc_pipeline(tmp_dir, dvc)
 
     files = Repo.ls(os.fspath(tmp_dir))
     match_files(
@@ -304,15 +305,15 @@
 
 
 def test_ls_remote_repo(erepo_dir):
     with erepo_dir.chdir():
         erepo_dir.scm_gen(FS_STRUCTURE, commit="init")
         erepo_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")
 
-    url = f"file://{erepo_dir}"
+    url = f"file://{erepo_dir.as_posix()}"
     files = Repo.ls(url)
     match_files(
         files,
         (
             ((".dvcignore",), False),
             ((".gitignore",), False),
             (("README.md",), False),
@@ -325,15 +326,15 @@
 
 
 def test_ls_remote_repo_recursive(erepo_dir):
     with erepo_dir.chdir():
         erepo_dir.scm_gen(FS_STRUCTURE, commit="init")
         erepo_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")
 
-    url = f"file://{erepo_dir}"
+    url = f"file://{erepo_dir.as_posix()}"
     files = Repo.ls(url, recursive=True)
     match_files(
         files,
         (
             ((".dvcignore",), False),
             ((".gitignore",), False),
             (("README.md",), False),
@@ -354,15 +355,15 @@
     )
 
 
 def test_ls_remote_git_only_repo_recursive(git_dir):
     with git_dir.chdir():
         git_dir.scm_gen(FS_STRUCTURE, commit="init")
 
-    url = f"file://{git_dir}"
+    url = f"file://{git_dir.as_posix()}"
     files = Repo.ls(url, recursive=True)
     match_files(
         files,
         (
             ((".gitignore",), False),
             (("README.md",), False),
             (("model", "script.py"), False),
@@ -372,15 +373,15 @@
 
 
 def test_ls_remote_repo_with_path_dir(erepo_dir):
     with erepo_dir.chdir():
         erepo_dir.scm_gen(FS_STRUCTURE, commit="init")
         erepo_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")
 
-    url = f"file://{erepo_dir}"
+    url = f"file://{erepo_dir.as_posix()}"
     path = "model"
     files = Repo.ls(url, path)
     match_files(
         files,
         (
             (("script.py",), False),
             (("train.py",), False),
@@ -393,15 +394,15 @@
 
 def test_ls_remote_repo_with_rev(erepo_dir):
     with erepo_dir.chdir():
         erepo_dir.scm_gen(FS_STRUCTURE, commit="init")
         erepo_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")
 
     rev = erepo_dir.scm.list_all_commits()[1]
-    url = f"file://{erepo_dir}"
+    url = f"file://{erepo_dir.as_posix()}"
     files = Repo.ls(url, rev=rev)
     match_files(
         files,
         (
             ((".dvcignore",), False),
             ((".gitignore",), False),
             (("README.md",), False),
@@ -412,15 +413,15 @@
 
 def test_ls_remote_repo_with_rev_recursive(erepo_dir):
     with erepo_dir.chdir():
         erepo_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")
         erepo_dir.scm_gen(FS_STRUCTURE, commit="init")
 
     rev = erepo_dir.scm.list_all_commits()[1]
-    url = f"file://{erepo_dir}"
+    url = f"file://{erepo_dir.as_posix()}"
     files = Repo.ls(url, rev=rev, recursive=True)
     match_files(
         files,
         (
             (("structure.xml.dvc",), False),
             (("model", "people.csv.dvc"), False),
             (("data", "subcontent", "data.xml.dvc"), False),
@@ -443,19 +444,19 @@
 
     dirname = "__{}_{}".format("not_existed", time())
     with pytest.raises(CloneError):
         Repo.ls(dirname, recursive=True)
 
 
 def test_ls_shows_pipeline_tracked_outs(tmp_dir, dvc, scm, run_copy):
-    from dvc.dvcfile import PIPELINE_FILE, PIPELINE_LOCK
+    from dvc.dvcfile import LOCK_FILE, PROJECT_FILE
 
     tmp_dir.gen("foo", "foo")
     run_copy("foo", "bar", name="copy-foo-bar")
-    dvc.scm.add([PIPELINE_FILE, PIPELINE_LOCK])
+    dvc.scm.add([PROJECT_FILE, LOCK_FILE])
     dvc.scm.commit("add pipeline stage")
 
     files = Repo.ls(os.curdir, dvc_only=True)
     match_files(files, ((("bar",), True),))
 
 
 def test_ls_granular(erepo_dir):
@@ -477,15 +478,15 @@
         {"isout": True, "isdir": False, "isexec": False, "path": "foo"},
     ]
 
     entries = Repo.ls(os.fspath(erepo_dir), "dir")
     assert entries == [
         {"isout": True, "isdir": False, "isexec": False, "path": "1"},
         {"isout": True, "isdir": False, "isexec": False, "path": "2"},
-        {"isout": False, "isdir": True, "isexec": False, "path": "subdir"},
+        {"isout": True, "isdir": True, "isexec": False, "path": "subdir"},
     ]
 
 
 @pytest.mark.parametrize("use_scm", [True, False])
 def test_ls_target(erepo_dir, use_scm):
     with erepo_dir.chdir():
         gen = erepo_dir.scm_gen if use_scm else erepo_dir.dvc_gen
@@ -522,61 +523,62 @@
     [
         (True, pytest.lazy_fixture("erepo_dir")),
         (False, pytest.lazy_fixture("git_dir")),
     ],
 )
 def test_subrepo(dvc_top_level, erepo):
     from tests.func.test_get import make_subrepo
-    from tests.utils import clean_staging
 
     dvc_files = {"foo.txt": "foo.txt", "dvc_dir": {"lorem": "lorem"}}
     scm_files = {"bar.txt": "bar.txt", "scm_dir": {"ipsum": "ipsum"}}
     subrepo = erepo / "subrepo"
     make_subrepo(subrepo, erepo.scm)
 
     for repo in [erepo, subrepo]:
         with repo.chdir():
             repo.scm_gen(scm_files, commit=f"scm track for top {repo}")
             if hasattr(repo, "dvc"):
                 repo.dvc_gen(dvc_files, commit=f"dvc track for {repo}")
-        clean_staging()
 
     def _list_files(repo, path=None):
         return set(map(itemgetter("path"), Repo.ls(os.fspath(repo), path)))
 
     extras = {".dvcignore", ".gitignore"}
     git_tracked_outputs = {"bar.txt", "scm_dir"}
     dvc_files = {"dvc_dir", "foo.txt", "foo.txt.dvc", "dvc_dir.dvc"}
     common_outputs = git_tracked_outputs | extras | dvc_files
 
-    top_level_outputs = (
-        common_outputs if dvc_top_level else git_tracked_outputs
-    )
+    top_level_outputs = common_outputs if dvc_top_level else git_tracked_outputs
     assert _list_files(erepo) == top_level_outputs
     assert _list_files(erepo, "scm_dir") == {"ipsum"}
     if dvc_top_level:
         assert _list_files(erepo, "dvc_dir") == {"lorem"}
 
     assert _list_files(subrepo, ".") == common_outputs
     assert _list_files(subrepo, "scm_dir") == {"ipsum"}
     assert _list_files(subrepo, "dvc_dir") == {"lorem"}
 
 
 def test_broken_symlink(tmp_dir, dvc):
-    from dvc.system import System
+    from dvc.fs import system
 
     tmp_dir.gen("file", "content")
-    System.symlink("file", "link")
+    system.symlink("file", "link")
 
     os.remove("file")
 
     entries = Repo.ls(os.fspath(tmp_dir))
 
     assert entries == [
         {
             "isout": False,
             "isdir": False,
             "isexec": False,
             "path": ".dvcignore",
         },
-        {"isout": False, "isdir": False, "isexec": False, "path": "link"},
+        {
+            "isout": False,
+            "isdir": False,
+            "isexec": False,
+            "path": "link",
+        },
     ]
```

### Comparing `dvc-2.9.5/tests/func/test_merge_driver.py` & `dvc-3.0.0a0/tests/func/test_merge_driver.py`

 * *Files 10% similar despite different names*

```diff
@@ -35,14 +35,30 @@
         ),
         ({}, {"foo": "foo"}, {"bar": "bar"}, {"foo": "foo", "bar": "bar"}),
         ({}, {}, {"bar": "bar"}, {"bar": "bar"}),
         ({}, {"foo": "foo"}, {}, {"foo": "foo"}),
         (None, {"foo": "foo"}, {"bar": "bar"}, {"foo": "foo", "bar": "bar"}),
         (None, None, {"bar": "bar"}, {"bar": "bar"}),
         (None, {"foo": "foo"}, None, {"foo": "foo"}),
+        (
+            {"foo": "foo"},
+            {"foo": "bar"},
+            {"foo": "foo", "baz": "baz"},
+            {"foo": "bar", "baz": "baz"},
+        ),
+        ({"foo": "foo"}, {}, {"foo": "foo", "bar": "bar"}, {"bar": "bar"}),
+        (
+            {"common": "common", "subdir": {"foo": "foo", "bar": "bar"}},
+            {"common": "common", "subdir": {"foo": "foo", "bar": "baz"}},
+            {"common": "common", "subdir": {"bar": "bar", "bizz": "bizz"}},
+            {
+                "common": "common",
+                "subdir": {"bar": "baz", "bizz": "bizz"},
+            },
+        ),
     ],
 )
 def test_merge(tmp_dir, dvc, ancestor, our, their, merged):
     _gen(tmp_dir, ancestor, "ancestor")
     _gen(tmp_dir, our, "our")
     _gen(tmp_dir, their, "their")
 
@@ -70,27 +86,21 @@
 @pytest.mark.parametrize(
     "ancestor, our, their, error",
     [
         (
             {"foo": "foo"},
             {"foo": "bar"},
             {"foo": "baz"},
-            (
-                "unable to auto-merge directories with "
-                "diff that contains 'change'ed files"
-            ),
+            "unable to auto-merge the following paths:\nfoo",
         ),
         (
             {"common": "common", "foo": "foo"},
             {"common": "common", "bar": "bar"},
             {"baz": "baz"},
-            (
-                "unable to auto-merge directories with "
-                "diff that contains 'remove'ed files"
-            ),
+            "unable to auto-merge the following paths:\nboth deleted: ('foo',)",
         ),
     ],
 )
 def test_merge_conflict(tmp_dir, dvc, ancestor, our, their, error, caplog):
     _gen(tmp_dir, ancestor, "ancestor")
     _gen(tmp_dir, our, "our")
     _gen(tmp_dir, their, "their")
@@ -113,17 +123,15 @@
 
     assert error in caplog.text
 
 
 def test_merge_different_output_options(tmp_dir, dvc, caplog):
     (tmp_dir / "ancestor").touch()
 
-    (tmp_dir / "our").write_text(
-        "outs:\n- md5: f123456789.dir\n  path: path\n"
-    )
+    (tmp_dir / "our").write_text("outs:\n- md5: f123456789.dir\n  path: path\n")
 
     (tmp_dir / "their").write_text(
         "outs:\n- md5: f987654321.dir\n  path: path\n  cache: false\n"
     )
 
     assert (
         main(
@@ -144,17 +152,15 @@
     error = "unable to auto-merge outputs with different options"
     assert error in caplog.text
 
 
 def test_merge_file(tmp_dir, dvc, caplog):
     (tmp_dir / "ancestor").touch()
 
-    (tmp_dir / "our").write_text(
-        "outs:\n- md5: f123456789.dir\n  path: path\n"
-    )
+    (tmp_dir / "our").write_text("outs:\n- md5: f123456789.dir\n  path: path\n")
 
     (tmp_dir / "their").write_text("outs:\n- md5: f987654321\n  path: path\n")
 
     assert (
         main(
             [
                 "git-hook",
```

### Comparing `dvc-2.9.5/tests/func/test_odb.py` & `dvc-3.0.0a0/tests/func/test_odb.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,74 +1,74 @@
 import os
 import stat
 
 import configobj
 import pytest
 
+from dvc.cachemgr import CacheManager
 from dvc.cli import main
-from dvc.data.db import ODBManager
-from dvc.hash_info import HashInfo
-from dvc.objects.errors import ObjectFormatError
 from dvc.utils import relpath
+from dvc_data.hashfile.hash_info import HashInfo
+from dvc_objects.errors import ObjectFormatError
 
 
 def test_cache(tmp_dir, dvc):
     cache1_md5 = "123"
     cache2_md5 = "234"
     cache1 = os.path.join(
-        dvc.odb.local.cache_dir,
+        dvc.cache.local.path,
         cache1_md5[0:2],
         cache1_md5[2:],
     )
     cache2 = os.path.join(
-        dvc.odb.local.cache_dir,
+        dvc.cache.local.path,
         cache2_md5[0:2],
         cache2_md5[2:],
     )
     tmp_dir.gen({cache1: "1", cache2: "2"})
 
     assert os.path.exists(cache1)
     assert os.path.exists(cache2)
 
-    odb = ODBManager(dvc)
+    odb = CacheManager(dvc)
 
     md5_list = list(odb.local.all())
     assert len(md5_list) == 2
     assert cache1_md5 in md5_list
     assert cache2_md5 in md5_list
 
-    odb_cache1 = odb.local.hash_to_path(cache1_md5)
-    odb_cache2 = odb.local.hash_to_path(cache2_md5)
+    odb_cache1 = odb.local.oid_to_path(cache1_md5)
+    odb_cache2 = odb.local.oid_to_path(cache2_md5)
     assert os.fspath(odb_cache1) == cache1
     assert os.fspath(odb_cache2) == cache2
 
 
 def test_cache_load_bad_dir_cache(tmp_dir, dvc):
-    from dvc.data import load
+    from dvc_data.hashfile import load
 
     dir_hash = "123.dir"
-    fname = os.fspath(dvc.odb.local.hash_to_path(dir_hash))
+    fname = os.fspath(dvc.cache.local.oid_to_path(dir_hash))
     tmp_dir.gen({fname: "<clearly>not,json"})
     with pytest.raises(ObjectFormatError):
-        load(dvc.odb.local, HashInfo("md5", dir_hash))
+        load(dvc.cache.local, HashInfo("md5", dir_hash))
 
     dir_hash = "234.dir"
-    fname = os.fspath(dvc.odb.local.hash_to_path(dir_hash))
+    fname = os.fspath(dvc.cache.local.oid_to_path(dir_hash))
     tmp_dir.gen({fname: '{"a": "b"}'})
     with pytest.raises(ObjectFormatError):
-        load(dvc.odb.local, HashInfo("md5", dir_hash))
+        load(dvc.cache.local, HashInfo("md5", dir_hash))
 
 
 def test_external_cache_dir(tmp_dir, dvc, make_tmp_dir):
     cache_dir = make_tmp_dir("cache")
 
     with dvc.config.edit() as conf:
         conf["cache"]["dir"] = cache_dir.fs_path
-    assert not os.path.exists(dvc.odb.local.cache_dir)
-    dvc.odb = ODBManager(dvc)
+    assert not os.path.exists(dvc.cache.local.path)
+    dvc.cache = CacheManager(dvc)
 
     tmp_dir.dvc_gen({"foo": "foo"})
 
     tmp_dir.dvc_gen(
         {
             "data_dir": {
                 "data": "data_dir/data",
@@ -83,17 +83,17 @@
 
 def test_remote_cache_references(tmp_dir, dvc):
     with dvc.config.edit() as conf:
         conf["remote"]["storage"] = {"url": "ssh://user@localhost:23"}
         conf["remote"]["cache"] = {"url": "remote://storage/tmp"}
         conf["cache"]["ssh"] = "cache"
 
-    dvc.odb = ODBManager(dvc)
+    dvc.cache = CacheManager(dvc)
 
-    assert dvc.odb.ssh.fs_path == "/tmp"
+    assert dvc.cache.ssh.path == "/tmp"
 
 
 def test_shared_cache_dir(tmp_dir):
     cache_dir = os.path.abspath(os.path.join(os.curdir, "cache"))
     for d in ["dir1", "dir2"]:
         os.mkdir(d)
         with (tmp_dir / d).chdir():
@@ -108,32 +108,29 @@
             (tmp_dir / d).gen({"common": "common", "unique": d})
 
             ret = main(["add", "common", "unique"])
             assert ret == 0
 
     assert not os.path.exists(os.path.join("dir1", ".dvc", "cache"))
     assert not os.path.exists(os.path.join("dir2", ".dvc", "cache"))
-
-    subdirs = list(
-        filter(
-            lambda x: os.path.isdir(os.path.join(cache_dir, x)),
-            os.listdir(cache_dir),
-        )
+    assert os.path.exists(
+        os.path.join(cache_dir, "dc", "f6c2fa538b445a3a095255c3641dfc")
+    )
+    assert os.path.exists(
+        os.path.join(cache_dir, "b4", "333c8cfa2ebba7ef20ec6c3265902b")
+    )
+    assert os.path.exists(
+        os.path.join(cache_dir, "9e", "fab2399c7c560b34de477b9aa0a465")
     )
-    assert len(subdirs) == 3
-    assert len(os.listdir(os.path.join(cache_dir, subdirs[0]))) == 1
-
-    assert len(os.listdir(os.path.join(cache_dir, subdirs[1]))) == 1
-    assert len(os.listdir(os.path.join(cache_dir, subdirs[2]))) == 1
 
 
 def test_cache_link_type(tmp_dir, scm, dvc):
     with dvc.config.edit() as conf:
         conf["cache"]["type"] = "reflink,copy"
-    dvc.odb = ODBManager(dvc)
+    dvc.cache = CacheManager(dvc)
 
     stages = tmp_dir.dvc_gen({"foo": "foo"})
     assert len(stages) == 1
     assert (tmp_dir / "foo").read_text().strip() == "foo"
 
 
 def test_cmd_cache_dir(tmp_dir, scm, dvc):
@@ -153,69 +150,51 @@
 def test_cmd_cache_relative_path(tmp_dir, scm, dvc, make_tmp_dir):
     cache_dir = make_tmp_dir("cache")
     dname = relpath(cache_dir)
     ret = main(["cache", "dir", dname])
     assert ret == 0
 
     dvc.config.load()
-    dvc.odb = ODBManager(dvc)
+    dvc.cache = CacheManager(dvc)
 
     # NOTE: we are in the repo's root and config is in .dvc/, so
     # dir path written to config should be just one level above.
     rel = os.path.join("..", dname)
     config = configobj.ConfigObj(dvc.config.files["repo"])
     assert config["cache"]["dir"] == rel.replace("\\", "/")
 
     tmp_dir.dvc_gen({"foo": "foo"})
 
-    subdirs = os.listdir(cache_dir)
-    assert len(subdirs) == 1
-    files = os.listdir(os.path.join(cache_dir, subdirs[0]))
-    assert len(files) == 1
+    assert os.path.exists(
+        os.path.join(cache_dir, "ac", "bd18db4cc2f85cedef654fccc4a4d8")
+    )
 
 
 def test_default_cache_type(dvc):
-    assert dvc.odb.local.cache_types == ["reflink", "copy"]
+    assert dvc.cache.local.cache_types == ["reflink", "copy"]
 
 
 @pytest.mark.skipif(os.name == "nt", reason="Not supported for Windows.")
 @pytest.mark.parametrize("group", [False, True])
 def test_shared_cache(tmp_dir, dvc, group):
-    from dvc.utils.fs import umask
+    from dvc.fs import system
 
     if group:
         with dvc.config.edit() as conf:
             conf["cache"].update({"shared": "group"})
-    dvc.odb = ODBManager(dvc)
-    cache_dir = dvc.odb.local.cache_dir
+    dvc.cache = CacheManager(dvc)
+    cache_dir = dvc.cache.local.path
 
     assert not os.path.exists(cache_dir)
 
-    tmp_dir.dvc_gen(
-        {"file": "file content", "dir": {"file2": "file 2 " "content"}}
-    )
-
-    actual = {}
-    for root, dnames, fnames in os.walk(cache_dir):
-        for name in dnames + fnames:
-            path = os.path.join(root, name)
-            actual[path] = oct(stat.S_IMODE(os.stat(path).st_mode))
+    tmp_dir.dvc_gen({"file": "file content", "dir": {"file2": "file 2 content"}})
 
     file_mode = oct(0o444)
-    dir_mode = oct(0o2775 if group else (0o777 & ~umask))
-
-    expected = {
-        os.path.join(cache_dir, "17"): dir_mode,
-        os.path.join(
-            cache_dir, "17", "4eaa1dd94050255b7b98a7e1924b31.dir"
-        ): file_mode,
-        os.path.join(cache_dir, "97"): dir_mode,
-        os.path.join(
-            cache_dir, "97", "e17781c198500e2766ea56bd697c03"
-        ): file_mode,
-        os.path.join(cache_dir, "d1"): dir_mode,
-        os.path.join(
-            cache_dir, "d1", "0b4c3ff123b26dc068d43a8bef2d23"
-        ): file_mode,
-    }
-
-    assert expected == actual
+    dir_mode = oct(0o2775 if group else (0o777 & ~system.umask))
+    for root, dnames, fnames in os.walk(cache_dir):
+        for dname in dnames:
+            path = os.path.join(root, dname)
+            assert oct(stat.S_IMODE(os.stat(path).st_mode)) == dir_mode
+
+        for fname in fnames:
+            path = os.path.join(root, fname)
+            assert oct(stat.S_IMODE(os.stat(path).st_mode)) == file_mode
```

### Comparing `dvc-2.9.5/tests/func/test_remote.py` & `dvc-3.0.0a0/tests/func/test_remote.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,78 +1,70 @@
 import errno
+import itertools
 import os
 import stat
 from unittest.mock import patch
 
 import configobj
 import pytest
 
 from dvc.cli import main
 from dvc.config import Config
 from dvc.exceptions import DownloadError, RemoteCacheRequiredError, UploadError
 from dvc.utils.fs import remove
-from tests.basic_env import TestDvc
 
 
-class TestRemote(TestDvc):
-    def test(self):
-        remotes = ["a", "b", "c"]
+def test_remote(dvc):
+    remotes = ["a", "b", "c"]
 
-        self.assertEqual(main(["remote", "list"]), 0)
-        self.assertNotEqual(main(["remote", "remove", remotes[0]]), 0)
+    assert main(["remote", "list"]) == 0
+    assert main(["remote", "remove", remotes[0]]) != 0
 
-        for r in remotes:
-            self.assertEqual(
-                main(["remote", "add", "--default", r, "s3://bucket/name"]), 0
-            )
+    for r in remotes:
+        assert main(["remote", "add", "--default", r, "s3://bucket/name"]) == 0
 
-        self.assertEqual(main(["remote", "list"]), 0)
+    assert main(["remote", "list"]) == 0
 
-        self.assertEqual(
-            main(["remote", "modify", remotes[0], "checksum_jobs", "1"]), 0
-        )
-        self.assertEqual(main(["remote", "remove", remotes[0]]), 0)
+    assert main(["remote", "modify", remotes[0], "checksum_jobs", "1"]) == 0
+    assert main(["remote", "remove", remotes[0]]) == 0
 
-        self.assertEqual(main(["remote", "list"]), 0)
+    assert main(["remote", "list"]) == 0
 
-    def test_relative_path(self):
-        dname = os.path.join("..", "path", "to", "dir")
-        ret = main(["remote", "add", "mylocal", dname])
-        self.assertEqual(ret, 0)
-
-        # NOTE: we are in the repo's root and config is in .dvc/, so
-        # dir path written to config should be just one level above.
-        rel = os.path.join("..", dname)
-        config = configobj.ConfigObj(self.dvc.config.files["repo"])
-        self.assertEqual(
-            config['remote "mylocal"']["url"], rel.replace("\\", "/")
-        )
 
-    def test_overwrite(self):
-        remote_name = "a"
-        remote_url = "s3://bucket/name"
-        self.assertEqual(main(["remote", "add", remote_name, remote_url]), 0)
-        self.assertEqual(main(["remote", "add", remote_name, remote_url]), 251)
-        self.assertEqual(
-            main(["remote", "add", "-f", remote_name, remote_url]), 0
-        )
+def test_remote_add_relative_path(dvc):
+    dname = os.path.join("..", "path", "to", "dir")
+    ret = main(["remote", "add", "mylocal", dname])
+    assert ret == 0
+
+    # NOTE: we are in the repo's root and config is in .dvc/, so
+    # dir path written to config should be just one level above.
+    rel = os.path.join("..", dname)
+    config = configobj.ConfigObj(dvc.config.files["repo"])
+    assert config['remote "mylocal"']["url"] == rel.replace("\\", "/")
 
-    def test_referencing_other_remotes(self):
-        assert main(["remote", "add", "foo", "ssh://localhost/"]) == 0
-        assert main(["remote", "add", "bar", "remote://foo/dvc-storage"]) == 0
 
-        config = configobj.ConfigObj(self.dvc.config.files["repo"])
-        assert config['remote "bar"']["url"] == "remote://foo/dvc-storage"
+def test_remote_overwrite(dvc):
+    remote_name = "a"
+    remote_url = "s3://bucket/name"
+    assert main(["remote", "add", remote_name, remote_url]) == 0
+    assert main(["remote", "add", remote_name, remote_url]) == 251
+    assert main(["remote", "add", "-f", remote_name, remote_url]) == 0
+
+
+def test_referencing_other_remotes(dvc):
+    assert main(["remote", "add", "foo", "ssh://localhost/"]) == 0
+    assert main(["remote", "add", "bar", "remote://foo/dvc-storage"]) == 0
+
+    config = configobj.ConfigObj(dvc.config.files["repo"])
+    assert config['remote "bar"']["url"] == "remote://foo/dvc-storage"
 
 
 def test_remove_default(tmp_dir, dvc):
     remote = "mys3"
-    assert (
-        main(["remote", "add", "--default", remote, "s3://bucket/name"]) == 0
-    )
+    assert main(["remote", "add", "--default", remote, "s3://bucket/name"]) == 0
     assert main(["remote", "modify", remote, "profile", "default"]) == 0
     assert main(["config", "--local", "core.remote", remote]) == 0
 
     config = configobj.ConfigObj(dvc.config.files["repo"])
     local_config = configobj.ConfigObj(dvc.config.files["local"])
     assert config["core"]["remote"] == remote
     assert local_config["core"]["remote"] == remote
@@ -81,45 +73,43 @@
 
     config = configobj.ConfigObj(dvc.config.files["repo"])
     local_config = configobj.ConfigObj(dvc.config.files["local"])
     assert config.get("core", {}).get("remote") is None
     assert local_config.get("core", {}).get("remote") is None
 
 
-class TestRemoteRemove(TestDvc):
-    def test(self):
-        ret = main(["config", "core.checksum_jobs", "1"])
-        self.assertEqual(ret, 0)
-
-        remote = "mys3"
-        ret = main(["remote", "add", remote, "s3://bucket/name"])
-        self.assertEqual(ret, 0)
-
-        ret = main(["remote", "remove", remote])
-        self.assertEqual(ret, 0)
-
-
-class TestRemoteDefault(TestDvc):
-    def test(self):
-        remote = "mys3"
-        ret = main(["remote", "add", "mys3", "s3://bucket/path"])
-        self.assertEqual(ret, 0)
-
-        ret = main(["remote", "default", "mys3"])
-        self.assertEqual(ret, 0)
-        config_file = os.path.join(self.dvc.dvc_dir, Config.CONFIG)
-        config = configobj.ConfigObj(config_file)
-        default = config["core"]["remote"]
-        self.assertEqual(default, remote)
-
-        ret = main(["remote", "default", "--unset"])
-        self.assertEqual(ret, 0)
-        config = configobj.ConfigObj(config_file)
-        default = config.get("core", {}).get("remote")
-        self.assertEqual(default, None)
+def test_remote_remove(dvc):
+    ret = main(["config", "core.checksum_jobs", "1"])
+    assert ret == 0
+
+    remote = "mys3"
+    ret = main(["remote", "add", remote, "s3://bucket/name"])
+    assert ret == 0
+
+    ret = main(["remote", "remove", remote])
+    assert ret == 0
+
+
+def test_remote_default_cmd(dvc):
+    remote = "mys3"
+    ret = main(["remote", "add", "mys3", "s3://bucket/path"])
+    assert ret == 0
+
+    ret = main(["remote", "default", "mys3"])
+    assert ret == 0
+    config_file = os.path.join(dvc.dvc_dir, Config.CONFIG)
+    config = configobj.ConfigObj(config_file)
+    default = config["core"]["remote"]
+    assert default == remote
+
+    ret = main(["remote", "default", "--unset"])
+    assert ret == 0
+    config = configobj.ConfigObj(config_file)
+    default = config.get("core", {}).get("remote")
+    assert default is None
 
 
 def test_show_default(dvc, capsys):
     assert main(["remote", "add", "foo", "s3://bucket/name"]) == 0
     assert main(["remote", "default", "foo"]) == 0
     assert main(["remote", "default"]) == 0
     out, _ = capsys.readouterr()
@@ -135,107 +125,131 @@
     assert ret == 0
 
     ret = main(["push", "-r", remote_name])
     assert ret == 0
 
 
 def test_dir_hash_should_be_key_order_agnostic(tmp_dir, dvc):
-    from dvc.data.stage import stage
-    from dvc.data.tree import Tree
+    from dvc_data.hashfile.build import build
+    from dvc_data.hashfile.tree import Tree
 
     tmp_dir.gen({"data": {"1": "1 content", "2": "2 content"}})
 
     path = (tmp_dir / "data").fs_path
 
-    tree = Tree.from_list(
-        [{"relpath": "1", "md5": "1"}, {"relpath": "2", "md5": "2"}]
-    )
+    tree = Tree.from_list([{"relpath": "1", "md5": "1"}, {"relpath": "2", "md5": "2"}])
     tree.digest()
-    with patch("dvc.data.stage._stage_tree", return_value=(None, tree)):
-        _, _, obj = stage(dvc.odb.local, path, dvc.odb.local.fs, "md5")
+    with patch("dvc_data.hashfile.build._build_tree", return_value=(None, tree)):
+        _, _, obj = build(dvc.cache.local, path, dvc.cache.local.fs, "md5")
         hash1 = obj.hash_info
 
-    tree = Tree.from_list(
-        [{"md5": "1", "relpath": "1"}, {"md5": "2", "relpath": "2"}]
-    )
+    # remove the raw dir obj to force building the tree on the next build call
+    dvc.cache.local.fs.remove(dvc.cache.local.oid_to_path(hash1.as_raw().value))
+
+    tree = Tree.from_list([{"md5": "1", "relpath": "1"}, {"md5": "2", "relpath": "2"}])
     tree.digest()
-    with patch("dvc.data.stage._stage_tree", return_value=(None, tree)):
-        _, _, obj = stage(dvc.odb.local, path, dvc.odb.local.fs, "md5")
+    with patch("dvc_data.hashfile.build._build_tree", return_value=(None, tree)):
+        _, _, obj = build(dvc.cache.local, path, dvc.cache.local.fs, "md5")
         hash2 = obj.hash_info
 
     assert hash1 == hash2
 
 
-def test_partial_push_n_pull(tmp_dir, dvc, tmp_path_factory, local_remote):
-    import dvc.fs.utils as fs_utils
+def test_partial_push_n_pull(  # noqa: C901
+    tmp_dir, dvc, tmp_path_factory, local_remote
+):
+    from dvc_objects.fs import generic
 
     foo = tmp_dir.dvc_gen({"foo": "foo content"})[0].outs[0]
     bar = tmp_dir.dvc_gen({"bar": "bar content"})[0].outs[0]
     baz = tmp_dir.dvc_gen({"baz": {"foo": "foo content"}})[0].outs[0]
 
     # Faulty upload version, failing on foo
-    original = fs_utils.transfer
+    original = generic.transfer
     odb = dvc.cloud.get_remote_odb("upstream")
 
     def unreliable_upload(from_fs, from_info, to_fs, to_info, **kwargs):
-        if os.path.abspath(to_info) == os.path.abspath(
-            odb.get(foo.hash_info).fs_path
-        ):
-            raise Exception("stop foo")
+        on_error = kwargs["on_error"]
+        assert on_error
+        if isinstance(from_info, str):
+            from_info = [from_info]
+        else:
+            from_info = list(from_info)
+        if isinstance(to_info, str):
+            to_info = [to_info]
+        else:
+            to_info = list(to_info)
+        for i in range(len(from_info) - 1, -1, -1):
+            from_i = from_info[i]
+            to_i = to_info[i]
+            if os.path.abspath(to_i) == os.path.abspath(
+                odb.get(foo.hash_info.value).path
+            ):
+                if on_error:
+                    on_error(from_i, to_i, Exception("stop foo"))
+                del from_info[i]
+                del to_info[i]
         return original(from_fs, from_info, to_fs, to_info, **kwargs)
 
-    with patch.object(fs_utils, "transfer", unreliable_upload):
+    with patch.object(generic, "transfer", unreliable_upload):
         with pytest.raises(UploadError) as upload_error_info:
             dvc.push()
         assert upload_error_info.value.amount == 2
 
-        assert not odb.exists(foo.hash_info)
-        assert odb.exists(bar.hash_info)
-        assert not odb.exists(baz.hash_info)
+        assert not odb.exists(foo.hash_info.value)
+        assert odb.exists(bar.hash_info.value)
+        assert not odb.exists(baz.hash_info.value)
 
     # Push everything and delete local cache
     dvc.push()
-    remove(dvc.odb.local.cache_dir)
+    dvc.cache.local.clear()
 
     baz._collect_used_dir_cache()
-    with patch.object(fs_utils, "transfer", side_effect=Exception):
+
+    def unreliable_download(_from_fs, from_info, _to_fs, to_info, **kwargs):
+        on_error = kwargs["on_error"]
+        assert on_error
+        if isinstance(from_info, str):
+            from_info = [from_info]
+        if isinstance(to_info, str):
+            to_info = [to_info]
+        for from_i, to_i in zip(from_info, to_info):
+            on_error(from_i, to_i, Exception())
+
+    with patch.object(generic, "transfer", unreliable_download):
         with pytest.raises(DownloadError) as download_error_info:
             dvc.pull()
         # error count should be len(.dir + standalone file checksums)
         # since files inside dir are ignored if dir cache entry is missing
         assert download_error_info.value.amount == 2
 
 
 def test_raise_on_too_many_open_files(
     tmp_dir, dvc, tmp_path_factory, mocker, local_remote
 ):
     tmp_dir.dvc_gen({"file": "file content"})
 
     mocker.patch(
-        "dvc.fs.utils.transfer",
+        "dvc_objects.fs.generic.transfer",
         side_effect=OSError(errno.EMFILE, "Too many open files"),
     )
 
-    with pytest.raises(OSError) as e:
+    with pytest.raises(OSError, match="Too many open files") as e:
         dvc.push()
-        assert e.errno == errno.EMFILE
+    assert e.value.errno == errno.EMFILE
 
 
 def test_modify_missing_remote(tmp_dir, dvc):
     assert main(["remote", "modify", "myremote", "user", "xxx"]) == 251
 
 
 def test_remote_modify_local_on_repo_config(tmp_dir, dvc):
     assert main(["remote", "add", "myremote", "http://example.com/path"]) == 0
-    assert (
-        main(["remote", "modify", "myremote", "user", "xxx", "--local"]) == 0
-    )
-    assert dvc.config.load_one("local")["remote"]["myremote"] == {
-        "user": "xxx"
-    }
+    assert main(["remote", "modify", "myremote", "user", "xxx", "--local"]) == 0
+    assert dvc.config.load_one("local")["remote"]["myremote"] == {"user": "xxx"}
     assert dvc.config.load_one("repo")["remote"]["myremote"] == {
         "url": "http://example.com/path"
     }
     dvc.config.load()
     assert dvc.config["remote"]["myremote"] == {
         "url": "http://example.com/path",
         "user": "xxx",
@@ -245,76 +259,69 @@
 
 def test_external_dir_resource_on_no_cache(tmp_dir, dvc, tmp_path_factory):
     # https://github.com/iterative/dvc/issues/2647, is some situations
     # (external dir dependency) cache is required to calculate dir md5
     external_dir = tmp_path_factory.mktemp("external_dir")
     file = external_dir / "file"
 
-    dvc.odb.local = None
+    dvc.cache.local = None
     with pytest.raises(RemoteCacheRequiredError):
         dvc.run(
             cmd=f"echo content > {file}",
             outs=[os.fspath(file)],
             name="echo",
             external=True,
         )
 
 
 def test_push_order(tmp_dir, dvc, tmp_path_factory, mocker, local_remote):
-    import dvc.fs.utils as fs_utils
+    from dvc_objects.fs import generic
 
     foo = tmp_dir.dvc_gen({"foo": {"bar": "bar content"}})[0].outs[0]
     tmp_dir.dvc_gen({"baz": "baz content"})
 
-    mocked_upload = mocker.spy(fs_utils, "transfer")
+    mocked_upload = mocker.spy(generic, "transfer")
     dvc.push()
 
     # foo .dir file should be uploaded after bar
     odb = dvc.cloud.get_remote_odb("upstream")
-    foo_path = odb.hash_to_path(foo.hash_info.value)
-    bar_path = odb.hash_to_path(foo.obj._trie[("bar",)][1].value)
-    paths = [args[3] for args, _ in mocked_upload.call_args_list]
+    foo_path = odb.oid_to_path(foo.hash_info.value)
+    bar_path = odb.oid_to_path(foo.obj._trie[("bar",)][1].value)
+    paths = list(
+        itertools.chain.from_iterable(
+            args[3] for args, _ in mocked_upload.call_args_list
+        )
+    )
     assert paths.index(foo_path) > paths.index(bar_path)
 
 
 def test_remote_modify_validation(dvc):
     remote_name = "drive"
     unsupported_config = "unsupported_config"
+    assert main(["remote", "add", "-d", remote_name, "gdrive://test/test"]) == 0
     assert (
-        main(["remote", "add", "-d", remote_name, "gdrive://test/test"]) == 0
-    )
-    assert (
-        main(
-            ["remote", "modify", remote_name, unsupported_config, "something"]
-        )
-        == 251
+        main(["remote", "modify", remote_name, unsupported_config, "something"]) == 251
     )
     config = configobj.ConfigObj(dvc.config.files["repo"])
     assert unsupported_config not in config[f'remote "{remote_name}"']
 
 
 def test_remote_modify_unset(dvc):
     assert main(["remote", "add", "-d", "myremote", "gdrive://test/test"]) == 0
     config = configobj.ConfigObj(dvc.config.files["repo"])
     assert config['remote "myremote"'] == {"url": "gdrive://test/test"}
 
-    assert (
-        main(["remote", "modify", "myremote", "gdrive_client_id", "something"])
-        == 0
-    )
+    assert main(["remote", "modify", "myremote", "gdrive_client_id", "something"]) == 0
     config = configobj.ConfigObj(dvc.config.files["repo"])
     assert config['remote "myremote"'] == {
         "url": "gdrive://test/test",
         "gdrive_client_id": "something",
     }
 
-    assert (
-        main(["remote", "modify", "myremote", "gdrive_client_id", "--unset"])
-        == 0
-    )
+    assert main(["remote", "modify", "myremote", "gdrive_client_id", "--unset"]) == 0
     config = configobj.ConfigObj(dvc.config.files["repo"])
     assert config['remote "myremote"'] == {"url": "gdrive://test/test"}
 
 
 def test_remote_modify_default(dvc):
     remote_repo = "repo_level"
     remote_local = "local_level"
@@ -405,46 +412,28 @@
 
 
 def test_protect_local_remote(tmp_dir, dvc, local_remote):
     (stage,) = tmp_dir.dvc_gen("file", "file content")
 
     dvc.push()
     odb = dvc.cloud.get_remote_odb("upstream")
-    remote_cache_file = odb.hash_to_path(stage.outs[0].hash_info.value)
+    remote_cache_file = odb.oid_to_path(stage.outs[0].hash_info.value)
 
     assert os.path.exists(remote_cache_file)
     assert stat.S_IMODE(os.stat(remote_cache_file).st_mode) == 0o444
 
 
 def test_push_incomplete_dir(tmp_dir, dvc, mocker, local_remote):
     (stage,) = tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}})
     remote_odb = dvc.cloud.get_remote_odb("upstream")
 
-    odb = dvc.odb.local
+    odb = dvc.cache.local
     out = stage.outs[0]
     file_objs = [entry_obj for _, _, entry_obj in out.obj]
 
     # remove one of the cache files for directory
-    remove(odb.hash_to_path(file_objs[0].value))
+    remove(odb.oid_to_path(file_objs[0].value))
 
     dvc.push()
-    assert not remote_odb.exists(out.hash_info)
-    assert not remote_odb.exists(file_objs[0])
-    assert remote_odb.exists(file_objs[1])
-
-
-def test_upload_exists(tmp_dir, dvc, local_remote):
-    tmp_dir.gen("foo", "foo")
-    odb = dvc.cloud.get_remote_odb("upstream")
-    # allow uploaded files to be writable for this test,
-    # normally they are set to read-only for DVC remotes
-    odb.fs.CACHE_MODE = 0o644
-
-    from_info = (tmp_dir / "foo").fs_path
-    to_info = odb.fs.path.join(odb.fs_path, "foo")
-    odb.fs.upload(from_info, to_info)
-    assert odb.fs.exists(to_info)
-
-    tmp_dir.gen("foo", "bar")
-    odb.fs.upload(from_info, to_info)
-    with odb.fs.open(to_info) as fobj:
-        assert fobj.read() == "bar"
+    assert not remote_odb.exists(out.hash_info.value)
+    assert not remote_odb.exists(file_objs[0].value)
+    assert remote_odb.exists(file_objs[1].value)
```

### Comparing `dvc-2.9.5/tests/func/test_remove.py` & `dvc-3.0.0a0/tests/func/test_remove.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 import os
 
 import pytest
 
 from dvc.cli import main
-from dvc.exceptions import DvcException
-from dvc.stage.exceptions import StageFileDoesNotExistError
-from dvc.system import System
+from dvc.fs import system
+from dvc.stage.exceptions import StageFileDoesNotExistError, StageFileIsNotDvcFileError
 from dvc.utils.fs import remove
+from dvc_objects.errors import ObjectDBError
 from tests.utils import get_gitignore_content
 
 
 @pytest.mark.parametrize("remove_outs", [True, False])
 def test_remove(tmp_dir, scm, dvc, run_copy, remove_outs):
     (stage1,) = tmp_dir.dvc_gen("foo", "foo")
     stage2 = run_copy("foo", "bar", single_stage=True)
@@ -19,39 +19,51 @@
     assert "/foo" in get_gitignore_content()
     assert "/bar" in get_gitignore_content()
     assert "/foobar" in get_gitignore_content()
 
     for stage in [stage1, stage2, stage3]:
         dvc.remove(stage.addressing, outs=remove_outs)
         out_exists = (out.exists for out in stage.outs)
-        assert stage not in dvc.stage.collect_repo()
+        assert stage not in dvc.index.stages
         if remove_outs:
             assert not any(out_exists)
         else:
             assert all(out_exists)
 
     assert not (tmp_dir / ".gitignore").exists()
 
 
+def test_remove_file_target(tmp_dir, dvc):
+    tmp_dir.dvc_gen("foo", "foo")
+
+    with pytest.raises(
+        StageFileIsNotDvcFileError,
+        match="'foo' is not a .dvc file. Do you mean 'foo.dvc'?",
+    ):
+        dvc.remove("foo")
+
+    dvc.remove("foo.dvc")
+
+
 def test_remove_non_existent_file(tmp_dir, dvc):
     with pytest.raises(StageFileDoesNotExistError):
         dvc.remove("non_existent_dvc_file.dvc")
     with pytest.raises(StageFileDoesNotExistError):
         dvc.remove("non_existent_stage_name")
 
 
 def test_remove_broken_symlink(tmp_dir, dvc):
     tmp_dir.gen("foo", "foo")
-    dvc.odb.local.cache_types = ["symlink"]
+    dvc.cache.local.cache_types = ["symlink"]
 
     (stage,) = dvc.add("foo")
-    remove(dvc.odb.local.cache_dir)
-    assert System.is_symlink("foo")
+    remove(dvc.cache.local.path)
+    assert system.is_symlink("foo")
 
-    with pytest.raises(DvcException):
+    with pytest.raises(ObjectDBError):
         dvc.remove(stage.addressing)
     assert os.path.lexists("foo")
     assert (tmp_dir / stage.relpath).exists()
 
     dvc.remove(stage.addressing, outs=True)
     assert not os.path.lexists("foo")
     assert not (tmp_dir / stage.relpath).exists()
```

### Comparing `dvc-2.9.5/tests/func/test_repo_index.py` & `dvc-3.0.0a0/tests/func/test_commit.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,293 +1,299 @@
 import os
-from itertools import chain
+import textwrap
 
 import pytest
-from pygtrie import Trie
 
-from dvc.repo.index import Index
-from dvc.stage import PipelineStage, Stage
-from dvc.utils import relpath
+from dvc.dependency.base import DependencyDoesNotExistError
+from dvc.dvcfile import PROJECT_FILE
+from dvc.output import OutputDoesNotExistError
+from dvc.stage.exceptions import StageCommitError
 
 
-def test_index(tmp_dir, scm, dvc, run_copy):
-    (stage1,) = tmp_dir.dvc_gen("foo", "foo")
-    stage2 = run_copy("foo", "bar", name="copy-foo-bar")
-    tmp_dir.commit([s.outs[0].fspath for s in (stage1, stage2)], msg="add")
+def test_commit_recursive(tmp_dir, dvc):
+    tmp_dir.gen({"dir": {"file": "text1", "subdir": {"file2": "text2"}}})
+    stages = dvc.add("dir", recursive=True, no_commit=True)
 
-    index = Index(dvc)
-    assert index.fs == dvc.fs
+    assert len(stages) == 2
+    assert dvc.status() != {}
 
-    assert len(index) == len(index.stages) == 2
-    assert set(index.stages) == set(index) == {stage1, stage2}
-    assert stage1 in index
-    assert stage2 in index
+    dvc.commit("dir", recursive=True)
+    assert dvc.status() == {}
 
-    assert index.outs_graph
-    assert index.graph
-    assert isinstance(index.outs_trie, Trie)
-    assert index.identifier
-    index.check_graph()
 
+def test_commit_force(tmp_dir, dvc):
+    tmp_dir.gen({"dir": {"file": "text1", "file2": "text2"}})
+    (stage,) = dvc.add("dir", no_commit=True)
 
-def test_repr(tmp_dir, scm, dvc):
-    tmp_dir.dvc_gen("foo", "foo", commit="add foo")
+    assert stage.outs[0].changed_cache()
 
-    brancher = dvc.brancher([scm.get_rev()])
-    rev = next(brancher)
-    assert rev == "workspace"
-    assert repr(Index(dvc)) == f"Index({dvc}, fs@{rev})"
+    tmp_dir.gen("dir/file", "file content modified")
 
-    rev = next(brancher)
-    assert rev == scm.get_rev()
-    assert repr(Index(dvc)) == f"Index({dvc}, fs@{rev[:7]})"
+    assert stage.outs[0].changed_cache()
 
+    with pytest.raises(StageCommitError):
+        dvc.commit(stage.path)
 
-def test_filter_index(tmp_dir, dvc, run_copy):
-    tmp_dir.dvc_gen("foo", "foo")
-    stage2 = run_copy("foo", "bar", name="copy-foo-bar")
+    assert stage.outs[0].changed_cache()
 
-    def filter_pipeline(stage):
-        return bool(stage.cmd)
+    dvc.commit(stage.path, force=True)
+    assert dvc.status([stage.path]) == {}
 
-    filtered_index = Index(dvc).filter(filter_pipeline)
-    assert list(filtered_index) == [stage2]
+
+def test_commit_preserve_fields(tmp_dir, dvc):
+    text = textwrap.dedent(
+        """\
+        # top comment
+        desc: top desc
+        outs:
+        - path: foo # out comment
+          desc: out desc
+          type: mytype
+          labels:
+          - label1
+          - label2
+          meta:
+            key1: value1
+            key2: value2
+          remote: testremote
+        meta: some metadata
+    """
+    )
+    tmp_dir.gen("foo.dvc", text)
+    tmp_dir.dvc_gen("foo", "foo", commit=False)
+    dvc.commit("foo")
+    assert (tmp_dir / "foo.dvc").read_text() == textwrap.dedent(
+        """\
+        # top comment
+        desc: top desc
+        outs:
+        - path: foo # out comment
+          desc: out desc
+          type: mytype
+          labels:
+          - label1
+          - label2
+          meta:
+            key1: value1
+            key2: value2
+          remote: testremote
+          md5: acbd18db4cc2f85cedef654fccc4a4d8
+          size: 3
+        meta: some metadata
+    """
+    )
 
 
-def test_slice_index(tmp_dir, dvc):
-    tmp_dir.gen({"dir1": {"foo": "foo"}, "dir2": {"bar": "bar"}})
-    with (tmp_dir / "dir1").chdir():
-        (stage1,) = dvc.add("foo")
-    with (tmp_dir / "dir2").chdir():
-        (stage2,) = dvc.add("bar")
+@pytest.mark.parametrize("run_kw", [{"single_stage": True}, {"name": "copy"}])
+def test_commit_with_deps(tmp_dir, dvc, run_copy, run_kw):
+    tmp_dir.gen("foo", "foo")
+    (foo_stage,) = dvc.add("foo", no_commit=True)
+    assert foo_stage is not None
+    assert len(foo_stage.outs) == 1
 
-    index = Index(dvc)
+    stage = run_copy("foo", "file", no_commit=True, **run_kw)
+    assert stage is not None
+    assert len(stage.outs) == 1
 
-    sliced = index.slice("dir1")
-    assert set(sliced) == {stage1}
-    assert sliced.stages is not index.stages  # sanity check
+    assert foo_stage.outs[0].changed_cache()
+    assert stage.outs[0].changed_cache()
 
-    sliced = index.slice(tmp_dir / "dir1")
-    assert set(sliced) == {stage1}
+    dvc.commit(stage.path, with_deps=True)
+    assert not foo_stage.outs[0].changed_cache()
+    assert not stage.outs[0].changed_cache()
 
-    sliced = index.slice("dir2")
-    assert set(sliced) == {stage2}
 
-    with (tmp_dir / "dir1").chdir():
-        sliced = index.slice(relpath(tmp_dir / "dir2"))
-        assert set(sliced) == {stage2}
+def test_commit_changed_md5(tmp_dir, dvc):
+    tmp_dir.gen({"file": "file content"})
+    (stage,) = dvc.add("file", no_commit=True)
 
+    stage_file_content = (tmp_dir / stage.path).parse()
+    stage_file_content["md5"] = "1111111111"
+    (tmp_dir / stage.path).dump(stage_file_content)
 
-def outputs_equal(actual, expected):
-    actual, expected = list(actual), list(expected)
+    with pytest.raises(StageCommitError):
+        dvc.commit(stage.path)
 
-    def sort_fn(output):
-        return output.fspath
+    dvc.commit(stage.path, force=True)
+    assert "md5" not in (tmp_dir / stage.path).parse()
 
-    assert len(actual) == len(expected)
-    pairs = zip(sorted(actual, key=sort_fn), sorted(expected, key=sort_fn))
-    assert all(actual.fspath == expected.fspath for actual, expected in pairs)
-    return True
 
+def test_commit_no_exec(tmp_dir, dvc):
+    tmp_dir.gen({"dep": "dep", "out": "out"})
+    stage = dvc.run(name="my", cmd="mycmd", deps=["dep"], outs=["out"], no_exec=True)
 
-def test_deps_outs_getters(tmp_dir, dvc, run_copy_metrics):
-    (foo_stage,) = tmp_dir.dvc_gen({"foo": "foo"})
-    tmp_dir.gen({"params.yaml": "param: 100\n"})
-    tmp_dir.gen({"m_temp.yaml": str(5)})
+    assert dvc.status(stage.path)
+    dvc.commit(stage.path, force=True)
+    assert dvc.status(stage.path) == {}
 
-    run_stage1 = run_copy_metrics(
-        "m_temp.yaml",
-        "m.yaml",
-        metrics=["m.yaml"],
-        params=["param"],
-        name="copy-metrics",
+
+def test_commit_granular_output(tmp_dir, dvc):
+    dvc.run(
+        name="mystage",
+        cmd=["echo foo>foo", "echo bar>bar"],
+        outs=["foo", "bar"],
+        no_commit=True,
     )
-    (tmp_dir / "metric_t.json").dump_json(
-        [{"a": 1, "b": 2}, {"a": 2, "b": 3}], sort_keys=True
+
+    cache = tmp_dir / ".dvc" / "cache"
+    assert not list(cache.glob("*/*"))
+
+    dvc.commit("foo")
+    assert list(cache.glob("*/*")) == [cache / "d3" / "b07384d113edec49eaa6238ad5ff00"]
+
+
+def test_commit_granular_output_file(tmp_dir, dvc):
+    tmp_dir.gen("foo", "foo")
+    dvc.add("foo", no_commit=True)
+    dvc.commit("foo")
+    assert dvc.status() == {}
+
+
+def test_commit_granular_output_dir(tmp_dir, dvc):
+    tmp_dir.gen(
+        {
+            "data": {
+                "foo": "foo",
+                "bar": "bar",
+                "subdir": {"subfoo": "subfoo", "subbar": "subbar"},
+            }
+        }
     )
-    run_stage2 = run_copy_metrics(
-        "metric_t.json",
-        "metric.json",
-        plots_no_cache=["metric.json"],
-        name="copy-metrics2",
+    dvc.add("data", no_commit=True)
+    dvc.commit("data")
+    assert dvc.status() == {}
+
+
+def test_commit_granular_dir(tmp_dir, dvc):
+    tmp_dir.gen(
+        {
+            "data": {
+                "foo": "foo",
+                "bar": "bar",
+                "subdir": {"subfoo": "subfoo", "subbar": "subbar"},
+            }
+        }
     )
+    dvc.add("data", no_commit=True)
 
-    index = Index(dvc)
+    cache = tmp_dir / ".dvc" / "cache"
 
-    stages = [foo_stage, run_stage1, run_stage2]
-    (metrics,) = run_stage1.outs
-    _, params = run_stage1.deps
-    (plots,) = run_stage2.outs
-
-    expected_outs = chain.from_iterable([stage.outs for stage in stages])
-    expected_deps = chain.from_iterable([stage.deps for stage in stages])
-
-    assert outputs_equal(index.outs, expected_outs)
-    assert outputs_equal(index.deps, expected_deps)
-    assert outputs_equal(index.decorated_outs, [metrics, plots])
-    assert outputs_equal(index.metrics, [metrics])
-    assert outputs_equal(index.plots, [plots])
-    assert outputs_equal(index.params, [params])
-
-
-def test_add_update(dvc):
-    """Test that add/update overwrites existing stages with the new ones.
-
-    The old stages and the new ones might have same hash, so we are
-    making sure that the old stages were removed and replaced by new ones
-    using `id`/`is` checks.
-    """
-    index = Index(dvc)
-    new_stage = Stage(dvc, path="path1")
-    new_index = index.add(new_stage)
-
-    assert not index.stages
-    assert new_index.stages == [new_stage]
-
-    dup_stage1 = Stage(dvc, path="path1")
-    dup_stage2 = Stage(dvc, path="path2")
-    dup_index = index.update([dup_stage1, dup_stage2])
-    assert not index.stages
-    assert len(new_index) == 1
-    assert new_index.stages[0] is new_stage
-    assert set(map(id, dup_index.stages)) == {id(dup_stage1), id(dup_stage2)}
-
-
-def assert_index_equal(first, second, strict=True, ordered=True):
-    assert len(first) == len(second), "Index have different no. of stages"
-    assert set(first) == set(second), "Index does not have same stages"
-    if ordered:
-        assert list(first) == list(
-            second
-        ), "Index does not have same sequence of stages"
-    if strict:
-        assert set(map(id, first)) == set(
-            map(id, second)
-        ), "Index is not strictly equal"
-
-
-def test_discard_remove(dvc):
-    stage = Stage(dvc, path="path1")
-    index = Index(dvc, stages=[stage])
-
-    assert list(index.discard(Stage(dvc, "path2"))) == list(index)
-    new_index = index.discard(stage)
-    assert len(new_index) == 0
-
-    with pytest.raises(ValueError):
-        index.remove(Stage(dvc, "path2"))
-    assert index.stages == [stage]
-    assert list(index.remove(stage)) == []
-
-
-def test_difference(dvc):
-    stages = [Stage(dvc, path=f"path{i}") for i in range(10)]
-    index = Index(dvc, stages=stages)
-
-    new_index = index.difference([*stages[:5], Stage(dvc, path="path100")])
-    assert index.stages == stages
-    assert set(new_index) == set(stages[5:])
-
-
-def test_dumpd(dvc):
-    stages = [
-        PipelineStage(dvc, "dvc.yaml", name="stage1"),
-        Stage(dvc, "path"),
-    ]
-    index = Index(dvc, stages=stages)
-    assert index.dumpd() == {"dvc.yaml:stage1": {}, "path": {}}
-    assert index.identifier == "d43da84e9001540c26abf2bf4541c275"
-
-
-def test_unique_identifier(tmp_dir, dvc, scm, run_copy):
-    dvc.scm_context.autostage = True
-    tmp_dir.dvc_gen("foo", "foo")
-    run_copy("foo", "bar", name="copy-foo-bar")
-
-    revs = []
-    n_commits = 5
-    for i in range(n_commits):
-        # create a few empty commits
-        scm.commit(f"commit {i}")
-        revs.append(scm.get_rev())
-    assert len(set(revs)) == n_commits  # the commit revs should be unique
-
-    ids = []
-    for _ in dvc.brancher(revs=revs):
-        index = Index(dvc)
-        assert index.stages
-        ids.append(index.identifier)
-
-    # we get "workspace" as well from the brancher by default
-    assert len(revs) + 1 == len(ids)
-    possible_ids = {
-        True: "2ba7c7c5b395d4211348d6274b869fc7",
-        False: "8406970ad2fcafaa84d9310330a67576",
+    assert set(cache.glob("*/*")) == set()
+
+    dvc.commit(os.path.join("data", "foo"))
+    assert set(cache.glob("*/*")) == {
+        cache / "1a" / "ca2c799df82929bbdd976557975546.dir",
+        cache / "ac" / "bd18db4cc2f85cedef654fccc4a4d8",
+    }
+
+    dvc.commit(os.path.join("data", "subdir"))
+    assert set(cache.glob("*/*")) == {
+        cache / "1a" / "ca2c799df82929bbdd976557975546.dir",
+        cache / "ac" / "bd18db4cc2f85cedef654fccc4a4d8",
+        cache / "4c" / "e8d2a2cf314a52fa7f315ca37ca445",
+        cache / "68" / "dde2c3c4e7953c2290f176bbdc9a54",
+    }
+
+    dvc.commit(os.path.join("data"))
+    assert set(cache.glob("*/*")) == {
+        cache / "1a" / "ca2c799df82929bbdd976557975546.dir",
+        cache / "ac" / "bd18db4cc2f85cedef654fccc4a4d8",
+        cache / "4c" / "e8d2a2cf314a52fa7f315ca37ca445",
+        cache / "68" / "dde2c3c4e7953c2290f176bbdc9a54",
+        cache / "37" / "b51d194a7513e45b56f6524f2d51f2",
     }
-    assert set(ids) == {possible_ids[os.name == "posix"]}
 
 
-def test_skip_graph_checks(dvc, mocker):
-    # See https://github.com/iterative/dvc/issues/2671 for more info
-    mock_build_graph = mocker.spy(Index.graph, "fget")
-
-    # sanity check
-    Index(dvc).check_graph()
-    assert mock_build_graph.called
-    mock_build_graph.reset_mock()
-
-    # check that our hack can be enabled
-    dvc._skip_graph_checks = True
-    Index(dvc).check_graph()
-    assert not mock_build_graph.called
-    mock_build_graph.reset_mock()
-
-    # check that our hack can be disabled
-    dvc._skip_graph_checks = False
-    Index(dvc).check_graph()
-    assert mock_build_graph.called
-
-
-def get_index(dvc, rev):
-    if rev:
-        brancher = dvc.brancher(revs=[rev])
-        if rev != "workspace":
-            assert next(brancher) == "workspace"
-        next(brancher)
-    return Index(dvc)
-
-
-@pytest.mark.parametrize("rev", ["workspace", "HEAD"])
-def test_used_objs(tmp_dir, scm, dvc, run_copy, rev):
-    from dvc.hash_info import HashInfo
-
-    dvc.config["core"]["autostage"] = True
-    tmp_dir.dvc_gen({"dir": {"subdir": {"file": "file"}}, "foo": "foo"})
-    run_copy("foo", "bar", name="copy-foo-bar")
-    scm.commit("commit")
-
-    index = get_index(dvc, rev)
-
-    expected_objs = [
-        HashInfo(
-            name="md5",
-            value="acbd18db4cc2f85cedef654fccc4a4d8",
-            obj_name="bar",
-        ),
-        HashInfo(
-            name="md5",
-            value="8c7dd922ad47494fc02c388e12c00eac",
-            obj_name="dir/subdir/file",
-        ),
-        HashInfo(
-            name="md5",
-            value="d28c9e28591aeb7e303dc6772ffa6f6b.dir",
-            obj_name="dir",
-        ),
-    ]
-
-    assert index.used_objs() == {None: set(expected_objs)}
-    assert index.used_objs("dir") == {None: set(expected_objs[1:])}
-    assert index.used_objs(".", recursive=True) == {None: set(expected_objs)}
-    assert index.used_objs("copy-foo-bar", with_deps=True) == {
-        None: {expected_objs[0]}
+def test_commit_no_exec_missing_dep(tmp_dir, dvc):
+    stage = dvc.run(name="my", cmd="mycmd", deps=["dep"], outs=["out"], no_exec=True)
+    assert dvc.status(stage.path)
+
+    with pytest.raises(DependencyDoesNotExistError):
+        dvc.commit(stage.path, force=True)
+
+
+def test_commit_no_exec_missing_out(tmp_dir, dvc):
+    stage = dvc.run(name="my", cmd="mycmd", outs=["out"], no_exec=True)
+    assert dvc.status(stage.path)
+
+    with pytest.raises(OutputDoesNotExistError):
+        dvc.commit(stage.path, force=True)
+
+
+def test_commit_pipeline_stage(tmp_dir, dvc, run_copy):
+    tmp_dir.gen("foo", "foo")
+    stage = run_copy("foo", "bar", no_commit=True, name="copy-foo-bar")
+    assert dvc.status(stage.addressing)
+    assert dvc.commit(stage.addressing, force=True) == [stage]
+    assert not dvc.status(stage.addressing)
+
+    # just to confirm different variants work
+    assert dvc.commit(f":{stage.addressing}") == [stage]
+    assert dvc.commit(f"{PROJECT_FILE}:{stage.addressing}") == [stage]
+    assert dvc.commit(PROJECT_FILE) == [stage]
+
+
+def test_imported_entries_unchanged(tmp_dir, dvc, erepo_dir):
+    with erepo_dir.chdir():
+        erepo_dir.dvc_gen("file", "file content", "initial commit")
+
+    stage = dvc.imp(os.fspath(erepo_dir), "file")
+
+    assert stage.changed_entries() == ([], [], None)
+
+
+def test_commit_updates_to_cloud_versioning_dir(tmp_dir, dvc):
+    data_dvc = tmp_dir / "data.dvc"
+    data_dvc.dump(
+        {
+            "outs": [
+                {
+                    "path": "data",
+                    "files": [
+                        {
+                            "size": 3,
+                            "version_id": "WYRG4BglP7pD.gEoJP6a4AqOhl.FRA.h",
+                            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
+                            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
+                            "relpath": "bar",
+                        },
+                        {
+                            "size": 3,
+                            "version_id": "0vL53tFVY5vVAoJ4HG2jCS1mEcohDPE0",
+                            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
+                            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
+                            "relpath": "foo",
+                        },
+                    ],
+                }
+            ]
+        }
+    )
+
+    data = tmp_dir / "data"
+    data.mkdir()
+    (data / "foo").write_text("foo")
+    (data / "bar").write_text("bar2")
+
+    dvc.commit("data", force=True)
+
+    assert (tmp_dir / "data.dvc").parse() == {
+        "outs": [
+            {
+                "path": "data",
+                "files": [
+                    {
+                        "size": 4,
+                        "md5": "224e2539f52203eb33728acd228b4432",
+                        "relpath": "bar",
+                    },
+                    {
+                        "size": 3,
+                        "version_id": "0vL53tFVY5vVAoJ4HG2jCS1mEcohDPE0",
+                        "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
+                        "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
+                        "relpath": "foo",
+                    },
+                ],
+            }
+        ]
     }
```

### Comparing `dvc-2.9.5/tests/func/test_repro.py` & `dvc-3.0.0a0/tests/func/test_repro.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,121 +1,115 @@
 import filecmp
 import os
 import re
 import shutil
 from pathlib import Path
-from unittest.mock import patch
 
 import pytest
 
 from dvc.cli import main
-from dvc.dvcfile import DVC_FILE, Dvcfile
-from dvc.exceptions import (
-    CyclicGraphError,
-    ReproductionError,
-    StagePathAsOutputError,
-)
-from dvc.fs.local import LocalFileSystem
+from dvc.dvcfile import DVC_FILE, load_file
+from dvc.exceptions import CyclicGraphError, ReproductionError, StagePathAsOutputError
+from dvc.fs import LocalFileSystem, system
 from dvc.output import Output
 from dvc.stage import Stage
 from dvc.stage.exceptions import StageFileDoesNotExistError
-from dvc.system import System
-from dvc.utils import file_md5, relpath
+from dvc.utils import relpath
 from dvc.utils.fs import remove
 from dvc.utils.serialize import dump_yaml, load_yaml
-from tests.basic_env import TestDvc
-
+from dvc_data.hashfile.hash import file_md5
 
-class SingleStageRun:
-    def _run(self, **kwargs):
-        kwargs["single_stage"] = True
-        kwargs.pop("name", None)
-        return self.dvc.run(**kwargs)  # noqa, pylint: disable=no-member
-
-    @staticmethod
-    def _get_stage_target(stage):
-        return stage.addressing
-
-
-class TestRepro(SingleStageRun, TestDvc):
-    def setUp(self):
-        super().setUp()
-
-        stages = self.dvc.add(self.FOO)
-        self.assertEqual(len(stages), 1)
-        self.foo_stage = stages[0]
-        self.assertTrue(self.foo_stage is not None)
-
-        self.file1 = "file1"
-        self.file1_stage = self.file1 + ".dvc"
-        self.stage = self._run(
-            fname=self.file1_stage,
-            outs=[self.file1],
-            deps=[self.FOO, self.CODE],
-            cmd=f"python {self.CODE} {self.FOO} {self.file1}",
-            name="run1",
-        )
 
+@pytest.fixture(
+    params=(
+        {"single_stage": True},
+        {"single_stage": False},
+    ),
+    ids=["single_stage", "multi_stage"],
+)
+def run_stage(dvc, request):
+    def inner(*args, name=None, **kwargs):
+        assert name
+        # these should not be passed
+        assert "single_stage" not in kwargs
+        assert "fname" not in kwargs
+
+        if request.param["single_stage"]:
+            kwargs.update(
+                {
+                    "fname": name + ".dvc",
+                    "single_stage": True,
+                }
+            )
+        else:
+            kwargs["name"] = name
+        return dvc.run(*args, **kwargs)
+
+    return inner
 
-class TestReproFail(TestRepro):
-    def test(self):
-        os.unlink(self.CODE)
-
-        ret = main(["repro", self._get_stage_target(self.stage)])
-        self.assertNotEqual(ret, 0)
 
+def test_repro_fail(tmp_dir, run_stage, copy_script):
+    tmp_dir.dvc_gen("foo", "foo")
+    stage = run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    os.unlink("copy.py")
+    assert main(["repro", stage.addressing]) != 0
 
-class TestReproCyclicGraph(SingleStageRun, TestDvc):
-    def test(self):
-        self._run(
-            deps=[self.FOO],
-            outs=["bar.txt"],
-            cmd="echo bar > bar.txt",
-            name="copybarbar-txt",
-        )
 
-        self._run(
-            deps=["bar.txt"],
-            outs=["baz.txt"],
-            cmd="echo baz > baz.txt",
-            name="copybazbaz-txt",
-        )
+def test_repro_cyclic_graph(tmp_dir, dvc, run_stage):
+    tmp_dir.gen("foo", "foo")
+    run_stage(
+        deps=["foo"],
+        outs=["bar.txt"],
+        cmd="echo bar > bar.txt",
+        name="copybarbar-txt",
+    )
+    run_stage(
+        deps=["bar.txt"],
+        outs=["baz.txt"],
+        cmd="echo baz > baz.txt",
+        name="copybazbaz-txt",
+    )
 
-        stage_dump = {
-            "cmd": "echo baz > foo",
-            "deps": [{"path": "baz.txt"}],
-            "outs": [{"path": self.FOO}],
-        }
-        dump_yaml("cycle.dvc", stage_dump)
+    stage_dump = {
+        "cmd": "echo baz > foo",
+        "deps": [{"path": "baz.txt"}],
+        "outs": [{"path": "foo"}],
+    }
+    dump_yaml("cycle.dvc", stage_dump)
 
-        with self.assertRaises(CyclicGraphError):
-            self.dvc.reproduce("cycle.dvc")
+    with pytest.raises(CyclicGraphError):
+        dvc.reproduce("cycle.dvc")
 
 
-class TestReproWorkingDirectoryAsOutput(TestDvc):
+class TestReproWorkingDirectoryAsOutput:
     """
     |  stage.cwd  |  out.path | cwd as output |
     |:-----------:|:---------:|:-------------:|
     |     dir     |    dir    |      True     |
     | dir/subdir/ |    dir    |      True     |
     |     dir     |   dir-1   |     False     |
     |      .      | something |     False     |
     """
 
-    def test(self):
+    def test(self, dvc):
         # File structure:
         #       .
         #       |-- dir1
         #       |  |__ dir2.dvc         (out.path == ../dir2)
         #       |__ dir2
         #           |__ something.dvc    (stage.cwd == ./dir2)
 
-        os.mkdir(os.path.join(self.dvc.root_dir, "dir1"))
+        os.mkdir(os.path.join(dvc.root_dir, "dir1"))
 
-        self.dvc.run(
+        dvc.run(
             fname=os.path.join("dir1", "dir2.dvc"),
             wdir="dir1",
             outs=[os.path.join("..", "dir2")],
             cmd="mkdir {path}".format(path=os.path.join("..", "dir2")),
             single_stage=True,
         )
 
@@ -124,18 +118,18 @@
         output = os.path.join("..", "something")
         stage_dump = {
             "cmd": f"echo something > {output}",
             "outs": [{"path": output}],
         }
         dump_yaml(faulty_stage_path, stage_dump)
 
-        with self.assertRaises(StagePathAsOutputError):
-            self.dvc.reproduce(faulty_stage_path)
+        with pytest.raises(StagePathAsOutputError):
+            dvc.reproduce(faulty_stage_path)
 
-    def test_nested(self):
+    def test_nested(self, mocker, dvc):
         #       .
         #       |-- a
         #       |  |__ nested
         #       |     |__ dir
         #       |       |__ error.dvc     (stage.cwd == 'a/nested/dir')
         #       |__ b
         #          |__ nested.dvc         (stage.out == 'a/nested')
@@ -144,15 +138,15 @@
 
         os.mkdir(dir1)
         os.mkdir(dir2)
 
         nested_dir = os.path.join(dir2, "nested")
         out_dir = relpath(nested_dir, dir1)
 
-        nested_stage = self.dvc.run(
+        nested_stage = dvc.run(
             fname=os.path.join(dir1, "b.dvc"),
             wdir=dir1,
             outs=[out_dir],  # ../a/nested
             cmd=f"mkdir {out_dir}",
             single_stage=True,
         )
 
@@ -165,773 +159,738 @@
             "cmd": f"echo something > {output}",
             "outs": [{"path": output}],
         }
         dump_yaml(error_stage_path, stage_dump)
 
         # NOTE: os.walk() walks in a sorted order and we need dir2 subdirs to
         # be processed before dir1 to load error.dvc first.
-        self.dvc.index = self.dvc.index.update(
+        dvc.index = dvc.index.update(
             [
                 nested_stage,
-                Dvcfile(self.dvc, error_stage_path).stage,
+                load_file(dvc, error_stage_path).stage,
             ]
         )
 
-        with patch.object(self.dvc, "_reset"):  # to prevent `stages` resetting
-            with self.assertRaises(StagePathAsOutputError):
-                self.dvc.reproduce(error_stage_path)
+        mocker.patch.object(dvc, "_reset")  # to prevent `stages` resetting
+        with pytest.raises(StagePathAsOutputError):
+            dvc.reproduce(error_stage_path)
 
-    def test_similar_paths(self):
+    def test_similar_paths(self, dvc):
         # File structure:
         #
         #       .
         #       |-- something.dvc   (out.path == something)
         #       |-- something
         #       |__ something-1
         #          |-- a
         #          |__ a.dvc        (stage.cwd == something-1)
 
-        self.dvc.run(
-            outs=["something"], cmd="mkdir something", single_stage=True
-        )
+        dvc.run(outs=["something"], cmd="mkdir something", single_stage=True)
 
         os.mkdir("something-1")
 
         stage = os.path.join("something-1", "a.dvc")
 
         stage_dump = {"cmd": "echo a > a", "outs": [{"path": "a"}]}
         dump_yaml(stage, stage_dump)
 
-        try:
-            self.dvc.reproduce(stage)
-        except StagePathAsOutputError:
-            self.fail("should not raise StagePathAsOutputError")
-
-
-class TestReproDepUnderDir(SingleStageRun, TestDvc):
-    def test(self):
-        stages = self.dvc.add(self.DATA_DIR)
-        self.assertTrue(stages and stages[0] is not None)
-
-        file1 = "file1"
-        stage = self._run(
-            fname=file1 + ".dvc",
-            outs=[file1],
-            deps=[self.DATA, self.CODE],
-            cmd=f"python {self.CODE} {self.DATA} {file1}",
-            name="copy-data-file1",
-        )
+        dvc.reproduce(stage)
 
-        self.assertTrue(filecmp.cmp(file1, self.DATA, shallow=False))
 
-        os.unlink(self.DATA)
-        shutil.copyfile(self.FOO, self.DATA)
+def test_repro_dep_under_dir(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("foo", "foo")
+    tmp_dir.dvc_gen("data", {"file": "file", "sub": {"foo": "foo"}})
 
-        stages = self.dvc.reproduce(self._get_stage_target(stage))
-        self.assertEqual(len(stages), 2)
-        self.assertTrue(filecmp.cmp(file1, self.FOO, shallow=False))
-
-
-class TestReproDepDirWithOutputsUnderIt(SingleStageRun, TestDvc):
-    def test(self):
-        stages = self.dvc.add(self.DATA)
-        self.assertEqual(len(stages), 1)
-        self.assertTrue(stages[0] is not None)
-
-        stages = self.dvc.add(self.DATA_SUB)
-        self.assertEqual(len(stages), 1)
-        self.assertTrue(stages[0] is not None)
-
-        deps = [self.DATA, self.DATA_SUB]
-        stage = self.dvc.run(
-            cmd="ls {}".format(" ".join(deps)),
-            fname="dvcfile2.dvc",
-            deps=deps,
-            single_stage=True,
-        )
-        self.assertTrue(stage is not None)
+    stage = run_stage(
+        outs=["file1"],
+        deps=["data/file", "copy.py"],
+        cmd="python copy.py data/file file1",
+        name="copy-data-file1",
+    )
 
-        file1 = "file1"
-        file1_stage = file1 + ".dvc"
-        stage = self._run(
-            fname=file1_stage,
-            deps=[self.DATA_DIR],
-            outs=[file1],
-            cmd=f"python {self.CODE} {self.DATA} {file1}",
-            name="copy-data-file1",
-        )
-        self.assertTrue(stage is not None)
+    assert filecmp.cmp("file1", "data/file", shallow=False)
 
-        os.unlink(self.DATA)
-        shutil.copyfile(self.FOO, self.DATA)
+    os.unlink("data/file")
+    shutil.copyfile("foo", "data/file")
 
-        stages = self.dvc.reproduce(self._get_stage_target(stage))
-        self.assertEqual(len(stages), 2)
+    stages = dvc.reproduce(stage.addressing)
+    assert len(stages) == 2
+    assert filecmp.cmp("file1", "foo", shallow=False)
 
 
-class TestReproForce(TestRepro):
-    def test(self):
-        stages = self.dvc.reproduce(
-            self._get_stage_target(self.stage), force=True
-        )
-        self.assertEqual(len(stages), 2)
+def test_repro_dep_dir_with_outputs_under_it(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("foo", "foo")
+    file_stage, _ = tmp_dir.dvc_gen(
+        {"data/file": "file", "data/sub": {"foo": "foo", "bar": "bar"}}
+    )
+    run_stage(
+        cmd="ls data/file data/sub",
+        deps=["data/file", "data/sub"],
+        name="list-files",
+    )
+    copy_stage = run_stage(
+        deps=["data"],
+        outs=["file1"],
+        cmd="python copy.py data file1",
+        name="copy-data-file1",
+    )
+    os.unlink("data/file")
+    shutil.copyfile("foo", "data/file")
+    assert dvc.reproduce(copy_stage.addressing) == [file_stage, copy_stage]
 
 
-class TestReproChangedCode(TestRepro):
-    def test(self):
-        self.swap_code()
+def test_repro_force(tmp_dir, dvc, run_stage, copy_script):
+    tmp_dir.dvc_gen("foo", "foo")
+    stage = run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    stages = dvc.reproduce(stage.addressing, force=True)
+    assert len(stages) == 2
 
-        stages = self.dvc.reproduce(self._get_stage_target(self.stage))
 
-        self.assertTrue(filecmp.cmp(self.file1, self.BAR, shallow=False))
-        self.assertEqual(len(stages), 1)
+def test_repro_changed_code(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    stage = run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    with (tmp_dir / "copy.py").open("a+", encoding="utf8") as f:
+        f.write("\nshutil.copyfile('bar', sys.argv[2])")
+    stages = dvc.reproduce(stage.addressing)
+
+    assert filecmp.cmp("file1", "bar", shallow=False)
+    assert len(stages) == 1
+
+
+def test_repro_changed_data(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    stage = run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    shutil.copyfile("bar", "foo")
 
-    def swap_code(self):
-        os.unlink(self.CODE)
-        new_contents = self.CODE_CONTENTS
-        new_contents += "\nshutil.copyfile('{}', " "sys.argv[2])\n".format(
-            self.BAR
-        )
-        self.create(self.CODE, new_contents)
+    stages = dvc.reproduce(stage.addressing)
 
+    assert filecmp.cmp("file1", "bar", shallow=False)
+    assert len(stages) == 2
 
-class TestReproChangedData(TestRepro):
-    def test(self):
-        self.swap_foo_with_bar()
 
-        stages = self.dvc.reproduce(self._get_stage_target(self.stage))
+def test_repro_dry(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    stage = run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    shutil.copyfile("bar", "foo")
 
-        self.assertTrue(filecmp.cmp(self.file1, self.BAR, shallow=False))
-        self.assertEqual(len(stages), 2)
+    stages = dvc.reproduce(stage.addressing, dry=True)
 
-    def swap_foo_with_bar(self):
-        os.unlink(self.FOO)
-        shutil.copyfile(self.BAR, self.FOO)
+    assert len(stages), 2
+    assert not filecmp.cmp("file1", "bar", shallow=False)
 
+    ret = main(["repro", "--dry", stage.addressing])
+    assert ret == 0
+    assert not filecmp.cmp("file1", "bar", shallow=False)
+
+
+def test_repro_up_to_date(tmp_dir, copy_script, run_stage):
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    stage = run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    ret = main(["repro", stage.addressing])
+    assert ret == 0
 
-class TestReproDry(TestReproChangedData):
-    def test(self):
-        self.swap_foo_with_bar()
 
-        stages = self.dvc.reproduce(
-            self._get_stage_target(self.stage), dry=True
-        )
+def test_repro_dry_no_exec(tmp_dir, dvc):
+    deps = []
+    for d in range(3):
+        idir = f"idir{d}"
+        odir = f"odir{d}"
 
-        self.assertTrue(len(stages), 2)
-        self.assertFalse(filecmp.cmp(self.file1, self.BAR, shallow=False))
+        deps.append("-d")
+        deps.append(odir)
 
-        ret = main(["repro", "--dry", self._get_stage_target(self.stage)])
-        self.assertEqual(ret, 0)
-        self.assertFalse(filecmp.cmp(self.file1, self.BAR, shallow=False))
-
-
-class TestReproUpToDate(TestRepro):
-    def test(self):
-        ret = main(["repro", self._get_stage_target(self.stage)])
-        self.assertEqual(ret, 0)
-
-
-class TestReproDryNoExec(TestDvc):
-    def test(self):
-        deps = []
-        for d in range(3):
-            idir = f"idir{d}"
-            odir = f"odir{d}"
-
-            deps.append("-d")
-            deps.append(odir)
-
-            os.mkdir(idir)
-
-            f = os.path.join(idir, "file")
-            with open(f, "w+", encoding="utf-8") as fobj:
-                fobj.write(str(d))
-
-            ret = main(
-                [
-                    "run",
-                    "--no-exec",
-                    "--single-stage",
-                    "-d",
-                    idir,
-                    "-o",
-                    odir,
-                    "python -c 'import shutil; "
-                    'shutil.copytree("{}", "{}")\''.format(idir, odir),
-                ]
-            )
-            self.assertEqual(ret, 0)
+        os.mkdir(idir)
+
+        f = os.path.join(idir, "file")
+        with open(f, "w+", encoding="utf-8") as fobj:
+            fobj.write(str(d))
 
         ret = main(
             [
                 "run",
                 "--no-exec",
                 "--single-stage",
-                "--file",
-                DVC_FILE,
-                *deps,
-                "ls {}".format(
-                    " ".join(dep for i, dep in enumerate(deps) if i % 2)
+                "-d",
+                idir,
+                "-o",
+                odir,
+                'python -c \'import shutil; shutil.copytree("{}", "{}")\''.format(
+                    idir, odir
                 ),
             ]
         )
-        self.assertEqual(ret, 0)
-
-        ret = main(["repro", "--dry", DVC_FILE])
-        self.assertEqual(ret, 0)
-
+        assert ret == 0
 
-class TestReproChangedDeepData(TestReproChangedData):
-    def setUp(self):
-        super().setUp()
-
-        self.file2 = "file2"
-        self.stage = self._run(
-            fname=self.file2 + ".dvc",
-            outs=[self.file2],
-            deps=[self.file1, self.CODE],
-            cmd=f"python {self.CODE} {self.file1} {self.file2}",
-            name="copy-file-file2",
-        )
-
-    def test(self):
-        self.swap_foo_with_bar()
-
-        stages = self.dvc.reproduce(self._get_stage_target(self.stage))
-
-        self.assertTrue(filecmp.cmp(self.file1, self.BAR, shallow=False))
-        self.assertTrue(filecmp.cmp(self.file2, self.BAR, shallow=False))
-        self.assertEqual(len(stages), 3)
-
-
-class TestReproForceDownstream(TestDvc):
-    def test(self):
-        stages = self.dvc.add(self.FOO)
-        self.assertEqual(len(stages), 1)
-        foo_stage = stages[0]
-        self.assertTrue(foo_stage is not None)
-
-        code1 = "code1.py"
-        shutil.copyfile(self.CODE, code1)
-        file1 = "file1"
-        file1_stage = self.dvc.run(
-            outs=[file1],
-            deps=[self.FOO, code1],
-            cmd=f"python {code1} {self.FOO} {file1}",
-            single_stage=True,
-        )
-        self.assertTrue(file1_stage is not None)
-
-        code2 = "code2.py"
-        shutil.copyfile(self.CODE, code2)
-        file2 = "file2"
-        file2_stage = self.dvc.run(
-            outs=[file2],
-            deps=[file1, code2],
-            cmd=f"python {code2} {file1} {file2}",
-            single_stage=True,
-        )
-        self.assertTrue(file2_stage is not None)
+    ret = main(
+        [
+            "run",
+            "--no-exec",
+            "--single-stage",
+            "--file",
+            DVC_FILE,
+            *deps,
+            "ls {}".format(" ".join(dep for i, dep in enumerate(deps) if i % 2)),
+        ]
+    )
+    assert ret == 0
 
-        code3 = "code3.py"
-        shutil.copyfile(self.CODE, code3)
-        file3 = "file3"
-        file3_stage = self.dvc.run(
-            outs=[file3],
-            deps=[file2, code3],
-            cmd=f"python {code3} {file2} {file3}",
-            single_stage=True,
-        )
-        self.assertTrue(file3_stage is not None)
+    ret = main(["repro", "--dry", DVC_FILE])
+    assert ret == 0
 
-        with open(code2, "a", encoding="utf-8") as fobj:
-            fobj.write("\n\n")
 
-        stages = self.dvc.reproduce(file3_stage.path, force_downstream=True)
-        self.assertEqual(len(stages), 2)
-        self.assertEqual(stages[0].path, file2_stage.path)
-        self.assertEqual(stages[1].path, file3_stage.path)
+def test_repro_changed_deep_data(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    file2_stage = run_stage(
+        outs=["file2"],
+        deps=["file1", "copy.py"],
+        cmd="python copy.py file1 file2",
+        name="copy-file-file2",
+    )
+    shutil.copyfile("bar", "foo")
+    stages = dvc.reproduce(file2_stage.addressing)
+    assert filecmp.cmp("file1", "bar", shallow=False)
+    assert filecmp.cmp("file2", "bar", shallow=False)
+    assert len(stages) == 3
 
 
-class TestReproPipeline(TestReproChangedDeepData):
-    def test(self):
-        stages = self.dvc.reproduce(
-            self._get_stage_target(self.stage), force=True, pipeline=True
-        )
-        self.assertEqual(len(stages), 3)
+def test_repro_force_downstream(tmp_dir, dvc, copy_script):
+    tmp_dir.gen("foo", "foo")
+    stages = dvc.add("foo")
+    assert len(stages) == 1
+    foo_stage = stages[0]
+    assert foo_stage is not None
+
+    shutil.copyfile("copy.py", "copy1.py")
+    file1 = "file1"
+    file1_stage = dvc.run(
+        outs=[file1],
+        deps=["foo", "copy1.py"],
+        cmd=f"python copy1.py foo {file1}",
+        single_stage=True,
+    )
+    assert file1_stage is not None
 
-    def test_cli(self):
-        ret = main(
-            ["repro", "--pipeline", "-f", self._get_stage_target(self.stage)]
-        )
-        self.assertEqual(ret, 0)
+    shutil.copyfile("copy.py", "copy2.py")
+    file2 = "file2"
+    file2_stage = dvc.run(
+        outs=[file2],
+        deps=[file1, "copy2.py"],
+        cmd=f"python copy2.py {file1} {file2}",
+        single_stage=True,
+    )
+    assert file2_stage is not None
 
+    shutil.copyfile("copy.py", "copy3.py")
+    file3 = "file3"
+    file3_stage = dvc.run(
+        outs=[file3],
+        deps=[file2, "copy3.py"],
+        cmd=f"python copy3.py {file2} {file3}",
+        single_stage=True,
+    )
+    assert file3_stage is not None
 
-class TestReproPipelines(SingleStageRun, TestDvc):
-    def setUp(self):
-        super().setUp()
-
-        stages = self.dvc.add(self.FOO)
-        self.assertEqual(len(stages), 1)
-        self.foo_stage = stages[0]
-        self.assertTrue(self.foo_stage is not None)
-
-        stages = self.dvc.add(self.BAR)
-        self.assertEqual(len(stages), 1)
-        self.bar_stage = stages[0]
-        self.assertTrue(self.bar_stage is not None)
-
-        self.file1 = "file1"
-        self.file1_stage = self.dvc.run(
-            fname=self.file1 + ".dvc",
-            outs=[self.file1],
-            deps=[self.FOO, self.CODE],
-            cmd=f"python {self.CODE} {self.FOO} {self.file1}",
-            single_stage=True,
-        )
+    with open("copy2.py", "a", encoding="utf-8") as fobj:
+        fobj.write("\n\n")
 
-        self.file2 = "file2"
-        self.file2_stage = self._run(
-            fname=self.file2 + ".dvc",
-            outs=[self.file2],
-            deps=[self.BAR, self.CODE],
-            cmd=f"python {self.CODE} {self.BAR} {self.file2}",
-            name="copy-BAR-file2",
-        )
+    stages = dvc.reproduce(file3_stage.path, force_downstream=True)
+    assert len(stages) == 2
+    assert stages[0].path == file2_stage.path
+    assert stages[1].path == file3_stage.path
+
+
+def test_repro_pipeline(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    stage = run_stage(
+        outs=["file2"],
+        deps=["file1", "copy.py"],
+        cmd="python copy.py file1 file2",
+        name="copy-file-file2",
+    )
+    stages = dvc.reproduce(stage.addressing, force=True, pipeline=True)
+    assert len(stages) == 3
 
-    def test(self):
-        stages = self.dvc.reproduce(all_pipelines=True, force=True)
-        self.assertEqual(len(stages), 4)
-        self.assertTrue(self.file1_stage in stages)
-        self.assertTrue(self.file2_stage in stages)
-
-    def test_cli(self):
-        ret = main(["repro", "-f", "-P"])
-        self.assertEqual(ret, 0)
-
-
-class TestReproFrozen(TestReproChangedData):
-    def test(self):
-        file2 = "file2"
-        file2_stage = self._run(
-            fname=file2 + ".dvc",
-            outs=[file2],
-            deps=[self.file1, self.CODE],
-            cmd=f"python {self.CODE} {self.file1} {file2}",
-            name="copy-file1-file2",
-        )
 
-        self.swap_foo_with_bar()
+def test_repro_pipeline_cli(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    stage = run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    ret = main(["repro", "--pipeline", "-f", stage.addressing])
+    assert ret == 0
 
-        ret = main(["freeze", self._get_stage_target(file2_stage)])
-        self.assertEqual(ret, 0)
-        stages = self.dvc.reproduce(self._get_stage_target(file2_stage))
-        self.assertEqual(len(stages), 0)
-
-        ret = main(["unfreeze", self._get_stage_target(file2_stage)])
-        self.assertEqual(ret, 0)
-        stages = self.dvc.reproduce(self._get_stage_target(file2_stage))
-        self.assertTrue(filecmp.cmp(self.file1, self.BAR, shallow=False))
-        self.assertTrue(filecmp.cmp(file2, self.BAR, shallow=False))
-        self.assertEqual(len(stages), 3)
-
-    def test_non_existing(self):
-        with self.assertRaises(StageFileDoesNotExistError):
-            self.dvc.freeze("Dvcfile")
-            self.dvc.freeze("pipelines.yaml")
-            self.dvc.freeze("pipelines.yaml:name")
-            self.dvc.freeze("Dvcfile:name")
-            self.dvc.freeze("stage.dvc")
-            self.dvc.freeze("stage.dvc:name")
-            self.dvc.freeze("not-existing-stage.json")
-
-        ret = main(["freeze", "non-existing-stage"])
-        self.assertNotEqual(ret, 0)
-
-
-class TestReproFrozenCallback(SingleStageRun, TestDvc):
-    def test(self):
-        file1 = "file1"
-        file1_stage = file1 + ".dvc"
-        # NOTE: purposefully not specifying deps or outs
-        # to create a callback stage.
-        stage = self._run(
-            fname=file1_stage,
-            cmd=f"python {self.CODE} {self.FOO} {file1}",
-            name="copy-FOO-file1",
-        )
-        self.assertTrue(stage is not None)
 
-        stages = self.dvc.reproduce(self._get_stage_target(stage))
-        self.assertEqual(len(stages), 1)
+def test_repro_pipelines(tmp_dir, dvc, copy_script, run_stage):
+    foo_stage, bar_stage = tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"})
+    file1_stage = run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="copy-FOO-file1",
+    )
+    file2_stage = run_stage(
+        outs=["file2"],
+        deps=["bar", "copy.py"],
+        cmd="python copy.py bar file2",
+        name="copy-BAR-file2",
+    )
+    assert set(dvc.reproduce(all_pipelines=True, force=True)) == {
+        foo_stage,
+        bar_stage,
+        file1_stage,
+        file2_stage,
+    }
 
-        self.dvc.freeze(self._get_stage_target(stage))
-        stages = self.dvc.reproduce(self._get_stage_target(stage))
-        self.assertEqual(len(stages), 0)
-
-        self.dvc.unfreeze(self._get_stage_target(stage))
-        stages = self.dvc.reproduce(self._get_stage_target(stage))
-        self.assertEqual(len(stages), 1)
-
-
-class TestReproFrozenUnchanged(TestRepro):
-    def test(self):
-        """
-        Check that freezing/unfreezing doesn't affect stage state
-        """
-        target = self._get_stage_target(self.stage)
-        self.dvc.freeze(target)
-        stages = self.dvc.reproduce(target)
-        self.assertEqual(len(stages), 0)
-
-        self.dvc.unfreeze(target)
-        stages = self.dvc.reproduce(target)
-        self.assertEqual(len(stages), 0)
-
-
-class TestReproMetricsAddUnchanged(TestDvc):
-    def test(self):
-        """
-        Check that adding/removing metrics doesn't affect stage state
-        """
-        stages = self.dvc.add(self.FOO)
-        self.assertEqual(len(stages), 1)
-        self.assertTrue(stages[0] is not None)
-
-        file1 = "file1"
-        file1_stage = file1 + ".dvc"
-        self.dvc.run(
-            fname=file1_stage,
-            outs_no_cache=[file1],
-            deps=[self.FOO, self.CODE],
-            cmd=f"python {self.CODE} {self.FOO} {file1}",
-            single_stage=True,
-        )
 
-        stages = self.dvc.reproduce(file1_stage)
-        self.assertEqual(len(stages), 0)
+def test_repro_pipelines_cli(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"})
+    run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="copy-FOO-file1",
+    )
+    run_stage(
+        outs=["file2"],
+        deps=["bar", "copy.py"],
+        cmd="python copy.py bar file2",
+        name="copy-BAR-file2",
+    )
+    assert main(["repro", "-f", "-P"]) == 0
 
-        d = load_yaml(file1_stage)
-        d["outs"][0]["metric"] = True
-        dump_yaml(file1_stage, d)
 
-        stages = self.dvc.reproduce(file1_stage)
-        self.assertEqual(len(stages), 0)
+def test_repro_frozen(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    file2_stage = run_stage(
+        outs=["file2"],
+        deps=["file1", "copy.py"],
+        cmd="python copy.py file1 file2",
+        name="copy-file1-file2",
+    )
 
-        d = load_yaml(file1_stage)
-        d["outs"][0]["metric"] = False
-        dump_yaml(file1_stage, d)
+    shutil.copyfile("bar", "foo")
 
-        stages = self.dvc.reproduce(file1_stage)
-        self.assertEqual(len(stages), 0)
+    ret = main(["freeze", file2_stage.addressing])
+    assert ret == 0
+    stages = dvc.reproduce(file2_stage.addressing)
+    assert len(stages) == 0
+
+    ret = main(["unfreeze", file2_stage.addressing])
+    assert ret == 0
+    stages = dvc.reproduce(file2_stage.addressing)
+    assert filecmp.cmp("file1", "bar", shallow=False)
+    assert filecmp.cmp("file2", "bar", shallow=False)
+    assert len(stages) == 3
 
 
-class TestReproPhony(TestReproChangedData):
-    def test(self):
-        stage = self._run(
-            cmd="cat " + self.file1, deps=[self.file1], name="no_cmd"
-        )
+@pytest.mark.parametrize(
+    "target",
+    [
+        "Dvcfile",
+        "pipelines.yaml",
+        "pipelines.yaml:name",
+        "Dvcfile:name",
+        "stage.dvc",
+        "stage.dvc:name",
+        "not-existing-stage.json",
+    ],
+)
+def test_freeze_non_existing(dvc, target):
+    with pytest.raises(StageFileDoesNotExistError):
+        dvc.freeze(target)
 
-        self.swap_foo_with_bar()
+    ret = main(["freeze", target])
+    assert ret != 0
 
-        self.dvc.reproduce(self._get_stage_target(stage))
 
-        self.assertTrue(filecmp.cmp(self.file1, self.BAR, shallow=False))
+def test_repro_frozen_callback(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("foo", "foo")
+    # NOTE: purposefully not specifying deps or outs
+    # to create a callback stage.
+    stage = run_stage(
+        cmd="python copy.py foo file1",
+        name="copy-FOO-file1",
+    )
 
+    stages = dvc.reproduce(stage.addressing)
+    assert len(stages) == 1
 
-class TestNonExistingOutput(TestRepro):
-    def test(self):
-        os.unlink(self.FOO)
+    dvc.freeze(stage.addressing)
+    stages = dvc.reproduce(stage.addressing)
+    assert len(stages) == 0
 
-        with self.assertRaises(ReproductionError):
-            self.dvc.reproduce(self._get_stage_target(self.stage))
+    dvc.unfreeze(stage.addressing)
+    stages = dvc.reproduce(stage.addressing)
+    assert len(stages) == 1
 
 
-class TestReproDataSource(TestReproChangedData):
-    def test(self):
-        self.swap_foo_with_bar()
+def test_repro_frozen_unchanged(tmp_dir, dvc, copy_script, run_stage):
+    """
+    Check that freezing/unfreezing doesn't affect stage state
+    """
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    stage = run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    target = stage.addressing
+    dvc.freeze(target)
+    stages = dvc.reproduce(target)
+    assert len(stages) == 0
 
-        stages = self.dvc.reproduce(self.foo_stage.path)
+    dvc.unfreeze(target)
+    stages = dvc.reproduce(target)
+    assert len(stages) == 0
 
-        self.assertTrue(filecmp.cmp(self.FOO, self.BAR, shallow=False))
-        self.assertEqual(
-            stages[0].outs[0].hash_info.value, file_md5(self.BAR, self.dvc.fs)
-        )
 
+def test_repro_metrics_add_unchanged(tmp_dir, dvc, copy_script):
+    """
+    Check that adding/removing metrics doesn't affect stage state
+    """
+    tmp_dir.gen("foo", "foo")
+    stages = dvc.add("foo")
+    assert len(stages) == 1
+    assert stages[0] is not None
+
+    file1 = "file1"
+    file1_stage = file1 + ".dvc"
+    dvc.run(
+        fname=file1_stage,
+        outs_no_cache=[file1],
+        deps=["foo", "copy.py"],
+        cmd=f"python copy.py foo {file1}",
+        single_stage=True,
+    )
 
-class TestReproChangedDir(SingleStageRun, TestDvc):
-    def test(self):
-        file_name = "file"
-        shutil.copyfile(self.FOO, file_name)
-
-        dir_name = "dir"
-        dir_code = "dir.py"
-        code = (
-            'import os; import shutil; os.mkdir("{}"); '
-            'shutil.copyfile("{}", os.path.join("{}", "{}"))'
-        )
+    stages = dvc.reproduce(file1_stage)
+    assert len(stages) == 0
 
-        with open(dir_code, "w+", encoding="utf-8") as fd:
-            fd.write(code.format(dir_name, file_name, dir_name, file_name))
+    d = load_yaml(file1_stage)
+    d["outs"][0]["metric"] = True
+    dump_yaml(file1_stage, d)
+
+    stages = dvc.reproduce(file1_stage)
+    assert len(stages) == 0
+
+    d = load_yaml(file1_stage)
+    d["outs"][0]["metric"] = False
+    dump_yaml(file1_stage, d)
+
+    stages = dvc.reproduce(file1_stage)
+    assert len(stages) == 0
+
+
+def test_repro_phony(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    stage = run_stage(cmd="cat file1", deps=["file1"], name="cat")
+    shutil.copyfile("bar", "foo")
 
-        stage = self._run(
-            outs=[dir_name],
-            deps=[file_name, dir_code],
-            cmd=f"python {dir_code}",
-            name="copy-in-dir",
-        )
-        target = self._get_stage_target(stage)
+    dvc.reproduce(stage.addressing)
 
-        stages = self.dvc.reproduce(target)
-        self.assertEqual(len(stages), 0)
+    assert filecmp.cmp("file1", "bar", shallow=False)
 
-        os.unlink(file_name)
-        shutil.copyfile(self.BAR, file_name)
 
-        stages = self.dvc.reproduce(target)
-        self.assertEqual(len(stages), 1)
+def test_non_existing_output(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    stage = run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    os.unlink("foo")
 
+    with pytest.raises(ReproductionError):
+        dvc.reproduce(stage.addressing)
 
-class TestReproChangedDirData(SingleStageRun, TestDvc):
-    def test(self):
-        dir_name = "dir"
-        dir_code = "dir_code.py"
 
-        with open(dir_code, "w+", encoding="utf-8") as fd:
-            fd.write(
-                "import os; import sys; import shutil; "
-                "shutil.copytree(sys.argv[1], sys.argv[2])"
-            )
+def test_repro_data_source(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    stage = run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    shutil.copyfile("bar", "foo")
 
-        stage = self._run(
-            outs=[dir_name],
-            deps=[self.DATA_DIR, dir_code],
-            cmd=f"python {dir_code} {self.DATA_DIR} {dir_name}",
-            name="copy-dir",
-        )
-        target = self._get_stage_target(stage)
+    stages = dvc.reproduce(stage.addressing)
 
-        self.assertTrue(stage is not None)
+    assert filecmp.cmp("foo", "bar", shallow=False)
+    assert stages[0].outs[0].hash_info.value == file_md5("bar")
 
-        stages = self.dvc.reproduce(target)
-        self.assertEqual(len(stages), 0)
 
-        with open(self.DATA_SUB, "a", encoding="utf-8") as fd:
-            fd.write("add")
+def test_repro_changed_dir(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen({"foo": "foo", "bar": "bar"})
+    shutil.copyfile("foo", "file")
 
-        stages = self.dvc.reproduce(target)
-        self.assertEqual(len(stages), 1)
-        self.assertTrue(stages[0] is not None)
+    stage = run_stage(
+        outs=["dir"],
+        deps=["file", "copy.py"],
+        cmd="mkdir dir && python copy.py foo dir/foo",
+        name="copy-in-dir",
+    )
 
-        # Check that dvc indeed registers changed output dir
-        shutil.move(self.BAR, dir_name)
-        stages = self.dvc.reproduce(target)
-        self.assertEqual(len(stages), 1)
-        self.assertTrue(stages[0] is not None)
+    stages = dvc.reproduce(stage.addressing)
+    assert len(stages) == 0
 
-        # Check that dvc registers mtime change for the directory.
-        System.hardlink(self.DATA_SUB, self.DATA_SUB + ".lnk")
-        stages = self.dvc.reproduce(target)
-        self.assertEqual(len(stages), 1)
-        self.assertTrue(stages[0] is not None)
+    os.unlink("file")
+    shutil.copyfile("bar", "file")
 
+    stages = dvc.reproduce(stage.addressing)
+    assert len(stages) == 1
 
-class TestReproMissingMd5InStageFile(TestRepro):
-    def test(self):
-        d = load_yaml(self.file1_stage)
-        del d[Stage.PARAM_OUTS][0][LocalFileSystem.PARAM_CHECKSUM]
-        del d[Stage.PARAM_DEPS][0][LocalFileSystem.PARAM_CHECKSUM]
-        dump_yaml(self.file1_stage, d)
 
-        stages = self.dvc.reproduce(self.file1_stage)
-        self.assertEqual(len(stages), 1)
+def test_repro_changed_dir_data(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen({"data": {"foo": "foo"}, "bar": "bar"})
+    stage = run_stage(
+        outs=["dir"],
+        deps=["data", "copy.py"],
+        cmd="python copy.py data dir",
+        name="copy-dir",
+    )
 
+    assert not dvc.reproduce(stage.addressing)
 
-class TestCmdRepro(TestReproChangedData):
-    def test(self):
-        self.swap_foo_with_bar()
+    with (tmp_dir / "data" / "foo").open("a", encoding="utf-8") as f:
+        f.write("add")
 
-        ret = main(["status"])
-        self.assertEqual(ret, 0)
+    stages = dvc.reproduce(stage.addressing)
+    assert len(stages) == 1
 
-        ret = main(["repro", self._get_stage_target(self.stage)])
-        self.assertEqual(ret, 0)
+    # Check that dvc indeed registers changed output dir
+    shutil.move("bar", "dir")
+    stages = dvc.reproduce(stage.addressing)
+    assert len(stages) == 1
+
+    file = os.path.join("data", "foo")
+    # Check that dvc registers mtime change for the directory.
+    system.hardlink(file, file + ".lnk")
+    stages = dvc.reproduce(stage.addressing)
+    assert len(stages) == 1
 
-        ret = main(["repro", "non-existing-file"])
-        self.assertNotEqual(ret, 0)
 
+def test_repro_missing_md5_in_stage_file(tmp_dir, dvc, copy_script):
+    tmp_dir.dvc_gen("foo", "foo")
+    stage = dvc.run(
+        fname="file1.dvc",
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        single_stage=True,
+    )
+    d = load_yaml(stage.relpath)
+    del d[Stage.PARAM_OUTS][0][LocalFileSystem.PARAM_CHECKSUM]
+    del d[Stage.PARAM_DEPS][0][LocalFileSystem.PARAM_CHECKSUM]
+    dump_yaml(stage.relpath, d)
+
+    stages = dvc.reproduce(stage.addressing)
+    assert len(stages) == 1
+
+
+def test_cmd_repro(tmp_dir, copy_script, run_stage):
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    stage = run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    shutil.copyfile("bar", "foo")
 
-class TestReproShell(TestDvc):
-    def test(self):
-        if os.name == "nt":
-            return
+    ret = main(["status"])
+    assert ret == 0
 
-        fname = "shell.txt"
-        stage = fname + ".dvc"
+    ret = main(["repro", stage.addressing])
+    assert ret == 0
 
-        self.dvc.run(
-            fname=stage,
-            outs=[fname],
-            cmd=f"echo $SHELL > {fname}",
-            single_stage=True,
-        )
+    ret = main(["repro", "non-existing-file"])
+    assert ret != 0
 
-        with open(fname, encoding="utf-8") as fd:
-            self.assertEqual(os.getenv("SHELL"), fd.read().strip())
 
-        os.unlink(fname)
+@pytest.mark.skipif(os.name == "nt", reason="not on nt")
+def test_repro_shell(tmp_dir, monkeypatch, dvc):
+    monkeypatch.setenv("SHELL", "/bin/sh")
+    dvc.run(
+        fname="shell.txt.dvc",
+        outs=["shell.txt"],
+        cmd="echo $SHELL > shell.txt",
+        single_stage=True,
+    )
+    shell = os.getenv("SHELL")
 
-        self.dvc.reproduce(stage)
+    assert (tmp_dir / "shell.txt").read_text().rstrip() == shell
+    (tmp_dir / "shell.txt").unlink()
 
-        with open(fname, encoding="utf-8") as fd:
-            self.assertEqual(os.getenv("SHELL"), fd.read().strip())
-
-
-class TestReproAllPipelines(SingleStageRun, TestDvc):
-    def test(self):
-        stages = [
-            self._run(
-                fname="start.dvc",
-                outs=["start.txt"],
-                cmd="echo start > start.txt",
-                name="start",
-            ),
-            self._run(
-                fname="middle.dvc",
-                deps=["start.txt"],
-                outs=["middle.txt"],
-                cmd="echo middle > middle.txt",
-                name="middle",
-            ),
-            self._run(
-                fname="final.dvc",
-                deps=["middle.txt"],
-                outs=["final.txt"],
-                cmd="echo final > final.txt",
-                name="final",
-            ),
-            self._run(
-                fname="disconnected.dvc",
-                outs=["disconnected.txt"],
-                cmd="echo other > disconnected.txt",
-                name="disconnected",
-            ),
-        ]
+    dvc.reproduce("shell.txt.dvc")
+    assert (tmp_dir / "shell.txt").read_text().rstrip() == shell
 
-        from dvc.state import StateNoop
 
-        self.dvc.state = StateNoop()
+def test_repro_all_pipelines(mocker, dvc, run_stage):
+    stages = [
+        run_stage(
+            outs=["start.txt"],
+            cmd="echo start > start.txt",
+            name="start",
+        ),
+        run_stage(
+            deps=["start.txt"],
+            outs=["middle.txt"],
+            cmd="echo middle > middle.txt",
+            name="middle",
+        ),
+        run_stage(
+            deps=["middle.txt"],
+            outs=["final.txt"],
+            cmd="echo final > final.txt",
+            name="final",
+        ),
+        run_stage(
+            outs=["disconnected.txt"],
+            cmd="echo other > disconnected.txt",
+            name="disconnected",
+        ),
+    ]
 
-        with patch.object(
-            Stage, "reproduce", side_effect=stages
-        ) as mock_reproduce:
-            ret = main(["repro", "--all-pipelines"])
-            self.assertEqual(ret, 0)
-            self.assertEqual(mock_reproduce.call_count, 4)
+    from dvc_data.hashfile.state import StateNoop
 
+    dvc.state = StateNoop()
 
-class TestReproNoCommit(TestRepro):
-    def test(self):
-        remove(self.dvc.odb.local.cache_dir)
-        ret = main(
-            ["repro", self._get_stage_target(self.stage), "--no-commit"]
-        )
-        self.assertEqual(ret, 0)
-        self.assertEqual(os.listdir(self.dvc.odb.local.cache_dir), ["runs"])
+    mock_reproduce = mocker.patch.object(Stage, "reproduce", side_effect=stages)
+    ret = main(["repro", "--all-pipelines"])
+    assert ret == 0
+    assert mock_reproduce.call_count == 4
+
+
+def test_repro_no_commit(tmp_dir, dvc, copy_script, run_stage):
+    tmp_dir.gen("bar", "bar")
+    tmp_dir.dvc_gen("foo", "foo")
+    stage = run_stage(
+        outs=["file1"],
+        deps=["foo", "copy.py"],
+        cmd="python copy.py foo file1",
+        name="run1",
+    )
+    remove(dvc.cache.local.path)
+    ret = main(["repro", stage.addressing, "--no-commit"])
+    assert ret == 0
+    # run-cache should be skipped if `-no-commit`.
+    assert not os.path.isdir(dvc.cache.local.path)
 
 
-class TestReproAlreadyCached(TestRepro):
-    def test(self):
-        stage = self._run(
-            fname="datetime.dvc",
+class TestReproAlreadyCached:
+    def test(self, dvc, run_stage):
+        stage = run_stage(
             always_changed=True,
             deps=[],
             outs=["datetime.txt"],
             cmd='python -c "import time; print(time.time())" > datetime.txt',
             name="datetime",
         )
         run_out = stage.outs[0]
-        repro_out = self.dvc.reproduce(self._get_stage_target(stage))[0].outs[
-            0
-        ]
+        repro_out = dvc.reproduce(stage.addressing)[0].outs[0]
 
-        self.assertNotEqual(run_out.hash_info, repro_out.hash_info)
+        assert run_out.hash_info != repro_out.hash_info
 
-    def test_force_with_dependencies(self):
-        run_out = self.dvc.run(
-            fname="datetime.dvc",
-            deps=[self.FOO],
+    def test_force_with_dependencies(self, tmp_dir, dvc, run_stage):
+        tmp_dir.dvc_gen("foo", "foo")
+        stage = run_stage(
+            name="datetime",
+            deps=["foo"],
             outs=["datetime.txt"],
             cmd='python -c "import time; print(time.time())" > datetime.txt',
-            single_stage=True,
-        ).outs[0]
-
-        ret = main(["repro", "--force", "datetime.dvc"])
-        self.assertEqual(ret, 0)
-
-        repro_out = Dvcfile(self.dvc, "datetime.dvc").stage.outs[0]
-
-        self.assertNotEqual(run_out.hash_info, repro_out.hash_info)
-
-    def test_force_import(self):
-        remove(self.BAR)
-        ret = main(["import-url", self.FOO, self.BAR])
-        self.assertEqual(ret, 0)
-
-        patch_download = patch.object(
-            LocalFileSystem,
-            "download",
-            side_effect=LocalFileSystem.download,
-            autospec=True,
-        )
-
-        patch_checkout = patch.object(
-            Output,
-            "checkout",
-            side_effect=Output.checkout,
-            autospec=True,
-        )
-
-        with patch_download as mock_download:
-            with patch_checkout as mock_checkout:
-                assert main(["unfreeze", "bar.dvc"]) == 0
-                ret = main(["repro", "--force", "bar.dvc"])
-                self.assertEqual(ret, 0)
-                self.assertEqual(mock_download.call_count, 1)
-                self.assertEqual(mock_checkout.call_count, 0)
-
-
-class TestShouldDisplayMetricsOnReproWithMetricsOption(TestDvc):
-    def test(self):
-        metrics_file = "metrics_file"
-        metrics_value = 0.123489015
-        ret = main(
-            [
-                "run",
-                "--single-stage",
-                "-m",
-                metrics_file,
-                f"echo {metrics_value} >> {metrics_file}",
-            ]
         )
-        self.assertEqual(0, ret)
 
-        self._caplog.clear()
-        self._capsys.readouterr()  # clearing the buffer
+        ret = main(["repro", "--force", stage.addressing])
+        assert ret == 0
 
-        from dvc.dvcfile import DVC_FILE_SUFFIX
+        saved_stage = dvc.stage.get_target(stage.addressing)
+        assert stage.outs[0].hash_info != saved_stage.outs[0].hash_info
 
-        ret = main(
-            ["repro", "--force", "--metrics", metrics_file + DVC_FILE_SUFFIX]
-        )
-        self.assertEqual(0, ret)
+    def test_force_import(self, mocker, tmp_dir, dvc):
+        tmp_dir.dvc_gen("foo", "foo")
 
-        expected_metrics_display = f"Path\n{metrics_file}  {metrics_value}\n"
-        actual, _ = self._capsys.readouterr()
-        self.assertIn(expected_metrics_display, actual)
+        ret = main(["import-url", "foo", "bar"])
+        assert ret == 0
+
+        spy_get = mocker.spy(LocalFileSystem, "get")
+        spy_checkout = mocker.spy(Output, "checkout")
+
+        assert main(["unfreeze", "bar.dvc"]) == 0
+        ret = main(["repro", "--force", "bar.dvc"])
+        assert ret == 0
+        assert spy_get.call_count == 1
+        assert spy_checkout.call_count == 0
 
 
 @pytest.fixture
 def repro_dir(tmp_dir, dvc, run_copy):
     # Creates repo with following structure:
     #    data_dir/dir_file              origin_data
     #         |       |                   |
@@ -992,24 +951,20 @@
         deps=deps,
         single_stage=True,
     )
     assert stage is not None
     stages["last_stage"] = stage
 
     # Unrelated are to verify that reproducing `dir` will not trigger them too
+    assert run_copy(os.fspath(origin_copy), "unrelated1", single_stage=True) is not None
     assert (
-        run_copy(os.fspath(origin_copy), "unrelated1", single_stage=True)
-        is not None
-    )
-    assert (
-        run_copy(os.fspath(dir_file_path), "unrelated2", single_stage=True)
-        is not None
+        run_copy(os.fspath(dir_file_path), "unrelated2", single_stage=True) is not None
     )
 
-    yield stages
+    return stages
 
 
 def _rewrite_file(path_elements, new_content):
     if isinstance(path_elements, str):
         path_elements = [path_elements]
     file = Path(os.sep.join(path_elements))
     file.unlink()
@@ -1108,17 +1063,15 @@
         repro_dir["origin_copy_2"].relpath, recursive=True, force=True
     )
     assert stages == [repro_dir["origin_copy"], repro_dir["origin_copy_2"]]
 
 
 def test_dvc_formatting_retained(tmp_dir, dvc, run_copy):
     tmp_dir.dvc_gen("foo", "foo content")
-    stage = run_copy(
-        "foo", "foo_copy", fname="foo_copy.dvc", single_stage=True
-    )
+    stage = run_copy("foo", "foo_copy", fname="foo_copy.dvc", single_stage=True)
     stage_path = tmp_dir / stage.relpath
 
     # Add comments and custom formatting to DVC-file
     lines = list(map(_format_dvc_line, stage_path.read_text().splitlines()))
     lines.insert(0, "# Starting comment")
     stage_text = "".join(line + "\n" for line in lines)
     stage_path.write_text(stage_text)
@@ -1158,20 +1111,16 @@
     #     D   F
     #    / \   \
     #   B   C   G
     #    \ /
     #     A
     #
     assert main(["run", "--single-stage", "-o", "A", "echo A>A"]) == 0
-    assert (
-        main(["run", "--single-stage", "-d", "A", "-o", "B", "echo B>B"]) == 0
-    )
-    assert (
-        main(["run", "--single-stage", "-d", "A", "-o", "C", "echo C>C"]) == 0
-    )
+    assert main(["run", "--single-stage", "-d", "A", "-o", "B", "echo B>B"]) == 0
+    assert main(["run", "--single-stage", "-d", "A", "-o", "C", "echo C>C"]) == 0
     assert (
         main(
             [
                 "run",
                 "--single-stage",
                 "-d",
                 "B",
@@ -1181,17 +1130,15 @@
                 "D",
                 "echo D>D",
             ]
         )
         == 0
     )
     assert main(["run", "--single-stage", "-o", "G", "echo G>G"]) == 0
-    assert (
-        main(["run", "--single-stage", "-d", "G", "-o", "F", "echo F>F"]) == 0
-    )
+    assert main(["run", "--single-stage", "-d", "G", "-o", "F", "echo F>F"]) == 0
     assert (
         main(
             [
                 "run",
                 "--single-stage",
                 "-d",
                 "D",
@@ -1242,14 +1189,10 @@
 
     m = mocker.patch("dvc.stage.run.cmd_run", wraps=cmd_run)
 
     data = SingleStageFile(dvc, stage.path)._load()[0]
     data["cmd"] = "  ".join(stage.cmd.split())  # change cmd spacing by two
     (tmp_dir / stage.path).dump(data)
 
-    assert dvc.status([stage.addressing]) == {
-        stage.addressing: ["changed checksum"]
-    }
+    assert dvc.status([stage.addressing]) == {stage.addressing: ["changed checksum"]}
     assert dvc.reproduce(stage.addressing)[0] == stage
-    m.assert_called_once_with(
-        stage, checkpoint_func=None, dry=False, run_env=None
-    )
+    m.assert_called_once_with(stage, checkpoint_func=None, dry=False, run_env=None)
```

### Comparing `dvc-2.9.5/tests/func/test_run_cache.py` & `dvc-3.0.0a0/tests/func/test_run_cache.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 import os
 
-from dvc.dvcfile import PIPELINE_LOCK
-from dvc.utils import relpath
+import pytest
+
+from dvc.dvcfile import LOCK_FILE
 from dvc.utils.fs import remove
 
 
 def _recurse_count_files(path):
     return len([os.path.join(r, f) for r, _, fs in os.walk(path) for f in fs])
 
 
@@ -25,22 +26,23 @@
     run_copy("foo", "bar", name="copy-foo-bar")
 
     mock_restore = mocker.spy(dvc.stage_cache, "restore")
     mock_run = mocker.patch("dvc.stage.run.cmd_run")
 
     # removing any information that `dvc` could use to re-generate from
     (tmp_dir / "bar").unlink()
-    (tmp_dir / PIPELINE_LOCK).unlink()
+    (tmp_dir / LOCK_FILE).unlink()
 
     (stage,) = dvc.reproduce("copy-foo-bar")
 
-    mock_restore.assert_called_once_with(stage)
+    mock_restore.assert_called_once_with(stage, dry=False)
     mock_run.assert_not_called()
-    assert (tmp_dir / "bar").exists() and not (tmp_dir / "foo").unlink()
-    assert (tmp_dir / PIPELINE_LOCK).exists()
+    assert (tmp_dir / "bar").exists()
+    assert not (tmp_dir / "foo").unlink()
+    assert (tmp_dir / LOCK_FILE).exists()
 
 
 def test_save(tmp_dir, dvc, run_copy):
     run_cache_dir = dvc.stage_cache.cache_dir
     assert not os.path.exists(run_cache_dir)
 
     tmp_dir.gen("foo", "foo")
@@ -61,61 +63,68 @@
 
     (stage,) = dvc.reproduce("copy-foo-bar", dry=True)
 
     assert _recurse_count_files(run_cache_dir) == 0
     assert not dvc.stage_cache._load(stage)
 
 
-def test_uncached_outs_are_cached(tmp_dir, dvc, run_copy):
+@pytest.mark.parametrize(
+    "out_type,run_cache",
+    [
+        ("metrics_no_cache", True),
+        ("plots_no_cache", True),
+        ("outs_no_cache", False),
+    ],
+)
+def test_outs_no_cache_deactivate_run_cache(tmp_dir, dvc, out_type, run_cache):
     tmp_dir.gen("foo", "foo")
-    stage = dvc.run(
+    dvc.run(
         deps=["foo"],
-        cmd="cp foo bar",
-        outs_no_cache=["bar"],
+        cmd="cp foo bar && cp foo goo",
+        outs=["goo"],
         name="copy-foo-bar",
+        **{out_type: ["bar"]},
     )
-    stage.outs[0].hash_info = stage.outs[0].get_hash()
-    assert os.path.exists(relpath(stage.outs[0].cache_path))
+    assert os.path.isdir(dvc.stage_cache.cache_dir) == run_cache
 
 
-def test_memory_for_multiple_runs_of_same_stage(
-    tmp_dir, dvc, run_copy, mocker
-):
+def test_memory_for_multiple_runs_of_same_stage(tmp_dir, dvc, run_copy, mocker):
     tmp_dir.gen("foo", "foo")
     assert not os.path.exists(dvc.stage_cache.cache_dir)
     run_copy("foo", "bar", name="copy-foo-bar")
     assert _recurse_count_files(dvc.stage_cache.cache_dir) == 1
     tmp_dir.gen("foo", "foobar")
     run_copy("foo", "bar", name="copy-foo-bar")
     assert _recurse_count_files(dvc.stage_cache.cache_dir) == 2
 
     from dvc.stage import run as _run
 
     mock_restore = mocker.spy(dvc.stage_cache, "restore")
     mock_run = mocker.spy(_run, "cmd_run")
 
     (tmp_dir / "bar").unlink()
-    (tmp_dir / PIPELINE_LOCK).unlink()
+    (tmp_dir / LOCK_FILE).unlink()
     (stage,) = dvc.reproduce("copy-foo-bar")
 
-    assert (tmp_dir / PIPELINE_LOCK).exists()
+    assert (tmp_dir / LOCK_FILE).exists()
     assert (tmp_dir / "bar").read_text() == "foobar"
     mock_run.assert_not_called()
-    mock_restore.assert_called_once_with(stage)
+    mock_restore.assert_called_once_with(stage, dry=False)
     mock_restore.reset_mock()
 
-    (tmp_dir / PIPELINE_LOCK).unlink()
+    (tmp_dir / LOCK_FILE).unlink()
     tmp_dir.gen("foo", "foo")
     dvc.reproduce("copy-foo-bar")
 
     assert (tmp_dir / "bar").read_text() == "foo"
     mock_run.assert_not_called()
-    mock_restore.assert_called_once_with(stage)
-    assert (tmp_dir / "bar").exists() and not (tmp_dir / "foo").unlink()
-    assert (tmp_dir / PIPELINE_LOCK).exists()
+    mock_restore.assert_called_once_with(stage, dry=False)
+    assert (tmp_dir / "bar").exists()
+    assert not (tmp_dir / "foo").unlink()
+    assert (tmp_dir / LOCK_FILE).exists()
 
 
 def test_memory_runs_of_multiple_stages(tmp_dir, dvc, run_copy, mocker):
     tmp_dir.gen("foo", "foo")
     assert not os.path.exists(dvc.stage_cache.cache_dir)
 
     run_copy("foo", "foo.bak", name="backup-foo")
@@ -128,48 +137,52 @@
     from dvc.stage import run as _run
 
     mock_restore = mocker.spy(dvc.stage_cache, "restore")
     mock_run = mocker.spy(_run, "cmd_run")
 
     (tmp_dir / "foo.bak").unlink()
     (tmp_dir / "bar.bak").unlink()
-    (tmp_dir / PIPELINE_LOCK).unlink()
+    (tmp_dir / LOCK_FILE).unlink()
     (stage,) = dvc.reproduce("backup-foo")
 
     assert (tmp_dir / "foo.bak").read_text() == "foo"
-    assert (tmp_dir / PIPELINE_LOCK).exists()
+    assert (tmp_dir / LOCK_FILE).exists()
     mock_run.assert_not_called()
-    mock_restore.assert_called_once_with(stage)
+    mock_restore.assert_called_once_with(stage, dry=False)
     mock_restore.reset_mock()
 
     (stage,) = dvc.reproduce("backup-bar")
 
     assert (tmp_dir / "bar.bak").read_text() == "bar"
-    assert (tmp_dir / PIPELINE_LOCK).exists()
+    assert (tmp_dir / LOCK_FILE).exists()
     mock_run.assert_not_called()
-    mock_restore.assert_called_once_with(stage)
+    mock_restore.assert_called_once_with(stage, dry=False)
 
 
 def test_restore_pull(tmp_dir, dvc, run_copy, mocker, local_remote):
     import dvc.output as dvc_output
 
     tmp_dir.gen("foo", "foo")
     stage = run_copy("foo", "bar", name="copy-foo-bar")
 
-    dvc.push()
+    dvc.push(run_cache=True)
 
     mock_restore = mocker.spy(dvc.stage_cache, "restore")
     mock_run = mocker.patch("dvc.stage.run.cmd_run")
     mock_checkout = mocker.spy(dvc_output, "checkout")
 
     # removing any information that `dvc` could use to re-generate from
     (tmp_dir / "bar").unlink()
-    (tmp_dir / PIPELINE_LOCK).unlink()
+    (tmp_dir / LOCK_FILE).unlink()
     remove(stage.outs[0].cache_path)
 
+    # removing local run cache
+    remove(dvc.stage_cache.cache_dir)
+
     (stage,) = dvc.reproduce("copy-foo-bar", pull=True)
 
-    mock_restore.assert_called_once_with(stage, pull=True)
+    mock_restore.assert_called_once_with(stage, pull=True, dry=False)
     mock_run.assert_not_called()
     assert mock_checkout.call_count == 2
-    assert (tmp_dir / "bar").exists() and not (tmp_dir / "foo").unlink()
-    assert (tmp_dir / PIPELINE_LOCK).exists()
+    assert (tmp_dir / "bar").exists()
+    assert not (tmp_dir / "foo").unlink()
+    assert (tmp_dir / LOCK_FILE).exists()
```

### Comparing `dvc-2.9.5/tests/func/test_run_multistage.py` & `dvc-3.0.0a0/tests/func/test_dvcfile.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,407 +1,426 @@
-import os
 import textwrap
 
 import pytest
 
-from dvc.exceptions import InvalidArgumentError
-from dvc.stage.exceptions import DuplicateStageName, InvalidStageName
-
-
-def test_run_with_name(tmp_dir, dvc, run_copy):
-    from dvc.dvcfile import PIPELINE_FILE, PIPELINE_LOCK
-    from dvc.stage import PipelineStage
-
-    tmp_dir.dvc_gen("foo", "foo")
-    assert not os.path.exists(PIPELINE_FILE)
-    stage = run_copy("foo", "bar", name="copy-foo-to-bar")
-    assert isinstance(stage, PipelineStage)
-    assert stage.name == "copy-foo-to-bar"
-    assert os.path.exists(PIPELINE_FILE)
-    assert os.path.exists(PIPELINE_LOCK)
-
-
-def test_run_no_exec(tmp_dir, dvc, run_copy):
-    from dvc.dvcfile import PIPELINE_FILE, PIPELINE_LOCK
-    from dvc.stage import PipelineStage
-
-    tmp_dir.dvc_gen("foo", "foo")
-    assert not os.path.exists(PIPELINE_FILE)
-    stage = run_copy("foo", "bar", name="copy-foo-to-bar", no_exec=True)
-    assert isinstance(stage, PipelineStage)
-    assert stage.name == "copy-foo-to-bar"
-    assert os.path.exists(PIPELINE_FILE)
-    assert not os.path.exists(PIPELINE_LOCK)
-
-    data, _ = stage.dvcfile._load()
-    assert data["stages"]["copy-foo-to-bar"] == {
-        "cmd": "python copy.py foo bar",
-        "deps": ["copy.py", "foo"],
-        "outs": ["bar"],
-    }
-
-
-def test_run_with_multistage_and_single_stage(tmp_dir, dvc, run_copy):
-    from dvc.stage import PipelineStage, Stage
-
-    tmp_dir.dvc_gen("foo", "foo")
-    stage1 = run_copy("foo", "foo1", single_stage=True)
-    stage2 = run_copy("foo1", "foo2", name="copy-foo1-foo2")
-    stage3 = run_copy("foo2", "foo3", single_stage=True)
-
-    assert isinstance(stage2, PipelineStage)
-    assert isinstance(stage1, Stage)
-    assert isinstance(stage3, Stage)
-    assert stage2.name == "copy-foo1-foo2"
-
-
-def test_run_multi_stage_repeat(tmp_dir, dvc, run_copy):
-    from dvc.dvcfile import PIPELINE_FILE, Dvcfile
-    from dvc.stage import PipelineStage
-
-    tmp_dir.dvc_gen("foo", "foo")
-    run_copy("foo", "foo1", name="copy-foo-foo1")
-    run_copy("foo1", "foo2", name="copy-foo1-foo2")
-    run_copy("foo2", "foo3", single_stage=True)
-
-    stages = list(Dvcfile(dvc, PIPELINE_FILE).stages.values())
-    assert len(stages) == 2
-    assert all(isinstance(stage, PipelineStage) for stage in stages)
-    assert {stage.name for stage in stages} == {
-        "copy-foo-foo1",
-        "copy-foo1-foo2",
+from dvc.annotations import Annotation
+from dvc.dvcfile import (
+    LOCK_FILE,
+    PROJECT_FILE,
+    ParametrizedDumpError,
+    SingleStageFile,
+    load_file,
+)
+from dvc.stage.exceptions import StageFileDoesNotExistError
+from dvc.stage.loader import StageNotFound
+from dvc.utils.strictyaml import YAMLValidationError
+
+STAGE_EXAMPLE = {
+    "stage1": {
+        "cmd": "cp foo bar",
+        "desc": "stage desc",
+        "meta": {"key1": "value1", "key2": "value2"},
+        "deps": ["foo"],
+        "outs": [{"bar": {"desc": "bar desc", "meta": {"key": "value"}}}],
     }
+}
 
 
-def test_multi_stage_run_cached(tmp_dir, dvc, run_copy, mocker):
-    from dvc.stage.run import subprocess
-
-    tmp_dir.dvc_gen("foo", "foo")
-
-    run_copy("foo", "foo2", name="copy-foo1-foo2")
-    spy = mocker.spy(subprocess, "Popen")
-    run_copy("foo", "foo2", name="copy-foo1-foo2")
-    assert not spy.called
-
-
-def test_multistage_dump_on_non_cached_outputs(tmp_dir, dvc):
-    tmp_dir.dvc_gen("foo")
-    dvc.run(
-        cmd="cp foo foo1",
+def test_run_load_one_for_multistage(tmp_dir, dvc):
+    tmp_dir.gen("foo", "foo")
+    stage1 = dvc.run(
+        cmd="cp foo foo2",
         deps=["foo"],
-        name="copy-foo1-foo2",
-        outs_no_cache=["foo1"],
+        name="copy-foo-foo2",
+        outs_persist_no_cache=["foo2"],
+        always_changed=True,
     )
+    stage2 = load_file(dvc, PROJECT_FILE).stages["copy-foo-foo2"]
+    assert stage1 == stage2
+    foo_out = stage2.outs[0]
+    assert stage2.cmd == "cp foo foo2"
+    assert stage2.name == "copy-foo-foo2"
+    assert foo_out.def_path == "foo2"
+    assert foo_out.persist
+    assert not foo_out.use_cache
+    assert stage2.deps[0].def_path == "foo"
+    assert dvc.reproduce(":copy-foo-foo2")
+
+
+def test_run_load_one_for_multistage_non_existing(tmp_dir, dvc):
+    with pytest.raises(StageFileDoesNotExistError):
+        assert load_file(dvc, PROJECT_FILE).stages.get("copy-foo-foo2")
 
 
-def test_multistage_with_wdir(tmp_dir, dvc):
-    from dvc.dvcfile import Dvcfile
-
-    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
+def test_run_load_one_for_multistage_non_existing_stage_name(tmp_dir, dvc):
+    tmp_dir.gen("foo", "foo")
     stage = dvc.run(
-        cmd="cp foo foo1",
+        cmd="cp foo foo2",
         deps=["foo"],
-        name="copy-foo1-foo2",
-        outs=["foo1"],
-        wdir="dir",
+        name="copy-foo-foo2",
+        metrics=["foo2"],
+        always_changed=True,
     )
+    with pytest.raises(StageNotFound):
+        assert load_file(dvc, stage.path).stages["random-name"]
 
-    data, _ = Dvcfile(dvc, stage.path)._load()
-    assert "dir" == data["stages"]["copy-foo1-foo2"]["wdir"]
 
-
-def test_multistage_always_changed(tmp_dir, dvc):
-    from dvc.dvcfile import Dvcfile
-
-    tmp_dir.gen({"foo": "foo", "bar": "bar"})
+def test_run_load_one_on_single_stage(tmp_dir, dvc):
+    tmp_dir.gen("foo", "foo")
     stage = dvc.run(
-        cmd="cp foo foo1",
+        cmd="cp foo foo2",
         deps=["foo"],
-        name="copy-foo1-foo2",
-        outs=["foo1"],
+        metrics=["foo2"],
         always_changed=True,
+        single_stage=True,
     )
+    assert isinstance(load_file(dvc, stage.path), SingleStageFile)
+    assert load_file(dvc, stage.path).stages.get("random-name") == stage
+    assert load_file(dvc, stage.path).stage == stage
 
-    data, _ = Dvcfile(dvc, stage.path)._load()
-    assert data["stages"]["copy-foo1-foo2"]["always_changed"]
+
+def test_has_stage_with_name(tmp_dir, dvc):
+    tmp_dir.gen("foo", "foo")
+    dvc.run(
+        cmd="cp foo foo2",
+        deps=["foo"],
+        name="copy-foo-foo2",
+        metrics=["foo2"],
+        always_changed=True,
+    )
+    dvcfile = load_file(dvc, PROJECT_FILE)
+    assert "copy-foo-foo2" in dvcfile.stages
+    assert "copy" not in dvcfile.stages
 
 
-def test_graph(tmp_dir, dvc):
-    from dvc.exceptions import CyclicGraphError
+def test_load_all_multistage(tmp_dir, dvc):
+    tmp_dir.gen("foo", "foo")
+    stage1 = dvc.run(
+        cmd="cp foo foo2",
+        deps=["foo"],
+        name="copy-foo-foo2",
+        metrics=["foo2"],
+        always_changed=True,
+    )
+    stages = load_file(dvc, PROJECT_FILE).stages.values()
+    assert len(stages) == 1
+    assert list(stages) == [stage1]
+
+    tmp_dir.gen("bar", "bar")
+    stage2 = dvc.run(
+        cmd="cp bar bar2",
+        deps=["bar"],
+        name="copy-bar-bar2",
+        metrics=["bar2"],
+        always_changed=True,
+    )
+    assert set(load_file(dvc, PROJECT_FILE).stages.values()) == {
+        stage2,
+        stage1,
+    }
 
-    tmp_dir.gen({"foo": "foo", "bar": "bar"})
 
-    dvc.run(deps=["foo"], outs=["bar"], cmd="echo foo > bar", name="1")
+def test_load_all_singlestage(tmp_dir, dvc):
+    tmp_dir.gen("foo", "foo")
+    stage1 = dvc.run(
+        cmd="cp foo foo2",
+        deps=["foo"],
+        metrics=["foo2"],
+        always_changed=True,
+        single_stage=True,
+    )
+    dvcfile = load_file(dvc, "foo2.dvc")
+    assert isinstance(dvcfile, SingleStageFile)
+    assert len(dvcfile.stages) == 1
+    stages = dvcfile.stages.values()
+    assert len(stages) == 1
+    assert list(stages) == [stage1]
 
-    dvc.run(deps=["bar"], outs=["baz"], cmd="echo bar > baz", name="2")
 
-    with pytest.raises(CyclicGraphError):
-        dvc.run(deps=["baz"], outs=["foo"], cmd="echo baz > foo", name="3")
+def test_try_get_single_stage_from_pipeline_file(tmp_dir, dvc):
+    from dvc.dvcfile import DvcException
 
+    tmp_dir.gen("foo", "foo")
+    dvc.run(
+        cmd="cp foo foo2",
+        deps=["foo"],
+        name="copy-foo-foo2",
+        metrics=["foo2"],
+        always_changed=True,
+    )
+    with pytest.raises(DvcException):
+        assert load_file(dvc, PROJECT_FILE).stage
 
-def test_run_dump_on_multistage(tmp_dir, dvc, run_head):
-    from dvc.dvcfile import PIPELINE_FILE, Dvcfile
 
+def test_stage_collection(tmp_dir, dvc):
     tmp_dir.gen(
         {
-            "dir": {
-                "foo": "foo\nfoo",
-                "bar": "bar\nbar",
-                "foobar": "foobar\foobar",
-            }
+            "dir": {"file1": "file1", "file2": "file2"},
+            "foo": "foo",
+            "bar": "bar",
         }
     )
-
-    dvc.run(
+    (stage1,) = dvc.add("dir")
+    stage2 = dvc.run(
         cmd="cp foo foo2",
         deps=["foo"],
         name="copy-foo-foo2",
-        wdir="dir",
-        outs_persist=["foo2"],
+        metrics=["foo2"],
         always_changed=True,
     )
-    data = Dvcfile(dvc, PIPELINE_FILE)._load()[0]
-    assert data == {
-        "stages": {
-            "copy-foo-foo2": {
-                "cmd": "cp foo foo2",
-                "deps": ["foo"],
-                "outs": [{"foo2": {"persist": True}}],
-                "always_changed": True,
-                "wdir": "dir",
-            }
-        }
-    }
+    stage3 = dvc.run(
+        cmd="cp bar bar2",
+        deps=["bar"],
+        metrics=["bar2"],
+        always_changed=True,
+        single_stage=True,
+    )
+    assert set(dvc.index.stages) == {stage1, stage3, stage2}
 
-    run_head(
-        "foo",
-        "bar",
-        "foobar",
-        name="head-files",
-        outs=["bar-1"],
-        outs_persist=["foo-1"],
-        metrics_no_cache=["foobar-1"],
-        wdir="dir",
-    )
-    assert Dvcfile(dvc, PIPELINE_FILE)._load()[0] == {
-        "stages": {
-            "head-files": {
-                "cmd": "python {} foo bar foobar".format(
-                    (tmp_dir / "head.py").resolve()
-                ),
-                "wdir": "dir",
-                "deps": ["bar", "foo", "foobar"],
-                "outs": ["bar-1", {"foo-1": {"persist": True}}],
-                "metrics": [{"foobar-1": {"cache": False}}],
-            },
-            **data["stages"],
-        }
-    }
 
+def test_remove_stage(tmp_dir, dvc, run_copy):
+    tmp_dir.gen("foo", "foo")
+    stage = run_copy("foo", "bar", name="copy-foo-bar")
+    stage2 = run_copy("bar", "foobar", name="copy-bar-foobar")
 
-@pytest.mark.parametrize(
-    "char", ["@:", "#", "$", ":", "/", "\\", ".", ";", ","]
-)
-def test_run_with_invalid_stage_name(run_copy, char):
-    with pytest.raises(InvalidStageName):
-        run_copy("foo", "bar", name=f"copy_name-{char}")
-
-
-def test_run_with_name_having_hyphen_underscore(tmp_dir, dvc, run_copy):
-    tmp_dir.dvc_gen("foo", "foo")
-    run_copy("foo", "bar", name="copy-foo_bar")
-
-
-def test_run_already_exists(tmp_dir, dvc, run_copy):
-    tmp_dir.dvc_gen("foo", "foo")
-    run_copy("foo", "bar", name="copy")
-    with pytest.raises(DuplicateStageName):
-        run_copy("bar", "foobar", name="copy", force=False)
-    run_copy("bar", "foobar", name="copy", force=True)
-
-
-supported_params = {
-    "name": "Answer",
-    "answer": 42,
-    "floats": 42.0,
-    "lists": [42, 42.0, "42"],
-    "nested": {"nested1": {"nested2": "42", "nested2-2": 41.99999}},
-}
+    dvc_file = load_file(dvc, PROJECT_FILE)
+    assert dvc_file.exists()
+    assert {"copy-bar-foobar", "copy-foo-bar"} == set(
+        dvc_file._load()[0]["stages"].keys()
+    )
 
+    dvc_file.remove_stage(stage)
 
-def test_run_params_default(tmp_dir, dvc):
-    from dvc.dependency import ParamsDependency
+    assert ["copy-bar-foobar"] == list(dvc_file._load()[0]["stages"].keys())
 
-    (tmp_dir / "params.yaml").dump(supported_params)
-    stage = dvc.run(
-        name="read_params",
-        params=["nested.nested1.nested2"],
-        cmd="cat params.yaml",
-    )
-    assert isinstance(stage.deps[0], ParamsDependency)
-    assert stage.deps[0].params == ["nested.nested1.nested2"]
-
-    lockfile = stage.dvcfile._lockfile
-    assert lockfile.load()["stages"]["read_params"]["params"] == {
-        "params.yaml": {"nested.nested1.nested2": "42"}
-    }
+    # sanity check
+    stage2.reload()
 
-    data, _ = stage.dvcfile._load()
-    assert data["stages"]["read_params"]["params"] == [
-        "nested.nested1.nested2"
-    ]
+    # re-check to see if it fails if there's no stage entry
+    dvc_file.remove_stage(stage)
+    dvc_file.remove(force=True)
+    # should not fail when there's no file at all.
+    dvc_file.remove_stage(stage)
 
 
-def test_run_params_custom_file(tmp_dir, dvc):
-    from dvc.dependency import ParamsDependency
+def test_remove_stage_lockfile(tmp_dir, dvc, run_copy):
+    tmp_dir.gen("foo", "foo")
+    stage = run_copy("foo", "bar", name="copy-foo-bar")
+    stage2 = run_copy("bar", "foobar", name="copy-bar-foobar")
 
-    (tmp_dir / "params2.yaml").dump(supported_params)
-    stage = dvc.run(
-        name="read_params",
-        params=["params2.yaml:lists"],
-        cmd="cat params2.yaml",
-    )
+    dvc_file = load_file(dvc, PROJECT_FILE)
+    lock_file = dvc_file._lockfile
+    assert dvc_file.exists()
+    assert lock_file.exists()
+    assert {"copy-bar-foobar", "copy-foo-bar"} == set(lock_file.load()["stages"].keys())
+    lock_file.remove_stage(stage)
 
-    isinstance(stage.deps[0], ParamsDependency)
-    assert stage.deps[0].params == ["lists"]
-    lockfile = stage.dvcfile._lockfile
-    assert lockfile.load()["stages"]["read_params"]["params"] == {
-        "params2.yaml": {"lists": [42, 42.0, "42"]}
-    }
+    assert ["copy-bar-foobar"] == list(lock_file.load()["stages"].keys())
 
-    data, _ = stage.dvcfile._load()
-    assert data["stages"]["read_params"]["params"] == [
-        {"params2.yaml": ["lists"]}
-    ]
+    # sanity check
+    stage2.reload()
 
+    # re-check to see if it fails if there's no stage entry
+    lock_file.remove_stage(stage)
+    lock_file.remove()
+    # should not fail when there's no file at all.
+    lock_file.remove_stage(stage)
 
-def test_run_params_no_exec(tmp_dir, dvc):
-    from dvc.dependency import ParamsDependency
 
-    (tmp_dir / "params2.yaml").dump(supported_params)
-    stage = dvc.run(
-        name="read_params",
-        params=["params2.yaml:lists"],
-        cmd="cat params2.yaml",
-        no_exec=True,
+def test_remove_stage_dvcfiles(tmp_dir, dvc, run_copy):
+    tmp_dir.gen("foo", "foo")
+    stage = run_copy("foo", "bar", single_stage=True)
+
+    dvc_file = load_file(dvc, stage.path)
+    assert dvc_file.exists()
+    dvc_file.remove_stage(stage)
+    assert not dvc_file.exists()
+
+    # re-check to see if it fails if there's no stage entry
+    dvc_file.remove_stage(stage)
+    dvc_file.remove(force=True)
+
+    # should not fail when there's no file at all.
+    dvc_file.remove_stage(stage)
+
+
+def test_remove_stage_on_lockfile_format_error(tmp_dir, dvc, run_copy):
+    tmp_dir.gen("foo", "foo")
+    stage = run_copy("foo", "bar", name="copy-foo-bar")
+    dvc_file = load_file(dvc, stage.path)
+    lock_file = dvc_file._lockfile
+
+    data = dvc_file._load()[0]
+    lock_data = lock_file.load()
+    lock_data["gibberish"] = True
+    data["gibberish"] = True
+    (tmp_dir / lock_file.relpath).dump(lock_data)
+    with pytest.raises(YAMLValidationError):
+        dvc_file.remove_stage(stage)
+
+    lock_file.remove()
+    dvc_file.dump(stage, update_pipeline=False)
+
+    (tmp_dir / dvc_file.relpath).dump(data)
+    with pytest.raises(YAMLValidationError):
+        dvc_file.remove_stage(stage)
+
+
+def test_remove_stage_preserves_comment(tmp_dir, dvc, run_copy):
+    tmp_dir.gen(
+        "dvc.yaml",
+        textwrap.dedent(
+            """\
+            stages:
+                generate-foo:
+                    cmd: "echo foo > foo"
+                    # This copies 'foo' text to 'foo' file.
+                    outs:
+                    - foo
+                copy-foo-bar:
+                    cmd: "python copy.py foo bar"
+                    deps:
+                    - foo
+                    outs:
+                    - bar"""
+        ),
+    )
+
+    dvc.reproduce(PROJECT_FILE)
+
+    dvc_file = load_file(dvc, PROJECT_FILE)
+
+    assert dvc_file.exists()
+    assert (tmp_dir / LOCK_FILE).exists()
+    assert (tmp_dir / "foo").exists()
+    assert (tmp_dir / "bar").exists()
+
+    dvc_file.remove_stage(dvc_file.stages["copy-foo-bar"])
+    assert (
+        "# This copies 'foo' text to 'foo' file."
+        in (tmp_dir / PROJECT_FILE).read_text()
     )
 
-    isinstance(stage.deps[0], ParamsDependency)
-    assert stage.deps[0].params == ["lists"]
-    assert not stage.dvcfile._lockfile.exists()
-
-    data, _ = stage.dvcfile._load()
-    assert data["stages"]["read_params"]["params"] == [
-        {"params2.yaml": ["lists"]}
-    ]
 
+def test_remove_stage_removes_dvcfiles_if_no_stages_left(tmp_dir, dvc, run_copy):
+    tmp_dir.gen("foo", "foo")
+    run_copy("foo", "bar", name="run_copy")
 
-@pytest.mark.parametrize(
-    "kwargs",
-    [
-        {"outs": ["foo"], "deps": ["bar"]},
-        {"outs": ["foo"], "deps": ["bar"], "name": "copy-foo-bar"},
-    ],
-)
-def test_run_without_cmd(tmp_dir, dvc, kwargs):
-    with pytest.raises(InvalidArgumentError) as exc:
-        dvc.run(**kwargs)
-    assert "command is not specified" == str(exc.value)
+    dvc_file = load_file(dvc, PROJECT_FILE)
+
+    assert dvc_file.exists()
+    assert (tmp_dir / LOCK_FILE).exists()
+    assert (tmp_dir / "foo").exists()
+
+    dvc_file.remove_stage(dvc_file.stages["run_copy"])
+    assert not dvc_file.exists()
+    assert not (tmp_dir / LOCK_FILE).exists()
 
 
-def test_run_overwrite_order(tmp_dir, dvc, run_copy):
-    from dvc.dvcfile import PIPELINE_FILE
+def test_dvcfile_dump_preserves_meta(tmp_dir, dvc, run_copy):
+    tmp_dir.gen("foo", "foo")
+    stage = run_copy("foo", "bar", name="run_copy")
+    dvcfile = stage.dvcfile
 
-    tmp_dir.gen({"foo": "foo", "foo1": "foo1"})
-    run_copy("foo", "bar", name="copy-foo-bar")
-    run_copy("bar", "foobar", name="copy-bar-foobar")
+    data = dvcfile._load()[0]
+    metadata = {"name": "copy-file"}
+    stage.meta = metadata
+    data["stages"]["run_copy"]["meta"] = metadata
 
-    run_copy("foo1", "bar1", name="copy-foo-bar", force=True)
+    dvcfile.dump(stage)
+    assert dvcfile._load()[0] == data
+    assert dvcfile._load()[0]["stages"]["run_copy"]["meta"] == metadata
 
-    data = (tmp_dir / PIPELINE_FILE).parse()
-    assert list(data["stages"].keys()) == ["copy-foo-bar", "copy-bar-foobar"]
 
+def test_dvcfile_dump_preserves_desc(tmp_dir, dvc, run_copy):
+    tmp_dir.gen("foo", "foo")
+    stage_desc = "test stage description"
+    out_desc = "test out description"
 
-def test_run_overwrite_preserves_meta_and_comment(tmp_dir, dvc, run_copy):
-    from dvc.dvcfile import PIPELINE_FILE
+    stage = run_copy("foo", "bar", name="run_copy", desc=stage_desc)
+    dvcfile = stage.dvcfile
 
-    tmp_dir.gen({"foo": "foo", "foo1": "foo1"})
+    data = dvcfile._load()[0]
+    data["stages"]["run_copy"]["outs"][0] = {"bar": {"desc": out_desc}}
+    (tmp_dir / dvcfile.path).dump(data)
+
+    assert stage.desc == stage_desc
+    stage.outs[0].annot.desc = out_desc
+    dvcfile.dump(stage)
+    loaded = dvcfile._load()[0]
+    assert loaded == data
+    assert loaded["stages"]["run_copy"]["desc"] == stage_desc
+    assert loaded["stages"]["run_copy"]["outs"][0]["bar"]["desc"] == out_desc
+
+
+def test_dvcfile_dump_preserves_comments(tmp_dir, dvc):
     text = textwrap.dedent(
         """\
         stages:
-          copy-foo-bar:
-            cmd: python copy.py {src} {dest}
-            deps:
-            - copy.py
-            - {src}
+          generate-foo:
+            cmd: echo foo > foo
+            # This copies 'foo' text to 'foo' file.
             outs:
-            # comments are preserved
-            - {dest}
-            meta:
-              name: meta is preserved too
-    """
+            - foo"""
     )
-    (tmp_dir / PIPELINE_FILE).write_text(text.format(src="foo", dest="bar"))
-    assert dvc.reproduce(PIPELINE_FILE)
+    tmp_dir.gen("dvc.yaml", text)
+    stage = dvc.stage.load_one(name="generate-foo")
+    stage.outs[0].use_cache = False
+    dvcfile = stage.dvcfile
 
-    assert run_copy("foo1", "bar1", name="copy-foo-bar", force=True)
+    dvcfile.dump(stage)
+    assert dvcfile._load()[1] == (text + ":\n\tcache: false\n".expandtabs())
 
-    assert (tmp_dir / PIPELINE_FILE).read_text() == text.format(
-        src="foo1", dest="bar1"
-    )
 
+@pytest.mark.parametrize(
+    "data, name",
+    [
+        ({"build-us": {"cmd": "echo ${foo}"}}, "build-us"),
+        (
+            {"build": {"foreach": ["us", "gb"], "do": {"cmd": "echo ${foo}"}}},
+            "build@us",
+        ),
+    ],
+)
+def test_dvcfile_try_dumping_parametrized_stage(tmp_dir, dvc, data, name):
+    (tmp_dir / "dvc.yaml").dump({"stages": data, "vars": [{"foo": "foobar"}]})
+
+    stage = dvc.stage.load_one(name=name)
+    dvcfile = stage.dvcfile
 
-def test_run_external_outputs(
-    tmp_dir,
-    dvc,
-    local_workspace,
-):
-    hash_name = "md5"
-    foo_hash = "acbd18db4cc2f85cedef654fccc4a4d8"
-    bar_hash = "37b51d194a7513e45b56f6524f2d51f2"
+    with pytest.raises(ParametrizedDumpError) as exc:
+        dvcfile.dump(stage)
 
-    local_workspace.gen("foo", "foo")
-    dvc.run(
-        name="mystage",
-        cmd="mycmd",
-        deps=["remote://workspace/foo"],
-        outs=["remote://workspace/bar"],
-        no_exec=True,
-    )
-
-    dvc_yaml = (
-        "stages:\n"
-        "  mystage:\n"
-        "    cmd: mycmd\n"
-        "    deps:\n"
-        "    - remote://workspace/foo\n"
-        "    outs:\n"
-        "    - remote://workspace/bar\n"
-    )
-
-    assert (tmp_dir / "dvc.yaml").read_text() == dvc_yaml
-    assert not (tmp_dir / "dvc.lock").exists()
-
-    local_workspace.gen("bar", "bar")
-    dvc.commit("dvc.yaml", force=True)
-
-    assert (tmp_dir / "dvc.yaml").read_text() == dvc_yaml
-    assert (tmp_dir / "dvc.lock").read_text() == (
-        "schema: '2.0'\n"
-        "stages:\n"
-        "  mystage:\n"
-        "    cmd: mycmd\n"
-        "    deps:\n"
-        "    - path: remote://workspace/foo\n"
-        f"      {hash_name}: {foo_hash}\n"
-        "      size: 3\n"
-        "    outs:\n"
-        "    - path: remote://workspace/bar\n"
-        f"      {hash_name}: {bar_hash}\n"
-        "      size: 3\n"
-    )
+    assert str(exc.value) == f"cannot dump a parametrized stage: '{name}'"
 
-    assert (local_workspace / "foo").read_text() == "foo"
-    assert (local_workspace / "bar").read_text() == "bar"
-    assert (
-        local_workspace / "cache" / bar_hash[:2] / bar_hash[2:]
-    ).read_text() == "bar"
+
+def test_dvcfile_load_dump_stage_with_desc_meta(tmp_dir, dvc):
+    data = {"stages": STAGE_EXAMPLE}
+    (tmp_dir / "dvc.yaml").dump(data)
+
+    stage = dvc.stage.load_one(name="stage1")
+    assert stage.meta == {"key1": "value1", "key2": "value2"}
+    assert stage.desc == "stage desc"
+    assert stage.outs[0].annot == Annotation(desc="bar desc", meta={"key": "value"})
+
+    # sanity check
+    stage.dump()
+    assert (tmp_dir / "dvc.yaml").parse() == data
+
+
+def test_dvcfile_load_with_plots(tmp_dir, dvc):
+    (tmp_dir / "dvc.yaml").dump(
+        {
+            "plots": [
+                {"path/to/plot": {"x": "value", "y": "value"}},
+                {"path/to/another/plot": {"x": "value", "y": "value"}},
+                {"path/to/empty/plot": None},
+                "path/to/plot/str",
+            ],
+            "stages": STAGE_EXAMPLE,
+        },
+    )
+    plots = list(dvc.plots.collect())
+    top_level_plots = plots[0]["workspace"]["definitions"]["data"]["dvc.yaml"]["data"]
+    assert all(
+        name in top_level_plots for name in ("path/to/plot", "path/to/another/plot")
+    )
```

### Comparing `dvc-2.9.5/tests/func/test_scm_context.py` & `dvc-3.0.0a0/tests/func/test_scm_context.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/func/test_stage.py` & `dvc-3.0.0a0/tests/func/test_stage.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,22 +1,23 @@
 import os
-import tempfile
 
 import pytest
 
-from dvc.cli import main
+from dvc.annotations import Annotation
 from dvc.dvcfile import SingleStageFile
-from dvc.fs.local import LocalFileSystem
+from dvc.exceptions import OutputDuplicationError
+from dvc.fs import LocalFileSystem
 from dvc.output import Output
 from dvc.repo import Repo, lock_repo
 from dvc.stage import PipelineStage, Stage
 from dvc.stage.run import run_stage
+from dvc.stage.utils import compute_md5
+from dvc.utils import dict_md5
 from dvc.utils.serialize import dump_yaml, load_yaml
 from dvc.utils.strictyaml import YAMLValidationError
-from tests.basic_env import TestDvc
 
 
 def test_cmd_obj():
     with pytest.raises(YAMLValidationError):
         SingleStageFile.validate({Stage.PARAM_CMD: {}})
 
 
@@ -64,144 +65,146 @@
 
     lst[0][Output.PARAM_CACHE] = True
     lst[1][Output.PARAM_CACHE] = False
     d = {Stage.PARAM_OUTS: lst}
     SingleStageFile.validate(d)
 
 
-class TestReload(TestDvc):
-    def test(self):
-        stages = self.dvc.add(self.FOO)
-        self.assertEqual(len(stages), 1)
-        stage = stages[0]
-        self.assertTrue(stage is not None)
-
-        d = load_yaml(stage.relpath)
-
-        # NOTE: checking that reloaded stage didn't change its checksum
-        md5 = "11111111111111111111111111111111"
-        d[stage.PARAM_MD5] = md5
-        dump_yaml(stage.relpath, d)
-
-        dvcfile = SingleStageFile(self.dvc, stage.relpath)
-        stage = dvcfile.stage
-
-        self.assertTrue(stage is not None)
-        dvcfile.dump(stage)
-
-        d = load_yaml(stage.relpath)
-        self.assertEqual(d[stage.PARAM_MD5], md5)
-
-
-class TestDefaultWorkingDirectory(TestDvc):
-    def test_ignored_in_checksum(self):
-        stage = self.dvc.run(
-            cmd=f"echo test > {self.FOO}",
-            deps=[self.BAR],
-            outs=[self.FOO],
-            single_stage=True,
-        )
-
-        d = stage.dumpd()
-        self.assertNotIn(Stage.PARAM_WDIR, d.keys())
-
-        d = load_yaml(stage.relpath)
-        self.assertNotIn(Stage.PARAM_WDIR, d.keys())
-
-        with self.dvc.lock:
-            stage = SingleStageFile(self.dvc, stage.relpath).stage
-            self.assertFalse(stage.changed())
-
-
-class TestExternalRemoteResolution(TestDvc):
-    def test_remote_output(self):
-        tmp_path = tempfile.mkdtemp()
-        storage = os.path.join(tmp_path, "storage")
-        file_path = os.path.join(storage, "file")
-
-        os.makedirs(storage)
-
-        assert main(["remote", "add", "tmp", tmp_path]) == 0
-        assert main(["remote", "add", "storage", "remote://tmp/storage"]) == 0
-        assert (
-            main(
-                [
-                    "run",
-                    "--single-stage",
-                    "-O",
-                    "remote://storage/file",
-                    f"echo file > {file_path}",
-                ]
-            )
-            == 0
-        )
-
-        assert os.path.exists(file_path)
-
-    def test_remote_dependency(self):
-        tmp_path = tempfile.mkdtemp()
-        storage = os.path.join(tmp_path, "storage")
-        file_path = os.path.join(storage, "file")
-
-        os.makedirs(storage)
-
-        with open(file_path, "w", encoding="utf-8") as fobj:
-            fobj.write("Isle of Dogs")
-
-        assert main(["remote", "add", "tmp", tmp_path]) == 0
-        assert main(["remote", "add", "storage", "remote://tmp/storage"]) == 0
-        assert main(["import-url", "remote://storage/file", "movie.txt"]) == 0
+def test_reload(tmp_dir, dvc):
+    (stage,) = tmp_dir.dvc_gen("foo", "foo")
+    d = load_yaml(stage.relpath)
+
+    # NOTE: checking that reloaded stage didn't change its checksum
+    md5 = "11111111111111111111111111111111"
+    d[stage.PARAM_MD5] = md5
+    dump_yaml(stage.relpath, d)
+
+    dvcfile = SingleStageFile(dvc, stage.relpath)
+    stage = dvcfile.stage
+
+    assert stage is not None
+    dvcfile.dump(stage)
+
+    d = load_yaml(stage.relpath)
+    assert d[stage.PARAM_MD5] == md5
+
+
+def test_default_wdir_ignored_in_checksum(tmp_dir, dvc):
+    tmp_dir.gen("bar", "bar")
+    stage = dvc.run(
+        cmd="cp bar foo",
+        deps=["bar"],
+        outs=["foo"],
+        single_stage=True,
+    )
 
-        assert os.path.exists("movie.txt")
+    d = stage.dumpd()
+    assert Stage.PARAM_WDIR not in d.keys()
+
+    d = load_yaml(stage.relpath)
+    assert Stage.PARAM_WDIR not in d.keys()
+
+    with dvc.lock:
+        stage = SingleStageFile(dvc, stage.relpath).stage
+        assert not stage.changed()
+
+
+def test_external_remote_output_resolution(tmp_dir, dvc, make_remote):
+    tmp_path = make_remote("tmp", default=False)
+    tmp_dir.add_remote(url="remote://tmp/storage", name="storage", default=False)
+    storage = tmp_path / "storage"
+    storage.mkdir()
+    file_path = storage / "file"
+
+    dvc.run(
+        cmd=f"echo file > {file_path}",
+        outs_no_cache=["remote://storage/file"],
+        single_stage=True,
+    )
+    assert os.path.exists(file_path)
+
+
+def test_external_remote_dependency_resolution(tmp_dir, dvc, make_remote):
+    tmp_path = make_remote("tmp", default=False)
+    tmp_dir.add_remote(url="remote://tmp/storage", name="storage", default=False)
+    storage = tmp_path / "storage"
+    storage.mkdir()
+    file_path = storage / "file"
+    file_path.write_text("Isle of Dogs", encoding="utf-8")
+
+    dvc.imp_url("remote://storage/file", "movie.txt")
+    assert (tmp_dir / "movie.txt").read_text() == "Isle of Dogs"
 
 
 def test_md5_ignores_comments(tmp_dir, dvc):
     (stage,) = tmp_dir.dvc_gen("foo", "foo content")
 
     with open(stage.path, "a", encoding="utf-8") as f:
         f.write("# End comment\n")
 
     new_stage = SingleStageFile(dvc, stage.path).stage
     assert not new_stage.changed_stage()
 
 
-def test_meta_is_preserved(tmp_dir, dvc):
-    (stage,) = tmp_dir.dvc_gen("foo", "foo content")
-
-    # Add meta to DVC-file
-    data = (tmp_dir / stage.path).parse()
-    data["meta"] = {"custom_key": 42}
-    (tmp_dir / stage.path).dump(data)
-
-    # Loading and dumping to test that it works and meta is retained
-    dvcfile = SingleStageFile(dvc, stage.path)
-    new_stage = dvcfile.stage
-    dvcfile.dump(new_stage)
-
-    new_data = (tmp_dir / stage.path).parse()
-    assert new_data["meta"] == data["meta"]
+def test_md5_ignores_annotations(tmp_dir, dvc):
+    data = {
+        "desc": "stage desc",
+        "meta": {"key1": "value1", "key2": "value2"},
+        "outs": [
+            {
+                "md5": "d3b07384d113edec49eaa6238ad5ff00",
+                "size": 4,
+                "path": "foo",
+                "desc": "foo desc",
+                "type": "mytype",
+                "labels": ["get-started", "dataset-registry"],
+                "meta": {"key1": "value1"},
+            }
+        ],
+    }
+    (tmp_dir / "foo.dvc").dump(data)
+    stage = dvc.stage.load_one("foo.dvc")
+    assert compute_md5(stage) == "1822617147b53ae6f9eb4b3c87c0b6f3"
+    assert (
+        dict_md5({"outs": [{"md5": "d3b07384d113edec49eaa6238ad5ff00", "path": "foo"}]})
+        == "1822617147b53ae6f9eb4b3c87c0b6f3"
+    )
 
 
-def test_desc_is_preserved(tmp_dir, dvc):
-    (stage,) = tmp_dir.dvc_gen("foo", "foo content")
+def test_meta_desc_is_preserved(tmp_dir, dvc):
+    data = {
+        "desc": "stage desc",
+        "meta": {"key1": "value1", "key2": "value2"},
+        "outs": [
+            {
+                "md5": "d3b07384d113edec49eaa6238ad5ff00",
+                "size": 4,
+                "path": "foo",
+                "desc": "foo desc",
+                "type": "mytype",
+                "labels": ["get-started", "dataset-registry"],
+                "meta": {"key": "value"},
+            }
+        ],
+    }
+    (tmp_dir / "foo.dvc").dump(data)
+    stage = dvc.stage.load_one("foo.dvc")
+
+    assert stage.meta == {"key1": "value1", "key2": "value2"}
+    assert stage.desc == "stage desc"
+    assert stage.outs[0].annot == Annotation(
+        desc="foo desc",
+        type="mytype",
+        labels=["get-started", "dataset-registry"],
+        meta={"key": "value"},
+    )
 
-    data = (tmp_dir / stage.path).parse()
-    stage_desc = "test stage description"
-    out_desc = "test out description"
-    data["desc"] = stage_desc
-    data["outs"][0]["desc"] = out_desc
-    (tmp_dir / stage.path).dump(data)
-
-    dvcfile = SingleStageFile(dvc, stage.path)
-    new_stage = dvcfile.stage
-    dvcfile.dump(new_stage)
-
-    new_data = (tmp_dir / stage.path).parse()
-    assert new_data["desc"] == stage_desc
-    assert new_data["outs"][0]["desc"] == out_desc
+    # sanity check
+    stage.dump()
+    assert (tmp_dir / "foo.dvc").parse() == data
 
 
 def test_parent_repo_collect_stages(tmp_dir, scm, dvc):
     tmp_dir.gen({"subdir": {}})
     tmp_dir.gen({"deep": {"dir": {}}})
     subrepo_dir = tmp_dir / "subdir"
     deep_subrepo_dir = tmp_dir / "deep" / "dir"
@@ -223,19 +226,27 @@
     deep_subrepo_stages = deep_subrepo.stage.collect(None)
 
     assert stages == []
     assert subrepo_stages != []
     assert deep_subrepo_stages != []
 
 
-def test_collect_repo_ignored_dir_unignored_pattern(tmp_dir, dvc, scm):
-    tmp_dir.gen({".gitignore": "data/**\n!data/**/\n!data/**/*.dvc"})
-    scm.add([".gitignore"])
-    (stage,) = tmp_dir.dvc_gen({"data/raw/tracked.csv": "5,6,7,8"})
-    assert dvc.stage.collect_repo() == [stage]
+@pytest.mark.parametrize("with_deps", (False, True))
+def test_collect_symlink(tmp_dir, dvc, with_deps):
+    tmp_dir.gen({"data": {"foo": "foo contents"}})
+    foo_path = os.path.join("data", "foo")
+    dvc.add(foo_path)
+
+    data_link = tmp_dir / "data_link"
+    data_link.symlink_to("data")
+    stage = list(
+        dvc.stage.collect(target=str(data_link / "foo.dvc"), with_deps=with_deps)
+    )[0]
+
+    assert stage.addressing == f"{foo_path}.dvc"
 
 
 def test_stage_strings_representation(tmp_dir, dvc, run_copy):
     tmp_dir.dvc_gen("foo", "foo")
     stage1 = run_copy("foo", "bar", single_stage=True)
     assert stage1.addressing == "bar.dvc"
     assert repr(stage1) == "Stage: 'bar.dvc'"
@@ -284,14 +295,16 @@
     dvc_file = stage.dvcfile
     with dvc.lock:
         stage.remove(purge=False)
     assert stage.name in dvc_file.stages
 
     with dvc.lock:
         stage.remove()
+
+    dvc_file._reset()
     assert stage.name not in dvc_file.stages
     assert "copy-bar-foobar" in dvc_file.stages
 
 
 def test_stage_remove_pointer_stage(tmp_dir, dvc, run_copy):
     (stage,) = tmp_dir.dvc_gen("foo", "foo")
 
@@ -317,7 +330,18 @@
         callback = None
 
     with lock_repo(dvc):
         run_stage(stage, checkpoint_func=callback)
     mock_cmd_run.assert_called_with(
         stage, checkpoint_func=callback, dry=False, run_env=None
     )
+
+
+def test_stage_add_duplicated_output(tmp_dir, dvc):
+    tmp_dir.dvc_gen("foo", "foo")
+    dvc.add("foo")
+
+    with pytest.raises(
+        OutputDuplicationError,
+        match="Use `dvc remove foo.dvc` to stop tracking the overlapping output.",
+    ):
+        dvc.stage.add(name="duplicated", cmd="echo bar > foo", outs=["foo"])
```

### Comparing `dvc-2.9.5/tests/func/test_stage_load.py` & `dvc-3.0.0a0/tests/func/test_stage_load.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import os
 from operator import itemgetter
 
 import pytest
 from funcy import raiser
 
-from dvc.dvcfile import PIPELINE_FILE, FileIsGitIgnored
+from dvc.dvcfile import PROJECT_FILE, FileIsGitIgnored
 from dvc.exceptions import NoOutputOrStageError
 from dvc.repo import Repo
 from dvc.stage.exceptions import (
     StageFileDoesNotExistError,
     StageNameUnspecified,
     StageNotFound,
 )
@@ -42,17 +42,15 @@
 
     run_copy("foo", "foobar", name="copy-foo-foobar")
     assert collect_outs(":copy-foo-foobar") == {"foobar"}
     assert collect_outs(":copy-foo-foobar", with_deps=True) == {
         "foobar",
         "foo",
     }
-    assert collect_outs("dvc.yaml:copy-foo-foobar", recursive=True) == {
-        "foobar"
-    }
+    assert collect_outs("dvc.yaml:copy-foo-foobar", recursive=True) == {"foobar"}
     assert collect_outs("copy-foo-foobar") == {"foobar"}
     assert collect_outs("copy-foo-foobar", with_deps=True) == {"foobar", "foo"}
     assert collect_outs("copy-foo-foobar", recursive=True) == {"foobar"}
 
     run_copy("foobar", "baz", name="copy-foobar-baz")
     assert collect_outs("dvc.yaml") == {"foobar", "baz"}
     assert collect_outs("dvc.yaml", with_deps=True) == {"foobar", "baz", "foo"}
@@ -67,30 +65,26 @@
     assert set(dvc.stage.collect("dir", recursive=True)) == {
         stage1,
         stage2,
         stage3,
     }
 
 
-def test_collect_with_not_existing_output_or_stage_name(
-    tmp_dir, dvc, run_copy
-):
+def test_collect_with_not_existing_output_or_stage_name(tmp_dir, dvc, run_copy):
     with pytest.raises(StageFileDoesNotExistError):
         dvc.stage.collect("some_file")
     tmp_dir.dvc_gen("foo", "foo")
     run_copy("foo", "bar", name="copy-foo-bar")
     with pytest.raises(StageNotFound):
         dvc.stage.collect("some_file")
 
 
 def test_stages(tmp_dir, dvc):
     def collect_stages():
-        return {
-            stage.relpath for stage in Repo(os.fspath(tmp_dir)).index.stages
-        }
+        return {stage.relpath for stage in Repo(os.fspath(tmp_dir)).index.stages}
 
     tmp_dir.dvc_gen({"file": "a", "dir/file": "b", "dir/subdir/file": "c"})
 
     assert collect_stages() == {
         "file.dvc",
         os.path.join("dir", "file.dvc"),
         os.path.join("dir", "subdir", "file.dvc"),
@@ -105,107 +99,75 @@
 def stages(tmp_dir, run_copy):
     stage1, stage2 = tmp_dir.dvc_gen({"foo": "foo", "lorem": "lorem"})
     return {
         "foo-generate": stage1,
         "lorem-generate": stage2,
         "copy-foo-bar": run_copy("foo", "bar", single_stage=True),
         "copy-bar-foobar": run_copy("bar", "foobar", name="copy-bar-foobar"),
-        "copy-lorem-ipsum": run_copy(
-            "lorem", "ipsum", name="copy-lorem-ipsum"
-        ),
+        "copy-lorem-ipsum": run_copy("lorem", "ipsum", name="copy-lorem-ipsum"),
     }
 
 
 def test_collect_not_a_group_stage_with_group_flag(tmp_dir, dvc, stages):
-    assert set(dvc.stage.collect("copy-bar-foobar", accept_group=True)) == {
-        stages["copy-bar-foobar"]
-    }
-    assert set(
-        dvc.stage.collect("copy-bar-foobar", accept_group=True, with_deps=True)
-    ) == {
+    assert set(dvc.stage.collect("copy-bar-foobar")) == {stages["copy-bar-foobar"]}
+    assert set(dvc.stage.collect("copy-bar-foobar", with_deps=True)) == {
         stages["copy-bar-foobar"],
         stages["copy-foo-bar"],
         stages["foo-generate"],
     }
-    assert set(
-        dvc.stage.collect_granular("copy-bar-foobar", accept_group=True)
-    ) == {(stages["copy-bar-foobar"], None)}
-    assert set(
-        dvc.stage.collect_granular(
-            "copy-bar-foobar", accept_group=True, with_deps=True
-        )
-    ) == {
+    assert set(dvc.stage.collect_granular("copy-bar-foobar")) == {
+        (stages["copy-bar-foobar"], None)
+    }
+    assert set(dvc.stage.collect_granular("copy-bar-foobar", with_deps=True)) == {
         (stages["copy-bar-foobar"], None),
         (stages["copy-foo-bar"], None),
         (stages["foo-generate"], None),
     }
 
 
 def test_collect_generated(tmp_dir, dvc):
     d = {
         "vars": [{"vars": [1, 2, 3, 4, 5]}],
-        "stages": {
-            "build": {"foreach": "${vars}", "do": {"cmd": "echo ${item}"}}
-        },
+        "stages": {"build": {"foreach": "${vars}", "do": {"cmd": "echo ${item}"}}},
     }
     (tmp_dir / "dvc.yaml").dump(d)
 
     all_stages = set(dvc.index.stages)
     assert len(all_stages) == 5
 
     assert set(dvc.stage.collect()) == all_stages
-    assert set(dvc.stage.collect("build", accept_group=True)) == all_stages
-    assert (
-        set(dvc.stage.collect("build", accept_group=True, with_deps=True))
-        == all_stages
-    )
+    assert set(dvc.stage.collect("build")) == all_stages
+    assert set(dvc.stage.collect("build", with_deps=True)) == all_stages
     assert set(dvc.stage.collect("build*", glob=True)) == all_stages
-    assert (
-        set(dvc.stage.collect("build*", glob=True, with_deps=True))
-        == all_stages
-    )
+    assert set(dvc.stage.collect("build*", glob=True, with_deps=True)) == all_stages
 
     stages_info = {(stage, None) for stage in all_stages}
-    assert (
-        set(dvc.stage.collect_granular("build", accept_group=True))
-        == stages_info
-    )
-    assert (
-        set(
-            dvc.stage.collect_granular(
-                "build", accept_group=True, with_deps=True
-            )
-        )
-        == stages_info
-    )
+    assert set(dvc.stage.collect_granular("build")) == stages_info
+    assert set(dvc.stage.collect_granular("build", with_deps=True)) == stages_info
 
 
 def test_collect_glob(tmp_dir, dvc, stages):
     assert set(dvc.stage.collect("copy*", glob=True)) == {
         stages[key] for key in ["copy-bar-foobar", "copy-lorem-ipsum"]
     }
-    assert set(
-        dvc.stage.collect("copy-lorem*", glob=True, with_deps=True)
-    ) == {stages[key] for key in ["copy-lorem-ipsum", "lorem-generate"]}
+    assert set(dvc.stage.collect("copy-lorem*", glob=True, with_deps=True)) == {
+        stages[key] for key in ["copy-lorem-ipsum", "lorem-generate"]
+    }
 
 
 def test_collect_granular_with_no_target(tmp_dir, dvc, stages):
-    assert set(map(itemgetter(0), dvc.stage.collect_granular())) == set(
-        stages.values()
+    assert set(map(itemgetter(0), dvc.stage.collect_granular())) == set(stages.values())
+    assert list(map(itemgetter(1), dvc.stage.collect_granular())) == [None] * len(
+        stages
     )
-    assert list(map(itemgetter(1), dvc.stage.collect_granular())) == [
-        None
-    ] * len(stages)
 
 
 def test_collect_granular_with_target(tmp_dir, dvc, stages):
-    assert dvc.stage.collect_granular("bar.dvc") == [
-        (stages["copy-foo-bar"], None)
-    ]
-    assert dvc.stage.collect_granular(PIPELINE_FILE) == [
+    assert dvc.stage.collect_granular("bar.dvc") == [(stages["copy-foo-bar"], None)]
+    assert dvc.stage.collect_granular(PROJECT_FILE) == [
         (stages["copy-bar-foobar"], None),
         (stages["copy-lorem-ipsum"], None),
     ]
     assert dvc.stage.collect_granular(":") == [
         (stages["copy-bar-foobar"], None),
         (stages["copy-lorem-ipsum"], None),
     ]
@@ -217,15 +179,15 @@
     ]
     assert dvc.stage.collect_granular("dvc.yaml:copy-bar-foobar") == [
         (stages["copy-bar-foobar"], None)
     ]
 
     with (tmp_dir / dvc.DVC_DIR).chdir():
         assert dvc.stage.collect_granular(
-            relpath(tmp_dir / PIPELINE_FILE) + ":copy-bar-foobar"
+            relpath(tmp_dir / PROJECT_FILE) + ":copy-bar-foobar"
         ) == [(stages["copy-bar-foobar"], None)]
 
     assert dvc.stage.collect_granular("foobar") == [
         (stages["copy-bar-foobar"], os.path.join(tmp_dir, "foobar"))
     ]
 
 
@@ -268,15 +230,15 @@
         stages["copy-bar-foobar"],
         stages["copy-foo-bar"],
         stages["foo-generate"],
     }
     assert set(
         map(
             itemgetter(0),
-            dvc.stage.collect_granular(PIPELINE_FILE, with_deps=True),
+            dvc.stage.collect_granular(PROJECT_FILE, with_deps=True),
         )
     ) == set(stages.values())
 
 
 def test_collect_granular_same_output_name_stage_name(tmp_dir, dvc, run_copy):
     (stage1,) = tmp_dir.dvc_gen("foo", "foo")
     (stage2,) = tmp_dir.dvc_gen("copy-foo-bar", "copy-foo-bar")
@@ -298,29 +260,23 @@
 
 def test_collect_granular_priority_on_collision(tmp_dir, dvc, run_copy):
     tmp_dir.gen({"dir": {"foo": "foo"}, "foo": "foo"})
     (stage1,) = dvc.add("dir", recursive=True)
     stage2 = run_copy("foo", "bar", name="dir")
 
     assert dvc.stage.collect_granular("dir") == [(stage2, None)]
-    assert dvc.stage.collect_granular("dir", recursive=True) == [
-        (stage1, None)
-    ]
+    assert dvc.stage.collect_granular("dir", recursive=True) == [(stage1, None)]
 
     remove(tmp_dir / "dir")
 
     assert dvc.stage.collect_granular("dir") == [(stage2, None)]
-    assert dvc.stage.collect_granular("dir", recursive=True) == [
-        (stage2, None)
-    ]
+    assert dvc.stage.collect_granular("dir", recursive=True) == [(stage2, None)]
 
 
-def test_collect_granular_collision_output_dir_stage_name(
-    tmp_dir, dvc, run_copy
-):
+def test_collect_granular_collision_output_dir_stage_name(tmp_dir, dvc, run_copy):
     stage1, *_ = tmp_dir.dvc_gen({"dir": {"foo": "foo"}, "foo": "foo"})
     stage3 = run_copy("foo", "bar", name="dir")
 
     assert dvc.stage.collect_granular("dir") == [(stage3, None)]
     assert not dvc.stage.collect_granular("dir", recursive=True)
     assert dvc.stage.collect_granular("./dir") == [
         (stage1, os.path.join(tmp_dir / "dir"))
@@ -328,41 +284,41 @@
 
 
 def test_collect_granular_not_existing_stage_name(tmp_dir, dvc, run_copy):
     tmp_dir.dvc_gen("foo", "foo")
     (stage,) = tmp_dir.dvc_gen("copy-foo-bar", "copy-foo-bar")
     run_copy("foo", "bar", name="copy-foo-bar")
 
-    assert dvc.stage.collect_granular(
-        "copy-foo-bar.dvc:stage_name_not_needed"
-    ) == [(stage, None)]
+    assert dvc.stage.collect_granular("copy-foo-bar.dvc:stage_name_not_needed") == [
+        (stage, None)
+    ]
     with pytest.raises(StageNotFound):
         dvc.stage.collect_granular("dvc.yaml:does-not-exist")
 
 
 def test_get_stages(tmp_dir, dvc, run_copy):
     with pytest.raises(StageFileDoesNotExistError):
         dvc.stage.load_all()
 
     tmp_dir.gen("foo", "foo")
     stage1 = run_copy("foo", "bar", name="copy-foo-bar")
     stage2 = run_copy("bar", "foobar", name="copy-bar-foobar")
 
     assert set(dvc.stage.load_all()) == {stage1, stage2}
-    assert set(dvc.stage.load_all(path=PIPELINE_FILE)) == {stage1, stage2}
+    assert set(dvc.stage.load_all(path=PROJECT_FILE)) == {stage1, stage2}
     assert set(dvc.stage.load_all(name="copy-bar-foobar")) == {stage2}
-    assert set(
-        dvc.stage.load_all(path=PIPELINE_FILE, name="copy-bar-foobar")
-    ) == {stage2}
+    assert set(dvc.stage.load_all(path=PROJECT_FILE, name="copy-bar-foobar")) == {
+        stage2
+    }
 
     with pytest.raises(StageFileDoesNotExistError):
-        dvc.stage.load_all(path=relpath(tmp_dir / ".." / PIPELINE_FILE))
+        dvc.stage.load_all(path=relpath(tmp_dir / ".." / PROJECT_FILE))
 
     with pytest.raises(StageNotFound):
-        dvc.stage.load_all(path=PIPELINE_FILE, name="copy")
+        dvc.stage.load_all(path=PROJECT_FILE, name="copy")
 
 
 def test_get_stages_old_dvcfile(tmp_dir, dvc):
     (stage1,) = tmp_dir.dvc_gen("foo", "foo")
     assert set(dvc.stage.load_all("foo.dvc")) == {stage1}
     assert set(dvc.stage.load_all("foo.dvc", name="foo-generate")) == {stage1}
 
@@ -374,19 +330,17 @@
     tmp_dir.gen("foo", "foo")
     stage1 = run_copy("foo", "bar", name="copy-foo-bar")
 
     with pytest.raises(StageNameUnspecified):
         dvc.stage.load_one()
 
     with pytest.raises(StageNameUnspecified):
-        dvc.stage.load_one(path=PIPELINE_FILE)
+        dvc.stage.load_one(path=PROJECT_FILE)
 
-    assert (
-        dvc.stage.load_one(path=PIPELINE_FILE, name="copy-foo-bar") == stage1
-    )
+    assert dvc.stage.load_one(path=PROJECT_FILE, name="copy-foo-bar") == stage1
     assert dvc.stage.load_one(name="copy-foo-bar") == stage1
 
     with pytest.raises(StageFileDoesNotExistError):
         dvc.stage.load_one(path="something.yaml", name="name")
 
     with pytest.raises(StageNotFound):
         dvc.stage.load_one(name="random_name")
@@ -402,74 +356,59 @@
 
 def test_collect_optimization(tmp_dir, dvc, mocker):
     (stage,) = tmp_dir.dvc_gen("foo", "foo text")
 
     # Forget cached stages and graph and error out on collection
     dvc._reset()
     mocker.patch(
-        "dvc.repo.index.Index.stages",
+        "dvc.repo.Repo.index",
         property(raiser(Exception("Should not collect"))),
     )
 
     # Should read stage directly instead of collecting the whole graph
     dvc.stage.collect(stage.path)
     dvc.stage.collect_granular(stage.path)
 
 
 def test_collect_optimization_on_stage_name(tmp_dir, dvc, mocker, run_copy):
     tmp_dir.dvc_gen("foo", "foo")
     stage = run_copy("foo", "bar", name="copy-foo-bar")
     # Forget cached stages and graph and error out on collection
     dvc._reset()
     mocker.patch(
-        "dvc.repo.index.Index.stages",
+        "dvc.repo.Repo.index",
         property(raiser(Exception("Should not collect"))),
     )
 
     # Should read stage directly instead of collecting the whole graph
     assert dvc.stage.collect("copy-foo-bar") == [stage]
     assert dvc.stage.collect_granular("copy-foo-bar") == [(stage, None)]
 
 
 def test_collect_repo_callback(tmp_dir, dvc, mocker):
     mock = mocker.Mock()
     dvc.stage_collection_error_handler = mock
 
     (stage,) = tmp_dir.dvc_gen("foo", "foo")
-    (tmp_dir / PIPELINE_FILE).dump({"stages": {"cmd": "echo hello world"}})
+    (tmp_dir / PROJECT_FILE).dump({"stages": {"cmd": "echo hello world"}})
 
     dvc._reset()
     assert dvc.index.stages == [stage]
     mock.assert_called_once()
 
     file_path, exc = mock.call_args[0]
-    assert file_path == PIPELINE_FILE
+    assert file_path == PROJECT_FILE
     assert isinstance(exc, YAMLValidationError)
 
 
-def test_gitignored_collect_repo(tmp_dir, dvc, scm):
-    (stage,) = tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar"}})
-
-    assert dvc.stage.collect_repo() == [stage]
-
-    scm.ignore(stage.path)
-    scm._reset()
-
-    assert not dvc.stage.collect_repo()
-
-
-def test_gitignored_file_try_collect_granular_for_data_files(
-    tmp_dir, dvc, scm
-):
+def test_gitignored_file_try_collect_granular_for_data_files(tmp_dir, dvc, scm):
     (stage,) = tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar"}})
     path = os.path.join("data", "foo")
 
-    assert dvc.stage.collect_granular(path) == [
-        (stage, os.path.join(tmp_dir, path))
-    ]
+    assert dvc.stage.collect_granular(path) == [(stage, os.path.join(tmp_dir, path))]
 
     scm.ignore(stage.path)
     dvc._reset()
 
     with pytest.raises(NoOutputOrStageError):
         dvc.stage.collect_granular(path)
```

### Comparing `dvc-2.9.5/tests/func/test_state.py` & `dvc-3.0.0a0/tests/func/test_state.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,35 +1,33 @@
 import os
-import re
 
-from dvc.hash_info import HashInfo
-from dvc.repo import Repo
-from dvc.state import State
-from dvc.utils import file_md5
+from dvc_data.hashfile.hash import file_md5
+from dvc_data.hashfile.hash_info import HashInfo
+from dvc_data.hashfile.state import State
 
 
 def test_state(tmp_dir, dvc):
     tmp_dir.gen("foo", "foo content")
     path = tmp_dir / "foo"
     hash_info = HashInfo("md5", file_md5(path, dvc.fs))
 
     state = State(dvc.root_dir, dvc.tmp_dir, dvc.dvcignore)
 
-    state.save(path, dvc.fs, hash_info)
-    assert state.get(path, dvc.fs)[1] == hash_info
+    state.save(str(path), dvc.fs, hash_info)
+    assert state.get(str(path), dvc.fs)[1] == hash_info
 
     path.unlink()
     path.write_text("1")
 
-    assert state.get(path, dvc.fs) == (None, None)
+    assert state.get(str(path), dvc.fs) == (None, None)
 
     hash_info = HashInfo("md5", file_md5(path, dvc.fs))
-    state.save(path, dvc.fs, hash_info)
+    state.save(str(path), dvc.fs, hash_info)
 
-    assert state.get(path, dvc.fs)[1] == hash_info
+    assert state.get(str(path), dvc.fs)[1] == hash_info
 
 
 def test_state_overflow(tmp_dir, dvc):
     # NOTE: trying to add more entries than state can handle,
     # to see if it will clean up and vacuum successfully
     dvc.config["state"]["row_limit"] = 10
 
@@ -67,21 +65,7 @@
     assert set(dvc.state.get_unused_links(links, dvc.fs)) == set()
     assert set(
         dvc.state.get_unused_links(
             (links[:1] + [os.path.join(dvc.root_dir, "not-existing-file")]),
             dvc.fs,
         )
     ) == {"bar"}
-
-
-def test_state_dir_config(make_tmp_dir, dvc):
-    assert dvc.state.tmp_dir == dvc.tmp_dir
-
-    index_dir = str(make_tmp_dir("tmp_index"))
-    repo = Repo(config={"state": {"dir": index_dir}})
-    assert os.path.dirname(repo.state.tmp_dir) == os.path.join(
-        index_dir, ".dvc"
-    )
-    assert re.match(
-        r"^test_state_dir_config0-([0-9a-f]+)$",
-        os.path.basename(repo.state.tmp_dir),
-    )
```

### Comparing `dvc-2.9.5/tests/func/test_update.py` & `dvc-3.0.0a0/tests/func/test_update.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 import os
 
 import pytest
 
-from dvc.dvcfile import Dvcfile
+from dvc.dependency import base
+from dvc.dvcfile import load_file
 from dvc.exceptions import InvalidArgumentError
-from tests.unit.fs.test_repo import make_subrepo
+from dvc.testing.tmp_dir import make_subrepo
 
 
 @pytest.mark.parametrize("cached", [True, False])
 def test_update_import(tmp_dir, dvc, erepo_dir, cached):
     gen = erepo_dir.dvc_gen if cached else erepo_dir.scm_gen
 
     with erepo_dir.branch("branch", new=True), erepo_dir.chdir():
@@ -44,26 +45,18 @@
         )
         new_rev = erepo_dir.scm.get_rev()
 
     assert old_rev != new_rev
 
     assert dvc.status() == {
         "dir.dvc": [
-            {
-                "changed deps": {
-                    f"dir ({os.fspath(erepo_dir)})": "update available"
-                }
-            }
+            {"changed deps": {f"dir ({os.fspath(erepo_dir)})": "update available"}}
         ],
         "version.dvc": [
-            {
-                "changed deps": {
-                    f"version ({os.fspath(erepo_dir)})": "update available"
-                }
-            }
+            {"changed deps": {f"version ({os.fspath(erepo_dir)})": "update available"}}
         ],
     }
 
     (stage,) = dvc.update(stage.path)
     (dir_stage,) = dvc.update(dir_stage.path)
     assert dvc.status() == {}
 
@@ -93,17 +86,15 @@
         "rev": "branch",
         "rev_lock": old_rev,
     }
 
     new_rev = None
     with erepo_dir.branch("branch", new=False), erepo_dir.chdir():
         erepo_dir.scm.gitpython.repo.index.remove(["version"])
-        erepo_dir.dvc_gen(
-            "version", "updated", commit="upgrade to DVC tracking"
-        )
+        erepo_dir.dvc_gen("version", "updated", commit="upgrade to DVC tracking")
         new_rev = erepo_dir.scm.get_rev()
 
     assert old_rev != new_rev
 
     (status,) = dvc.status([stage.path])["version.dvc"]
     (changed_dep,) = list(status["changed deps"].items())
     assert changed_dep[0].startswith("version ")
@@ -112,15 +103,15 @@
     dvc.update([stage.path])
 
     assert dvc.status([stage.path]) == {}
 
     assert imported.is_file()
     assert imported.read_text() == "updated"
 
-    stage = Dvcfile(dvc, stage.path).stage
+    stage = load_file(dvc, stage.path).stage
     assert stage.deps[0].def_repo == {
         "url": os.fspath(erepo_dir),
         "rev": "branch",
         "rev_lock": new_rev,
     }
 
 
@@ -138,28 +129,65 @@
         git_dir.dvc_gen("file", "second version", commit="with dvc")
         new_rev = git_dir.scm.get_rev()
 
     assert old_rev != new_rev
 
     assert dvc.status([stage.path]) == {
         "file.dvc": [
-            {
-                "changed deps": {
-                    f"file ({os.fspath(git_dir)})": "update available"
-                }
-            }
+            {"changed deps": {f"file ({os.fspath(git_dir)})": "update available"}}
         ]
     }
 
     dvc.update([stage.path])
 
     assert (tmp_dir / "file").read_text() == "second version"
     assert dvc.status([stage.path]) == {}
 
 
+def test_update_unchanged(tmp_dir, dvc, erepo_dir, mocker):
+    with erepo_dir.chdir():
+        erepo_dir.dvc_gen("file", "file content", commit="add file")
+
+    assert (erepo_dir / "file").exists()
+    stage = dvc.imp(os.fspath(erepo_dir), "file")
+
+    spy = mocker.spy(base, "fs_download")
+    dvc.update([stage.path])
+
+    assert not spy.called
+
+
+@pytest.mark.parametrize("outs_exist", (False, True))
+def test_update_no_download(tmp_dir, dvc, erepo_dir, outs_exist, mocker):
+    with erepo_dir.chdir():
+        erepo_dir.dvc_gen("file", "file content", commit="add file")
+        initial_rev = erepo_dir.scm.get_rev()
+
+    stage = dvc.imp(os.fspath(erepo_dir), "file", no_download=not outs_exist)
+
+    assert stage.deps[0].def_repo["rev_lock"] == initial_rev
+
+    dst = tmp_dir / "file"
+    assert dst.exists() is outs_exist
+
+    with erepo_dir.chdir():
+        erepo_dir.dvc_gen("file", "updated file content", commit="update file")
+        new_rev = erepo_dir.scm.get_rev()
+
+    updated_stage = dvc.update([stage.path], rev=new_rev, no_download=True)[0]
+    assert not dst.exists()
+
+    assert updated_stage.deps[0].def_repo["rev_lock"] == new_rev
+
+    # output must have no information since no_download=True
+    out = updated_stage.outs[0]
+    assert out.hash_info.value is None
+    assert out.meta.size is None
+
+
 def test_update_import_url(tmp_dir, dvc, workspace):
     workspace.gen("file", "file content")
 
     dst = tmp_dir / "imported_file"
     stage = dvc.imp_url("remote://workspace/file", os.fspath(dst))
 
     assert dst.is_file()
@@ -172,14 +200,55 @@
     dvc.update([stage.path])
     assert dvc.status([stage.path]) == {}
 
     assert dst.is_file()
     assert dst.read_text() == "updated file content"
 
 
+@pytest.mark.parametrize("outs_exist", (False, True))
+def test_update_import_url_no_download(tmp_dir, dvc, workspace, outs_exist, mocker):
+    workspace.gen("file", "file content")
+
+    dst = tmp_dir / "imported_file"
+    stage = dvc.imp_url(
+        "remote://workspace/file", os.fspath(dst), no_download=not outs_exist
+    )
+
+    assert dst.exists() is outs_exist
+    hash_info = stage.deps[0].hash_info
+    assert hash_info.value == "d10b4c3ff123b26dc068d43a8bef2d23"
+
+    workspace.gen("file", "updated file content")
+
+    updated_stage = dvc.update([stage.path], no_download=True)[0]
+    assert not dst.exists()
+
+    updated_hash_info = updated_stage.deps[0].hash_info
+    assert updated_hash_info != hash_info
+    assert updated_hash_info.value == "6ffba511ce3aa40b8231d1b1f8c5fba5"
+
+    # output must have no information since no_download=True
+    out = updated_stage.outs[0]
+    assert out.hash_info.value is None
+    assert out.hash_info.name is None
+    assert out.meta.size is None
+
+
+def test_update_import_url_unchanged(tmp_dir, dvc, workspace, mocker):
+    workspace.gen("file", "file content")
+
+    dst = tmp_dir / "imported_file"
+    stage = dvc.imp_url("remote://workspace/file", os.fspath(dst))
+
+    spy = mocker.spy(base, "fs_download")
+
+    dvc.update([stage.path])
+    assert not spy.called
+
+
 def test_update_rev(tmp_dir, dvc, scm, git_dir):
     with git_dir.chdir():
         git_dir.scm_gen({"foo": "foo"}, commit="first")
 
     dvc.imp(os.fspath(git_dir), "foo")
     assert (tmp_dir / "foo.dvc").exists()
 
@@ -194,24 +263,24 @@
     stage = dvc.update(["foo.dvc"], rev="branch1")[0]
     assert stage.deps[0].def_repo == {
         "url": os.fspath(git_dir),
         "rev": "branch1",
         "rev_lock": branch1_head,
     }
     with open(tmp_dir / "foo", encoding="utf-8") as f:
-        assert "foobar" == f.read()
+        assert f.read() == "foobar"
 
     stage = dvc.update(["foo.dvc"], rev="branch2")[0]
     assert stage.deps[0].def_repo == {
         "url": os.fspath(git_dir),
         "rev": "branch2",
         "rev_lock": branch2_head,
     }
     with open(tmp_dir / "foo", encoding="utf-8") as f:
-        assert "foobar foo" == f.read()
+        assert f.read() == "foobar foo"
 
 
 def test_update_recursive(tmp_dir, dvc, erepo_dir):
     with erepo_dir.branch("branch", new=True), erepo_dir.chdir():
         erepo_dir.scm_gen(
             {"foo1": "text1", "foo2": "text2", "foo3": "text3"},
             commit="add foo files",
@@ -232,20 +301,16 @@
         os.fspath(erepo_dir),
         "foo3",
         os.path.join("dir", "subdir", "foo3"),
         rev="branch",
     )
 
     assert (tmp_dir / os.path.join("dir", "foo1")).read_text() == "text1"
-    assert (
-        tmp_dir / os.path.join("dir", "subdir", "foo2")
-    ).read_text() == "text2"
-    assert (
-        tmp_dir / os.path.join("dir", "subdir", "foo3")
-    ).read_text() == "text3"
+    assert (tmp_dir / os.path.join("dir", "subdir", "foo2")).read_text() == "text2"
+    assert (tmp_dir / os.path.join("dir", "subdir", "foo3")).read_text() == "text3"
 
     assert stage1.deps[0].def_repo["rev_lock"] == old_rev
     assert stage2.deps[0].def_repo["rev_lock"] == old_rev
     assert stage3.deps[0].def_repo["rev_lock"] == old_rev
 
     with erepo_dir.branch("branch", new=False), erepo_dir.chdir():
         erepo_dir.scm_gen(
@@ -255,17 +320,17 @@
         )
         new_rev = erepo_dir.scm.get_rev()
 
     assert old_rev != new_rev
 
     dvc.update(["dir"], recursive=True)
 
-    stage1 = Dvcfile(dvc, stage1.path).stage
-    stage2 = Dvcfile(dvc, stage2.path).stage
-    stage3 = Dvcfile(dvc, stage3.path).stage
+    stage1 = load_file(dvc, stage1.path).stage
+    stage2 = load_file(dvc, stage2.path).stage
+    stage3 = load_file(dvc, stage3.path).stage
     assert stage1.deps[0].def_repo["rev_lock"] == new_rev
     assert stage2.deps[0].def_repo["rev_lock"] == new_rev
     assert stage3.deps[0].def_repo["rev_lock"] == new_rev
 
 
 @pytest.mark.parametrize("is_dvc", [True, False])
 def test_update_from_subrepos(tmp_dir, dvc, erepo_dir, is_dvc):
@@ -368,22 +433,18 @@
         "bar": {"baz": "baz", "baz2": "baz2"},
     }
 
 
 def test_update_import_url_to_remote_directory_same_hash(
     tmp_dir, dvc, local_workspace, local_remote
 ):
-    local_workspace.gen(
-        {"data": {"foo": "foo", "bar": {"baz": "baz"}, "same": "same"}}
-    )
+    local_workspace.gen({"data": {"foo": "foo", "bar": {"baz": "baz"}, "same": "same"}})
     stage = dvc.imp_url("remote://workspace/data", to_remote=True)
 
-    local_workspace.gen(
-        {"data": {"foo": "baz", "bar": {"baz": "foo"}, "same": "same"}}
-    )
+    local_workspace.gen({"data": {"foo": "baz", "bar": {"baz": "foo"}, "same": "same"}})
     stage = dvc.update(stage.path, to_remote=True)
 
     dvc.pull("data")
     assert (tmp_dir / "data").read_text() == {
         "foo": "baz",
         "bar": {"baz": "foo"},
         "same": "same",
```

### Comparing `dvc-2.9.5/tests/func/test_utils.py` & `dvc-3.0.0a0/tests/func/test_utils.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,16 +1,13 @@
-from dvc import utils
-from dvc.fs.local import LocalFileSystem
+import re
 
+import pytest
 
-def test_file_md5_crlf(tmp_dir):
-    fs = LocalFileSystem()
-    tmp_dir.gen("cr", b"a\nb\nc")
-    tmp_dir.gen("crlf", b"a\r\nb\r\nc")
-    assert utils.file_md5("cr", fs) == utils.file_md5("crlf", fs)
+from dvc import utils
+from dvc.exceptions import DvcException
 
 
 def test_dict_md5():
     d = {
         "cmd": "python code.py foo file1",
         "locked": "true",
         "outs": [
@@ -38,7 +35,14 @@
         "|                 |\n"
         "|     message     |\n"
         "|                 |\n"
         "+-----------------+\n"
     )
 
     assert expected == utils.boxify("message")
+
+
+def test_glob_no_match():
+    with pytest.raises(
+        DvcException, match=re.escape("Glob ['invalid*'] has no matches.")
+    ):
+        utils.glob_targets(["invalid*"], glob=True)
```

### Comparing `dvc-2.9.5/tests/func/utils/test_strict_yaml.py` & `dvc-3.0.0a0/tests/func/utils/test_strict_yaml.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 import os
 
 import pytest
 from ruamel.yaml import __with_libyaml__ as ruamel_clib
 
 from dvc.cli import main
-from dvc.ui import ui
 
 DUPLICATE_KEYS = """\
 stages:
   stage1:
     cmd: python train.py
     cmd: python train.py
 """
@@ -321,49 +320,87 @@
         FOREACH_WITH_CMD_DO_MISSING_OUTPUT,
     ),
     # merge conflicts
     "merge_conflicts": (MERGE_CONFLICTS, MERGE_CONFLICTS_OUTPUT),
 }
 
 
-@pytest.mark.parametrize(
-    "text, expected", examples.values(), ids=examples.keys()
-)
-def test_exceptions(tmp_dir, dvc, capsys, text, expected, mocker):
+@pytest.fixture
+def force_posixpath(mocker):
     # make it always return posix path, easier for validating error messages
     mocker.patch(
-        "dvc.utils.strictyaml.make_relpath", return_value="./dvc.yaml"
+        "dvc.utils.strictyaml.make_relpath",
+        return_value="./dvc.yaml",
     )
 
-    console = ui.error_console
-    original_printer = console.print
 
-    def print_with_fixed_width(*args, **kwargs):
-        console.options.min_width = console.options.max_width = 80
-        console.width = kwargs["width"] = 80
-        return original_printer(*args, **kwargs)
+@pytest.fixture
+def fixed_width_term(mocker):
+    """Fixed width console."""
+    from rich.console import Console
+
+    mocker.patch.object(
+        Console, "width", new_callable=mocker.PropertyMock(return_value=80)
+    )
 
-    mocker.patch.object(console, "print", print_with_fixed_width)
 
+@pytest.mark.parametrize("text, expected", examples.values(), ids=examples.keys())
+def test_exceptions(
+    tmp_dir,
+    dvc,
+    capsys,
+    force_posixpath,
+    fixed_width_term,
+    text,
+    expected,
+):
     tmp_dir.gen("dvc.yaml", text)
 
     capsys.readouterr()  # clear outputs
     assert main(["stage", "list"]) != 0
     out, err = capsys.readouterr()
 
     assert not out
 
     # strip whitespace on the right: output is always left-justified
     # by rich.syntax.Syntax:
-    for expected_line, err_line in zip(
-        expected.splitlines(), err.splitlines()
-    ):
+    for expected_line, err_line in zip(expected.splitlines(), err.splitlines()):
         assert expected_line == err_line.rstrip(" ")
 
 
+@pytest.mark.parametrize(
+    "text, expected",
+    [
+        (DUPLICATE_KEYS, "'./dvc.yaml' is invalid in revision '{short_rev}'."),
+        (
+            MISSING_CMD,
+            "'./dvc.yaml' validation failed in revision '{short_rev}'.",
+        ),
+    ],
+)
+def test_on_revision(
+    tmp_dir,
+    scm,
+    dvc,
+    force_posixpath,
+    fixed_width_term,
+    capsys,
+    text,
+    expected,
+):
+    tmp_dir.scm_gen("dvc.yaml", text, commit="add dvc.yaml")
+    capsys.readouterr()  # clear outputs
+
+    assert main(["ls", f"file://{tmp_dir.as_posix()}", "--rev", "HEAD"]) != 0
+
+    out, err = capsys.readouterr()
+    assert not out
+    assert expected.format(short_rev=scm.get_rev()[:7]) in err
+
+
 def test_make_relpath(tmp_dir, monkeypatch):
     from dvc.utils.strictyaml import make_relpath
 
     path = tmp_dir / "dvc.yaml"
     expected_path = "./dvc.yaml" if os.name == "posix" else ".\\dvc.yaml"
     assert make_relpath(path) == expected_path
 
@@ -386,17 +423,14 @@
         side_effect=ValueError,
     )
 
     # syntax errors
     dvc_file = tmp_dir / "dvc.yaml"
     dvc_file.write_text(MAPPING_VALUES_NOT_ALLOWED)
     assert main(["stage", "list"]) != 0
-    assert (
-        "unable to read: 'dvc.yaml', "
-        "YAML file structure is corrupted" in caplog.text
-    )
+    assert "unable to read: 'dvc.yaml', YAML file structure is corrupted" in caplog.text
 
     caplog.clear()
     # validation error
     dvc_file.dump({"stages": {"stage1": None}})
     assert main(["stage", "list"]) != 0
     assert "dvc.yaml' validation failed" in caplog.text
```

### Comparing `dvc-2.9.5/tests/remotes/git_server.py` & `dvc-3.0.0a0/tests/remotes/git_server.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 import pytest
 from dvc_ssh.tests.cloud import SSH, TEST_SSH_KEY_PATH, TEST_SSH_USER
 
 
 class GitSSH(SSH):
     @staticmethod
-    def get_url(host, port):  # pylint: disable=arguments-differ
+    def get_url(host, port):
         return f"ssh://{host}:{port}/tmp/data/git"
 
 
 @pytest.fixture
-def git_server(test_config, docker_compose, docker_services):
+def git_server(request, test_config):
     import asyncssh
     from sshfs import SSHFileSystem
 
     test_config.requires("ssh")
+    docker_services = request.getfixturevalue("docker_services")
     conn_info = {
         "host": "127.0.0.1",
         "port": docker_services.port_for("git-server", 2222),
     }
 
     def get_fs():
         return SSHFileSystem(
@@ -40,15 +41,15 @@
     return conn_info
 
 
 @pytest.fixture
 def git_ssh_connection(git_server):
     from sshfs import SSHFileSystem
 
-    yield SSHFileSystem(
+    return SSHFileSystem(
         host=git_server["host"],
         port=git_server["port"],
         username=TEST_SSH_USER,
         client_keys=[TEST_SSH_KEY_PATH],
     )
```

### Comparing `dvc-2.9.5/tests/remotes/user.key` & `dvc-3.0.0a0/tests/remotes/user.key`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/remotes_env.sample` & `dvc-3.0.0a0/tests/remotes_env.sample`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/unit/command/ls/test_ls.py` & `dvc-3.0.0a0/tests/unit/command/ls/test_ls.py`

 * *Files 12% similar despite different names*

```diff
@@ -14,50 +14,40 @@
     assert cmd.run() == 0
     return m
 
 
 def test_list(mocker):
     url = "local_dir"
     m = _test_cli(mocker, url)
-    m.assert_called_once_with(
-        url, None, recursive=False, rev=None, dvc_only=False
-    )
+    m.assert_called_once_with(url, None, recursive=False, rev=None, dvc_only=False)
 
 
 def test_list_recursive(mocker):
     url = "local_dir"
     m = _test_cli(mocker, url, "-R")
-    m.assert_called_once_with(
-        url, None, recursive=True, rev=None, dvc_only=False
-    )
+    m.assert_called_once_with(url, None, recursive=True, rev=None, dvc_only=False)
 
 
 def test_list_git_ssh_rev(mocker):
     url = "git@github.com:repo"
     m = _test_cli(mocker, url, "--rev", "123")
-    m.assert_called_once_with(
-        url, None, recursive=False, rev="123", dvc_only=False
-    )
+    m.assert_called_once_with(url, None, recursive=False, rev="123", dvc_only=False)
 
 
 def test_list_targets(mocker):
     url = "local_dir"
     target = "subdir"
     m = _test_cli(mocker, url, target)
-    m.assert_called_once_with(
-        url, target, recursive=False, rev=None, dvc_only=False
-    )
+    m.assert_called_once_with(url, target, recursive=False, rev=None, dvc_only=False)
 
 
 def test_list_outputs_only(mocker):
     url = "local_dir"
     m = _test_cli(mocker, url, None, "--dvc-only")
-    m.assert_called_once_with(
-        url, None, recursive=False, rev=None, dvc_only=True
-    )
+    m.assert_called_once_with(url, None, recursive=False, rev=None, dvc_only=True)
 
 
 def test_show_json(mocker, capsys):
     cli_args = parse_args(["list", "local_dir", "--json"])
     assert cli_args.func == CmdList
 
     cmd = cli_args.func(cli_args)
@@ -71,17 +61,15 @@
 
 
 def test_show_colors(mocker, capsys, monkeypatch):
     cli_args = parse_args(["list", "local_dir"])
     assert cli_args.func == CmdList
     cmd = cli_args.func(cli_args)
 
-    monkeypatch.setenv(
-        "LS_COLORS", "ex=01;32:rs=0:di=01;34:*.xml=01;31:*.dvc=01;33:"
-    )
+    monkeypatch.setenv("LS_COLORS", "ex=01;32:rs=0:di=01;34:*.xml=01;31:*.dvc=01;33:")
     result = [
         {"isdir": False, "isexec": 0, "isout": False, "path": ".dvcignore"},
         {"isdir": False, "isexec": 0, "isout": False, "path": ".gitignore"},
         {"isdir": False, "isexec": 0, "isout": False, "path": "README.md"},
         {"isdir": True, "isexec": 0, "isout": True, "path": "data"},
         {"isdir": False, "isexec": 0, "isout": True, "path": "structure.xml"},
         {
```

### Comparing `dvc-2.9.5/tests/unit/command/ls/test_ls_colors.py` & `dvc-3.0.0a0/tests/unit/command/ls/test_ls_colors.py`

 * *Files 1% similar despite different names*

```diff
@@ -22,17 +22,15 @@
 def test_ls_colors_out_dir():
     ls_colors = LsColors(LsColors.default)
     assert colorize(ls_colors)("dir", "do") == "\x1b[01;34mdir\x1b[0m"
 
 
 def test_ls_colors_out_exec():
     ls_colors = LsColors(LsColors.default)
-    assert (
-        colorize(ls_colors)("script.sh", "eo") == "\x1b[01;32mscript.sh\x1b[0m"
-    )
+    assert colorize(ls_colors)("script.sh", "eo") == "\x1b[01;32mscript.sh\x1b[0m"
 
 
 def test_ls_colors_out_ext():
     ls_colors = LsColors(LsColors.default + ":*.xml=01;33")
     assert colorize(ls_colors)("file.xml", "o") == "\x1b[01;33mfile.xml\x1b[0m"
 
 
@@ -44,17 +42,15 @@
 def test_ls_colors_dir():
     ls_colors = LsColors(LsColors.default)
     assert colorize(ls_colors)("dir", "d") == "\x1b[01;34mdir\x1b[0m"
 
 
 def test_ls_colors_exec():
     ls_colors = LsColors(LsColors.default)
-    assert (
-        colorize(ls_colors)("script.sh", "e") == "\x1b[01;32mscript.sh\x1b[0m"
-    )
+    assert colorize(ls_colors)("script.sh", "e") == "\x1b[01;32mscript.sh\x1b[0m"
 
 
 def test_ls_colors_ext():
     ls_colors = LsColors(LsColors.default + ":*.xml=01;33")
     assert colorize(ls_colors)("file.xml") == "\x1b[01;33mfile.xml\x1b[0m"
 
 
@@ -64,10 +60,8 @@
     colorizer = colorize(ls_colors)
 
     assert colorizer(".dvcignore") == ".dvcignore"
     assert colorizer(".gitignore") == ".gitignore"
     assert colorizer("README.md") == "README.md"
     assert colorizer("data", "d") == "\x1b[01;34mdata\x1b[0m"
     assert colorizer("structure.xml") == "\x1b[01;31mstructure.xml\x1b[0m"
-    assert (
-        colorizer("structure.xml.dvc") == "\x1b[01;33mstructure.xml.dvc\x1b[0m"
-    )
+    assert colorizer("structure.xml.dvc") == "\x1b[01;33mstructure.xml.dvc\x1b[0m"
```

### Comparing `dvc-2.9.5/tests/unit/command/test_add.py` & `dvc-3.0.0a0/tests/unit/command/test_add.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,16 +10,14 @@
             "add",
             "--recursive",
             "--no-commit",
             "--external",
             "--glob",
             "--file",
             "file",
-            "--desc",
-            "stage description",
             "data",
         ]
     )
     assert cli_args.func == CmdAdd
 
     cmd = cli_args.func(cli_args)
     m = mocker.patch.object(cmd.repo, "add", autospec=True)
@@ -32,16 +30,16 @@
         no_commit=True,
         glob=True,
         fname="file",
         external=True,
         out=None,
         remote=None,
         to_remote=False,
-        desc="stage description",
         jobs=None,
+        force=False,
     )
 
 
 def test_add_to_remote(mocker):
     cli_args = parse_args(
         [
             "add",
@@ -66,23 +64,21 @@
         no_commit=False,
         glob=False,
         fname=None,
         external=False,
         out="bar",
         remote="remote",
         to_remote=True,
-        desc=None,
         jobs=None,
+        force=False,
     )
 
 
 def test_add_to_remote_invalid_combinations(mocker, caplog):
-    cli_args = parse_args(
-        ["add", "s3://bucket/foo", "s3://bucket/bar", "--to-remote"]
-    )
+    cli_args = parse_args(["add", "s3://bucket/foo", "s3://bucket/bar", "--to-remote"])
     assert cli_args.func == CmdAdd
 
     cmd = cli_args.func(cli_args)
     with caplog.at_level(logging.ERROR, logger="dvc"):
         assert cmd.run() == 1
         expected_msg = "multiple targets can't be used with --to-remote"
         assert expected_msg in caplog.text
@@ -94,17 +90,15 @@
         with caplog.at_level(logging.ERROR, logger="dvc"):
             assert cmd.run() == 1
             expected_msg = f"{option} can't be used without --to-remote"
             assert expected_msg in caplog.text
 
 
 def test_add_to_cache_invalid_combinations(mocker, caplog):
-    cli_args = parse_args(
-        ["add", "s3://bucket/foo", "s3://bucket/bar", "-o", "foo"]
-    )
+    cli_args = parse_args(["add", "s3://bucket/foo", "s3://bucket/bar", "-o", "foo"])
     assert cli_args.func == CmdAdd
 
     cmd = cli_args.func(cli_args)
     with caplog.at_level(logging.ERROR, logger="dvc"):
         assert cmd.run() == 1
         expected_msg = "multiple targets can't be used with -o"
         assert expected_msg in caplog.text
```

### Comparing `dvc-2.9.5/tests/unit/command/test_cache.py` & `dvc-3.0.0a0/tests/unit/command/test_cache.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/unit/command/test_checkout.py` & `dvc-3.0.0a0/tests/unit/command/test_checkout.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,15 +1,13 @@
 from dvc.cli import parse_args
 from dvc.commands.checkout import CmdCheckout, log_changes
 
 
 def test_checkout(tmp_dir, dvc, mocker):
-    cli_args = parse_args(
-        ["checkout", "foo.dvc", "bar.dvc", "--relink", "--with-deps"]
-    )
+    cli_args = parse_args(["checkout", "foo.dvc", "bar.dvc", "--relink", "--with-deps"])
     assert cli_args.func == CmdCheckout
 
     cmd = cli_args.func(cli_args)
     m = mocker.patch("dvc.repo.Repo.checkout")
 
     assert cmd.run() == 0
     m.assert_called_once_with(
```

### Comparing `dvc-2.9.5/tests/unit/command/test_config.py` & `dvc-3.0.0a0/tests/unit/command/test_config.py`

 * *Files 21% similar despite different names*

```diff
@@ -19,13 +19,11 @@
         "section_foo.option_bar=True",
         "section_foo.option_baz=False",
         "section_foo2.option_bar2.option_baz2=True",
         "section_foo2.option_baz3.option_baz4=False",
     )
 
 
-@pytest.mark.parametrize(
-    "name", ["way.too.long", "no_option", "remote.way.too.long"]
-)
+@pytest.mark.parametrize("name", ["way.too.long", "no_option", "remote.way.too.long"])
 def test_config_bad_name(name):
     with pytest.raises(DvcParserError):
         parse_args(["config", name])
```

### Comparing `dvc-2.9.5/tests/unit/command/test_data_sync.py` & `dvc-3.0.0a0/tests/unit/command/test_data_sync.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,14 +54,15 @@
             "--all-tags",
             "--all-commits",
             "--with-deps",
             "--force",
             "--recursive",
             "--run-cache",
             "--glob",
+            "--allow-missing",
         ]
     )
     assert cli_args.func == CmdDataPull
 
     cmd = cli_args.func(cli_args)
     m = mocker.patch.object(cmd.repo, "pull", autospec=True)
 
@@ -75,14 +76,15 @@
         all_tags=True,
         all_commits=True,
         with_deps=True,
         force=True,
         recursive=True,
         run_cache=True,
         glob=True,
+        allow_missing=True,
     )
 
 
 def test_push(mocker):
     cli_args = parse_args(
         [
             "push",
```

### Comparing `dvc-2.9.5/tests/unit/command/test_diff.py` & `dvc-3.0.0a0/tests/unit/command/test_diff.py`

 * *Files 4% similar despite different names*

```diff
@@ -35,15 +35,15 @@
                 "hash": "11111111",
             }
         ],
         "not in cache": [],
     }
     mocker.patch("dvc.repo.Repo.diff", return_value=diff)
 
-    assert 0 == cmd.run()
+    assert cmd.run() == 0
     assert (
         "Added:\n"
         "    file\n"
         "\n"
         "Renamed:\n"
         "    data{sep}file_old -> data{sep}file_new\n"
         "\n"
@@ -73,25 +73,25 @@
                 },
                 "hash": "11111111",
             }
         ],
         "not in cache": [],
     }
     mocker.patch("dvc.repo.Repo.diff", return_value=diff)
-    assert 0 == cmd.run()
+    assert cmd.run() == 0
 
     out, _ = capsys.readouterr()
     assert (
-        "Deleted:\n"
-        "    XXXXXXXX  " + os.path.join("data", "") + "\n"
-        "    00000000  " + os.path.join("data", "bar") + "\n"
-        "    11111111  " + os.path.join("data", "foo") + "\n"
-        "\n"
-        "Renamed:\n"
-        "    11111111  "
+        "Deleted:\n    XXXXXXXX  "
+        + os.path.join("data", "")
+        + "\n    00000000  "
+        + os.path.join("data", "bar")
+        + "\n    11111111  "
+        + os.path.join("data", "foo")
+        + "\n\nRenamed:\n    11111111  "
         + os.path.join("data", "file_old")
         + " -> "
         + os.path.join("data", "file_new")
         + "\n"
         "\n"
         "Modified:\n"
         "    CCCCCCCC..DDDDDDDD  file1\n"
@@ -111,15 +111,15 @@
         ],
         "deleted": [],
         "modified": [],
         "not in cache": [],
     }
     mocker.patch("dvc.repo.Repo.diff", return_value=diff)
 
-    assert 0 == cmd.run()
+    assert cmd.run() == 0
     out, _ = capsys.readouterr()
     assert '"added": [{"path": "file1"}, {"path": "file2"}]' in out
     assert '"deleted": []' in out
     assert '"modified": []' in out
     assert '"not in cache": []' in out
 
 
@@ -141,15 +141,15 @@
                 "hash": "11111111",
             }
         ],
         "not in cache": [],
     }
     mocker.patch("dvc.repo.Repo.diff", return_value=diff)
 
-    assert 0 == cmd.run()
+    assert cmd.run() == 0
     out, _ = capsys.readouterr()
     assert (
         '"added": [{"path": "file1", "hash": "11111111"}, '
         '{"path": "file2", "hash": "22222222"}]' in out
     )
     assert '"deleted": []' in out
     assert '"modified": []' in out
@@ -176,15 +176,15 @@
                 "hash": "11111111",
             }
         ],
         "not in cache": [],
     }
     mocker.patch("dvc.repo.Repo.diff", return_value=diff)
 
-    assert 0 == cmd.run()
+    assert cmd.run() == 0
     out, _ = capsys.readouterr()
     assert '"added": [{"path": "file1"}, {"path": "file2"}]' in out
     assert '"deleted": []' in out
     assert '"renamed": [{"path": {"old": "file_old", "new": "file_new"}' in out
     assert '"modified": []' in out
     assert '"not in cache": []' not in out
 
@@ -196,32 +196,43 @@
     cmd = args.func(args)
 
     diff = {}
     show_hash = show_hash if show_hash else False
     mock_show_markdown = mocker.patch("dvc.commands.diff._show_markdown")
     mocker.patch("dvc.repo.Repo.diff", return_value=diff.copy())
 
-    assert 0 == cmd.run()
+    assert cmd.run() == 0
     mock_show_markdown.assert_called_once_with(diff, show_hash, False)
 
 
-def test_no_changes(mocker, capsys):
-    args = parse_args(["diff", "--json"])
+@pytest.mark.parametrize(
+    "opts",
+    (
+        [],
+        ["a_rev", "b_rev"],
+        ["--targets", "."],
+        ["--hide-missing"],
+    ),
+)
+@pytest.mark.parametrize(
+    "show, expected",
+    (
+        ([], ""),
+        (["--json"], "{}"),
+        (["--md"], "| Status   | Path   |\n|----------|--------|"),
+    ),
+)
+def test_no_changes(mocker, capsys, opts, show, expected):
+    args = parse_args(["diff", *opts, *show])
     cmd = args.func(args)
     mocker.patch("dvc.repo.Repo.diff", return_value={})
 
-    assert 0 == cmd.run()
-    out, _ = capsys.readouterr()
-    assert "{}" in out
-
-    args = parse_args(["diff"])
-    cmd = args.func(args)
-    assert 0 == cmd.run()
+    assert cmd.run() == 0
     out, _ = capsys.readouterr()
-    assert not out
+    assert expected == out.strip()
 
 
 def test_show_markdown(capsys):
     diff = {
         "deleted": [
             {"path": "zoo"},
             {"path": os.path.join("data", "")},
@@ -255,17 +266,15 @@
     diff = {
         "deleted": [
             {"path": "zoo", "hash": "22222"},
             {"path": os.path.join("data", ""), "hash": "XXXXXXXX.dir"},
             {"path": os.path.join("data", "foo"), "hash": "11111111"},
             {"path": os.path.join("data", "bar"), "hash": "00000000"},
         ],
-        "modified": [
-            {"path": "file", "hash": {"old": "AAAAAAAA", "new": "BBBBBBBB"}}
-        ],
+        "modified": [{"path": "file", "hash": {"old": "AAAAAAAA", "new": "BBBBBBBB"}}],
         "added": [{"path": "file", "hash": "00000000"}],
         "renamed": [
             {
                 "path": {"old": "file_old", "new": "file_new"},
                 "hash": "11111111",
             }
         ],
@@ -334,19 +343,19 @@
                 "hash": "11111111",
             }
         ],
         "not in cache": [],
     }
     mocker.patch("dvc.repo.Repo.diff", return_value=diff)
 
-    assert 0 == cmd.run()
+    assert cmd.run() == 0
     out, _ = capsys.readouterr()
     assert (
         "Added:\n"
         "    file\n"
         "\n"
         "Renamed:\n"
         "    file_old -> file_new\n"
         "\n"
-        "files summary: 1 added, 1 renamed"
-    ) in out
+        "files summary: 1 added, 1 renamed" in out
+    )
     assert "not in cache" not in out
```

### Comparing `dvc-2.9.5/tests/unit/command/test_get.py` & `dvc-3.0.0a0/tests/unit/command/test_get.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,22 +20,20 @@
 
     cmd = cli_args.func(cli_args)
     m = mocker.patch("dvc.repo.Repo.get")
 
     assert cmd.run() == 0
 
     m.assert_called_once_with(
-        "repo_url", path="src", out="out", rev="version", jobs=4
+        "repo_url", path="src", out="out", rev="version", jobs=4, force=False
     )
 
 
 def test_get_url(mocker, capsys):
-    cli_args = parse_args(
-        ["get", "repo_url", "src", "--rev", "version", "--show-url"]
-    )
+    cli_args = parse_args(["get", "repo_url", "src", "--rev", "version", "--show-url"])
     assert cli_args.func == CmdGet
 
     cmd = cli_args.func(cli_args)
     m = mocker.patch("dvc.api.get_url", return_value="resource_url")
 
     assert cmd.run() == 0
     out, _ = capsys.readouterr()
```

### Comparing `dvc-2.9.5/tests/unit/command/test_git_hook.py` & `dvc-3.0.0a0/tests/unit/command/test_git_hook.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/unit/command/test_imp.py` & `dvc-3.0.0a0/tests/unit/command/test_imp.py`

 * *Files 18% similar despite different names*

```diff
@@ -10,16 +10,14 @@
             "src",
             "--out",
             "out",
             "--file",
             "file",
             "--rev",
             "version",
-            "--desc",
-            "description",
             "--jobs",
             "3",
         ]
     )
     assert cli_args.func == CmdImport
 
     cmd = cli_args.func(cli_args)
@@ -30,15 +28,15 @@
     m.assert_called_once_with(
         "repo_url",
         path="src",
         out="out",
         fname="file",
         rev="version",
         no_exec=False,
-        desc="description",
+        no_download=False,
         jobs=3,
     )
 
 
 def test_import_no_exec(mocker):
     cli_args = parse_args(
         [
@@ -48,16 +46,14 @@
             "--out",
             "out",
             "--file",
             "file",
             "--rev",
             "version",
             "--no-exec",
-            "--desc",
-            "description",
         ]
     )
 
     cmd = cli_args.func(cli_args)
     m = mocker.patch.object(cmd.repo, "imp", autospec=True)
 
     assert cmd.run() == 0
@@ -65,10 +61,43 @@
     m.assert_called_once_with(
         "repo_url",
         path="src",
         out="out",
         fname="file",
         rev="version",
         no_exec=True,
-        desc="description",
+        no_download=False,
+        jobs=None,
+    )
+
+
+def test_import_no_download(mocker):
+    cli_args = parse_args(
+        [
+            "import",
+            "repo_url",
+            "src",
+            "--out",
+            "out",
+            "--file",
+            "file",
+            "--rev",
+            "version",
+            "--no-download",
+        ]
+    )
+
+    cmd = cli_args.func(cli_args)
+    m = mocker.patch.object(cmd.repo, "imp", autospec=True)
+
+    assert cmd.run() == 0
+
+    m.assert_called_once_with(
+        "repo_url",
+        path="src",
+        out="out",
+        fname="file",
+        rev="version",
+        no_exec=False,
+        no_download=True,
         jobs=None,
     )
```

### Comparing `dvc-2.9.5/tests/unit/command/test_imp_url.py` & `dvc-3.0.0a0/tests/unit/command/test_imp_url.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,11 @@
 import logging
 
+import pytest
+
 from dvc.cli import parse_args
 from dvc.commands.imp_url import CmdImportUrl
 from dvc.exceptions import DvcException
 
 
 def test_import_url(mocker):
     cli_args = parse_args(
@@ -11,34 +13,34 @@
             "import-url",
             "src",
             "out",
             "--file",
             "file",
             "--jobs",
             "4",
-            "--desc",
-            "description",
         ]
     )
     assert cli_args.func == CmdImportUrl
 
     cmd = cli_args.func(cli_args)
     m = mocker.patch.object(cmd.repo, "imp_url", autospec=True)
 
     assert cmd.run() == 0
 
     m.assert_called_once_with(
         "src",
         out="out",
         fname="file",
         no_exec=False,
+        no_download=False,
         remote=None,
         to_remote=False,
-        desc="description",
         jobs=4,
+        force=False,
+        version_aware=False,
     )
 
 
 def test_failed_import_url(mocker, caplog):
     cli_args = parse_args(["import-url", "http://somesite.com/file_name"])
     assert cli_args.func == CmdImportUrl
 
@@ -50,97 +52,109 @@
             "failed to import http://somesite.com/file_name. "
             "You could also try downloading it manually, and "
             "adding it with `dvc add`."
         )
         assert expected_error in caplog.text
 
 
-def test_import_url_no_exec(mocker):
+@pytest.mark.parametrize(
+    "flag,expected",
+    [
+        ("--no-exec", {"no_exec": True, "no_download": False}),
+        ("--no-download", {"no_download": True, "no_exec": False}),
+    ],
+)
+def test_import_url_no_exec_download_flags(mocker, flag, expected):
     cli_args = parse_args(
         [
             "import-url",
-            "--no-exec",
+            flag,
             "src",
             "out",
             "--file",
             "file",
-            "--desc",
-            "description",
         ]
     )
 
     cmd = cli_args.func(cli_args)
     m = mocker.patch.object(cmd.repo, "imp_url", autospec=True)
 
     assert cmd.run() == 0
 
     m.assert_called_once_with(
         "src",
         out="out",
         fname="file",
-        no_exec=True,
         remote=None,
         to_remote=False,
-        desc="description",
         jobs=None,
+        force=False,
+        version_aware=False,
+        **expected,
     )
 
 
 def test_import_url_to_remote(mocker):
     cli_args = parse_args(
         [
             "import-url",
             "s3://bucket/foo",
             "bar",
             "--to-remote",
             "--remote",
             "remote",
-            "--desc",
-            "description",
         ]
     )
     assert cli_args.func == CmdImportUrl
 
     cmd = cli_args.func(cli_args)
     m = mocker.patch.object(cmd.repo, "imp_url", autospec=True)
 
     assert cmd.run() == 0
 
     m.assert_called_once_with(
         "s3://bucket/foo",
         out="bar",
         fname=None,
         no_exec=False,
+        no_download=False,
         remote="remote",
         to_remote=True,
-        desc="description",
         jobs=None,
+        force=False,
+        version_aware=False,
     )
 
 
-def test_import_url_to_remote_invalid_combination(mocker, caplog):
+@pytest.mark.parametrize("flag", ["--no-exec", "--no-download", "--version-aware"])
+def test_import_url_to_remote_invalid_combination(dvc, mocker, caplog, flag):
     cli_args = parse_args(
         [
             "import-url",
             "s3://bucket/foo",
             "bar",
             "--to-remote",
             "--remote",
             "remote",
-            "--no-exec",
+            flag,
         ]
     )
     assert cli_args.func == CmdImportUrl
 
     cmd = cli_args.func(cli_args)
     with caplog.at_level(logging.ERROR, logger="dvc"):
         assert cmd.run() == 1
-        expected_msg = "--no-exec can't be combined with --to-remote"
+        expected_msg = (
+            "--no-exec/--no-download/--version-aware cannot be combined with "
+            "--to-remote"
+        )
         assert expected_msg in caplog.text
 
+
+def test_import_url_to_remote_flag(dvc, mocker, caplog):
     cli_args = parse_args(
         ["import-url", "s3://bucket/foo", "bar", "--remote", "remote"]
     )
 
     cmd = cli_args.func(cli_args)
     with caplog.at_level(logging.ERROR, logger="dvc"):
         assert cmd.run() == 1
```

### Comparing `dvc-2.9.5/tests/unit/command/test_live.py` & `dvc-3.0.0a0/tests/unit/command/test_update.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,40 +1,63 @@
 from dvc.cli import parse_args
-from dvc.commands.live import CmdLiveDiff, CmdLiveShow
+from dvc.commands.update import CmdUpdate
 
 
-def test_live_diff(dvc, mocker):
+def test_update(dvc, mocker):
     cli_args = parse_args(
         [
-            "live",
-            "diff",
-            "--out",
-            "result.extension",
-            "target",
-            "--revs",
-            "HEAD",
-            "rev1",
+            "update",
+            "target1",
+            "target2",
+            "--rev",
+            "REV",
+            "--recursive",
+            "-j",
+            "8",
         ]
     )
-    assert cli_args.func == CmdLiveDiff
-
+    assert cli_args.func == CmdUpdate
     cmd = cli_args.func(cli_args)
-    m = mocker.patch("dvc.repo.live.Live.show", return_value=({}, {}))
+    m = mocker.patch("dvc.repo.Repo.update")
 
-    assert cmd.run() == 1
+    assert cmd.run() == 0
 
-    m.assert_called_once_with(target="target", revs=["HEAD", "rev1"])
+    m.assert_called_once_with(
+        targets=["target1", "target2"],
+        rev="REV",
+        recursive=True,
+        to_remote=False,
+        no_download=False,
+        remote=None,
+        jobs=8,
+    )
 
 
-def test_live_show(dvc, mocker):
+def test_update_to_remote(dvc, mocker):
     cli_args = parse_args(
-        ["live", "show", "-o", "result.extension", "datafile"]
+        [
+            "update",
+            "target1",
+            "target2",
+            "--to-remote",
+            "-j",
+            "5",
+            "-r",
+            "remote",
+            "--recursive",
+        ]
     )
-    assert cli_args.func == CmdLiveShow
-
+    assert cli_args.func == CmdUpdate
     cmd = cli_args.func(cli_args)
+    m = mocker.patch("dvc.repo.Repo.update")
 
-    m = mocker.patch("dvc.repo.live.Live.show", return_value=({}, {}))
-
-    assert cmd.run() == 1
+    assert cmd.run() == 0
 
-    m.assert_called_once_with(target="datafile", revs=None)
+    m.assert_called_once_with(
+        targets=["target1", "target2"],
+        rev=None,
+        recursive=True,
+        to_remote=True,
+        no_download=False,
+        remote="remote",
+        jobs=5,
+    )
```

### Comparing `dvc-2.9.5/tests/unit/command/test_machine.py` & `dvc-3.0.0a0/tests/unit/command/test_machine.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 import os
+from unittest.mock import call
 
 import configobj
 import pytest
-from mock import call
 
 from dvc.cli import parse_args
 from dvc.commands.machine import (
     CmdMachineAdd,
     CmdMachineCreate,
     CmdMachineDestroy,
     CmdMachineList,
@@ -52,52 +52,44 @@
 
 
 def test_create(tmp_dir, dvc, mocker):
     cli_args = parse_args(["machine", "create", "foo"])
     assert cli_args.func == CmdMachineCreate
 
     cmd = cli_args.func(cli_args)
-    m = mocker.patch.object(
-        cmd.repo.machine, "create", autospec=True, return_value=0
-    )
+    m = mocker.patch.object(cmd.repo.machine, "create", autospec=True, return_value=0)
 
     assert cmd.run() == 0
     m.assert_called_once_with("foo")
 
 
 def test_status(tmp_dir, scm, dvc, mocker):
     tmp_dir.gen(DATA)
     cli_args = parse_args(["machine", "status", "foo"])
     assert cli_args.func == CmdMachineStatus
 
     cmd = cli_args.func(cli_args)
-    m = mocker.patch.object(
-        cmd.repo.machine, "status", autospec=True, return_value=[]
-    )
+    m = mocker.patch.object(cmd.repo.machine, "status", autospec=True, return_value=[])
     assert cmd.run() == 0
     m.assert_called_once_with("foo")
 
     cli_args = parse_args(["machine", "status"])
     cmd = cli_args.func(cli_args)
-    m = mocker.patch.object(
-        cmd.repo.machine, "status", autospec=True, return_value=[]
-    )
+    m = mocker.patch.object(cmd.repo.machine, "status", autospec=True, return_value=[])
     assert cmd.run() == 0
     assert m.call_count == 2
     m.assert_has_calls([call("foo"), call("myaws")])
 
 
 def test_destroy(tmp_dir, dvc, mocker):
     cli_args = parse_args(["machine", "destroy", "foo"])
     assert cli_args.func == CmdMachineDestroy
 
     cmd = cli_args.func(cli_args)
-    m = mocker.patch.object(
-        cmd.repo.machine, "destroy", autospec=True, return_value=0
-    )
+    m = mocker.patch.object(cmd.repo.machine, "destroy", autospec=True, return_value=0)
 
     assert cmd.run() == 0
     m.assert_called_once_with("foo")
 
 
 def test_ssh(tmp_dir, dvc, mocker):
     cli_args = parse_args(["machine", "ssh", "foo"])
@@ -114,15 +106,15 @@
 
 @pytest.mark.parametrize("show_origin", [["--show-origin"], []])
 def test_list(tmp_dir, mocker, show_origin):
     from dvc.compare import TabularData
     from dvc.ui import ui
 
     tmp_dir.gen(DATA)
-    cli_args = parse_args(["machine", "list"] + show_origin + ["foo"])
+    cli_args = parse_args(["machine", "list", *show_origin, "foo"])
     assert cli_args.func == CmdMachineList
     cmd = cli_args.func(cli_args)
     if show_origin:
         m = mocker.patch.object(ui, "write", autospec=True)
     else:
         m = mocker.patch.object(TabularData, "render", autospec=True)
     assert cmd.run() == 0
@@ -164,15 +156,13 @@
         ("destroy", False),
         ("rename", True),
         ("status", False),
         ("ssh", False),
     ],
 )
 def test_help_message(tmp_dir, scm, dvc, cmd, use_config, capsys):
-
     try:
         parse_args(["machine", cmd, "--help"])
     except SystemExit:
         pass
     cap = capsys.readouterr()
-    print(cap.out)
     assert ("--global" in cap.out) is use_config
```

### Comparing `dvc-2.9.5/tests/unit/command/test_metrics.py` & `dvc-3.0.0a0/tests/unit/command/test_metrics.py`

 * *Files 6% similar despite different names*

```diff
@@ -21,17 +21,15 @@
         ]
     )
 
     assert cli_args.func == CmdMetricsDiff
 
     cmd = cli_args.func(cli_args)
     diff = {"metrics.yaml": {"": {"old": 1, "new": 3}}}
-    metrics_diff = mocker.patch(
-        "dvc.repo.metrics.diff.diff", return_value=diff
-    )
+    metrics_diff = mocker.patch("dvc.repo.metrics.diff.diff", return_value=diff)
     show_diff_mock = mocker.patch("dvc.compare.show_diff")
 
     assert cmd.run() == 0
 
     metrics_diff.assert_called_once_with(
         cmd.repo,
         targets=["target1", "target2"],
@@ -71,17 +69,15 @@
         ]
     )
 
     assert cli_args.func == CmdMetricsDiff
     cmd = cli_args.func(cli_args)
 
     diff = {"metrics.yaml": {"": {"old": 1, "new": 3}}}
-    metrics_diff = mocker.patch(
-        "dvc.repo.metrics.diff.diff", return_value=diff
-    )
+    metrics_diff = mocker.patch("dvc.repo.metrics.diff.diff", return_value=diff)
     show_diff_mock = mocker.patch("dvc.compare.show_diff")
 
     assert cmd.run() == 0
     out, _ = capsys.readouterr()
     metrics_diff.assert_called_once_with(
         cmd.repo,
         targets=["target1", "target2"],
```

### Comparing `dvc-2.9.5/tests/unit/command/test_params.py` & `dvc-3.0.0a0/tests/unit/command/test_params.py`

 * *Files 2% similar despite different names*

```diff
@@ -65,16 +65,14 @@
         b_rev=None,
     )
 
 
 def test_params_diff_show_json(dvc, mocker, capsys):
     cli_args = parse_args(["params", "diff", "HEAD~10", "HEAD~1", "--json"])
     cmd = cli_args.func(cli_args)
-    mocker.patch(
-        "dvc.repo.params.diff.diff", return_value={"params.yaml": {"a": "b"}}
-    )
+    mocker.patch("dvc.repo.params.diff.diff", return_value={"params.yaml": {"a": "b"}})
     show_diff_mock = mocker.patch("dvc.compare.show_diff")
 
     assert cmd.run() == 0
     out, _ = capsys.readouterr()
     assert '{"params.yaml": {"a": "b"}}\n' in out
     show_diff_mock.assert_not_called()
```

### Comparing `dvc-2.9.5/tests/unit/command/test_plots.py` & `dvc-3.0.0a0/tests/unit/command/test_plots.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,27 +1,35 @@
 import json
 import os
 import posixpath
 from pathlib import Path
 
 import pytest
-from funcy import pluck_attr
+from funcy import set_in
 
 from dvc.cli import parse_args
 from dvc.commands.plots import CmdPlotsDiff, CmdPlotsShow, CmdPlotsTemplates
+from dvc.render.match import RendererWithErrors
+from dvc.utils.serialize import YAMLFileCorruptedError
 
 
 @pytest.fixture
 def plots_data():
-    yield {
+    return {
         "revision": {
-            "data": {
-                "plot.csv": {"data": [{"val": 1}, {"val": 2}], "props": {}},
-                "other.jpg": {"data": b"content"},
-            }
+            "sources": {
+                "data": {
+                    "plot.csv": {
+                        "data": [{"val": 1}, {"val": 2}],
+                        "props": {},
+                    },
+                    "other.jpg": {"data": b"content"},
+                }
+            },
+            "definitions": {"data": {"dvc.yaml": {"data": {"plot.csv": {}}}}},
         }
     }
 
 
 def test_plots_diff(dvc, mocker, plots_data):
     cli_args = parse_args(
         [
@@ -50,17 +58,15 @@
             "tag2",
         ]
     )
     assert cli_args.func == CmdPlotsDiff
 
     cmd = cli_args.func(cli_args)
     m = mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)
-    render_mock = mocker.patch(
-        "dvc.render.utils.render", return_value="html_path"
-    )
+    render_mock = mocker.patch("dvc_render.render_html", return_value="html_path")
 
     assert cmd.run() == 0
 
     m.assert_called_once_with(
         cmd.repo,
         targets=["datafile"],
         revs=["HEAD", "tag1", "tag2"],
@@ -95,17 +101,15 @@
 
     cmd = cli_args.func(cli_args)
 
     m = mocker.patch(
         "dvc.repo.plots.Plots.show",
         return_value=plots_data,
     )
-    render_mock = mocker.patch(
-        "dvc.render.utils.render", return_value="html_path"
-    )
+    render_mock = mocker.patch("dvc_render.render_html", return_value="html_path")
 
     assert cmd.run() == 0
 
     m.assert_called_once_with(
         targets=["datafile"],
         props={"template": "template", "header": False},
     )
@@ -123,18 +127,18 @@
             "--targets",
             "plot.csv",
         ]
     )
     cmd = cli_args.func(cli_args)
     mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)
     mocker.patch(
-        "dvc.render.VegaRenderer.asdict",
+        "dvc_render.VegaRenderer.get_filled_template",
         return_value={"this": "is vega json"},
     )
-    render_mock = mocker.patch("dvc.render.utils.render")
+    render_mock = mocker.patch("dvc_render.render_html")
     assert cmd.run() == 0
 
     out, _ = capsys.readouterr()
 
     assert json.dumps({"this": "is vega json"}) in out
     render_mock.assert_not_called()
 
@@ -152,59 +156,52 @@
         args.append("--open")
 
     cli_args = parse_args(args)
     cmd = cli_args.func(cli_args)
     mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)
 
     index_path = tmp_dir / "dvc_plots" / "index.html"
-    mocker.patch("dvc.render.utils.render", return_value=index_path)
+    mocker.patch("dvc_render.render_html", return_value=index_path)
 
     assert cmd.run() == 0
     mocked_open.assert_called_once_with(index_path.as_uri())
 
     out, _ = capsys.readouterr()
     assert index_path.as_uri() in out
 
 
-def test_plots_diff_open_WSL(tmp_dir, dvc, mocker, plots_data):
+def test_plots_diff_open_wsl(tmp_dir, dvc, mocker, plots_data):
     mocked_open = mocker.patch("webbrowser.open", return_value=True)
     mocked_uname_result = mocker.MagicMock()
-    mocked_uname_result.release = "Microsoft"
+    mocked_uname_result.release = "microsoft"
     mocker.patch("platform.uname", return_value=mocked_uname_result)
 
-    cli_args = parse_args(
-        ["plots", "diff", "--targets", "plots.csv", "--open"]
-    )
+    cli_args = parse_args(["plots", "diff", "--targets", "plots.csv", "--open"])
     cmd = cli_args.func(cli_args)
     mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)
 
     index_path = tmp_dir / "dvc_plots" / "index.html"
-    mocker.patch("dvc.render.utils.render", return_value=index_path)
+    mocker.patch("dvc_render.render_html", return_value=index_path)
 
     assert cmd.run() == 0
     mocked_open.assert_called_once_with(str(Path("dvc_plots") / "index.html"))
 
 
 def test_plots_diff_open_failed(tmp_dir, dvc, mocker, capsys, plots_data):
     mocked_open = mocker.patch("webbrowser.open", return_value=False)
-    cli_args = parse_args(
-        ["plots", "diff", "--targets", "plots.csv", "--open"]
-    )
+    cli_args = parse_args(["plots", "diff", "--targets", "plots.csv", "--open"])
     cmd = cli_args.func(cli_args)
-    mocker.patch(
-        "dvc.repo.plots.diff.diff", return_value={"datafile": plots_data}
-    )
+    mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)
 
     assert cmd.run() == 1
     expected_url = tmp_dir / "dvc_plots" / "index.html"
     mocked_open.assert_called_once_with(expected_url.as_uri())
 
     error_message = (
-        f"Failed to open {expected_url.as_uri()}. "
-        "Please try opening it manually."
+        f"Failed to open {expected_url.as_uri()}. Please try opening it manually."
     )
 
     out, err = capsys.readouterr()
     assert expected_url.as_uri() in out
     assert error_message in err
 
 
@@ -221,160 +218,187 @@
         ),
     ],
     ids=["quote", "resolve"],
 )
 def test_plots_path_is_quoted_and_resolved_properly(
     tmp_dir, dvc, mocker, capsys, output, expected_url_path, plots_data
 ):
-    cli_args = parse_args(
-        ["plots", "diff", "--targets", "datafile", "--out", output]
-    )
+    cli_args = parse_args(["plots", "diff", "--targets", "datafile", "--out", output])
     cmd = cli_args.func(cli_args)
-    mocker.patch(
-        "dvc.repo.plots.diff.diff", return_value={"datafile": plots_data}
-    )
+    mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)
 
     assert cmd.run() == 0
     expected_url = posixpath.join(tmp_dir.as_uri(), expected_url_path)
 
     out, _ = capsys.readouterr()
     assert expected_url in out
 
 
-@pytest.mark.parametrize(
-    "output", ("some_out", os.path.join("to", "subdir"), None)
-)
-def test_should_call_render(tmp_dir, mocker, capsys, plots_data, output):
+def test_should_pass_template_dir(tmp_dir, dvc, mocker, capsys):
     cli_args = parse_args(
-        ["plots", "diff", "--targets", "plots.csv", "--out", output]
+        [
+            "plots",
+            "diff",
+            "HEAD~1",
+            "--json",
+            "--targets",
+            "plot.csv",
+        ]
     )
     cmd = cli_args.func(cli_args)
+
+    data = mocker.MagicMock()
+    mocker.patch("dvc.repo.plots.diff.diff", return_value=data)
+
+    renderers = mocker.MagicMock()
+    match_renderers = mocker.patch(
+        "dvc.render.match.match_defs_renderers", return_value=renderers
+    )
+
+    assert cmd.run() == 0
+
+    match_renderers.assert_called_once_with(
+        data=data,
+        out="dvc_plots",
+        templates_dir=str(tmp_dir / ".dvc/plots"),
+    )
+
+
+@pytest.mark.parametrize("output", ("some_out", os.path.join("to", "subdir"), None))
+def test_should_call_render(tmp_dir, mocker, capsys, plots_data, output):
+    cli_args = parse_args(["plots", "diff", "--targets", "plots.csv", "--out", output])
+    cmd = cli_args.func(cli_args)
     mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)
 
     output = output or "dvc_plots"
     index_path = tmp_dir / output / "index.html"
-    renderers = mocker.MagicMock()
-    mocker.patch("dvc.render.utils.match_renderers", return_value=renderers)
-    render_mock = mocker.patch(
-        "dvc.render.utils.render", return_value=index_path
+    renderer = mocker.MagicMock()
+    mocker.patch(
+        "dvc.render.match.match_defs_renderers",
+        return_value=[RendererWithErrors(renderer, {}, {})],
     )
+    render_mock = mocker.patch("dvc_render.render_html", return_value=index_path)
 
     assert cmd.run() == 0
 
     out, _ = capsys.readouterr()
     assert index_path.as_uri() in out
 
     render_mock.assert_called_once_with(
-        cmd.repo, renderers, path=tmp_dir / output, html_template_path=None
+        renderers=[renderer],
+        output_file=Path(tmp_dir / output / "index.html"),
+        html_template=None,
     )
 
 
 def test_plots_diff_json(dvc, mocker, capsys):
     cli_args = parse_args(
         [
             "plots",
             "diff",
             "HEAD~10",
             "HEAD~1",
             "--json",
+            "--split",
             "--targets",
             "plot.csv",
             "-o",
             "out",
         ]
     )
     cmd = cli_args.func(cli_args)
 
     data = mocker.MagicMock()
     mocker.patch("dvc.repo.plots.diff.diff", return_value=data)
 
     renderers = mocker.MagicMock()
-    mocker.patch("dvc.render.utils.match_renderers", return_value=renderers)
-    render_mock = mocker.patch("dvc.render.utils.render")
+    mocker.patch("dvc.render.match.match_defs_renderers", return_value=renderers)
+    render_mock = mocker.patch("dvc_render.render_html")
 
     show_json_mock = mocker.patch("dvc.commands.plots._show_json")
 
     assert cmd.run() == 0
 
-    show_json_mock.assert_called_once_with(renderers, "out")
+    show_json_mock.assert_called_once_with(renderers, True, errors={})
 
     render_mock.assert_not_called()
 
 
-def test_show_json_requires_out(dvc, mocker, capsys):
-    cli_args = parse_args(
-        [
-            "plots",
-            "diff",
-            "HEAD~10",
-            "HEAD~1",
-            "--json",
-            "--targets",
-            "plot.csv",
-        ]
-    )
+@pytest.mark.parametrize(
+    "target,expected_out,expected_rtn",
+    (("t1", "\"{'t1'}\"", 0), (None, "t1\nt2", 0), ("t3", "", 1)),
+)
+def test_plots_templates(dvc, mocker, capsys, target, expected_out, expected_rtn):
+    t1 = mocker.Mock()
+    t1.DEFAULT_NAME = "t1"
+    t1.DEFAULT_CONTENT = "{'t1'}"
+
+    t2 = mocker.Mock()
+    t2.DEFAULT_NAME = "t2"
+    t2.DEFAULT_CONTENT = "{'t2'}"
+
+    mocker.patch("dvc_render.vega_templates.TEMPLATES", [t1, t2])
+
+    arguments = ["plots", "templates"]
+    if target:
+        arguments += [target]
+
+    cli_args = parse_args(arguments)
+    assert cli_args.func == CmdPlotsTemplates
+
     cmd = cli_args.func(cli_args)
 
-    mocker.patch("dvc.repo.plots.diff.diff")
+    rtn = cmd.run()
 
-    renderer = mocker.MagicMock()
-    renderer.needs_output_path = False
-    renderer.filename = "foo"
-    renderer.as_json.return_value = "{}"
-    mocker.patch("dvc.render.utils.match_renderers", return_value=[renderer])
+    out, _ = capsys.readouterr()
 
-    assert cmd.run() == 0
+    assert out.strip() == expected_out
+    assert rtn == expected_rtn
 
-    renderer.needs_output_path = True
 
-    assert cmd.run() == 1
+@pytest.mark.parametrize("split", (True, False))
+def test_show_json(split, mocker, capsys):
+    import dvc.commands.plots
 
-    cli_args = parse_args(
-        [
-            "plots",
-            "diff",
-            "HEAD~10",
-            "HEAD~1",
-            "--json",
-            "--targets",
-            "plot.csv",
-            "-o",
-            "out",
-        ]
+    renderer = mocker.MagicMock()
+    renderer_obj = RendererWithErrors(renderer, {}, {})
+    renderer.name = "rname"
+    to_json_mock = mocker.patch(
+        "dvc.render.convert.to_json", return_value={"renderer": "json"}
     )
-    cmd = cli_args.func(cli_args)
-    assert cmd.run() == 0
 
+    dvc.commands.plots._show_json([renderer_obj], split)
 
-@pytest.mark.parametrize("target", (("t1"), (None)))
-def test_plots_templates(tmp_dir, dvc, mocker, capsys, target):
-    assert not os.path.exists(dvc.plots.templates.templates_dir)
-    mocker.patch(
-        "dvc.commands.plots.CmdPlotsTemplates.TEMPLATES_CHOICES",
-        ["t1", "t2"],
-    )
+    to_json_mock.assert_called_once_with(renderer, split)
 
-    arguments = ["plots", "templates", "--out", "output"]
-    if target:
-        arguments += [target]
+    out, _ = capsys.readouterr()
+    assert json.dumps({"rname": {"renderer": "json"}}) in out
 
-    cli_args = parse_args(arguments)
-    assert cli_args.func == CmdPlotsTemplates
 
-    init_mock = mocker.patch("dvc.repo.plots.template.PlotTemplates.init")
-    cmd = cli_args.func(cli_args)
+def test_show_json_no_renderers(capsys):
+    import dvc.commands.plots
+
+    dvc.commands.plots._show_json([])
 
-    assert cmd.run() == 0
     out, _ = capsys.readouterr()
+    assert json.dumps({}) in out
 
-    init_mock.assert_called_once_with(
-        output=os.path.abspath("output"), targets=[target] if target else None
-    )
-    assert "Templates have been written into 'output'." in out
 
+def test_show_json_with_error(dvc, mocker, capsys):
+    cli_args = parse_args(["plots", "show", "--json"])
+    cmd = cli_args.func(cli_args)
 
-def test_plots_templates_choices(tmp_dir, dvc):
-    from dvc.repo.plots.template import PlotTemplates
+    e = YAMLFileCorruptedError("dvc.yaml")
+    data = set_in({}, ["workspace", "definitions", "error"], e)
+    cmd._func = mocker.MagicMock(return_value=data)
 
-    assert CmdPlotsTemplates.TEMPLATES_CHOICES == list(
-        pluck_attr("DEFAULT_NAME", PlotTemplates.TEMPLATES)
-    )
+    cmd.run()
+    out, _ = capsys.readouterr()
+    assert json.loads(out) == {
+        "errors": [
+            {
+                "rev": "workspace",
+                "type": type(e).__name__,
+                "msg": e.args[0],
+            }
+        ]
+    }
```

### Comparing `dvc-2.9.5/tests/unit/command/test_repro.py` & `dvc-3.0.0a0/tests/unit/command/test_repro.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,37 +1,39 @@
 from dvc.cli import parse_args
 from dvc.commands.repro import CmdRepro
 
-default_arguments = {
+common_arguments = {
     "all_pipelines": False,
     "downstream": False,
     "dry": False,
     "force": False,
-    "run_cache": True,
     "interactive": False,
-    "no_commit": False,
     "pipeline": False,
     "single_item": False,
     "recursive": False,
     "force_downstream": False,
     "pull": False,
-    "glob": False,
+    "allow_missing": False,
     "targets": [],
 }
+repro_arguments = {
+    "run_cache": True,
+    "no_commit": False,
+    "glob": False,
+}
 
 
 def test_default_arguments(dvc, mocker):
     cmd = CmdRepro(parse_args(["repro"]))
     mocker.patch.object(cmd.repo, "reproduce")
     cmd.run()
-    # pylint: disable=no-member
-    cmd.repo.reproduce.assert_called_with(**default_arguments)
+    cmd.repo.reproduce.assert_called_with(**common_arguments, **repro_arguments)
 
 
 def test_downstream(dvc, mocker):
     cmd = CmdRepro(parse_args(["repro", "--downstream"]))
     mocker.patch.object(cmd.repo, "reproduce")
     cmd.run()
-    arguments = default_arguments.copy()
+    arguments = common_arguments.copy()
+    arguments.update(repro_arguments)
     arguments.update({"downstream": True})
-    # pylint: disable=no-member
     cmd.repo.reproduce.assert_called_with(**arguments)
```

### Comparing `dvc-2.9.5/tests/unit/command/test_run.py` & `dvc-3.0.0a0/tests/unit/command/test_run.py`

 * *Files 16% similar despite different names*

```diff
@@ -4,35 +4,29 @@
 
 
 def test_run(mocker, dvc):
     cli_args = parse_args(
         [
             "run",
             "--name",
-            "nam",
+            "name",
             "--deps",
             "deps",
             "--outs",
             "outs",
             "--outs-no-cache",
             "outs-no-cache",
             "--metrics",
             "metrics",
             "--metrics-no-cache",
             "metrics-no-cache",
             "--plots",
             "plots",
             "--plots-no-cache",
             "plots-no-cache",
-            "--live",
-            "live",
-            "--live-no-cache",
-            "live-no-cache",
-            "--live-no-summary",
-            "--live-no-html",
             "--file",
             "file",
             "--wdir",
             "wdir",
             "--no-exec",
             "--force",
             "--no-run-cache",
@@ -70,27 +64,23 @@
         metrics_no_cache=["metrics-no-cache"],
         plots=["plots"],
         plots_no_cache=["plots-no-cache"],
         outs_persist=["outs-persist"],
         outs_persist_no_cache=["outs-persist-no-cache"],
         checkpoints=["checkpoints"],
         params=["file:param1,param2", "param3"],
-        live="live",
-        live_no_cache="live-no-cache",
-        live_no_summary=True,
-        live_no_html=True,
         fname="file",
         wdir="wdir",
         no_exec=True,
         force=True,
         run_cache=False,
         no_commit=True,
         always_changed=True,
         cmd="command",
-        name="nam",
+        name="name",
         single_stage=False,
         external=True,
         desc="description",
     )
 
 
 def test_run_args_from_cli(mocker, dvc):
@@ -103,18 +93,14 @@
         deps=[],
         outs=[],
         outs_no_cache=[],
         metrics=[],
         metrics_no_cache=[],
         plots=[],
         plots_no_cache=[],
-        live=None,
-        live_no_cache=None,
-        live_no_summary=False,
-        live_no_html=False,
         outs_persist=[],
         outs_persist_no_cache=[],
         checkpoints=[],
         params=[],
         fname=None,
         wdir=None,
         no_exec=False,
@@ -140,18 +126,14 @@
         deps=[],
         outs=[],
         outs_no_cache=[],
         metrics=[],
         metrics_no_cache=[],
         plots=[],
         plots_no_cache=[],
-        live=None,
-        live_no_cache=None,
-        live_no_summary=False,
-        live_no_html=False,
         outs_persist=[],
         outs_persist_no_cache=[],
         checkpoints=[],
         params=[],
         fname=None,
         wdir=None,
         no_exec=False,
```

### Comparing `dvc-2.9.5/tests/unit/command/test_status.py` & `dvc-3.0.0a0/tests/unit/command/test_status.py`

 * *Files 18% similar despite different names*

```diff
@@ -72,20 +72,22 @@
     assert cmd.run() == ret
     assert not caplog.messages
     captured = capsys.readouterr()
     assert not captured.out
 
 
 def test_status_empty(dvc, mocker, capsys):
+    from dvc.repo.index import Index
+
     cli_args = parse_args(["status"])
     assert cli_args.func == CmdDataStatus
 
     cmd = cli_args.func(cli_args)
 
-    spy = mocker.spy(cmd.repo.stage, "_collect_repo")
+    spy = mocker.spy(Index, "from_repo")
 
     assert cmd.run() == 0
 
     captured = capsys.readouterr()
     assert "no data or pipelines tracked" in captured.out
     # stages should only be collected once
     assert spy.call_count == 1
@@ -96,21 +98,22 @@
     [
         (["--cloud"], "Cache and remote 'default' are in sync"),
         (["--remote", "remote1"], "Cache and remote 'remote1' are in sync"),
         ([], "Data and pipelines are up to date"),
     ],
 )
 def test_status_up_to_date(dvc, mocker, capsys, cloud_opts, expected_message):
+    from dvc.repo.index import Index
+
     cli_args = parse_args(["status", *cloud_opts])
     assert cli_args.func == CmdDataStatus
 
     cmd = cli_args.func(cli_args)
 
     mocker.patch.dict(cmd.repo.config, {"core": {"remote": "default"}})
     mocker.patch.object(cmd.repo, "status", autospec=True, return_value={})
-    mocker.patch.object(
-        cmd.repo.stage, "_collect_repo", return_value=[object()], autospec=True
-    )
+    mocker.patch("dvc.repo.Repo.index", return_value=Index(dvc, [object()]))
+    cmd.repo._reset = mocker.Mock()
 
     assert cmd.run() == 0
     captured = capsys.readouterr()
     assert expected_message in captured.out
```

### Comparing `dvc-2.9.5/tests/unit/dependency/test_params.py` & `dvc-3.0.0a0/tests/unit/dependency/test_params.py`

 * *Files 23% similar despite different names*

```diff
@@ -36,17 +36,57 @@
 
     assert isinstance(deps[2], ParamsDependency)
     assert deps[2].def_path == "b_file"
     assert deps[2].params == ["cat"]
     assert not deps[2].hash_info
 
 
-@pytest.mark.parametrize("params", [[3], [{"b_file": "cat"}]])
-def test_params_error(dvc, params):
-    with pytest.raises(ValueError):
+def test_loads_params_without_any_specific_targets(dvc):
+    stage = Stage(dvc)
+    deps = loads_params(
+        stage,
+        [
+            "foo",
+            {"params.yaml": None},
+            {"a_file": []},
+            {"b_file": ["baz"]},
+            {"b_file": ["bat"]},
+            {"a_file": ["foobar"]},
+        ],
+    )
+    assert len(deps) == 3
+
+    assert isinstance(deps[0], ParamsDependency)
+    assert deps[0].def_path == ParamsDependency.DEFAULT_PARAMS_FILE
+    assert deps[0].params == []
+    assert not deps[0].hash_info
+
+    assert isinstance(deps[1], ParamsDependency)
+    assert deps[1].def_path == "a_file"
+    assert deps[1].params == []
+    assert not deps[1].hash_info
+
+    assert isinstance(deps[2], ParamsDependency)
+    assert deps[2].def_path == "b_file"
+    assert deps[2].params == ["baz", "bat"]
+    assert not deps[2].hash_info
+
+
+@pytest.mark.parametrize(
+    "params, errmsg",
+    [
+        ([3], "Only list of str/dict is supported. Got: 'int'"),
+        (
+            [{"b_file": "cat"}],
+            "Expected list of params for custom params file 'b_file', got 'str'.",
+        ),
+    ],
+)
+def test_params_error(dvc, params, errmsg):
+    with pytest.raises(ValueError, match=errmsg):
         loads_params(Stage(dvc), params)
 
 
 def test_loadd_from(dvc):
     stage = Stage(dvc)
     deps = loadd_from(stage, [{"params": PARAMS}])
     assert len(deps) == 1
@@ -78,17 +118,15 @@
     tmp_dir.gen(DEFAULT_PARAMS_FILE, b"\0\1\2\3\4\5\6\7")
     dep = ParamsDependency(Stage(dvc), None, ["foo"])
     with pytest.raises(BadParamFileError):
         dep.read_params()
 
 
 def test_read_params_nested(tmp_dir, dvc):
-    dump_yaml(
-        DEFAULT_PARAMS_FILE, {"some": {"path": {"foo": ["val1", "val2"]}}}
-    )
+    dump_yaml(DEFAULT_PARAMS_FILE, {"some": {"path": {"foo": ["val1", "val2"]}}})
     dep = ParamsDependency(Stage(dvc), None, ["some.path.foo"])
     assert dep.read_params() == {"some.path.foo": ["val1", "val2"]}
 
 
 def test_read_params_default_loader(tmp_dir, dvc):
     parameters_file = "parameters.foo"
     dump_yaml(parameters_file, {"some": {"path": {"foo": ["val1", "val2"]}}})
@@ -111,23 +149,25 @@
     assert dep.read_params() == {"some.path.foo": ["val1", "val2"]}
 
 
 def test_read_params_py(tmp_dir, dvc):
     parameters_file = "parameters.py"
     tmp_dir.gen(
         parameters_file,
-        "INT: int = 5\n"
-        "FLOAT = 0.001\n"
-        "STR = 'abc'\n"
-        "BOOL: bool = True\n"
-        "DICT = {'a': 1}\n"
-        "LIST = [1, 2, 3]\n"
-        "SET = {4, 5, 6}\n"
-        "TUPLE = (10, 100)\n"
-        "NONE = None\n",
+        (
+            "INT: int = 5\n"
+            "FLOAT = 0.001\n"
+            "STR = 'abc'\n"
+            "BOOL: bool = True\n"
+            "DICT = {'a': 1}\n"
+            "LIST = [1, 2, 3]\n"
+            "SET = {4, 5, 6}\n"
+            "TUPLE = (10, 100)\n"
+            "NONE = None\n"
+        ),
     )
     dep = ParamsDependency(
         Stage(dvc),
         parameters_file,
         [
             "INT",
             "FLOAT",
@@ -148,38 +188,54 @@
         "DICT": {"a": 1},
         "LIST": [1, 2, 3],
         "SET": {4, 5, 6},
         "TUPLE": (10, 100),
         "NONE": None,
     }
 
-    tmp_dir.gen(
-        parameters_file, "class Train:\n    foo = 'val1'\n    bar = 'val2'\n"
-    )
+    tmp_dir.gen(parameters_file, "class Train:\n    foo = 'val1'\n    bar = 'val2'\n")
     dep = ParamsDependency(Stage(dvc), parameters_file, ["Train.foo"])
     assert dep.read_params() == {"Train.foo": "val1"}
 
     dep = ParamsDependency(Stage(dvc), parameters_file, ["Train"])
     assert dep.read_params() == {"Train": {"foo": "val1", "bar": "val2"}}
 
     tmp_dir.gen(
         parameters_file,
-        "x = 4\n"
-        "config.x = 3\n"
-        "class Klass:\n"
-        "    def __init__(self):\n"
-        "        self.a = 'val1'\n"
-        "        container.a = 2\n"
-        "        self.container.a = 1\n"
-        "        a = 'val2'\n",
+        (
+            "x = 4\n"
+            "config.x = 3\n"
+            "class Klass:\n"
+            "    def __init__(self):\n"
+            "        self.a = 'val1'\n"
+            "        container.a = 2\n"
+            "        self.container.a = 1\n"
+            "        a = 'val2'\n"
+        ),
     )
     dep = ParamsDependency(Stage(dvc), parameters_file, ["x", "Klass.a"])
     assert dep.read_params() == {"x": 4, "Klass.a": "val1"}
 
 
+def test_params_py_tuple_status(tmp_dir, dvc):
+    """https://github.com/iterative/dvc/issues/8803"""
+    parameters_file = "parameters.py"
+    tmp_dir.gen(parameters_file, "TUPLE = (10, 100)\n")
+    dep = ParamsDependency(Stage(dvc), parameters_file, ["TUPLE"])
+    # lock file uses YAML so the tuple will be loaded as a list
+    dep.fill_values({"TUPLE": [10, 100]})
+    assert dep.status() == {}
+    dep.fill_values({"TUPLE": [11, 100]})
+    assert dep.status() == {"parameters.py": {"TUPLE": "modified"}}
+    dep.fill_values({"TUPLE": [10]})
+    assert dep.status() == {"parameters.py": {"TUPLE": "modified"}}
+    dep.fill_values({"TUPLE": {10: "foo", 100: "bar"}})
+    assert dep.status() == {"parameters.py": {"TUPLE": "modified"}}
+
+
 def test_get_hash_missing_config(dvc):
     dep = ParamsDependency(Stage(dvc), None, ["foo"])
     with pytest.raises(MissingParamsError):
         dep.get_hash()
 
 
 def test_get_hash_missing_param(tmp_dir, dvc):
@@ -195,7 +251,29 @@
     key = "param"
     dep = ParamsDependency(Stage(dvc), DEFAULT_PARAMS_FILE, [key])
     (tmp_dir / DEFAULT_PARAMS_FILE).write_text(f"{key}: {param_value}")
 
     dep.fill_values(load_yaml(DEFAULT_PARAMS_FILE))
 
     assert dep.status() == {}
+
+
+def test_params_status_without_targets(tmp_dir, dvc):
+    params_file = tmp_dir / "params.yaml"
+    dep = ParamsDependency(Stage(dvc), str(params_file), [])
+
+    assert dep.hash_info.value is None
+    assert dep.status() == {"params.yaml": "deleted"}
+
+    params_file.dump({"foo": "foo", "bar": "bar"})
+
+    assert dep.status() == {"params.yaml": "new"}
+
+    dep.fill_values({})
+    assert dep.hash_info.value == {}
+    assert dep.status() == {"params.yaml": {"bar": "new", "foo": "new"}}
+
+    dep.fill_values({"foo": "foobar", "lorem": "ipsum"})
+    assert dep.hash_info.value == {"foo": "foobar", "lorem": "ipsum"}
+    assert dep.status() == {
+        "params.yaml": {"bar": "new", "foo": "modified", "lorem": "deleted"}
+    }
```

### Comparing `dvc-2.9.5/tests/unit/fs/test_azure.py` & `dvc-3.0.0a0/tests/unit/fs/test_azure.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,9 @@
 import pytest
-
-from dvc.fs.azure import AzureAuthError, AzureFileSystem
+from dvc_azure import AzureAuthError, AzureFileSystem
 
 container_name = "container-name"
 connection_string = (
     "DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;"
     "AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsu"
     "Fq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;"
     "BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;"
@@ -16,44 +15,36 @@
     monkeypatch.setenv("AZURE_STORAGE_CONNECTION_STRING", connection_string)
 
     assert AzureFileSystem._strip_protocol("azure://") == container_name
 
 
 def test_strip_protocol(dvc):
     assert (
-        AzureFileSystem._strip_protocol(f"azure://{container_name}")
-        == container_name
+        AzureFileSystem._strip_protocol(f"azure://{container_name}") == container_name
     )
 
 
 def test_init(dvc):
     config = {"connection_string": connection_string}
     fs = AzureFileSystem(**config)
     assert fs.fs_args["connection_string"] == connection_string
 
 
 def test_azure_login_methods():
     def get_login_method(config):
         fs = AzureFileSystem(**config)
-        # pylint: disable=pointless-statement
         return fs.login_method
 
     with pytest.raises(AzureAuthError):
         get_login_method({})
 
+    assert get_login_method({"connection_string": "test"}) == "connection string"
+    assert get_login_method({"account_name": "test"}).startswith("default credentials")
     assert (
-        get_login_method({"connection_string": "test"}) == "connection string"
-    )
-    assert get_login_method({"account_name": "test"}).startswith(
-        "default credentials"
-    )
-    assert (
-        get_login_method(
-            {"account_name": "test", "allow_anonymous_login": True}
-        )
+        get_login_method({"account_name": "test", "allow_anonymous_login": True})
         == "anonymous login"
     )
 
     with pytest.raises(AzureAuthError):
         get_login_method(
             {"tenant_id": "test", "client_id": "test", "client_secret": "test"}
         )
@@ -71,16 +62,15 @@
     )
 
     assert (
         get_login_method({"account_name": "test", "account_key": "test"})
         == "account key"
     )
     assert (
-        get_login_method({"account_name": "test", "sas_token": "test"})
-        == "SAS token"
+        get_login_method({"account_name": "test", "sas_token": "test"}) == "SAS token"
     )
     assert (
         get_login_method(
             {
                 "connection_string": "test",
                 "account_name": "test",
                 "sas_token": "test",
```

### Comparing `dvc-2.9.5/tests/unit/fs/test_dvc.py` & `dvc-3.0.0a0/tests/unit/fs/test_data.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,81 +1,81 @@
-import os
+import posixpath
 import shutil
 
 import pytest
 
-from dvc.config import NoRemoteError
-from dvc.data.stage import stage
-from dvc.fs.dvc import DvcFileSystem
-from dvc.hash_info import HashInfo
+import dvc_data
+from dvc.fs.data import DataFileSystem
 from dvc.utils.fs import remove
+from dvc_data.hashfile.build import build
+from dvc_data.hashfile.hash_info import HashInfo
 
 
 @pytest.mark.parametrize(
     "path, key",
     [
-        ("", ("repo",)),
-        (".", ("repo",)),
-        ("foo", ("repo", "foo")),
-        (os.path.join("dir", "foo"), ("repo", "dir", "foo")),
+        ("", ()),
+        (".", ()),
+        ("/", ()),
+        ("foo", ("foo",)),
+        ("dir/foo", ("dir", "foo")),
     ],
 )
 def test_get_key(tmp_dir, dvc, path, key):
-    fs = DvcFileSystem(repo=dvc)
-    assert fs._get_key(path) == key
+    fs = DataFileSystem(index=dvc.index.data["repo"])
+    assert fs.fs._get_key(path) == key
 
 
 def test_exists(tmp_dir, dvc):
     tmp_dir.gen("foo", "foo")
     dvc.add("foo")
     (tmp_dir / "foo").unlink()
 
-    fs = DvcFileSystem(repo=dvc)
+    fs = DataFileSystem(index=dvc.index.data["repo"])
     assert fs.exists("foo")
 
 
 def test_open(tmp_dir, dvc):
     tmp_dir.gen("foo", "foo")
     dvc.add("foo")
     (tmp_dir / "foo").unlink()
 
-    fs = DvcFileSystem(repo=dvc)
+    fs = DataFileSystem(index=dvc.index.data["repo"])
     with fs.open("foo", "r") as fobj:
         assert fobj.read() == "foo"
 
 
 def test_open_dirty_hash(tmp_dir, dvc):
     tmp_dir.dvc_gen("file", "file")
     (tmp_dir / "file").write_text("something")
 
-    fs = DvcFileSystem(repo=dvc)
+    fs = DataFileSystem(index=dvc.index.data["repo"])
     with fs.open("file", "r") as fobj:
-        # NOTE: Unlike RepoFileSystem, DvcFileSystem should not
+        # NOTE: Unlike DVCFileSystem, DataFileSystem should not
         # be affected by a dirty workspace.
         assert fobj.read() == "file"
 
 
 def test_open_no_remote(tmp_dir, dvc):
     tmp_dir.dvc_gen("file", "file")
     (tmp_dir / "file").unlink()
-    remove(dvc.odb.local.cache_dir)
+    remove(dvc.cache.local.path)
 
-    fs = DvcFileSystem(repo=dvc)
-    with pytest.raises(FileNotFoundError) as exc_info:
+    fs = DataFileSystem(index=dvc.index.data["repo"])
+    with pytest.raises(FileNotFoundError):
         with fs.open("file", "r"):
             pass
-    assert isinstance(exc_info.value.__cause__, NoRemoteError)
 
 
 def test_open_dirty_no_hash(tmp_dir, dvc):
     tmp_dir.gen("file", "file")
     (tmp_dir / "file.dvc").write_text("outs:\n- path: file\n")
 
-    fs = DvcFileSystem(repo=dvc)
-    # NOTE: Unlike RepoFileSystem, DvcFileSystem should not
+    fs = DataFileSystem(index=dvc.index.data["repo"])
+    # NOTE: Unlike DVCFileSystem, DataFileSystem should not
     # be affected by a dirty workspace.
     with pytest.raises(FileNotFoundError):
         with fs.open("file", "r"):
             pass
 
 
 def test_open_in_history(tmp_dir, scm, dvc):
@@ -85,48 +85,46 @@
     dvc.scm.commit("foo")
 
     tmp_dir.gen("foo", "foofoo")
     dvc.add("foo")
     dvc.scm.add(["foo.dvc", ".gitignore"])
     dvc.scm.commit("foofoo")
 
-    for rev in dvc.brancher(revs=["HEAD~1"]):
-        if rev == "workspace":
-            continue
-
-        fs = DvcFileSystem(repo=dvc)
+    with dvc.switch("HEAD~1"):
+        fs = DataFileSystem(index=dvc.index.data["repo"])
         with fs.open("foo", "r") as fobj:
             assert fobj.read() == "foo"
 
 
 def test_isdir_isfile(tmp_dir, dvc):
     tmp_dir.gen({"datafile": "data", "datadir": {"foo": "foo", "bar": "bar"}})
 
-    fs = DvcFileSystem(repo=dvc)
+    fs = DataFileSystem(index=dvc.index.data["repo"])
     assert not fs.isdir("datadir")
     assert not fs.isfile("datadir")
     assert not fs.isdir("datafile")
     assert not fs.isfile("datafile")
 
     dvc.add(["datadir", "datafile"])
     shutil.rmtree(tmp_dir / "datadir")
     (tmp_dir / "datafile").unlink()
 
+    fs = DataFileSystem(index=dvc.index.data["repo"])
     assert fs.isdir("datadir")
     assert not fs.isfile("datadir")
     assert not fs.isdir("datafile")
     assert fs.isfile("datafile")
 
 
 def test_isdir_mixed(tmp_dir, dvc):
     tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
 
     dvc.add(str(tmp_dir / "dir" / "foo"))
 
-    fs = DvcFileSystem(repo=dvc)
+    fs = DataFileSystem(index=dvc.index.data["repo"])
     assert fs.isdir("dir")
     assert not fs.isfile("dir")
 
 
 def test_walk(tmp_dir, dvc):
     tmp_dir.gen(
         {
@@ -136,30 +134,30 @@
                 "foo": "foo",
                 "bar": "bar",
             }
         }
     )
 
     dvc.add("dir", recursive=True)
-    fs = DvcFileSystem(repo=dvc)
+    fs = DataFileSystem(index=dvc.index.data["repo"])
 
     expected = [
-        os.path.join("dir", "subdir1"),
-        os.path.join("dir", "subdir2"),
-        os.path.join("dir", "subdir1", "foo1"),
-        os.path.join("dir", "subdir1", "bar1"),
-        os.path.join("dir", "subdir2", "foo2"),
-        os.path.join("dir", "foo"),
-        os.path.join("dir", "bar"),
+        "dir/subdir1",
+        "dir/subdir2",
+        "dir/subdir1/foo1",
+        "dir/subdir1/bar1",
+        "dir/subdir2/foo2",
+        "dir/foo",
+        "dir/bar",
     ]
 
     actual = []
     for root, dirs, files in fs.walk("dir"):
         for entry in dirs + files:
-            actual.append(os.path.join(root, entry))
+            actual.append(posixpath.join(root, entry))
 
     assert set(actual) == set(expected)
     assert len(actual) == len(expected)
 
 
 def test_walk_dir(tmp_dir, dvc):
     tmp_dir.gen(
@@ -170,106 +168,98 @@
                 "foo": "foo",
                 "bar": "bar",
             }
         }
     )
 
     dvc.add("dir")
-    fs = DvcFileSystem(repo=dvc)
+    fs = DataFileSystem(index=dvc.index.data["repo"])
 
     expected = [
-        os.path.join("dir", "subdir1"),
-        os.path.join("dir", "subdir2"),
-        os.path.join("dir", "subdir1", "foo1"),
-        os.path.join("dir", "subdir1", "bar1"),
-        os.path.join("dir", "subdir2", "foo2"),
-        os.path.join("dir", "foo"),
-        os.path.join("dir", "bar"),
+        "dir/subdir1",
+        "dir/subdir2",
+        "dir/subdir1/foo1",
+        "dir/subdir1/bar1",
+        "dir/subdir2/foo2",
+        "dir/foo",
+        "dir/bar",
     ]
 
     actual = []
     for root, dirs, files in fs.walk("dir"):
         for entry in dirs + files:
-            actual.append(os.path.join(root, entry))
+            actual.append(posixpath.join(root, entry))
 
     assert set(actual) == set(expected)
     assert len(actual) == len(expected)
 
 
 def test_walk_missing(tmp_dir, dvc):
-    fs = DvcFileSystem(repo=dvc)
+    fs = DataFileSystem(index=dvc.index.data["repo"])
 
     for _ in fs.walk("dir"):
         pass
 
 
 def test_walk_not_a_dir(tmp_dir, dvc):
     tmp_dir.dvc_gen("foo", "foo")
-    fs = DvcFileSystem(repo=dvc)
+    fs = DataFileSystem(index=dvc.index.data["repo"])
 
     for _ in fs.walk("foo"):
         pass
 
 
 def test_isdvc(tmp_dir, dvc):
     tmp_dir.gen({"foo": "foo", "bar": "bar"})
     dvc.add("foo")
-    fs = DvcFileSystem(repo=dvc)
+    fs = DataFileSystem(index=dvc.index.data["repo"])
     assert fs.isdvc("foo")
     assert not fs.isdvc("bar")
 
 
 def test_get_hash_file(tmp_dir, dvc):
     tmp_dir.dvc_gen({"foo": "foo"})
-    fs = DvcFileSystem(repo=dvc)
+    fs = DataFileSystem(index=dvc.index.data["repo"])
     assert fs.info("foo")["md5"] == "acbd18db4cc2f85cedef654fccc4a4d8"
 
 
 def test_get_hash_dir(tmp_dir, dvc, mocker):
-    import dvc as dvc_module
-
-    tmp_dir.dvc_gen(
-        {"dir": {"foo": "foo", "bar": "bar", "subdir": {"data": "data"}}}
-    )
-    fs = DvcFileSystem(repo=dvc)
-    get_file_hash_spy = mocker.spy(dvc_module.data.stage, "get_file_hash")
+    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar", "subdir": {"data": "data"}}})
+    fs = DataFileSystem(index=dvc.index.data["repo"])
+    hash_file_spy = mocker.spy(dvc_data.hashfile.hash, "hash_file")
     assert fs.info("dir")["md5"] == "8761c4e9acad696bee718615e23e22db.dir"
-    assert not get_file_hash_spy.called
+    assert not hash_file_spy.called
 
 
 def test_get_hash_granular(tmp_dir, dvc):
-    tmp_dir.dvc_gen(
-        {"dir": {"foo": "foo", "bar": "bar", "subdir": {"data": "data"}}}
-    )
-    fs = DvcFileSystem(repo=dvc)
-    subdir = os.path.join("dir", "subdir")
+    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar", "subdir": {"data": "data"}}})
+    fs = DataFileSystem(index=dvc.index.data["repo"])
+    subdir = "dir/subdir"
     assert fs.info(subdir).get("md5") is None
-    _, _, obj = stage(dvc.odb.local, subdir, fs, "md5", dry_run=True)
-    assert obj.hash_info == HashInfo(
-        "md5", "af314506f1622d107e0ed3f14ec1a3b5.dir"
-    )
-    data = os.path.join(subdir, "data")
+    _, _, obj = build(dvc.cache.local, subdir, fs, "md5", dry_run=True)
+    assert obj.hash_info == HashInfo("md5", "af314506f1622d107e0ed3f14ec1a3b5.dir")
+    data = posixpath.join(subdir, "data")
     assert fs.info(data)["md5"] == "8d777f385d3dfec8815d20f7496026dc"
-    _, _, obj = stage(dvc.odb.local, data, fs, "md5", dry_run=True)
+    _, _, obj = build(dvc.cache.local, data, fs, "md5", dry_run=True)
     assert obj.hash_info == HashInfo("md5", "8d777f385d3dfec8815d20f7496026dc")
 
 
 def test_get_hash_dirty_file(tmp_dir, dvc):
     tmp_dir.dvc_gen("file", "file")
     (tmp_dir / "file").write_text("something")
 
-    fs = DvcFileSystem(repo=dvc)
+    fs = DataFileSystem(index=dvc.index.data["repo"])
     expected = "8c7dd922ad47494fc02c388e12c00eac"
     assert fs.info("file").get("md5") == expected
-    _, _, obj = stage(dvc.odb.local, "file", fs, "md5", dry_run=True)
+    _, _, obj = build(dvc.cache.local, "file", fs, "md5", dry_run=True)
     assert obj.hash_info == HashInfo("md5", expected)
 
 
 def test_get_hash_dirty_dir(tmp_dir, dvc):
     tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}})
     (tmp_dir / "dir" / "baz").write_text("baz")
 
-    fs = DvcFileSystem(repo=dvc)
+    fs = DataFileSystem(index=dvc.index.data["repo"])
     expected = "5ea40360f5b4ec688df672a4db9c17d1.dir"
     assert fs.info("dir").get("md5") == expected
-    _, _, obj = stage(dvc.odb.local, "dir", fs, "md5", dry_run=True)
+    _, _, obj = build(dvc.cache.local, "dir", fs, "md5", dry_run=True)
     assert obj.hash_info == HashInfo("md5", expected)
```

### Comparing `dvc-2.9.5/tests/unit/fs/test_path.py` & `dvc-3.0.0a0/tests/unit/fs/test_path.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 import pytest
 
-from dvc.fs.path import Path
+from dvc.fs import Path
 
 
 @pytest.mark.parametrize("prefix", ["", "/"])
 @pytest.mark.parametrize("postfix", ["", "/"])
 @pytest.mark.parametrize(
     "path,expected",
     [
```

### Comparing `dvc-2.9.5/tests/unit/fs/test_repo.py` & `dvc-3.0.0a0/tests/unit/fs/test_dvc.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,73 +1,71 @@
 import os
+import posixpath
 import shutil
 from unittest import mock
 
 import pytest
 
-from dvc.data.stage import stage
-from dvc.fs.repo import RepoFileSystem
-from dvc.hash_info import HashInfo
-from tests.utils import clean_staging
+from dvc.fs.dvc import DVCFileSystem
+from dvc.testing.tmp_dir import make_subrepo
+from dvc_data.hashfile.build import build
+from dvc_data.hashfile.hash_info import HashInfo
 
 
 def test_exists(tmp_dir, dvc):
     tmp_dir.gen("foo", "foo")
     dvc.add("foo")
     (tmp_dir / "foo").unlink()
 
-    fs = RepoFileSystem(repo=dvc)
+    fs = DVCFileSystem(repo=dvc)
     assert fs.exists("foo")
 
 
 def test_open(tmp_dir, dvc):
     tmp_dir.gen("foo", "foo")
     dvc.add("foo")
     (tmp_dir / "foo").unlink()
 
-    fs = RepoFileSystem(repo=dvc)
-    with fs.open((tmp_dir / "foo").fs_path, "r") as fobj:
+    fs = DVCFileSystem(repo=dvc)
+    with fs.open("foo", "r") as fobj:
         assert fobj.read() == "foo"
 
 
 def test_open_dirty_hash(tmp_dir, dvc):
     tmp_dir.dvc_gen("file", "file")
     (tmp_dir / "file").write_text("something")
 
-    fs = RepoFileSystem(repo=dvc)
-    with fs.open((tmp_dir / "file").fs_path, "r") as fobj:
+    fs = DVCFileSystem(repo=dvc)
+    with fs.open("file", "r") as fobj:
         assert fobj.read() == "something"
 
 
 def test_open_dirty_no_hash(tmp_dir, dvc):
     tmp_dir.gen("file", "file")
     (tmp_dir / "file.dvc").write_text("outs:\n- path: file\n")
 
-    fs = RepoFileSystem(repo=dvc)
-    with fs.open((tmp_dir / "file").fs_path, "r") as fobj:
+    fs = DVCFileSystem(repo=dvc)
+    with fs.open("file", "r") as fobj:
         assert fobj.read() == "file"
 
 
 def test_open_in_history(tmp_dir, scm, dvc):
     tmp_dir.gen("foo", "foo")
     dvc.add("foo")
     dvc.scm.add(["foo.dvc", ".gitignore"])
     dvc.scm.commit("foo")
 
     tmp_dir.gen("foo", "foofoo")
     dvc.add("foo")
     dvc.scm.add(["foo.dvc", ".gitignore"])
     dvc.scm.commit("foofoo")
 
-    for rev in dvc.brancher(revs=["HEAD~1"]):
-        if rev == "workspace":
-            continue
-
-        fs = RepoFileSystem(repo=dvc)
-        with fs.open((tmp_dir / "foo").fs_path, "r") as fobj:
+    with dvc.switch("HEAD~1"):
+        fs = DVCFileSystem(repo=dvc)
+        with fs.open("foo", "r") as fobj:
             assert fobj.read() == "foo"
 
 
 def test_isdir_isfile(tmp_dir, dvc):
     tmp_dir.gen(
         {
             "datafile": "data",
@@ -81,15 +79,15 @@
                     "abc": "abc",
                     "xyz": "xyz",
                 },
             },
         },
     )
 
-    fs = RepoFileSystem(repo=dvc)
+    fs = DVCFileSystem(repo=dvc)
     assert fs.isdir("datadir")
     assert not fs.isfile("datadir")
     assert not fs.isdvc("datadir")
     assert not fs.isdir("datafile")
     assert fs.isfile("datafile")
     assert not fs.isdvc("datafile")
 
@@ -102,84 +100,82 @@
         ]
     )
     shutil.rmtree(tmp_dir / "datadir")
     shutil.rmtree(tmp_dir / "subdir" / "data")
     (tmp_dir / "datafile").unlink()
     (tmp_dir / "subdir" / "baz").unlink()
 
+    fs = DVCFileSystem(repo=dvc)
     assert fs.isdir("datadir")
     assert not fs.isfile("datadir")
     assert fs.isdvc("datadir")
     assert not fs.isdir("datafile")
     assert fs.isfile("datafile")
     assert fs.isdvc("datafile")
 
     assert fs.isdir("subdir")
     assert not fs.isfile("subdir")
     assert not fs.isdvc("subdir")
-    assert fs.isfile(os.path.join("subdir", "baz"))
-    assert fs.isdir(os.path.join("subdir", "data"))
+    assert fs.isfile("subdir/baz")
+    assert fs.isdir("subdir/data")
 
 
 def test_exists_isdir_isfile_dirty(tmp_dir, dvc):
-    tmp_dir.dvc_gen(
-        {"datafile": "data", "datadir": {"foo": "foo", "bar": "bar"}}
-    )
+    tmp_dir.dvc_gen({"datafile": "data", "datadir": {"foo": "foo", "bar": "bar"}})
 
-    fs = RepoFileSystem(repo=dvc)
+    fs = DVCFileSystem(repo=dvc)
     shutil.rmtree(tmp_dir / "datadir")
     (tmp_dir / "datafile").unlink()
 
-    root = tmp_dir
-    assert fs.exists(root / "datafile")
-    assert fs.exists(root / "datadir")
-    assert fs.exists(root / "datadir" / "foo")
-    assert fs.isfile(root / "datafile")
-    assert not fs.isfile(root / "datadir")
-    assert fs.isfile(root / "datadir" / "foo")
-    assert not fs.isdir(root / "datafile")
-    assert fs.isdir(root / "datadir")
-    assert not fs.isdir(root / "datadir" / "foo")
+    assert fs.exists("datafile")
+    assert fs.exists("datadir")
+    assert fs.exists("datadir/foo")
+    assert fs.isfile("datafile")
+    assert not fs.isfile("datadir")
+    assert fs.isfile("datadir/foo")
+    assert not fs.isdir("datafile")
+    assert fs.isdir("datadir")
+    assert not fs.isdir("datadir/foo")
 
     # NOTE: creating file instead of dir and dir instead of file
     tmp_dir.gen({"datadir": "data", "datafile": {"foo": "foo", "bar": "bar"}})
-    assert fs.exists(root / "datafile")
-    assert fs.exists(root / "datadir")
-    assert not fs.exists(root / "datadir" / "foo")
-    assert fs.exists(root / "datafile" / "foo")
-    assert not fs.isfile(root / "datafile")
-    assert fs.isfile(root / "datadir")
-    assert not fs.isfile(root / "datadir" / "foo")
-    assert fs.isfile(root / "datafile" / "foo")
-    assert fs.isdir(root / "datafile")
-    assert not fs.isdir(root / "datadir")
-    assert not fs.isdir(root / "datadir" / "foo")
-    assert not fs.isdir(root / "datafile" / "foo")
+    assert fs.exists("datafile")
+    assert fs.exists("datadir")
+    assert not fs.exists("datadir/foo")
+    assert fs.exists("datafile/foo")
+    assert not fs.isfile("datafile")
+    assert fs.isfile("datadir")
+    assert not fs.isfile("datadir/foo")
+    assert fs.isfile("datafile/foo")
+    assert fs.isdir("datafile")
+    assert not fs.isdir("datadir")
+    assert not fs.isdir("datadir/foo")
+    assert not fs.isdir("datafile/foo")
 
 
 def test_isdir_mixed(tmp_dir, dvc):
     tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
 
     dvc.add(str(tmp_dir / "dir" / "foo"))
 
-    fs = RepoFileSystem(repo=dvc)
+    fs = DVCFileSystem(repo=dvc)
     assert fs.isdir("dir")
     assert not fs.isfile("dir")
 
 
 @pytest.mark.parametrize(
     "dvcfiles,extra_expected",
     [
         (False, []),
         (
             True,
             [
-                os.path.join("dir", "subdir1", "foo1.dvc"),
-                os.path.join("dir", "subdir1", "bar1.dvc"),
-                os.path.join("dir", "subdir2", "foo2.dvc"),
+                "dir/subdir1/foo1.dvc",
+                "dir/subdir1/bar1.dvc",
+                "dir/subdir2/foo2.dvc",
             ],
         ),
     ],
 )
 def test_walk(tmp_dir, dvc, dvcfiles, extra_expected):
     tmp_dir.gen(
         {
@@ -187,30 +183,30 @@
                 "subdir1": {"foo1": "foo1", "bar1": "bar1"},
                 "subdir2": {"foo2": "foo2"},
             }
         }
     )
     dvc.add(str(tmp_dir / "dir"), recursive=True)
     tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
-    fs = RepoFileSystem(repo=dvc)
+    fs = DVCFileSystem(repo=dvc)
 
     expected = [
-        os.path.join("dir", "subdir1"),
-        os.path.join("dir", "subdir2"),
-        os.path.join("dir", "subdir1", "foo1"),
-        os.path.join("dir", "subdir1", "bar1"),
-        os.path.join("dir", "subdir2", "foo2"),
-        os.path.join("dir", "foo"),
-        os.path.join("dir", "bar"),
+        "dir/subdir1",
+        "dir/subdir2",
+        "dir/subdir1/foo1",
+        "dir/subdir1/bar1",
+        "dir/subdir2/foo2",
+        "dir/foo",
+        "dir/bar",
     ]
 
     actual = []
     for root, dirs, files in fs.walk("dir", dvcfiles=dvcfiles):
         for entry in dirs + files:
-            actual.append(os.path.join(root, entry))
+            actual.append(posixpath.join(root, entry))
 
     expected += extra_expected
     assert set(actual) == set(expected)
     assert len(actual) == len(expected)
 
 
 def test_walk_dirty(tmp_dir, dvc):
@@ -222,340 +218,315 @@
                 "subdir2": {"foo2": "foo2"},
             }
         }
     )
     tmp_dir.gen({"dir": {"bar": "bar", "subdir3": {"foo3": "foo3"}}})
     (tmp_dir / "dir" / "foo").unlink()
 
-    fs = RepoFileSystem(repo=dvc)
+    fs = DVCFileSystem(repo=dvc)
     expected = [
-        os.path.join("dir", "subdir1"),
-        os.path.join("dir", "subdir2"),
-        os.path.join("dir", "subdir3"),
-        os.path.join("dir", "subdir1", "foo1"),
-        os.path.join("dir", "subdir1", "bar1"),
-        os.path.join("dir", "subdir2", "foo2"),
-        os.path.join("dir", "subdir3", "foo3"),
-        os.path.join("dir", "bar"),
+        "dir/subdir1",
+        "dir/subdir2",
+        "dir/subdir3",
+        "dir/subdir1/foo1",
+        "dir/subdir1/bar1",
+        "dir/subdir2/foo2",
+        "dir/subdir3/foo3",
+        "dir/bar",
+        "dir/foo",
     ]
 
     actual = []
     for root, dirs, files in fs.walk("dir"):
         for entry in dirs + files:
-            actual.append(os.path.join(root, entry))
+            actual.append(posixpath.join(root, entry))
 
     assert set(actual) == set(expected)
     assert len(actual) == len(expected)
 
 
 def test_walk_dirty_cached_dir(tmp_dir, scm, dvc):
     tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar"}}, commit="add data")
     (tmp_dir / "data" / "foo").unlink()
 
-    fs = RepoFileSystem(repo=dvc)
-
-    data = tmp_dir / "data"
+    fs = DVCFileSystem(repo=dvc)
 
     actual = []
-    for root, dirs, files in fs.walk(data):
+    for root, dirs, files in fs.walk("data"):
         for entry in dirs + files:
-            actual.append(os.path.join(root, entry))
+            actual.append(posixpath.join(root, entry))
 
-    assert actual == [(data / "bar").fs_path]
+    expected = ["data/foo", "data/bar"]
+    assert set(actual) == set(expected)
+    assert len(actual) == len(expected)
 
 
 def test_walk_mixed_dir(tmp_dir, scm, dvc):
     tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
     tmp_dir.dvc.add(os.path.join("dir", "foo"))
     tmp_dir.scm.add(
         [
             os.path.join("dir", "bar"),
             os.path.join("dir", ".gitignore"),
             os.path.join("dir", "foo.dvc"),
         ]
     )
     tmp_dir.scm.commit("add dir")
 
-    fs = RepoFileSystem(repo=dvc)
+    fs = DVCFileSystem(repo=dvc)
 
     expected = [
-        os.path.join("dir", "foo"),
-        os.path.join("dir", "bar"),
-        os.path.join("dir", ".gitignore"),
+        "dir/foo",
+        "dir/bar",
+        "dir/.gitignore",
     ]
     actual = []
     for root, dirs, files in fs.walk("dir"):
         for entry in dirs + files:
-            actual.append(os.path.join(root, entry))
+            actual.append(posixpath.join(root, entry))
 
     assert set(actual) == set(expected)
     assert len(actual) == len(expected)
 
 
 def test_walk_missing(tmp_dir, dvc):
-    fs = RepoFileSystem(repo=dvc)
+    fs = DVCFileSystem(repo=dvc)
 
     for _ in fs.walk("dir"):
         pass
 
 
 def test_walk_not_a_dir(tmp_dir, dvc):
     tmp_dir.dvc_gen("foo", "foo")
-    fs = RepoFileSystem(repo=dvc)
+    fs = DVCFileSystem(repo=dvc)
 
     for _ in fs.walk("foo"):
         pass
 
 
 def test_isdvc(tmp_dir, dvc):
     tmp_dir.gen({"foo": "foo", "bar": "bar", "dir": {"baz": "baz"}})
     dvc.add("foo")
     dvc.add("dir")
-    fs = RepoFileSystem(repo=dvc)
+    fs = DVCFileSystem(repo=dvc)
     assert fs.isdvc("foo")
     assert not fs.isdvc("bar")
     assert fs.isdvc("dir")
-    assert fs.isdvc(os.path.join("dir", "baz"))
-    assert fs.isdvc(os.path.join("dir", "baz"), recursive=True)
-
-
-def make_subrepo(dir_, scm, config=None):
-    dir_.mkdir(parents=True, exist_ok=True)
-    with dir_.chdir():
-        dir_.scm = scm
-        dir_.init(dvc=True, subdir=True)
-        if config:
-            dir_.add_remote(config=config)
+    assert fs.isdvc("dir/baz")
+    assert fs.isdvc("dir/baz", recursive=True)
 
 
 def test_subrepos(tmp_dir, scm, dvc, mocker):
     tmp_dir.scm_gen(
-        {"dir": {"repo.txt": "file to confuse RepoFileSystem"}},
+        {"dir": {"repo.txt": "file to confuse DVCFileSystem"}},
         commit="dir/repo.txt",
     )
 
     subrepo1 = tmp_dir / "dir" / "repo"
     subrepo2 = tmp_dir / "dir" / "repo2"
 
     for repo in [subrepo1, subrepo2]:
         make_subrepo(repo, scm)
 
     with subrepo1.chdir():
         subrepo1.dvc_gen({"foo": "foo", "dir1": {"bar": "bar"}}, commit="FOO")
     with subrepo2.chdir():
-        subrepo2.dvc_gen(
-            {"lorem": "lorem", "dir2": {"ipsum": "ipsum"}}, commit="BAR"
-        )
+        subrepo2.dvc_gen({"lorem": "lorem", "dir2": {"ipsum": "ipsum"}}, commit="BAR")
 
     dvc._reset()
-    fs = RepoFileSystem(repo=dvc, subrepos=True)
+    fs = DVCFileSystem(repo=dvc, subrepos=True)
 
     def assert_fs_belongs_to_repo(ret_val):
-        method = fs._get_repo
+        method = fs.fs._get_repo
 
         def f(*args, **kwargs):
             r = method(*args, **kwargs)
             assert r.root_dir == ret_val.root_dir
             return r
 
         return f
 
     with mock.patch.object(
-        fs, "_get_repo", side_effect=assert_fs_belongs_to_repo(subrepo1.dvc)
+        fs.fs, "_get_repo", side_effect=assert_fs_belongs_to_repo(subrepo1.dvc)
     ):
-        assert fs.exists((subrepo1 / "foo").fs_path) is True
-        assert fs.exists((subrepo1 / "bar").fs_path) is False
+        assert fs.exists("dir/repo/foo") is True
+        assert fs.exists("dir/repo/bar") is False
 
-        assert fs.isfile((subrepo1 / "foo").fs_path) is True
-        assert fs.isfile((subrepo1 / "dir1" / "bar").fs_path) is True
-        assert fs.isfile((subrepo1 / "dir1").fs_path) is False
-
-        assert fs.isdir((subrepo1 / "dir1").fs_path) is True
-        assert fs.isdir((subrepo1 / "dir1" / "bar").fs_path) is False
-        assert fs.isdvc((subrepo1 / "foo").fs_path) is True
+        assert fs.isfile("dir/repo/foo") is True
+        assert fs.isfile("dir/repo/dir1/bar") is True
+        assert fs.isfile("dir/repo/dir1") is False
+
+        assert fs.isdir("dir/repo/dir1") is True
+        assert fs.isdir("dir/repo/dir1/bar") is False
+        assert fs.isdvc("dir/repo/foo") is True
 
     with mock.patch.object(
-        fs, "_get_repo", side_effect=assert_fs_belongs_to_repo(subrepo2.dvc)
+        fs.fs, "_get_repo", side_effect=assert_fs_belongs_to_repo(subrepo2.dvc)
     ):
-        assert fs.exists((subrepo2 / "lorem").fs_path) is True
-        assert fs.exists((subrepo2 / "ipsum").fs_path) is False
+        assert fs.exists("dir/repo2/lorem") is True
+        assert fs.exists("dir/repo2/ipsum") is False
 
-        assert fs.isfile((subrepo2 / "lorem").fs_path) is True
-        assert fs.isfile((subrepo2 / "dir2" / "ipsum").fs_path) is True
-        assert fs.isfile((subrepo2 / "dir2").fs_path) is False
-
-        assert fs.isdir((subrepo2 / "dir2").fs_path) is True
-        assert fs.isdir((subrepo2 / "dir2" / "ipsum").fs_path) is False
-        assert fs.isdvc((subrepo2 / "lorem").fs_path) is True
+        assert fs.isfile("dir/repo2/lorem") is True
+        assert fs.isfile("dir/repo2/dir2/ipsum") is True
+        assert fs.isfile("dir/repo2/dir2") is False
+
+        assert fs.isdir("dir/repo2/dir2") is True
+        assert fs.isdir("dir/repo2/dir2/ipsum") is False
+        assert fs.isdvc("dir/repo2/lorem") is True
 
 
 @pytest.mark.parametrize(
     "dvcfiles,extra_expected",
     [
         (False, []),
         (
             True,
             [
-                os.path.join("dir", "repo", "foo.dvc"),
-                os.path.join("dir", "repo", ".dvcignore"),
-                os.path.join("dir", "repo", "dir1.dvc"),
-                os.path.join("dir", "repo2", ".dvcignore"),
-                os.path.join("dir", "repo2", "lorem.dvc"),
-                os.path.join("dir", "repo2", "dir2.dvc"),
+                "dir/repo/foo.dvc",
+                "dir/repo/.dvcignore",
+                "dir/repo/dir1.dvc",
+                "dir/repo2/.dvcignore",
+                "dir/repo2/lorem.dvc",
+                "dir/repo2/dir2.dvc",
             ],
         ),
     ],
 )
 def test_subrepo_walk(tmp_dir, scm, dvc, dvcfiles, extra_expected):
     tmp_dir.scm_gen(
-        {"dir": {"repo.txt": "file to confuse RepoFileSystem"}},
+        {"dir": {"repo.txt": "file to confuse DVCFileSystem"}},
         commit="dir/repo.txt",
     )
 
     subrepo1 = tmp_dir / "dir" / "repo"
     subrepo2 = tmp_dir / "dir" / "repo2"
 
     subdirs = [subrepo1, subrepo2]
     for dir_ in subdirs:
         make_subrepo(dir_, scm)
 
     with subrepo1.chdir():
         subrepo1.dvc_gen({"foo": "foo", "dir1": {"bar": "bar"}}, commit="FOO")
     with subrepo2.chdir():
-        subrepo2.dvc_gen(
-            {"lorem": "lorem", "dir2": {"ipsum": "ipsum"}}, commit="BAR"
-        )
+        subrepo2.dvc_gen({"lorem": "lorem", "dir2": {"ipsum": "ipsum"}}, commit="BAR")
 
     # using fs that does not have dvcignore
     dvc._reset()
-    fs = RepoFileSystem(repo=dvc)
+    fs = DVCFileSystem(repo=dvc)
     expected = [
-        os.path.join("dir", "repo"),
-        os.path.join("dir", "repo.txt"),
-        os.path.join("dir", "repo2"),
-        os.path.join("dir", "repo", ".gitignore"),
-        os.path.join("dir", "repo", "foo"),
-        os.path.join("dir", "repo", "dir1"),
-        os.path.join("dir", "repo", "dir1", "bar"),
-        os.path.join("dir", "repo2", ".gitignore"),
-        os.path.join("dir", "repo2", "lorem"),
-        os.path.join("dir", "repo2", "dir2"),
-        os.path.join("dir", "repo2", "dir2", "ipsum"),
+        "dir/repo",
+        "dir/repo.txt",
+        "dir/repo2",
+        "dir/repo/.gitignore",
+        "dir/repo/foo",
+        "dir/repo/dir1",
+        "dir/repo/dir1/bar",
+        "dir/repo2/.gitignore",
+        "dir/repo2/lorem",
+        "dir/repo2/dir2",
+        "dir/repo2/dir2/ipsum",
     ]
 
     actual = []
     for root, dirs, files in fs.walk(
-        os.path.join(fs.root_dir, "dir"),
+        "dir",
         dvcfiles=dvcfiles,
         ignore_subrepos=False,
     ):
         for entry in dirs + files:
-            actual.append(os.path.join(root, entry))
+            actual.append(posixpath.join(root, entry))
 
-    expected = [
-        os.path.join(fs.root_dir, path) for path in expected + extra_expected
-    ]
+    expected += extra_expected
     assert set(actual) == set(expected)
     assert len(actual) == len(expected)
 
 
-def test_repo_fs_no_subrepos(tmp_dir, dvc, scm):
+def test_dvcfs_no_subrepos(tmp_dir, dvc, scm):
     tmp_dir.scm_gen(
-        {"dir": {"repo.txt": "file to confuse RepoFileSystem"}},
+        {"dir": {"repo.txt": "file to confuse DVCFileSystem"}},
         commit="dir/repo.txt",
     )
     tmp_dir.dvc_gen({"lorem": "lorem"}, commit="add foo")
 
     subrepo = tmp_dir / "dir" / "repo"
     make_subrepo(subrepo, scm)
     with subrepo.chdir():
         subrepo.dvc_gen({"foo": "foo", "dir1": {"bar": "bar"}}, commit="FOO")
         subrepo.scm_gen({"ipsum": "ipsum"}, commit="BAR")
 
     # using fs that does not have dvcignore
     dvc._reset()
-    fs = RepoFileSystem(repo=dvc)
+    fs = DVCFileSystem(repo=dvc)
     expected = [
-        tmp_dir / ".dvcignore",
-        tmp_dir / ".gitignore",
-        tmp_dir / "lorem",
-        tmp_dir / "lorem.dvc",
-        tmp_dir / "dir",
-        tmp_dir / "dir" / "repo.txt",
+        "/.dvcignore",
+        "/.gitignore",
+        "/lorem",
+        "/lorem.dvc",
+        "/dir",
+        "/dir/repo.txt",
     ]
 
     actual = []
-    for root, dirs, files in fs.walk(tmp_dir.fs_path, dvcfiles=True):
+    for root, dirs, files in fs.walk("/", dvcfiles=True):
         for entry in dirs + files:
-            actual.append(os.path.normpath(os.path.join(root, entry)))
+            actual.append(posixpath.join(root, entry))
 
-    expected = [str(path) for path in expected]
     assert set(actual) == set(expected)
     assert len(actual) == len(expected)
 
-    assert fs.isfile(tmp_dir / "lorem") is True
-    assert fs.isfile(tmp_dir / "dir" / "repo" / "foo") is False
-    assert fs.isdir(tmp_dir / "dir" / "repo") is False
-    assert fs.isdir(tmp_dir / "dir") is True
+    assert fs.isfile("lorem") is True
+    assert fs.isfile("dir/repo/foo") is False
+    assert fs.isdir("dir/repo") is False
+    assert fs.isdir("dir") is True
 
-    assert fs.isdvc(tmp_dir / "lorem") is True
-    assert fs.isdvc(tmp_dir / "dir" / "repo" / "dir1") is False
+    assert fs.isdvc("lorem") is True
+    assert fs.isdvc("dir/repo/dir1") is False
 
-    assert fs.exists(tmp_dir / "dir" / "repo.txt") is True
-    assert fs.exists(tmp_dir / "repo" / "ipsum") is False
+    assert fs.exists("dir/repo.txt") is True
+    assert fs.exists("repo/ipsum") is False
 
 
 def test_get_hash_cached_file(tmp_dir, dvc, mocker):
     tmp_dir.dvc_gen({"foo": "foo"})
-    fs = RepoFileSystem(repo=dvc)
+    fs = DVCFileSystem(repo=dvc)
     expected = "acbd18db4cc2f85cedef654fccc4a4d8"
-    assert fs.info((tmp_dir / "foo").fs_path).get("md5") is None
-    _, _, obj = stage(dvc.odb.local, (tmp_dir / "foo").fs_path, fs, "md5")
+    assert fs.info("foo").get("md5") is None
+    _, _, obj = build(dvc.cache.local, "foo", fs, "md5")
     assert obj.hash_info == HashInfo("md5", expected)
     (tmp_dir / "foo").unlink()
-    assert fs.info((tmp_dir / "foo").fs_path)["md5"] == expected
+    assert fs.info("foo")["md5"] == expected
 
 
 def test_get_hash_cached_dir(tmp_dir, dvc, mocker):
-    tmp_dir.dvc_gen(
-        {"dir": {"foo": "foo", "bar": "bar", "subdir": {"data": "data"}}}
-    )
-    fs = RepoFileSystem(repo=dvc)
+    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar", "subdir": {"data": "data"}}})
+    fs = DVCFileSystem(repo=dvc)
     expected = "8761c4e9acad696bee718615e23e22db.dir"
-    assert fs.info((tmp_dir / "dir").fs_path).get("md5") is None
-    _, _, obj = stage(dvc.odb.local, (tmp_dir / "dir").fs_path, fs, "md5")
-    assert obj.hash_info == HashInfo(
-        "md5", "8761c4e9acad696bee718615e23e22db.dir"
-    )
+    assert fs.info("dir").get("md5") is None
+    _, _, obj = build(dvc.cache.local, "dir", fs, "md5")
+    assert obj.hash_info == HashInfo("md5", "8761c4e9acad696bee718615e23e22db.dir")
 
     shutil.rmtree(tmp_dir / "dir")
-    assert fs.info((tmp_dir / "dir").fs_path)["md5"] == expected
-    _, _, obj = stage(dvc.odb.local, (tmp_dir / "dir").fs_path, fs, "md5")
-    assert obj.hash_info == HashInfo(
-        "md5", "8761c4e9acad696bee718615e23e22db.dir"
-    )
+    assert fs.info("dir")["md5"] == expected
+    _, _, obj = build(dvc.cache.local, "dir", fs, "md5")
+    assert obj.hash_info == HashInfo("md5", "8761c4e9acad696bee718615e23e22db.dir")
 
 
 def test_get_hash_cached_granular(tmp_dir, dvc, mocker):
-    tmp_dir.dvc_gen(
-        {"dir": {"foo": "foo", "bar": "bar", "subdir": {"data": "data"}}}
-    )
-    fs = RepoFileSystem(repo=dvc)
-    subdir = tmp_dir / "dir" / "subdir"
-    assert fs.info(subdir.fs_path).get("md5") is None
-    _, _, obj = stage(dvc.odb.local, subdir.fs_path, fs, "md5")
-    assert obj.hash_info == HashInfo(
-        "md5", "af314506f1622d107e0ed3f14ec1a3b5.dir"
-    )
-    assert fs.info((subdir / "data").fs_path).get("md5") is None
-    _, _, obj = stage(dvc.odb.local, (subdir / "data").fs_path, fs, "md5")
+    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar", "subdir": {"data": "data"}}})
+    fs = DVCFileSystem(repo=dvc)
+    subdir = "dir/subdir"
+    assert fs.info(subdir).get("md5") is None
+    _, _, obj = build(dvc.cache.local, subdir, fs, "md5")
+    assert obj.hash_info == HashInfo("md5", "af314506f1622d107e0ed3f14ec1a3b5.dir")
+    assert fs.info(posixpath.join(subdir, "data")).get("md5") is None
+    _, _, obj = build(dvc.cache.local, posixpath.join(subdir, "data"), fs, "md5")
     assert obj.hash_info == HashInfo("md5", "8d777f385d3dfec8815d20f7496026dc")
     (tmp_dir / "dir" / "subdir" / "data").unlink()
     assert (
-        fs.info((subdir / "data").fs_path)["md5"]
+        fs.info(posixpath.join(subdir, "data"))["md5"]
         == "8d777f385d3dfec8815d20f7496026dc"
     )
 
 
 def test_get_hash_mixed_dir(tmp_dir, scm, dvc):
     tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
     tmp_dir.dvc.add(os.path.join("dir", "foo"))
@@ -563,75 +534,57 @@
         [
             os.path.join("dir", "bar"),
             os.path.join("dir", ".gitignore"),
             os.path.join("dir", "foo.dvc"),
         ]
     )
     tmp_dir.scm.commit("add dir")
-    clean_staging()
 
-    fs = RepoFileSystem(repo=dvc)
-    _, _, obj = stage(dvc.odb.local, (tmp_dir / "dir").fs_path, fs, "md5")
-    assert obj.hash_info == HashInfo(
-        "md5", "e1d9e8eae5374860ae025ec84cfd85c7.dir"
-    )
+    fs = DVCFileSystem(repo=dvc)
+    _, _, obj = build(dvc.cache.local, "dir", fs, "md5")
+    assert obj.hash_info == HashInfo("md5", "e1d9e8eae5374860ae025ec84cfd85c7.dir")
 
 
 def test_get_hash_dirty_file(tmp_dir, dvc):
-    from dvc.data import check
-    from dvc.data.stage import get_file_hash
-    from dvc.objects.errors import ObjectFormatError
+    from dvc_data.hashfile import check
+    from dvc_data.hashfile.hash import hash_file
 
     tmp_dir.dvc_gen("file", "file")
     file_hash_info = HashInfo("md5", "8c7dd922ad47494fc02c388e12c00eac")
 
     (tmp_dir / "file").write_text("something")
     something_hash_info = HashInfo("md5", "437b930db84b8079c2dd804a71936b5f")
 
-    clean_staging()
-
     # file is modified in workspace
-    # get_file_hash(file) should return workspace hash, not DVC cached hash
-    fs = RepoFileSystem(repo=dvc)
-    assert fs.info((tmp_dir / "file").fs_path).get("md5") is None
-    staging, _, obj = stage(
-        dvc.odb.local, (tmp_dir / "file").fs_path, fs, "md5"
-    )
+    # hash_file(file) should return workspace hash, not DVC cached hash
+    fs = DVCFileSystem(repo=dvc)
+    assert fs.info("file").get("md5") is None
+    staging, _, obj = build(dvc.cache.local, "file", fs, "md5")
     assert obj.hash_info == something_hash_info
     check(staging, obj)
 
-    # file is removed in workspace
-    # any staged object referring to modified workspace obj is now invalid
+    # hash_file(file) should return DVC cached hash
     (tmp_dir / "file").unlink()
-    with pytest.raises(ObjectFormatError):
-        check(staging, obj)
-
-    # get_file_hash(file) should return DVC cached hash
-    assert fs.info((tmp_dir / "file").fs_path)["md5"] == file_hash_info.value
-    _, hash_info = get_file_hash(
-        (tmp_dir / "file").fs_path, fs, "md5", state=dvc.state
-    )
+    assert fs.info("file")["md5"] == file_hash_info.value
+    _, hash_info = hash_file("file", fs, "md5", state=dvc.state)
     assert hash_info == file_hash_info
 
-    # tmp_dir/file can be staged even though it is missing in workspace since
+    # tmp_dir/file can be built even though it is missing in workspace since
     # repofs will use the DVC cached hash (and refer to the local cache object)
-    _, _, obj = stage(dvc.odb.local, (tmp_dir / "file").fs_path, fs, "md5")
+    _, _, obj = build(dvc.cache.local, "file", fs, "md5")
     assert obj.hash_info == file_hash_info
 
 
 def test_get_hash_dirty_dir(tmp_dir, dvc):
     tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}})
     (tmp_dir / "dir" / "baz").write_text("baz")
-    clean_staging()
 
-    fs = RepoFileSystem(repo=dvc)
-    _, meta, obj = stage(dvc.odb.local, (tmp_dir / "dir").fs_path, fs, "md5")
-    assert obj.hash_info == HashInfo(
-        "md5", "ba75a2162ca9c29acecb7957105a0bc2.dir"
-    )
+    fs = DVCFileSystem(repo=dvc)
+    _, meta, obj = build(dvc.cache.local, "dir", fs, "md5")
+    assert obj.hash_info == HashInfo("md5", "ba75a2162ca9c29acecb7957105a0bc2.dir")
     assert meta.nfiles == 3
 
 
 @pytest.mark.parametrize("traverse_subrepos", [True, False])
 def test_walk_nested_subrepos(tmp_dir, dvc, scm, traverse_subrepos):
     # generate a dvc and fs structure, with suffix based on repo's basename
     def fs_structure(suffix):
@@ -649,36 +602,37 @@
     paths = ["subrepo1", "subrepo2", os.path.join("subrepo1", "subrepo3")]
     subrepos = [tmp_dir / path for path in paths]
     for repo_dir in subrepos:
         make_subrepo(repo_dir, scm)
 
     extras = {".gitignore"}  # these files are always there
     expected = {}
-    for repo_dir in subrepos + [tmp_dir]:
+    for repo_dir in [*subrepos, tmp_dir]:
         base = os.path.basename(repo_dir)
         scm_files = fs_structure(base)
         dvc_files = dvc_structure(base)
         with repo_dir.chdir():
             repo_dir.scm_gen(scm_files, commit=f"git add in {repo_dir}")
             repo_dir.dvc_gen(dvc_files, commit=f"dvc add in {repo_dir}")
 
         if traverse_subrepos or repo_dir == tmp_dir:
-            expected[str(repo_dir)] = set(
-                scm_files.keys() | dvc_files.keys() | extras
+            repo_dir_path = (
+                "/" + repo_dir.relative_to(tmp_dir).as_posix()
+                if repo_dir != tmp_dir
+                else "/"
             )
+            expected[repo_dir_path] = set(scm_files.keys() | dvc_files.keys() | extras)
             # files inside a dvc directory
-            expected[str(repo_dir / f"dvc-{base}")] = {f"ipsum-{base}"}
+            expected[posixpath.join(repo_dir_path, f"dvc-{base}")] = {f"ipsum-{base}"}
             # files inside a git directory
-            expected[str(repo_dir / f"dir-{base}")] = {f"bar-{base}"}
+            expected[posixpath.join(repo_dir_path, f"dir-{base}")] = {f"bar-{base}"}
 
     if traverse_subrepos:
         # update subrepos
-        expected[str(tmp_dir)].update(["subrepo1", "subrepo2"])
-        expected[str(tmp_dir / "subrepo1")].add("subrepo3")
+        expected["/"].update(["subrepo1", "subrepo2"])
+        expected["/subrepo1"].add("subrepo3")
 
     actual = {}
-    fs = RepoFileSystem(repo=dvc)
-    for root, dirs, files in fs.walk(
-        str(tmp_dir), ignore_subrepos=not traverse_subrepos
-    ):
+    fs = DVCFileSystem(repo=dvc)
+    for root, dirs, files in fs.walk("/", ignore_subrepos=not traverse_subrepos):
         actual[root] = set(dirs + files)
     assert expected == actual
```

### Comparing `dvc-2.9.5/tests/unit/fs/test_s3.py` & `dvc-3.0.0a0/tests/unit/fs/test_s3.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import os
 
 import pytest
+from dvc_s3 import S3FileSystem
 
-from dvc.config import ConfigError
-from dvc.fs.s3 import S3FileSystem
+from dvc.fs import ConfigError
 
 bucket_name = "bucket-name"
 prefix = "some/prefix"
 url = f"s3://{bucket_name}/{prefix}"
 key_id = "key-id"
 key_secret = "key-secret"
 session_token = "session-token"
@@ -101,16 +101,15 @@
         "grant_write_acp": "id=write-acp-permission-id",
         "grant_full_control": "id=full-control-permission-id",
     }
     fs = S3FileSystem(**config)
 
     extra_args = fs.fs_args["s3_additional_kwargs"]
     assert (
-        extra_args["GrantRead"]
-        == "id=read-permission-id,id=other-read-permission-id"
+        extra_args["GrantRead"] == "id=read-permission-id,id=other-read-permission-id"
     )
     assert extra_args["GrantReadACP"] == "id=read-acp-permission-id"
     assert extra_args["GrantWriteACP"] == "id=write-acp-permission-id"
     assert extra_args["GrantFullControl"] == "id=full-control-permission-id"
 
 
 def test_grants_mutually_exclusive_acl_error(dvc, grants):
```

### Comparing `dvc-2.9.5/tests/unit/fs/test_tree.py` & `dvc-3.0.0a0/tests/unit/fs/test_tree.py`

 * *Files 23% similar despite different names*

```diff
@@ -3,34 +3,30 @@
 from dvc.config import ConfigError
 from dvc.fs import get_cloud_fs
 
 
 def test_get_cloud_fs(tmp_dir, dvc):
     tmp_dir.add_remote(name="base", url="s3://bucket/path", default=False)
     tmp_dir.add_remote(name="first", url="remote://base/first", default=False)
-    tmp_dir.add_remote(
-        name="second", url="remote://first/second", default=False
-    )
+    tmp_dir.add_remote(name="second", url="remote://first/second", default=False)
 
     base = "bucket/path"
     first = f"{base}/first"
     second = f"{first}/second"
 
     _, _, path = get_cloud_fs(dvc, name="base")
     assert path == base
     _, _, path = get_cloud_fs(dvc, name="first")
     assert path == first
     _, _, path = get_cloud_fs(dvc, name="second")
     assert path == second
 
 
 def test_get_cloud_fs_validate(tmp_dir, dvc):
-    tmp_dir.add_remote(
-        name="base", url="ssh://example.com/path", default=False
-    )
+    tmp_dir.add_remote(name="base", url="ssh://example.com/path", default=False)
     tmp_dir.add_remote(
         name="first",
         config={"url": "remote://base/first", "type": "symlink"},
         default=False,
     )
     tmp_dir.add_remote(
         name="second",
```

### Comparing `dvc-2.9.5/tests/unit/machine/test_machine.py` & `dvc-3.0.0a0/tests/unit/machine/test_machine.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/unit/objects/db/test_local.py` & `dvc-3.0.0a0/tests/unit/data/db/test_local.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,42 +1,42 @@
 import errno
 import os
 
 import pytest
 
-from dvc.data.db.local import LocalObjectDB
-from dvc.fs.local import LocalFileSystem
-from dvc.hash_info import HashInfo
+from dvc.fs import LocalFileSystem
+from dvc_data.hashfile.db.local import LocalHashFileDB
+from dvc_data.hashfile.hash_info import HashInfo
 
 
 def test_status_download_optimization(mocker, dvc):
     """When comparing the status to pull a remote cache,
     And the desired files to fetch are already on the local cache,
     Don't check the existence of the desired files on the remote cache
     """
-    from dvc.data.status import compare_status
+    from dvc_data.hashfile.status import compare_status
 
-    odb = LocalObjectDB(LocalFileSystem(), os.getcwd())
+    odb = LocalHashFileDB(LocalFileSystem(), os.getcwd())
     obj_ids = {
         HashInfo("md5", "acbd18db4cc2f85cedef654fccc4a4d8"),
         HashInfo("md5", "37b51d194a7513e45b56f6524f2d51f2"),
     }
 
     local_exists = [hash_info.value for hash_info in obj_ids]
-    mocker.patch.object(odb, "hashes_exist", return_value=local_exists)
+    mocker.patch.object(odb, "oids_exist", return_value=local_exists)
 
     src_odb = mocker.Mock()
 
     compare_status(src_odb, odb, obj_ids, check_deleted=False)
-    assert src_odb.hashes_exist.call_count == 0
+    assert src_odb.oids_exist.call_count == 0
 
 
 @pytest.mark.parametrize("link_name", ["hardlink", "symlink"])
 def test_is_protected(tmp_dir, dvc, link_name):
-    odb = dvc.odb.local
+    odb = dvc.cache.local
     fs = odb.fs
     link_method = getattr(fs, link_name)
 
     (tmp_dir / "foo").write_text("foo")
 
     foo = tmp_dir / "foo"
     link = tmp_dir / "link"
@@ -62,81 +62,71 @@
         assert odb.is_protected(foo)
 
 
 @pytest.mark.parametrize("err", [errno.EPERM, errno.EACCES, errno.EROFS])
 def test_protect_ignore_errors(tmp_dir, dvc, mocker, err):
     tmp_dir.gen("foo", "foo")
 
-    mock_chmod = mocker.patch(
-        "os.chmod", side_effect=OSError(err, "something")
-    )
-    dvc.odb.local.protect("foo")
+    mock_chmod = mocker.patch("os.chmod", side_effect=OSError(err, "something"))
+    dvc.cache.local.protect("foo")
     assert mock_chmod.called
 
 
 @pytest.mark.parametrize("err", [errno.EPERM, errno.EACCES, errno.EROFS])
 def test_set_exec_ignore_errors(tmp_dir, dvc, mocker, err):
     tmp_dir.gen("foo", "foo")
 
-    mock_chmod = mocker.patch(
-        "os.chmod", side_effect=OSError(err, "something")
-    )
-    dvc.odb.local.set_exec("foo")
+    mock_chmod = mocker.patch("os.chmod", side_effect=OSError(err, "something"))
+    dvc.cache.local.set_exec("foo")
     assert mock_chmod.called
 
 
 def test_staging_file(tmp_dir, dvc):
-    from dvc.data import check
-    from dvc.data.stage import stage
-    from dvc.data.transfer import transfer
+    from dvc_data.hashfile import check
+    from dvc_data.hashfile.build import build
+    from dvc_data.hashfile.transfer import transfer
 
     tmp_dir.gen("foo", "foo")
     fs = LocalFileSystem()
 
-    local_odb = dvc.odb.local
-    staging_odb, _, obj = stage(
-        local_odb, (tmp_dir / "foo").fs_path, fs, "md5"
-    )
+    local_odb = dvc.cache.local
+    staging_odb, _, obj = build(local_odb, (tmp_dir / "foo").fs_path, fs, "md5")
 
-    assert not local_odb.exists(obj.hash_info)
-    assert staging_odb.exists(obj.hash_info)
+    assert not local_odb.exists(obj.hash_info.value)
+    assert staging_odb.exists(obj.hash_info.value)
 
     with pytest.raises(FileNotFoundError):
         check(local_odb, obj)
     check(staging_odb, obj)
 
     transfer(staging_odb, local_odb, {obj.hash_info}, hardlink=True)
     check(local_odb, obj)
     check(staging_odb, obj)
 
-    path = local_odb.hash_to_path(obj.hash_info.value)
+    path = local_odb.oid_to_path(obj.hash_info.value)
     assert fs.exists(path)
 
 
 def test_staging_dir(tmp_dir, dvc):
-    from dvc.data import check
-    from dvc.data.stage import stage
-    from dvc.data.transfer import transfer
+    from dvc_data.hashfile import check
+    from dvc_data.hashfile.build import build
+    from dvc_data.hashfile.transfer import transfer
 
     tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
     fs = LocalFileSystem()
-    local_odb = dvc.odb.local
+    local_odb = dvc.cache.local
 
-    staging_odb, _, obj = stage(
-        local_odb, (tmp_dir / "dir").fs_path, fs, "md5"
-    )
+    staging_odb, _, obj = build(local_odb, (tmp_dir / "dir").fs_path, fs, "md5")
 
-    assert not local_odb.exists(obj.hash_info)
-    assert staging_odb.exists(obj.hash_info)
+    assert not local_odb.exists(obj.hash_info.value)
+    assert staging_odb.exists(obj.hash_info.value)
 
     with pytest.raises(FileNotFoundError):
         check(local_odb, obj)
     check(staging_odb, obj)
 
-    transfer(
-        staging_odb, local_odb, {obj.hash_info}, shallow=False, hardlink=True
-    )
+    transfer(staging_odb, local_odb, {obj.hash_info}, shallow=False, hardlink=True)
     check(local_odb, obj)
     check(staging_odb, obj)
 
-    path = local_odb.hash_to_path(obj.hash_info.value)
+    path = local_odb.oid_to_path(obj.hash_info.value)
     assert fs.exists(path)
```

### Comparing `dvc-2.9.5/tests/unit/output/test_load.py` & `dvc-3.0.0a0/tests/unit/output/test_load.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,12 @@
-# pylint: disable=unsubscriptable-object
-
 import pytest
+from dvc_s3 import S3FileSystem
 
 from dvc import output
-from dvc.fs.local import LocalFileSystem
-from dvc.fs.s3 import S3FileSystem
+from dvc.fs import LocalFileSystem
 from dvc.output import Output
 from dvc.stage import Stage
 
 
 @pytest.mark.parametrize(
     "out_type,type_test_func",
     [
@@ -56,29 +54,33 @@
             {"file2": {"persist": True}},
         ],
         "outs",
     )
     for out in outs:
         assert isinstance(out, Output)
         assert isinstance(out.fs, LocalFileSystem)
-        assert not out.plot and not out.metric
+        assert not out.plot
+        assert not out.metric
         assert not out.hash_info
 
-    assert outs[0].use_cache and not outs[0].persist
-    assert not outs[1].use_cache and outs[1].persist
+    assert outs[0].use_cache
+    assert not outs[0].persist
+    assert not outs[1].use_cache
+    assert outs[1].persist
 
 
 def test_load_remote_files_from_pipeline(dvc):
     stage = Stage(dvc)
     (out,) = output.load_from_pipeline(
         stage, [{"s3://dvc-test/file.txt": {"cache": False}}], typ="metrics"
     )
     assert isinstance(out, Output)
     assert isinstance(out.fs, S3FileSystem)
-    assert not out.plot and out.metric
+    assert not out.plot
+    assert out.metric
     assert not out.persist
     assert not out.hash_info
 
 
 def test_load_remote(dvc):
     stage = Stage(dvc)
     (foo, bar) = output.load_from_pipeline(
@@ -87,24 +89,29 @@
     )
     assert foo.remote is None
     assert bar.remote == "myremote"
 
 
 @pytest.mark.parametrize("typ", [None, "", "illegal"])
 def test_load_from_pipeline_error_on_typ(dvc, typ):
-    with pytest.raises(ValueError):
+    with pytest.raises(
+        ValueError, match=f"'{typ}' key is not allowed for pipeline files."
+    ):
         output.load_from_pipeline(Stage(dvc), ["file1"], typ)
 
 
 @pytest.mark.parametrize("key", [3, "list".split()])
 def test_load_from_pipeline_illegal_type(dvc, key):
     stage = Stage(dvc)
-    with pytest.raises(ValueError):
+    with pytest.raises(ValueError, match=f"'{type(key).__name__}' not supported."):
         output.load_from_pipeline(stage, [key], "outs")
-    with pytest.raises(ValueError):
+    with pytest.raises(
+        ValueError,
+        match=f"Expected dict for 'key', got: '{type(key).__name__}'",
+    ):
         output.load_from_pipeline(stage, [{"key": key}], "outs")
 
 
 def test_plots_load_from_pipeline(dvc):
     outs = output.load_from_pipeline(
         Stage(dvc),
         [
@@ -119,15 +126,17 @@
             },
         ],
         "plots",
     )
     assert isinstance(outs[0], Output)
     assert isinstance(outs[0].fs, LocalFileSystem)
     assert outs[0].use_cache
-    assert outs[0].plot is True and not outs[0].metric
+    assert outs[0].plot is True
+    assert not outs[0].metric
     assert not outs[0].persist
 
     assert isinstance(outs[1], Output)
     assert isinstance(outs[1].fs, LocalFileSystem)
     assert not outs[1].use_cache
-    assert outs[1].plot == {"x": 3} and not outs[1].metric
+    assert outs[1].plot == {"x": 3}
+    assert not outs[1].metric
     assert outs[1].persist
```

### Comparing `dvc-2.9.5/tests/unit/remote/test_oss.py` & `dvc-3.0.0a0/tests/unit/remote/test_oss.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from dvc.fs.oss import OSSFileSystem
+from dvc_oss import OSSFileSystem
 
 bucket_name = "bucket-name"
 endpoint = "endpoint"
 key_id = "Fq2UVErCz4I6tq"
 key_secret = "Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsu"
```

### Comparing `dvc-2.9.5/tests/unit/remote/test_webdav.py` & `dvc-3.0.0a0/tests/unit/remote/test_webdav.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,34 +1,34 @@
-from unittest.mock import patch
-
 import pytest
+from dvc_webdav import WebDAVFileSystem, WebDAVSFileSystem
 
 from dvc.fs import get_cloud_fs
-from dvc.fs.webdav import WebDAVFileSystem, WebDAVSFileSystem
 from tests.utils.asserts import issubset
 
 url_fmt = "{scheme}://{user}@example.com/public.php/webdav"
 url = "webdav://example.com/public.php/webdav"
 user = "username"
 password = "password"
 token = "4MgjsNM5aSJjxIKM"
+custom_auth_header = "Custom-Header"
 
 
 def test_common():
     fs = WebDAVFileSystem(
         url=url,
         cert_path="cert/path",
         key_path="key/path",
         ssl_verify="bundle.pem",
         timeout=10,
         prefix="/public.php/webdav",
         user=None,
         password=None,
         ask_password=False,
         token=None,
+        custom_auth_header=None,
     )
     assert issubset(
         {
             "headers": {},
             "auth": None,
             "base_url": url,
             "cert": ("cert/path", "key/path"),
@@ -62,17 +62,16 @@
     fs = WebDAVFileSystem(**config)
     assert issubset(
         {"headers": {"Authorization": f"Bearer {token}"}, "auth": None},
         fs.fs_args,
     )
 
 
-@patch("dvc.fs.webdav.ask_password")
-def test_ask_password(ask_password_mocked):
-    ask_password_mocked.return_value = "pass"
+def test_ask_password(mocker):
+    ask_password_mocked = mocker.patch("dvc_webdav.ask_password", return_value="pass")
     host = "host"
 
     # it should not ask for password as password is set
     config = {
         "url": url,
         "user": user,
         "password": password,
@@ -84,14 +83,50 @@
 
     config.pop("password")
     fs = WebDAVFileSystem(**config)
     assert issubset({"auth": (user, "pass"), "headers": {}}, fs.fs_args)
     ask_password_mocked.assert_called_once_with(host, user)
 
 
+def test_custom_auth_header():
+    config = {
+        "url": url,
+        "custom_auth_header": custom_auth_header,
+        "password": password,
+    }
+    fs = WebDAVFileSystem(**config)
+    assert issubset(
+        {"headers": {custom_auth_header: password}, "auth": None},
+        fs.fs_args,
+    )
+
+
+def test_ask_password_custom_auth_header(mocker):
+    ask_password_mocked = mocker.patch("dvc_webdav.ask_password", return_value="pass")
+    host = "host"
+
+    # it should not ask for password as password is set
+    config = {
+        "url": url,
+        "custom_auth_header": custom_auth_header,
+        "password": password,
+        "ask_password": True,
+        "host": host,
+    }
+    fs = WebDAVFileSystem(**config)
+    assert issubset(
+        {"headers": {custom_auth_header: password}, "auth": None}, fs.fs_args
+    )
+
+    config.pop("password")
+    fs = WebDAVFileSystem(**config)
+    assert issubset({"headers": {custom_auth_header: "pass"}, "auth": None}, fs.fs_args)
+    ask_password_mocked.assert_called_once_with(host, custom_auth_header)
+
+
 def test_ssl_verify_custom_cert():
     config = {
         "url": url,
         "ssl_verify": "/path/to/custom/cabundle.pem",
     }
 
     fs = WebDAVFileSystem(**config)
```

### Comparing `dvc-2.9.5/tests/unit/remote/test_webhdfs.py` & `dvc-3.0.0a0/tests/unit/remote/test_webhdfs.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,27 +1,24 @@
-from unittest.mock import Mock, create_autospec
-
 import pytest
 import requests
-
-from dvc.fs.webhdfs import WebHDFSFileSystem
+from dvc_webhdfs import WebHDFSFileSystem
 
 host = "host"
 kerberos = False
 kerberos_principal = "principal"
 port = 12345
 proxy_to = "proxy"
 ssl_verify = False
 token = "token"
 use_https = True
 user = "test"
 
 
-@pytest.fixture()
-def webhdfs_config():
+@pytest.fixture(name="webhdfs_config")
+def fixture_webhdfs_config():
     url = f"webhdfs://{user}@{host}:{port}"
     url_config = WebHDFSFileSystem._get_kwargs_from_urls(url)
     return {
         "kerberos": kerberos,
         "kerberos_principal": kerberos_principal,
         "proxy_to": proxy_to,
         "ssl_verify": ssl_verify,
@@ -39,16 +36,16 @@
     assert fs.fs_args["port"] == port
     assert fs.fs_args["kerberos"] == kerberos
     assert fs.fs_args["kerb_kwargs"] == {"principal": kerberos_principal}
     assert fs.fs_args["proxy_to"] == proxy_to
     assert fs.fs_args["use_https"] == use_https
 
 
-def test_verify_ssl(dvc, webhdfs_config, monkeypatch):
-    mock_session = create_autospec(requests.Session)
-    monkeypatch.setattr(requests, "Session", Mock(return_value=mock_session))
+def test_verify_ssl(dvc, webhdfs_config, monkeypatch, mocker):
+    mock_session = mocker.create_autospec(requests.Session)
+    monkeypatch.setattr(requests, "Session", mocker.Mock(return_value=mock_session))
     # can't have token at the same time as user or proxy_to
     del webhdfs_config["token"]
     fs = WebHDFSFileSystem(**webhdfs_config)
     # ssl verify can't be set until after the file system is instantiated
-    fs.fs  # pylint: disable=pointless-statement
+    assert fs.fs
     assert mock_session.verify == ssl_verify
```

### Comparing `dvc-2.9.5/tests/unit/repo/plots/test_diff.py` & `dvc-3.0.0a0/tests/unit/repo/plots/test_diff.py`

 * *Files 8% similar despite different names*

```diff
@@ -26,17 +26,15 @@
     [
         (["v1"], "v0", ["v1", "v0"]),
         (["v1"], None, ["v1", "workspace"]),
         (["v1", "v2"], "v0", ["v1", "v2"]),
         (["v1", "v2"], None, ["v1", "v2"]),
     ],
 )
-def test_revisions_experiment(
-    mocker, arg_revisions, baseline, expected_revisions
-):
+def test_revisions_experiment(mocker, arg_revisions, baseline, expected_revisions):
     mock_scm = mocker.Mock()
     mock_scm.configure_mock(
         **{"is_dirty.return_value": False, "get_ref.return_value": None}
     )
     mock_experiments = mocker.Mock()
     mock_experiments.configure_mock(**{"get_baseline.return_value": baseline})
     mock_repo = mocker.Mock(scm=mock_scm, experiments=mock_experiments)
```

### Comparing `dvc-2.9.5/tests/unit/repo/test_repo.py` & `dvc-3.0.0a0/tests/unit/repo/test_repo.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import os
 import shutil
 
 import pytest
 
 from dvc.exceptions import OutputDuplicationError
-from dvc.hash_info import HashInfo
 from dvc.repo import NotDvcRepoError, Repo, locked
+from dvc_data.hashfile.hash_info import HashInfo
 
 
 def test_is_dvc_internal(dvc):
     assert dvc.is_dvc_internal(os.path.join("path", "to", ".dvc", "file"))
     assert not dvc.is_dvc_internal(os.path.join("path", "to-non-.dvc", "file"))
 
 
@@ -18,17 +18,15 @@
     [
         os.path.join("dir", "subdir", "file"),
         os.path.join("dir", "subdir"),
         "dir",
     ],
 )
 def test_find_outs_by_path(tmp_dir, dvc, path):
-    (stage,) = tmp_dir.dvc_gen(
-        {"dir": {"subdir": {"file": "file"}, "other": "other"}}
-    )
+    (stage,) = tmp_dir.dvc_gen({"dir": {"subdir": {"file": "file"}, "other": "other"}})
 
     outs = dvc.find_outs_by_path(path, strict=False)
     assert len(outs) == 1
     assert outs[0].fs_path == stage.outs[0].fs_path
 
 
 def test_find_outs_by_path_does_graph_checks(tmp_dir, dvc):
@@ -87,23 +85,21 @@
     run_copy("foo", "bar", single_stage=True)
     assert mock_build_graph.called
 
     # check that our hack can be enabled
     mock_build_graph.reset_mock()
     dvc._skip_graph_checks = True
     tmp_dir.gen("baz", "baz text")
-    dvc.add("baz")
     run_copy("baz", "qux", single_stage=True)
     assert not mock_build_graph.called
 
     # check that our hack can be disabled
     mock_build_graph.reset_mock()
     dvc._skip_graph_checks = False
     tmp_dir.gen("quux", "quux text")
-    dvc.add("quux")
     run_copy("quux", "quuz", single_stage=True)
     assert mock_build_graph.called
 
 
 def test_branch_config(tmp_dir, scm):
     tmp_dir.scm_gen("foo", "foo", commit="init")
```

### Comparing `dvc-2.9.5/tests/unit/repo/test_scm_context.py` & `dvc-3.0.0a0/tests/unit/repo/test_scm_context.py`

 * *Files 5% similar despite different names*

```diff
@@ -17,15 +17,15 @@
     # we'll test `ignore` and `ignore_remove` in a functional test.
     return SCMContext(
         scm=mocker.MagicMock(
             spec=spec,
             **{
                 "ignore_remove.return_value": ".gitignore",
                 "ignore.return_value": ".gitignore",
-            }
+            },
         )
     )
 
 
 def test_scm_track_file(scm_context):
     scm_context.track_file("foo")
     assert scm_context.files_to_track == {"foo"}
@@ -76,29 +76,27 @@
     scm_context.scm.add.assert_called_once_with({"foo"})
 
 
 def test_scm_context_clears_ignores_on_error(scm_context):
     class CustomException(Exception):
         pass
 
-    with pytest.raises(CustomException), scm_context():
+    with pytest.raises(CustomException), scm_context():  # noqa: PT012
         scm_context.ignore("foo")
         assert scm_context.ignored_paths == ["foo"]
         raise CustomException
 
     scm_context.scm.ignore_remove.assert_called_once_with("foo")
     assert scm_context.files_to_track == {".gitignore"}
     assert not scm_context.ignored_paths
 
 
 @pytest.mark.parametrize("autostage", [True, False])
 @pytest.mark.parametrize("quiet", [True, False])
-def test_scm_context_on_no_files_to_track(
-    caplog, scm_context, autostage, quiet
-):
+def test_scm_context_on_no_files_to_track(caplog, scm_context, autostage, quiet):
     with scm_context(autostage=autostage, quiet=quiet):
         pass
 
     scm_context.scm.assert_not_called()
     assert not caplog.text
 
 
@@ -109,15 +107,16 @@
         assert context.files_to_track == {"foo", "lorem ipsum"}
 
     if isinstance(scm_context.scm, NoSCM):
         assert not caplog.text
     else:
         assert "To track the changes with git, run:" in caplog.text
         match = re.search(r"git add(?: (('.*?')|(\S+)))*", caplog.text)
-        assert match and set(match.groups()) == {"'lorem ipsum'", "foo"}
+        assert match
+        assert set(match.groups()) == {"'lorem ipsum'", "foo"}
 
 
 def test_scm_context_remind_disable(caplog, scm_context):
     with scm_context(quiet=True) as context:
         context.track_file("foo")
         assert context.files_to_track == {"foo"}
     assert not caplog.text
@@ -131,14 +130,14 @@
 
 
 def test_scm_context_decorator(scm_context, mocker):
     from dvc.repo.scm_context import scm_context as decorator
 
     repo = mocker.MagicMock(scm_context=scm_context)
 
-    def test_method(repo, *args, **kwargs):  # pylint: disable=unused-argument
+    def test_method(repo, *args, **kwargs):
         scm_context.track_file("foo")
 
     method = mocker.MagicMock(wraps=test_method)
     decorator(method, autostage=True)(repo, "arg", kw=1)
     method.assert_called_once_with(repo, "arg", kw=1)
     scm_context.scm.add.assert_called_once_with({"foo"})
```

### Comparing `dvc-2.9.5/tests/unit/stage/test_loader_pipeline_file.py` & `dvc-3.0.0a0/tests/unit/stage/test_loader_pipeline_file.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 import os
 from copy import deepcopy
 from itertools import chain
 
 import pytest
 
-from dvc.dvcfile import PIPELINE_FILE, Dvcfile
-from dvc.hash_info import HashInfo
+from dvc.dvcfile import PROJECT_FILE, load_file
 from dvc.stage import PipelineStage, create_stage
 from dvc.stage.loader import StageLoader
 from dvc.stage.serialize import split_params_deps
+from dvc_data.hashfile.hash_info import HashInfo
 
 
 @pytest.fixture
 def stage_data():
     return {"cmd": "command", "deps": ["foo"], "outs": ["bar"]}
 
 
@@ -22,29 +22,27 @@
         "cmd": "command",
         "deps": [{"path": "foo", "md5": "foo_checksum"}],
         "outs": [{"path": "bar", "md5": "bar_checksum"}],
     }
 
 
 def test_fill_from_lock_deps_outs(dvc, lock_data):
-    stage = create_stage(
-        PipelineStage, dvc, PIPELINE_FILE, deps=["foo"], outs=["bar"]
-    )
+    stage = create_stage(PipelineStage, dvc, PROJECT_FILE, deps=["foo"], outs=["bar"])
 
     for item in chain(stage.deps, stage.outs):
         assert not item.hash_info
 
     StageLoader.fill_from_lock(stage, lock_data)
 
     assert stage.deps[0].hash_info == HashInfo("md5", "foo_checksum")
     assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")
 
 
 def test_fill_from_lock_outs_isexec(dvc):
-    stage = create_stage(PipelineStage, dvc, PIPELINE_FILE, outs=["foo"])
+    stage = create_stage(PipelineStage, dvc, PROJECT_FILE, outs=["foo"])
 
     assert not stage.outs[0].meta.isexec
 
     StageLoader.fill_from_lock(
         stage,
         {
             "cmd": "command",
@@ -57,15 +55,15 @@
     assert stage.outs[0].meta.isexec
 
 
 def test_fill_from_lock_params(dvc, lock_data):
     stage = create_stage(
         PipelineStage,
         dvc,
-        PIPELINE_FILE,
+        PROJECT_FILE,
         deps=["foo"],
         outs=["bar"],
         params=[
             "lorem",
             "lorem.ipsum",
             {"myparams.yaml": ["ipsum", "foobar"]},
         ],
@@ -84,174 +82,174 @@
     assert set(params_deps[0].params) == {"lorem", "lorem.ipsum"}
     assert set(params_deps[1].params) == {"ipsum", "foobar"}
     assert not params_deps[0].hash_info
     assert not params_deps[1].hash_info
 
     StageLoader.fill_from_lock(stage, lock_data)
     assert params_deps[0].hash_info.value == lock_data["params"]["params.yaml"]
-    assert (
-        params_deps[1].hash_info.value == lock_data["params"]["myparams.yaml"]
-    )
+    assert params_deps[1].hash_info.value == lock_data["params"]["myparams.yaml"]
 
 
 def test_fill_from_lock_missing_params_section(dvc, lock_data):
     stage = create_stage(
         PipelineStage,
         dvc,
-        PIPELINE_FILE,
+        PROJECT_FILE,
         deps=["foo"],
         outs=["bar"],
         params=["lorem", "lorem.ipsum", {"myparams.yaml": ["ipsum"]}],
     )
     params_deps = split_params_deps(stage)[0]
     StageLoader.fill_from_lock(stage, lock_data)
-    assert not params_deps[0].hash_info and not params_deps[1].hash_info
+    assert not params_deps[0].hash_info
+    assert not params_deps[1].hash_info
 
 
 def test_fill_from_lock_missing_checksums(dvc, lock_data):
     stage = create_stage(
         PipelineStage,
         dvc,
-        PIPELINE_FILE,
+        PROJECT_FILE,
         deps=["foo", "foo1"],
         outs=["bar", "bar1"],
     )
 
     StageLoader.fill_from_lock(stage, lock_data)
 
     assert stage.deps[0].hash_info == HashInfo("md5", "foo_checksum")
     assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")
-    assert not stage.deps[1].hash_info and not stage.outs[1].hash_info
+    assert not stage.deps[1].hash_info
+    assert not stage.outs[1].hash_info
 
 
 def test_fill_from_lock_use_appropriate_checksum(dvc, lock_data):
     stage = create_stage(
         PipelineStage,
         dvc,
-        PIPELINE_FILE,
+        PROJECT_FILE,
         deps=["s3://dvc-temp/foo"],
         outs=["bar"],
     )
     lock_data["deps"] = [{"path": "s3://dvc-temp/foo", "etag": "e-tag"}]
     StageLoader.fill_from_lock(stage, lock_data)
     assert stage.deps[0].hash_info == HashInfo("etag", "e-tag")
     assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")
 
 
 def test_fill_from_lock_with_missing_sections(dvc, lock_data):
-    stage = create_stage(
-        PipelineStage, dvc, PIPELINE_FILE, deps=["foo"], outs=["bar"]
-    )
+    stage = create_stage(PipelineStage, dvc, PROJECT_FILE, deps=["foo"], outs=["bar"])
     lock = deepcopy(lock_data)
     del lock["deps"]
     StageLoader.fill_from_lock(stage, lock)
     assert not stage.deps[0].hash_info
     assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")
 
     lock = deepcopy(lock_data)
     del lock["outs"]
     StageLoader.fill_from_lock(stage, lock)
     assert stage.deps[0].hash_info == HashInfo("md5", "foo_checksum")
     assert not stage.outs[0].hash_info
 
 
 def test_fill_from_lock_empty_data(dvc):
-    stage = create_stage(
-        PipelineStage, dvc, PIPELINE_FILE, deps=["foo"], outs=["bar"]
-    )
+    stage = create_stage(PipelineStage, dvc, PROJECT_FILE, deps=["foo"], outs=["bar"])
     StageLoader.fill_from_lock(stage, None)
-    assert not stage.deps[0].hash_info and not stage.outs[0].hash_info
+    assert not stage.deps[0].hash_info
+    assert not stage.outs[0].hash_info
     StageLoader.fill_from_lock(stage, {})
-    assert not stage.deps[0].hash_info and not stage.outs[0].hash_info
+    assert not stage.deps[0].hash_info
+    assert not stage.outs[0].hash_info
 
 
 def test_load_stage(dvc, stage_data, lock_data):
-    dvcfile = Dvcfile(dvc, PIPELINE_FILE)
+    dvcfile = load_file(dvc, PROJECT_FILE)
     stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)
 
     assert stage.wdir == os.path.abspath(os.curdir)
     assert stage.name == "stage-1"
     assert stage.cmd == "command"
-    assert stage.path == os.path.abspath(PIPELINE_FILE)
+    assert stage.path == os.path.abspath(PROJECT_FILE)
     assert stage.deps[0].def_path == "foo"
     assert stage.deps[0].hash_info == HashInfo("md5", "foo_checksum")
     assert stage.outs[0].def_path == "bar"
     assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")
 
 
 def test_load_stage_cmd_with_list(dvc, stage_data, lock_data):
     stage_data["cmd"] = ["cmd-0", "cmd-1"]
-    dvcfile = Dvcfile(dvc, PIPELINE_FILE)
+    dvcfile = load_file(dvc, PROJECT_FILE)
     stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)
     assert stage.cmd == ["cmd-0", "cmd-1"]
 
 
 def test_load_stage_outs_with_flags(dvc, stage_data, lock_data):
     stage_data["outs"] = [{"foo": {"cache": False}}]
-    dvcfile = Dvcfile(dvc, PIPELINE_FILE)
+    dvcfile = load_file(dvc, PROJECT_FILE)
     stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)
     assert stage.outs[0].use_cache is False
 
 
 def test_load_stage_no_lock(dvc, stage_data):
-    dvcfile = Dvcfile(dvc, PIPELINE_FILE)
+    dvcfile = load_file(dvc, PROJECT_FILE)
     stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data)
-    assert stage.deps[0].def_path == "foo" and stage.outs[0].def_path == "bar"
+    assert stage.deps[0].def_path == "foo"
+    assert stage.outs[0].def_path == "bar"
     assert not stage.deps[0].hash_info
     assert not stage.outs[0].hash_info
 
 
 def test_load_stage_with_params(dvc, stage_data, lock_data):
     lock_data["params"] = {"params.yaml": {"lorem": "ipsum"}}
     stage_data["params"] = ["lorem"]
-    dvcfile = Dvcfile(dvc, PIPELINE_FILE)
+    dvcfile = load_file(dvc, PROJECT_FILE)
     stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)
 
     params, deps = split_params_deps(stage)
-    assert deps[0].def_path == "foo" and stage.outs[0].def_path == "bar"
+    assert deps[0].def_path == "foo"
+    assert stage.outs[0].def_path == "bar"
     assert params[0].def_path == "params.yaml"
     assert params[0].hash_info == HashInfo("params", {"lorem": "ipsum"})
     assert deps[0].hash_info == HashInfo("md5", "foo_checksum")
     assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")
 
 
 @pytest.mark.parametrize("typ", ["metrics", "plots"])
 def test_load_stage_with_metrics_and_plots(dvc, stage_data, lock_data, typ):
     stage_data[typ] = stage_data.pop("outs")
-    dvcfile = Dvcfile(dvc, PIPELINE_FILE)
+    dvcfile = load_file(dvc, PROJECT_FILE)
     stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)
 
     assert stage.outs[0].def_path == "bar"
     assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")
 
 
 def test_load_changed_command(dvc, stage_data, lock_data):
-    dvcfile = Dvcfile(dvc, PIPELINE_FILE)
+    dvcfile = load_file(dvc, PROJECT_FILE)
     stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data)
     assert not stage.cmd_changed
     assert stage.cmd == "command"
 
     lock_data["cmd"] = "different-command"
     stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)
     assert stage.cmd_changed
     assert stage.cmd == "command"
 
 
 def test_load_stage_wdir_and_path_correctly(dvc, stage_data, lock_data):
     stage_data["wdir"] = "dir"
-    dvcfile = Dvcfile(dvc, PIPELINE_FILE)
+    dvcfile = load_file(dvc, PROJECT_FILE)
     stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)
 
     assert stage.wdir == os.path.abspath("dir")
-    assert stage.path == os.path.abspath(PIPELINE_FILE)
+    assert stage.path == os.path.abspath(PROJECT_FILE)
 
 
 def test_load_stage_mapping(dvc, stage_data, lock_data):
-    dvcfile = Dvcfile(dvc, PIPELINE_FILE)
-    loader = StageLoader(
-        dvcfile, {"stages": {"stage": stage_data}}, {"stage": lock_data}
-    )
-    assert len(loader) == 1
-    assert "stage" in loader
-    assert "stage1" not in loader
-    assert loader.keys() == {"stage"}
-    assert isinstance(loader["stage"], PipelineStage)
+    dvcfile = load_file(dvc, PROJECT_FILE)
+    dvcfile.contents = {"stages": {"stage": stage_data}}
+    dvcfile.lockfile_contents = {"stage": lock_data}
+
+    assert len(dvcfile.stages) == 1
+    assert "stage" in dvcfile.stages
+    assert "stage1" not in dvcfile.stages
+    assert dvcfile.stages.keys() == {"stage"}
+    assert isinstance(dvcfile.stages["stage"], PipelineStage)
```

### Comparing `dvc-2.9.5/tests/unit/stage/test_serialize_pipeline_file.py` & `dvc-3.0.0a0/tests/unit/stage/test_serialize_pipeline_file.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 import os
 
 import pytest
 from voluptuous import Schema as _Schema
 
 from dvc import output
-from dvc.dvcfile import PIPELINE_FILE
+from dvc.dvcfile import PROJECT_FILE
 from dvc.schema import SINGLE_PIPELINE_STAGE_SCHEMA
 from dvc.stage import PipelineStage, create_stage
 from dvc.stage.serialize import to_pipeline_file as _to_pipeline_file
 
-kwargs = {"name": "something", "cmd": "command", "path": PIPELINE_FILE}
+kwargs = {"name": "something", "cmd": "command", "path": PROJECT_FILE}
 Schema = _Schema(SINGLE_PIPELINE_STAGE_SCHEMA)
 
 
 def to_pipeline_file(stage):
     """Validate schema on each serialization."""
     e = _to_pipeline_file(stage)
     assert len(Schema(e)) == 1
@@ -96,14 +96,33 @@
         "ipsum",
         "lorem",
         {"a-file-of-params.yaml": ["barr"]},
         {"custom.yaml": ["baz", "pqrs", "wxyz"]},
     ]
 
 
+def test_params_file_without_targets(dvc):
+    params = [
+        "foo",
+        "bar",
+        {"params.yaml": None},
+        {"custom.yaml": ["wxyz", "pqrs", "baz"]},
+        {"a-file-of-params.yaml": None},
+        {"a-file-of-params.yaml": ["barr"]},
+    ]
+    stage = create_stage(
+        PipelineStage, dvc, outs=["bar"], deps=["foo"], params=params, **kwargs
+    )
+    assert to_pipeline_file(stage)["something"][stage.PARAM_PARAMS] == [
+        {"a-file-of-params.yaml": None},
+        {"custom.yaml": ["baz", "pqrs", "wxyz"]},
+        {"params.yaml": None},
+    ]
+
+
 @pytest.mark.parametrize(
     "typ, extra",
     [("plots", {"plot": True}), ("metrics", {"metric": True}), ("outs", {})],
 )
 def test_outs_and_outs_flags_are_sorted(dvc, typ, extra):
     stage = create_stage(PipelineStage, dvc, deps=["input"], **kwargs)
     stage.outs += output.loads_from(stage, ["barr"], use_cache=False, **extra)
@@ -130,36 +149,27 @@
 
     assert to_pipeline_file(stage)["something"][stage.PARAM_PLOTS] == [
         {"plot_file": props}
     ]
 
 
 def test_frozen(dvc):
-    stage = create_stage(
-        PipelineStage, dvc, outs=["output"], deps=["input"], **kwargs
-    )
+    stage = create_stage(PipelineStage, dvc, outs=["output"], deps=["input"], **kwargs)
     assert stage.PARAM_FROZEN not in to_pipeline_file(stage)["something"]
 
     stage = create_stage(PipelineStage, dvc, **kwargs, frozen=True)
     assert to_pipeline_file(stage)["something"][stage.PARAM_FROZEN] is True
 
 
 def test_always_changed(dvc):
-    stage = create_stage(
-        PipelineStage, dvc, outs=["output"], deps=["input"], **kwargs
-    )
-    assert (
-        stage.PARAM_ALWAYS_CHANGED not in to_pipeline_file(stage)["something"]
-    )
+    stage = create_stage(PipelineStage, dvc, outs=["output"], deps=["input"], **kwargs)
+    assert stage.PARAM_ALWAYS_CHANGED not in to_pipeline_file(stage)["something"]
 
     stage = create_stage(PipelineStage, dvc, **kwargs, always_changed=True)
-    assert (
-        to_pipeline_file(stage)["something"][stage.PARAM_ALWAYS_CHANGED]
-        is True
-    )
+    assert to_pipeline_file(stage)["something"][stage.PARAM_ALWAYS_CHANGED] is True
 
 
 def test_order(dvc):
     stage = create_stage(
         PipelineStage,
         dvc,
         outs=["output"],
@@ -176,21 +186,19 @@
         "deps",
         "outs",
         "frozen",
         "always_changed",
     ]
 
 
-@pytest.mark.parametrize(
-    "typ", ["outs", "metrics", "plots", "params", "deps", None]
-)
+@pytest.mark.parametrize("typ", ["outs", "metrics", "plots", "params", "deps", None])
 def test_order_deps_outs(dvc, typ):
     all_types = ["deps", "params", "outs", "metrics", "plots"]
     all_types = [item for item in all_types if item != typ]
     extra = {key: [f"foo-{i}"] for i, key in enumerate(all_types)}
 
     stage = create_stage(PipelineStage, dvc, **kwargs, **extra)
     assert typ not in to_pipeline_file(stage)["something"]
-    assert (
-        list(to_pipeline_file(stage)["something"].keys())
-        == ["cmd"] + all_types
-    )
+    assert list(to_pipeline_file(stage)["something"].keys()) == [
+        "cmd",
+        *all_types,
+    ]
```

### Comparing `dvc-2.9.5/tests/unit/stage/test_serialize_pipeline_lock.py` & `dvc-3.0.0a0/tests/unit/stage/test_serialize_pipeline_lock.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,23 +1,21 @@
 from collections import OrderedDict
 
 import pytest
 from voluptuous import Schema as _Schema
 
-from dvc.dvcfile import PIPELINE_FILE
-from dvc.hash_info import HashInfo
+from dvc.dvcfile import PROJECT_FILE
 from dvc.schema import LOCK_FILE_STAGE_SCHEMA, LOCKFILE_STAGES_SCHEMA
 from dvc.stage import PipelineStage, create_stage
 from dvc.stage.serialize import DEFAULT_PARAMS_FILE, to_lockfile
-from dvc.stage.serialize import (
-    to_single_stage_lockfile as _to_single_stage_lockfile,
-)
+from dvc.stage.serialize import to_single_stage_lockfile as _to_single_stage_lockfile
 from dvc.stage.utils import split_params_deps
+from dvc_data.hashfile.hash_info import HashInfo
 
-kwargs = {"name": "something", "cmd": "command", "path": PIPELINE_FILE}
+kwargs = {"name": "something", "cmd": "command", "path": PROJECT_FILE}
 Schema = _Schema(LOCK_FILE_STAGE_SCHEMA)
 
 
 def to_single_stage_lockfile(stage):
     """Validate schema on each serialization."""
     e = _to_single_stage_lockfile(stage)
     assert Schema(e)
@@ -37,17 +35,15 @@
             ("cmd", "command"),
             ("deps", [OrderedDict([("path", "input"), ("md5", "md-five")])]),
         ]
     )
 
 
 def test_lock_deps_order(dvc):
-    stage = create_stage(
-        PipelineStage, dvc, deps=["input1", "input0"], **kwargs
-    )
+    stage = create_stage(PipelineStage, dvc, deps=["input1", "input0"], **kwargs)
     stage.deps[0].hash_info = HashInfo("md5", "md-one1")
     stage.deps[1].hash_info = HashInfo("md5", "md-zer0")
     assert to_single_stage_lockfile(stage) == OrderedDict(
         [
             ("cmd", "command"),
             (
                 "deps",
@@ -57,17 +53,15 @@
                 ],
             ),
         ]
     )
 
 
 def test_lock_params(dvc):
-    stage = create_stage(
-        PipelineStage, dvc, params=["lorem.ipsum", "abc"], **kwargs
-    )
+    stage = create_stage(PipelineStage, dvc, params=["lorem.ipsum", "abc"], **kwargs)
     stage.deps[0].hash_info = HashInfo(
         "params", {"lorem.ipsum": {"lorem1": 1, "lorem2": 2}, "abc": 3}
     )
     assert to_single_stage_lockfile(stage)["params"][
         DEFAULT_PARAMS_FILE
     ] == OrderedDict([("abc", 3), ("lorem.ipsum", {"lorem1": 1, "lorem2": 2})])
 
@@ -78,52 +72,65 @@
         dvc,
         params=[
             "lorem.ipsum",
             "abc",
             {"myparams.yaml": ["foo", "foobar"]},
             {"a-params-file.yaml": ["bar", "barr"]},
         ],
-        **kwargs
+        **kwargs,
     )
     stage.deps[0].hash_info = HashInfo(
         "params", {"lorem.ipsum": {"lorem1": 1, "lorem2": 2}, "abc": 3}
     )
     stage.deps[1].hash_info = HashInfo(
         "params", {"foo": ["f", "o", "o"], "foobar": "foobar"}
     )
     stage.deps[2].hash_info = HashInfo(
         "params", {"bar": ["b", "a", "r"], "barr": "barr"}
     )
     assert to_single_stage_lockfile(stage)["params"] == OrderedDict(
         [
             (
                 DEFAULT_PARAMS_FILE,
-                OrderedDict(
-                    [("abc", 3), ("lorem.ipsum", {"lorem1": 1, "lorem2": 2})]
-                ),
+                OrderedDict([("abc", 3), ("lorem.ipsum", {"lorem1": 1, "lorem2": 2})]),
             ),
             (
                 "a-params-file.yaml",
                 OrderedDict([("bar", ["b", "a", "r"]), ("barr", "barr")]),
             ),
             (
                 "myparams.yaml",
                 OrderedDict([("foo", ["f", "o", "o"]), ("foobar", "foobar")]),
             ),
         ]
     )
 
 
 def test_lock_params_no_values_filled(dvc):
-    stage = create_stage(
-        PipelineStage, dvc, params=["lorem.ipsum", "abc"], **kwargs
-    )
+    stage = create_stage(PipelineStage, dvc, params=["lorem.ipsum", "abc"], **kwargs)
     assert to_single_stage_lockfile(stage) == {"cmd": "command"}
 
 
+@pytest.mark.parametrize(
+    "info, expected",
+    [
+        (None, {}),
+        ({}, {}),
+        ({"foo": "foo", "bar": "bar"}, {"bar": "bar", "foo": "foo"}),
+    ],
+)
+def test_lock_params_without_targets(dvc, info, expected):
+    stage = create_stage(PipelineStage, dvc, params=[{"params.yaml": None}], **kwargs)
+    stage.deps[0].fill_values(info)
+    assert to_single_stage_lockfile(stage) == {
+        "cmd": "command",
+        "params": {"params.yaml": OrderedDict(expected)},
+    }
+
+
 @pytest.mark.parametrize("typ", ["plots", "metrics", "outs"])
 def test_lock_outs(dvc, typ):
     stage = create_stage(PipelineStage, dvc, **{typ: ["input"]}, **kwargs)
     stage.outs[0].hash_info = HashInfo("md5", "md-five")
     assert to_single_stage_lockfile(stage) == OrderedDict(
         [
             ("cmd", "command"),
@@ -154,17 +161,15 @@
             ),
         ]
     )
 
 
 @pytest.mark.parametrize("typ", ["plots", "metrics", "outs"])
 def test_lock_outs_order(dvc, typ):
-    stage = create_stage(
-        PipelineStage, dvc, **{typ: ["input1", "input0"]}, **kwargs
-    )
+    stage = create_stage(PipelineStage, dvc, **{typ: ["input1", "input0"]}, **kwargs)
     stage.outs[0].hash_info = HashInfo("md5", "md-one1")
     stage.outs[1].hash_info = HashInfo("md5", "md-zer0")
     assert to_single_stage_lockfile(stage) == OrderedDict(
         [
             ("cmd", "command"),
             (
                 "outs",
@@ -174,41 +179,35 @@
                 ],
             ),
         ]
     )
 
 
 def test_dump_nondefault_hash(dvc):
-    stage = create_stage(
-        PipelineStage, dvc, deps=["s3://dvc-temp/file"], **kwargs
-    )
+    stage = create_stage(PipelineStage, dvc, deps=["s3://dvc-temp/file"], **kwargs)
     stage.deps[0].hash_info = HashInfo("md5", "value")
     assert to_single_stage_lockfile(stage) == OrderedDict(
         [
             ("cmd", "command"),
             (
                 "deps",
-                [
-                    OrderedDict(
-                        [("path", "s3://dvc-temp/file"), ("md5", "value")]
-                    )
-                ],
+                [OrderedDict([("path", "s3://dvc-temp/file"), ("md5", "value")])],
             ),
         ]
     )
 
 
 def test_order(dvc):
     stage = create_stage(
         PipelineStage,
         dvc,
         deps=["input"],
         outs=["output"],
         params=["foo-param"],
-        **kwargs
+        **kwargs,
     )
     params, deps = split_params_deps(stage)
 
     deps[0].hash_info = HashInfo("md5", "md-five")
     params[0].hash_info = HashInfo("params", {"foo-param": "value"})
     stage.outs[0].hash_info = HashInfo("md5", "md5-output")
 
@@ -232,7 +231,32 @@
         "something": OrderedDict(
             [
                 ("cmd", "command"),
                 ("deps", [{"path": "input", "md5": "md-five"}]),
             ]
         )
     }
+
+
+def test_to_single_stage_lockfile_cloud_versioning_dir(dvc):
+    stage = create_stage(PipelineStage, dvc, outs=["dir"], **kwargs)
+    stage.outs[0].hash_info = HashInfo("md5", "md-five.dir")
+    files = [
+        {
+            "size": 3,
+            "version_id": "WYRG4BglP7pD.gEoJP6a4AqOhl.FRA.h",
+            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
+            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
+            "relpath": "bar",
+        },
+        {
+            "size": 3,
+            "version_id": "0vL53tFVY5vVAoJ4HG2jCS1mEcohDPE0",
+            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
+            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
+            "relpath": "foo",
+        },
+    ]
+    stage.outs[0].files = files
+    e = _to_single_stage_lockfile(stage, with_files=True)
+    assert Schema(e)
+    assert e["outs"][0] == {"path": "dir", "files": files}
```

### Comparing `dvc-2.9.5/tests/unit/stage/test_stage.py` & `dvc-3.0.0a0/tests/unit/stage/test_stage.py`

 * *Files 6% similar despite different names*

```diff
@@ -51,37 +51,39 @@
 def test_path_conversion(dvc):
     stage = Stage(dvc, "path")
 
     stage.wdir = os.path.join("..", "..")
     assert stage.dumpd()["wdir"] == "../.."
 
 
-def test_stage_update(mocker):
-    dep = RepoDependency({"url": "example.com"}, None, "dep_path")
+def test_stage_update(dvc, mocker):
+    stage = Stage(dvc, "path", "cmd")
+    dep = RepoDependency({"url": "example.com"}, stage, "dep_path")
     mocker.patch.object(dep, "update", return_value=None)
 
-    stage = Stage(None, "path", deps=[dep])
+    stage = Stage(dvc, "path", deps=[dep])
     reproduce = mocker.patch.object(stage, "reproduce")
     is_repo_import = mocker.patch(
         __name__ + ".Stage.is_repo_import", new_callable=mocker.PropertyMock
     )
 
     is_repo_import.return_value = True
-    stage.update()
+    with dvc.lock:
+        stage.update()
     assert reproduce.called_once_with()
 
     is_repo_import.return_value = False
     with pytest.raises(StageUpdateError):
         stage.update()
 
 
 @pytest.mark.skipif(
     not isinstance(
         threading.current_thread(),
-        threading._MainThread,  # noqa, pylint: disable=protected-access
+        threading._MainThread,
     ),
     reason="Not running in the main thread.",
 )
 def test_stage_run_ignore_sigint(dvc, mocker):
     proc = mocker.Mock()
     communicate = mocker.Mock()
     proc.configure_mock(returncode=0, communicate=communicate)
@@ -89,15 +91,14 @@
     signal_mock = mocker.patch("signal.signal")
 
     dvc.run(cmd="path", single_stage=True)
 
     assert popen.called_once()
     assert communicate.called_once_with()
     signal_mock.assert_any_call(signal.SIGINT, signal.SIG_IGN)
-    # pylint: disable=comparison-with-callable
     assert signal.getsignal(signal.SIGINT) == signal.default_int_handler
 
 
 def test_always_changed(dvc):
     stage = Stage(dvc, "path", always_changed=True)
     stage.save()
     with dvc.lock:
@@ -133,11 +134,10 @@
     mocker.patch.object(stage, "_read_env", return_value={"foo": "foo"})
     env = stage.env()
     assert env == {DVC_ROOT: dvc.root_dir, "foo": "foo"}
 
     def mock_read_env(out, **kwargs):
         return {"foo": str(out)}
 
-    with pytest.raises(DvcException) as exc:
-        mocker.patch.object(stage, "_read_env", mock_read_env)
+    mocker.patch.object(stage, "_read_env", mock_read_env)
+    with pytest.raises(DvcException, match="Conflicting values for env variable"):
         _ = stage.env()
-        assert exc.value == "Conflicting values for env variable"
```

### Comparing `dvc-2.9.5/tests/unit/stage/test_utils.py` & `dvc-3.0.0a0/tests/unit/stage/test_utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 import os
 
+from dvc.fs import localfs
 from dvc.stage.utils import resolve_paths
 
 
 def test_resolve_paths():
     p = os.path.join("dir", "subdir")
     file_path = os.path.join(p, "dvc.yaml")
 
-    path, wdir = resolve_paths(path=file_path, wdir="dir")
+    path, wdir = resolve_paths(fs=localfs, path=file_path, wdir="dir")
     assert path == os.path.abspath(file_path)
     assert wdir == os.path.abspath(os.path.join(p, "dir"))
 
-    path, wdir = resolve_paths(path=file_path)
+    path, wdir = resolve_paths(fs=localfs, path=file_path)
     assert path == os.path.abspath(file_path)
     assert wdir == os.path.abspath(p)
 
-    path, wdir = resolve_paths(path=file_path, wdir="../../some-dir")
+    path, wdir = resolve_paths(fs=localfs, path=file_path, wdir="../../some-dir")
     assert path == os.path.abspath(file_path)
     assert wdir == os.path.abspath("some-dir")
```

### Comparing `dvc-2.9.5/tests/unit/test_analytics.py` & `dvc-3.0.0a0/tests/unit/test_analytics.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,24 +1,27 @@
 import json
 import platform
 
-import mock
 import pytest
 from voluptuous import Any, Schema
 
 from dvc import analytics
 from dvc.cli import parse_args
 
 
 @pytest.fixture
 def tmp_global_dir(mocker, tmp_path):
     """
     Fixture to prevent modifying the actual global config
     """
-    mocker.patch("dvc.config.Config.get_dir", return_value=str(tmp_path))
+
+    def _user_config_dir(appname, *_args, **_kwargs):
+        return str(tmp_path / appname)
+
+    mocker.patch("iterative_telemetry.user_config_dir", _user_config_dir)
 
 
 def test_collect_and_send_report(mocker, tmp_global_dir):
     mock_json = mocker.patch("json.dump")
     mock_daemon = mocker.patch("dvc.daemon._spawn")
     analytics.collect_and_send_report()
     report = mock_json.call_args[0][0]
@@ -42,23 +45,25 @@
     schema = Schema(
         {
             "dvc_version": str,
             "is_binary": bool,
             "scm_class": Any("Git", None),
             "user_id": str,
             "system_info": dict,
+            "group_id": Any(str, None),
         },
         required=True,
     )
 
     assert schema(analytics._runtime_info())
 
 
-@mock.patch("requests.post")
-def test_send(mock_post, tmp_path):
+def test_send(mocker, tmp_path):
+    mock_post = mocker.patch("requests.post")
+
     import requests
 
     url = "https://analytics.dvc.org"
     report = {"name": "dummy report"}
     report_file = tmp_path / "report"
 
     report_file.write_text(json.dumps(report))
@@ -101,17 +106,15 @@
         ("false", "true", False),
         ("false", "false", False),
         ("true", None, True),
         ("true", "true", False),
         ("true", "false", False),  # we checking if env is set
     ],
 )
-def test_is_enabled_env_neg(
-    dvc, config, env, result, monkeypatch, tmp_global_dir
-):
+def test_is_enabled_env_neg(dvc, config, env, result, monkeypatch, tmp_global_dir):
     # reset DVC_TEST env var, which affects `is_enabled()`
     monkeypatch.delenv("DVC_TEST")
     monkeypatch.delenv("DVC_NO_ANALYTICS", raising=False)
 
     with dvc.config.edit() as conf:
         conf["core"] = {}
 
@@ -151,14 +154,7 @@
                 "linux_distro": str,
                 "linux_distro_like": str,
                 "linux_distro_version": str,
             }
         )
 
     assert schema(analytics._system_info())
-
-
-def test_find_or_create_user_id(tmp_global_dir):
-    created = analytics._find_or_create_user_id()
-    found = analytics._find_or_create_user_id()
-
-    assert created == found
```

### Comparing `dvc-2.9.5/tests/unit/test_compare.py` & `dvc-3.0.0a0/tests/unit/test_compare.py`

 * *Files 5% similar despite different names*

```diff
@@ -63,37 +63,29 @@
             "HEAD": "1",
             "workspace": "2",
         }
     ]
 
 
 def test_diff_table_precision():
-    diff = {
-        "metrics.json": {
-            "a.b.c": {"old": 1.1234, "new": 2.2345, "diff": 3.3456}
-        }
-    }
+    diff = {"metrics.json": {"a.b.c": {"old": 1.1234, "new": 2.2345, "diff": 3.3456}}}
     td = diff_table(diff, title="Metric", precision=3)
     assert td.as_dict() == [
         {
             "Path": "metrics.json",
             "Metric": "a.b.c",
             "HEAD": "1.12",
             "workspace": "2.23",
             "Change": "3.35",
         }
     ]
 
 
 def test_diff_table_rounding():
-    diff = {
-        "metrics.json": {
-            "a.b.c": {"old": 1.1234, "new": 2.2345, "diff": 3.3456}
-        }
-    }
+    diff = {"metrics.json": {"a.b.c": {"old": 1.1234, "new": 2.2345, "diff": 3.3456}}}
     td = diff_table(diff, title="Metric", precision=3, round_digits=True)
     assert td.as_dict() == [
         {
             "Path": "metrics.json",
             "Metric": "a.b.c",
             "HEAD": "1.123",
             "workspace": "2.235",
@@ -105,15 +97,15 @@
 @pytest.mark.parametrize(
     "extra, expected", [({"on_empty_diff": "no diff"}, "no diff"), ({}, "-")]
 )
 def test_diff_unsupported_diff_message(extra, expected):
     td = diff_table(
         {"metrics.json": {"": {"old": "1", "new": "2"}}},
         title="Metric",
-        **extra
+        **extra,
     )
     assert td.as_dict() == [
         {
             "Path": "metrics.json",
             "Metric": "",
             "HEAD": "1",
             "workspace": "2",
@@ -185,17 +177,15 @@
 
 
 @pytest.mark.parametrize(
     "composite, expected",
     [([2, 3], "[2, 3]"), ({"foo": 3, "bar": 3}, "{'foo': 3, 'bar': 3}")],
 )
 def test_diff_list(composite, expected):
-    td = diff_table(
-        {"params.yaml": {"a.b.c": {"old": 1, "new": composite}}}, "Param"
-    )
+    td = diff_table({"params.yaml": {"a.b.c": {"old": 1, "new": composite}}}, "Param")
     assert td.as_dict() == [
         {
             "Path": "params.yaml",
             "Param": "a.b.c",
             "HEAD": "1",
             "workspace": expected,
             "Change": "-",
@@ -275,17 +265,15 @@
 
 
 def test_metrics_show_with_valid_falsey_values():
     td = metrics_table(
         {
             "branch_1": {
                 "data": {
-                    "metrics.json": {
-                        "data": {"a": 0, "b": {"ad": 0.0, "bc": 0.0}}
-                    }
+                    "metrics.json": {"data": {"a": 0, "b": {"ad": 0.0, "bc": 0.0}}}
                 }
             }
         },
         all_branches=True,
     )
     assert td.as_dict() == [
         {
@@ -299,17 +287,15 @@
 
 
 def test_metrics_show_with_no_revision():
     td = metrics_table(
         {
             "branch_1": {
                 "data": {
-                    "metrics.json": {
-                        "data": {"a": 0, "b": {"ad": 0.0, "bc": 0.0}}
-                    }
+                    "metrics.json": {"data": {"a": 0, "b": {"ad": 0.0, "bc": 0.0}}}
                 }
             }
         },
         all_branches=False,
     )
     assert td.as_dict() == [
         {"Path": "metrics.json", "a": "0", "b.ad": "0.0", "b.bc": "0.0"}
@@ -317,31 +303,25 @@
 
 
 def test_metrics_show_with_non_dict_values():
     td = metrics_table(
         {"branch_1": {"data": {"metrics.json": {"data": 1}}}},
         all_branches=True,
     )
-    assert td.as_dict() == [
-        {"Revision": "branch_1", "Path": "metrics.json", "": "1"}
-    ]
+    assert td.as_dict() == [{"Revision": "branch_1", "Path": "metrics.json", "": "1"}]
 
 
 def test_metrics_show_with_multiple_revision():
     td = metrics_table(
         {
             "branch_1": {
-                "data": {
-                    "metrics.json": {"data": {"a": 1, "b": {"ad": 1, "bc": 2}}}
-                }
+                "data": {"metrics.json": {"data": {"a": 1, "b": {"ad": 1, "bc": 2}}}}
             },
             "branch_2": {
-                "data": {
-                    "metrics.json": {"data": {"a": 1, "b": {"ad": 3, "bc": 4}}}
-                }
+                "data": {"metrics.json": {"data": {"a": 1, "b": {"ad": 3, "bc": 4}}}}
             },
         },
         all_branches=True,
     )
     assert td.as_dict() == [
         {
             "Revision": "branch_1",
@@ -361,20 +341,16 @@
 
 
 def test_metrics_show_with_one_revision_multiple_paths():
     td = metrics_table(
         {
             "branch_1": {
                 "data": {
-                    "metrics.json": {
-                        "data": {"a": 1, "b": {"ad": 0.1, "bc": 1.03}}
-                    },
-                    "metrics_1.json": {
-                        "data": {"a": 2.3, "b": {"ad": 6.5, "bc": 7.9}}
-                    },
+                    "metrics.json": {"data": {"a": 1, "b": {"ad": 0.1, "bc": 1.03}}},
+                    "metrics_1.json": {"data": {"a": 2.3, "b": {"ad": 6.5, "bc": 7.9}}},
                 }
             }
         },
         all_branches=True,
     )
     assert td.as_dict() == [
         {
@@ -394,22 +370,18 @@
     ]
 
 
 def test_metrics_show_with_different_metrics_header():
     td = metrics_table(
         {
             "branch_1": {
-                "data": {
-                    "metrics.json": {"data": {"b": {"ad": 1, "bc": 2}, "c": 4}}
-                }
+                "data": {"metrics.json": {"data": {"b": {"ad": 1, "bc": 2}, "c": 4}}}
             },
             "branch_2": {
-                "data": {
-                    "metrics.json": {"data": {"a": 1, "b": {"ad": 3, "bc": 4}}}
-                }
+                "data": {"metrics.json": {"data": {"a": 1, "b": {"ad": 3, "bc": 4}}}}
             },
         },
         all_branches=True,
     )
     assert td.as_dict() == [
         {
             "Revision": "branch_1",
@@ -451,17 +423,15 @@
             "Path": "metrics.json",
             "a": "1.099",
             "b.ad": "1.534",
             "b.bc": "2.988",
         }
     ]
 
-    td = metrics_table(
-        metrics, all_branches=True, precision=4, round_digits=True
-    )
+    td = metrics_table(metrics, all_branches=True, precision=4, round_digits=True)
     assert td.as_dict() == [
         {
             "Revision": "branch_1",
             "Path": "metrics.json",
             "a": "1.0988",
             "b.ad": "1.5343",
             "b.bc": "2.9877",
@@ -498,23 +468,19 @@
     ret.render.assert_called_once_with(markdown=markdown)
 
 
 def test_metrics_show_default(capsys):
     show_metrics(
         metrics={
             "branch_1": {
-                "data": {
-                    "metrics.json": {"data": {"b": {"ad": 1, "bc": 2}, "c": 4}}
-                },
+                "data": {"metrics.json": {"data": {"b": {"ad": 1, "bc": 2}, "c": 4}}},
                 "error": Exception("Failed just a little bit"),
             },
             "branch_2": {
-                "data": {
-                    "metrics.json": {"data": {"a": 1, "b": {"ad": 3, "bc": 4}}}
-                }
+                "data": {"metrics.json": {"data": {"a": 1, "b": {"ad": 3, "bc": 4}}}}
             },
         },
         all_branches=True,
     )
     out, _ = capsys.readouterr()
     assert out == textwrap.dedent(
         """\
@@ -525,22 +491,18 @@
     )
 
 
 def test_metrics_show_markdown(capsys):
     show_metrics(
         metrics={
             "branch_1": {
-                "data": {
-                    "metrics.json": {"data": {"b": {"ad": 1, "bc": 2}, "c": 4}}
-                }
+                "data": {"metrics.json": {"data": {"b": {"ad": 1, "bc": 2}, "c": 4}}}
             },
             "branch_2": {
-                "data": {
-                    "metrics.json": {"data": {"a": 1, "b": {"ad": 3, "bc": 4}}}
-                }
+                "data": {"metrics.json": {"data": {"a": 1, "b": {"ad": 3, "bc": 4}}}}
             },
             "branch_3": {"error": YAMLFileCorruptedError("failed")},
         },
         all_branches=True,
         markdown=True,
     )
     out, _ = capsys.readouterr()
```

### Comparing `dvc-2.9.5/tests/unit/test_context.py` & `dvc-3.0.0a0/tests/unit/test_context.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from dataclasses import asdict
 from math import pi
 
 import pytest
 
-from dvc.fs.local import LocalFileSystem
+from dvc.fs import LocalFileSystem
 from dvc.parsing import DEFAULT_PARAMS_FILE
 from dvc.parsing.context import (
     Context,
     CtxDict,
     CtxList,
     KeyNotInContext,
     MergeError,
@@ -160,19 +160,21 @@
     context = Context({"dct": {"foo": "bar"}}, lst=[1, 2, 3], foo="foo")
 
     assert context.select("dct.foo", unwrap=True) == "bar"
     assert context.select("lst.0", unwrap=True) == 1
     assert context.select("foo", unwrap=True) == "foo"
 
     node = context.select("dct", unwrap=True)
-    assert isinstance(node, dict) and recurse_not_a_node(node)
+    assert isinstance(node, dict)
+    assert recurse_not_a_node(node)
     assert node == {"foo": "bar"}
 
     node = context.select("lst", unwrap=True)
-    assert isinstance(node, list) and recurse_not_a_node(node)
+    assert isinstance(node, list)
+    assert recurse_not_a_node(node)
     assert node == [1, 2, 3]
 
 
 def test_merge_dict():
     d1 = {"Train": {"us": {"lr": 10}}}
     d2 = {"Train": {"us": {"layers": 100}}}
 
@@ -284,22 +286,21 @@
         "lst": [
             {"foo0": "foo0", "bar0": "bar0"},
             {"foo1": "foo1", "bar1": "bar1"},
         ],
         "dct": {"foo": "foo", "bar": "bar", "baz": "baz"},
     }
     fs = LocalFileSystem()
-    path = tmp_dir / "params.yaml"
-    path.dump(d, fs=fs)
+    (tmp_dir / "params.yaml").dump(d, fs=fs)
 
-    context = Context.load_from(fs, path)
+    context = Context.load_from(fs, "params.yaml")
 
     def key_tracked(d, key):
         assert len(d) == 1
-        return key in d[relpath(path)]
+        return key in d["params.yaml"]
 
     with context.track() as tracked:
         context.select("lst")
         assert key_tracked(tracked, "lst")
 
         context.select("dct")
         assert not key_tracked(tracked, "dct")
@@ -319,68 +320,58 @@
 
 
 def test_track_from_multiple_files(tmp_dir):
     d1 = {"Train": {"us": {"lr": 10}}}
     d2 = {"Train": {"us": {"layers": 100}}}
 
     fs = LocalFileSystem()
-    path1 = tmp_dir / "params.yaml"
-    path2 = tmp_dir / "params2.yaml"
-    path1.dump(d1, fs=fs)
-    path2.dump(d2, fs=fs)
+    path1 = "params.yaml"
+    path2 = "params2.yaml"
+    (tmp_dir / path1).dump(d1, fs=fs)
+    (tmp_dir / path2).dump(d2, fs=fs)
 
     context = Context.load_from(fs, path1)
     c = Context.load_from(fs, path2)
     context.merge_update(c)
 
     def key_tracked(d, path, key):
         return key in d[relpath(path)]
 
     with context.track() as tracked:
         context.select("Train")
-        assert not (
-            key_tracked(tracked, path1, "Train")
-            or key_tracked(tracked, path2, "Train")
-        )
+        assert not key_tracked(tracked, path1, "Train")
+        assert not key_tracked(tracked, path2, "Train")
 
         context.select("Train.us")
-        assert not (
-            key_tracked(tracked, path1, "Train.us")
-            or key_tracked(tracked, path2, "Train.us")
-        )
+        assert not key_tracked(tracked, path1, "Train.us")
+        assert not key_tracked(tracked, path2, "Train.us")
 
         context.select("Train.us.lr")
-        assert key_tracked(tracked, path1, "Train.us.lr") and not key_tracked(
-            tracked, path2, "Train.us.lr"
-        )
+        assert key_tracked(tracked, path1, "Train.us.lr")
+        assert not key_tracked(tracked, path2, "Train.us.lr")
         context.select("Train.us.layers")
-        assert not key_tracked(
-            tracked, path1, "Train.us.layers"
-        ) and key_tracked(tracked, path2, "Train.us.layers")
+        assert not key_tracked(tracked, path1, "Train.us.layers")
+        assert key_tracked(tracked, path2, "Train.us.layers")
 
     context = Context.clone(context)
     assert not context._tracked_data
 
     # let's see with an alias
     context["us"] = context["Train"]["us"]
     with context.track() as tracked:
         context.select("us")
-        assert not (
-            key_tracked(tracked, path1, "Train.us")
-            or key_tracked(tracked, path2, "Train.us")
-        )
+        assert not key_tracked(tracked, path1, "Train.us")
+        assert not key_tracked(tracked, path2, "Train.us")
 
         context.select("us.lr")
-        assert key_tracked(tracked, path1, "Train.us.lr") and not key_tracked(
-            tracked, path2, "Train.us.lr"
-        )
+        assert key_tracked(tracked, path1, "Train.us.lr")
+        assert not key_tracked(tracked, path2, "Train.us.lr")
         context.select("Train.us.layers")
-        assert not key_tracked(
-            tracked, path1, "Train.us.layers"
-        ) and key_tracked(tracked, path2, "Train.us.layers")
+        assert not key_tracked(tracked, path1, "Train.us.layers")
+        assert key_tracked(tracked, path2, "Train.us.layers")
 
 
 def test_node_value():
     d = {"dct": {"foo": "bar"}, "lst": [1, 2, 3], "foo": "foo"}
     context = Context(d)
     assert isinstance(context, (Context, CtxDict))
     assert isinstance(context["dct"], CtxDict)
@@ -424,20 +415,19 @@
 
     assert context.resolve_str("--flag ${enabled}") == "--flag true"
     assert context.resolve_str("--flag ${disabled}") == "--flag false"
 
 
 def test_load_from_raises_if_file_not_exist(tmp_dir, dvc):
     with pytest.raises(ParamsLoadError) as exc_info:
-        Context.load_from(dvc.fs, tmp_dir / DEFAULT_PARAMS_FILE)
+        Context.load_from(dvc.fs, DEFAULT_PARAMS_FILE)
 
     assert str(exc_info.value) == "'params.yaml' does not exist"
 
 
 def test_load_from_raises_if_file_is_directory(tmp_dir, dvc):
-    data_dir = tmp_dir / "data"
-    data_dir.mkdir()
+    (tmp_dir / "data").mkdir()
 
     with pytest.raises(ParamsLoadError) as exc_info:
-        Context.load_from(dvc.fs, data_dir)
+        Context.load_from(dvc.fs, "data")
 
     assert str(exc_info.value) == "'data' is a directory"
```

### Comparing `dvc-2.9.5/tests/unit/test_daemon.py` & `dvc-3.0.0a0/tests/unit/test_daemon.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,15 +15,15 @@
         args = mock_windows.call_args[0]
     else:
         mock_windows.assert_not_called()
         mock_posix.assert_called()
         args = mock_posix.call_args[0]
 
     env = args[1]
-    assert "PYTHONPATH" in env.keys()
+    assert "PYTHONPATH" in env
 
     file_path = os.path.abspath(inspect.stack()[0][1])
     file_dir = os.path.dirname(file_path)
     test_dir = os.path.dirname(file_dir)
     dvc_dir = os.path.dirname(test_dir)
     assert env["PYTHONPATH"] == dvc_dir
     assert env[daemon.DVC_DAEMON] == "1"
```

### Comparing `dvc-2.9.5/tests/unit/test_external_repo.py` & `dvc-3.0.0a0/tests/unit/repo/test_open_repo.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import os
 from unittest.mock import call
 
 import pytest
 
-from dvc.external_repo import external_repo
-from tests.unit.fs.test_repo import make_subrepo
+from dvc.repo.open_repo import _external_repo as external_repo
+from dvc.testing.tmp_dir import make_subrepo
 
 
 def test_hook_is_called(tmp_dir, erepo_dir, mocker):
     subrepo_paths = [
         "subrepo1",
         "subrepo2",
         os.path.join("dir", "subrepo3"),
@@ -16,32 +16,33 @@
         "subrepo5",
         os.path.join("subrepo5", "subrepo6"),
     ]
     subrepos = [erepo_dir / path for path in subrepo_paths]
     for repo in subrepos:
         make_subrepo(repo, erepo_dir.scm)
 
-    for repo in subrepos + [erepo_dir]:
+    for repo in [*subrepos, erepo_dir]:
         with repo.chdir():
             repo.scm_gen("foo", "foo", commit=f"git add {repo}/foo")
             repo.dvc_gen("bar", "bar", commit=f"dvc add {repo}/bar")
 
-    with external_repo(str(erepo_dir)) as repo:
-        spy = mocker.spy(repo.repo_fs, "repo_factory")
+    with external_repo(str(erepo_dir), subrepos=True, uninitialized=True) as repo:
+        spy = mocker.spy(repo.dvcfs.fs, "repo_factory")
 
-        list(repo.repo_fs.walk(repo.root_dir, ignore_subrepos=False))  # drain
+        list(repo.dvcfs.walk("", ignore_subrepos=False))  # drain
         assert spy.call_count == len(subrepos)
 
-        paths = [os.path.join(repo.root_dir, path) for path in subrepo_paths]
+        paths = ["/" + path.replace("\\", "/") for path in subrepo_paths]
         spy.assert_has_calls(
             [
                 call(
                     path,
                     fs=repo.fs,
-                    repo_factory=repo.repo_fs.repo_factory,
+                    scm=repo.scm,
+                    repo_factory=repo.dvcfs.fs.repo_factory,
                 )
                 for path in paths
             ],
             any_order=True,
         )
 
 
@@ -50,40 +51,40 @@
     tmp_dir, scm, mocker, make_tmp_dir, root_is_dvc
 ):
     if root_is_dvc:
         make_subrepo(tmp_dir, scm)
 
     subrepo = tmp_dir / "subrepo"
     make_subrepo(subrepo, scm)
-    local_cache = subrepo.dvc.odb.local.cache_dir
+    local_cache = subrepo.dvc.cache.local.path
 
     tmp_dir.scm_gen("bar", "bar", commit="add bar")
     subrepo.dvc_gen("foo", "foo", commit="add foo")
 
     cache_dir = make_tmp_dir("temp-cache")
     with external_repo(
-        str(tmp_dir), cache_dir=str(cache_dir), cache_types=["symlink"]
+        str(tmp_dir),
+        subrepos=True,
+        uninitialized=True,
+        config={"cache": {"dir": str(cache_dir), "type": ["symlink"]}},
     ) as repo:
-        spy = mocker.spy(repo.repo_fs, "repo_factory")
+        spy = mocker.spy(repo.dvcfs.fs, "repo_factory")
 
-        list(repo.repo_fs.walk(repo.root_dir, ignore_subrepos=False))  # drain
+        list(repo.dvcfs.walk("", ignore_subrepos=False))  # drain
         assert spy.call_count == 1
         subrepo = spy.spy_return
 
         assert repo.url == str(tmp_dir)
         assert repo.config["cache"]["dir"] == str(cache_dir)
-        assert repo.odb.local.cache_dir == str(cache_dir)
-        assert subrepo.odb.local.cache_dir == str(cache_dir)
+        assert repo.cache.local.path == str(cache_dir)
+        assert subrepo.cache.local.path == str(cache_dir)
 
         assert repo.config["cache"]["type"] == ["symlink"]
-        assert repo.odb.local.cache_types == ["symlink"]
-        assert subrepo.odb.local.cache_types == ["symlink"]
+        assert repo.cache.local.cache_types == ["symlink"]
+        assert subrepo.cache.local.cache_types == ["symlink"]
 
-        assert (
-            subrepo.config["remote"]["auto-generated-upstream"]["url"]
-            == local_cache
-        )
+        assert subrepo.config["remote"]["auto-generated-upstream"]["url"] == local_cache
         if root_is_dvc:
-            main_cache = tmp_dir.dvc.odb.local.cache_dir
-            assert repo.config["remote"]["auto-generated-upstream"][
-                "url"
-            ] == str(main_cache)
+            main_cache = tmp_dir.dvc.cache.local.path
+            assert repo.config["remote"]["auto-generated-upstream"]["url"] == str(
+                main_cache
+            )
```

### Comparing `dvc-2.9.5/tests/unit/test_ignore.py` & `dvc-3.0.0a0/tests/unit/test_ignore.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,21 +3,21 @@
 
 import pytest
 
 from dvc.ignore import DvcIgnorePatterns
 
 
 def mock_dvcignore(dvcignore_path, patterns):
+    from dvc.fs import localfs
+
     fs = MagicMock()
+    fs.path = localfs.path
+    fs.sep = localfs.sep
     with patch.object(fs, "open", mock_open(read_data="\n".join(patterns))):
-        ignore_patterns = DvcIgnorePatterns.from_file(
-            dvcignore_path, fs, "mocked"
-        )
-
-    return ignore_patterns
+        return DvcIgnorePatterns.from_file(dvcignore_path, fs, "mocked")
 
 
 @pytest.mark.parametrize(
     "file_to_ignore_relpath, patterns,  expected_match",
     [
         # all rules from https://git-scm.com/docs/gitignore
         ("to_ignore", ["to_ignore"], True),
@@ -37,15 +37,15 @@
         # An optional prefix "!" which negates the pattern; any matching file
         # excluded by a previous pattern will become included again.
         ("to_ignore.txt", ["to_ignore*"], True),
         ("to_ignore.txt", ["to_ignore*", "!to_ignore.txt"], False),
         ("to_ignore.txt", ["!to_ignore.txt", "to_ignore*"], True),
         # It is not possible to re-include a file if a parent directory of
         # that file is excluded.
-        # Git doesn’t list excluded directories for performance reasons,
+        # Git doesn't list excluded directories for performance reasons,
         # so any patterns on contained files have no effect,
         # no matter where they are defined.
         # see (`tests/func/test_ignore.py::test_ignore_parent_path`)
         # Put a backslash ("\") in front of the first "!"
         # for patterns that begin with a literal "!",
         # for example, "\!important!.txt".
         ("!to_ignore.txt", ["\\!to_ignore.txt"], True),
@@ -83,15 +83,15 @@
         # however frotz/ matches frotz and a/frotz that is a directory
         # (all paths are relative from the .gitignore file).
         # see (`tests/func/test_ignore.py::test_ignore_directory`)
         # An asterisk "*" matches anything except a slash.
         ("to_ignore.txt", ["/*.txt"], True),
         (os.path.join("path", "to_ignore.txt"), ["/*.txt"], False),
         (os.path.join("data", "file.txt"), ["data/*"], True),
-        (os.path.join("data", "subdir", "file.txt"), ["data/*"], False),
+        (os.path.join("data", "subdir", "file.txt"), ["data/*"], True),
         (os.path.join("data", "file.txt"), ["data/"], True),
         (os.path.join("data", "subdir", "file.txt"), ["data/"], True),
         (os.path.join("data", "subdir", "file.txt"), ["subdir/"], True),
         (os.path.join("data", "subdir", "file.txt"), ["/subdir/"], False),
         (os.path.join("data", "path"), ["path/"], False),
         (os.path.join(".git", "file.txt"), [".git/"], True),
         (os.path.join("data", ".dvc", "file.txt"), [".dvc/"], True),
@@ -170,36 +170,32 @@
         (
             os.path.join("rel", "path", "path2", "to_ignore"),
             ["rel/***/to_ignore"],
             False,
         ),
     ],
 )
-def test_match_ignore_from_file(
-    file_to_ignore_relpath, patterns, expected_match
-):
-
+def test_match_ignore_from_file(file_to_ignore_relpath, patterns, expected_match):
     dvcignore_path = os.path.join(
         os.path.sep, "full", "path", "to", "ignore", "file", ".dvcignore"
     )
     dvcignore_dirname = os.path.dirname(dvcignore_path)
 
     ignore_file = mock_dvcignore(dvcignore_path, patterns)
 
     assert (
-        ignore_file.matches(dvcignore_dirname, file_to_ignore_relpath)
-        == expected_match
+        ignore_file.matches(dvcignore_dirname, file_to_ignore_relpath) == expected_match
     )
 
 
 @pytest.mark.parametrize("sub_dir", ["", "dir"])
 @pytest.mark.parametrize("omit_dir", [".git", ".hg", ".dvc"])
 def test_should_ignore_dir(omit_dir, sub_dir):
     root = os.path.join(os.path.sep, "walk", "dir", "root")
-    ignore = DvcIgnorePatterns([".git/", ".hg/", ".dvc/"], root)
+    ignore = DvcIgnorePatterns([".git/", ".hg/", ".dvc/"], root, os.sep)
 
     dirs = [omit_dir, "dir1", "dir2"]
     files = [omit_dir, "file1", "file2"]
 
     if sub_dir:
         current = os.path.join(root, sub_dir)
     else:
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `dvc-2.9.5/tests/unit/test_imports.py` & `dvc-3.0.0a0/tests/unit/test_imports.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/unit/test_info.py` & `dvc-3.0.0a0/tests/unit/test_info.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import os
 import re
 import shutil
 
 import pytest
 
-from dvc.info import get_dvc_info
+from dvc.info import SUBPROJECTS, get_dvc_info
 
 # Python's version is in the shape of:
 # <major>.<minor>.<patch>[{a|b|rc}N][.postN][.devN]
 # `patch` is more than enough for the tests.
 # Refer PEP-0440 for complete regex just in-case.
 PYTHON_VERSION_REGEX = r"Python \d\.\d+\.\d+\S*"
 DVC_VERSION_REGEX = r"\d+\.\d+\.(\d+\.)?.*"
@@ -26,37 +26,38 @@
         return []
 
     remotes = {}
     for line in lines[index:]:
         if not line.startswith("\t"):
             break
 
-        remote_name, _, raw_dependencies = (
-            line.strip().strip(",").partition(" ")
-        )
+        remote_name, _, raw_dependencies = line.strip().strip(",").partition(" ")
         remotes[remote_name] = {
             dependency: version
             for dependency, _, version in [
                 dependency.partition(" = ")
                 for dependency in raw_dependencies[1:-1].split(", ")
             ]
         }
     return remotes
 
 
 @pytest.mark.parametrize("scm_init", [True, False])
 def test_info_in_repo(scm_init, tmp_dir):
     tmp_dir.init(scm=scm_init, dvc=True)
     # Create `.dvc/cache`, that is needed to check supported link types.
-    os.mkdir(tmp_dir.dvc.odb.local.cache_dir)
+    os.mkdir(tmp_dir.dvc.cache.local.path)
 
     dvc_info = get_dvc_info()
 
     assert re.search(rf"DVC version: {DVC_VERSION_REGEX}", dvc_info)
     assert re.search(f"Platform: {PYTHON_VERSION_REGEX} on .*", dvc_info)
+    for subproject in SUBPROJECTS:
+        assert re.search(rf"{subproject} = .*", dvc_info)
+
     assert find_supported_remotes(dvc_info)
     assert re.search(r"Cache types: .*", dvc_info)
 
     if scm_init:
         assert "Repo: dvc, git" in dvc_info
     else:
         assert "Repo: dvc (no_scm)" in dvc_info
@@ -80,17 +81,15 @@
     shutil.rmtree(dvc.scm.dir)
     dvc_info = get_dvc_info()
 
     assert "Repo: dvc, git (broken)" in dvc_info
 
 
 def test_caches(tmp_dir, dvc, caplog):
-    tmp_dir.add_remote(
-        name="sshcache", url="ssh://example.com/path", default=False
-    )
+    tmp_dir.add_remote(name="sshcache", url="ssh://example.com/path", default=False)
     with tmp_dir.dvc.config.edit() as conf:
         conf["cache"]["ssh"] = "sshcache"
 
     dvc_info = get_dvc_info()
 
     # Order of cache types is runtime dependent
     assert re.search("Caches: (local, ssh|ssh, local)", dvc_info)
@@ -101,26 +100,24 @@
     dvc_info = get_dvc_info()
 
     assert "Remotes: None" in dvc_info
 
 
 def test_remotes(tmp_dir, dvc, caplog):
     tmp_dir.add_remote(name="server", url="ssh://localhost", default=False)
-    tmp_dir.add_remote(
-        name="r1", url="azure://example.com/path", default=False
-    )
+    tmp_dir.add_remote(name="r1", url="azure://example.com/path", default=False)
     tmp_dir.add_remote(name="r2", url="remote://server/path", default=False)
 
     dvc_info = get_dvc_info()
 
     assert re.search("Remotes: (ssh, azure|azure, ssh)", dvc_info)
 
 
 def test_fs_info_in_repo(tmp_dir, dvc, caplog):
-    os.mkdir(dvc.odb.local.cache_dir)
+    os.mkdir(dvc.cache.local.path)
     dvc_info = get_dvc_info()
 
     assert re.search(r"Cache directory: .* on .*", dvc_info)
     assert re.search(r"Workspace directory: .* on .*", dvc_info)
 
 
 def test_info_outside_of_repo(tmp_dir, caplog):
@@ -137,14 +134,14 @@
     dvc_info = get_dvc_info()
     assert re.search(rf"DVC version: {DVC_VERSION_REGEX}", dvc_info)
     assert re.search(f"Platform: {PYTHON_VERSION_REGEX} on .*", dvc_info)
     assert find_supported_remotes(dvc_info)
 
 
 def test_plugin_versions(tmp_dir, dvc):
-    from dvc.fs import FS_MAP
+    from dvc.fs import registry
 
     dvc_info = get_dvc_info()
     remotes = find_supported_remotes(dvc_info)
 
     for remote, dependencies in remotes.items():
-        assert dependencies.keys() == FS_MAP[remote].REQUIRES.keys()
+        assert dependencies.keys() == registry[remote].REQUIRES.keys()
```

### Comparing `dvc-2.9.5/tests/unit/test_interpolate.py` & `dvc-3.0.0a0/tests/unit/test_interpolate.py`

 * *Files 1% similar despite different names*

```diff
@@ -92,10 +92,8 @@
     assert resolved == RESOLVED_DVC_YAML_DATA
     assert recurse_not_a_node(resolved)
 
 
 def test_resolve_unicode():
     context = Context({"नेपाली": {"चिया": ["चि", "या"]}})
     assert context.resolve_str("${नेपाली.चिया[0]}${नेपाली.चिया[1]}") == "चिया"
-    assert (
-        context.resolve_str("${नेपाली[चिया][0]}${नेपाली[चिया][1]}") == "चिया"
-    )
+    assert context.resolve_str("${नेपाली[चिया][0]}${नेपाली[चिया][1]}") == "चिया"
```

### Comparing `dvc-2.9.5/tests/unit/test_lockfile.py` & `dvc-3.0.0a0/tests/unit/test_lockfile.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,15 +13,15 @@
         "schema": "2.0",
         "stages": {"s1": {"cmd": "command"}},
     }
 
 
 def test_stage_dump_when_already_exists(tmp_dir, dvc):
     data = {"s1": {"cmd": "command", "deps": [], "outs": []}}
-    (tmp_dir / "path.lock").dump(data)
+    (tmp_dir / "path.lock").dump({"schema": "2.0", "stages": data})
     stage = PipelineStage(name="s2", repo=dvc, path="path", cmd="command2")
     lockfile = Lockfile(dvc, "path.lock")
     lockfile.dump(stage)
     assert lockfile.load() == {
         "schema": "2.0",
         "stages": {**data, "s2": {"cmd": "command2"}},
     }
@@ -31,15 +31,15 @@
     data = {
         "s1": {
             "cmd": "command",
             "deps": [{"md5": "1.txt", "path": "checksum"}],
             "outs": [{"md5": "2.txt", "path": "checksum"}],
         }
     }
-    (tmp_dir / "path.lock").dump(data)
+    (tmp_dir / "path.lock").dump({"schema": "2.0", "stages": data})
     lockfile = Lockfile(dvc, "path.lock")
     stage = PipelineStage(name="s2", repo=dvc, path="path", cmd="command2")
     lockfile.dump(stage)
     assert lockfile.load() == {
         "schema": "2.0",
         "stages": {**data, "s2": {"cmd": "command2"}},
     }
@@ -65,17 +65,15 @@
     "corrupt_data",
     [
         {"s1": {"outs": []}},
         {"s1": {}},
         {
             "s1": {
                 "cmd": "command",
-                "outs": [
-                    {"md5": "checksum", "path": "path", "random": "value"}
-                ],
+                "outs": [{"md5": "checksum", "path": "path", "random": "value"}],
             }
         },
         {"s1": {"cmd": "command", "deps": [{"md5": "checksum"}]}},
     ],
 )
 def test_load_when_lockfile_is_corrupted(tmp_dir, dvc, corrupt_data):
     (tmp_dir / "Dvcfile.lock").dump(corrupt_data)
```

### Comparing `dvc-2.9.5/tests/unit/test_logger.py` & `dvc-3.0.0a0/tests/unit/test_logger.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,39 +16,38 @@
     "green": colorama.Fore.GREEN,
     "red": colorama.Fore.RED,
     "yellow": colorama.Fore.YELLOW,
     "nc": colorama.Fore.RESET,
 }
 
 
-@pytest.fixture()
+@pytest.fixture
 def dt(mocker):
     mocker.patch(
         "time.time", return_value=time.mktime(datetime(2020, 2, 2).timetuple())
     )
-    yield "2020-02-02 00:00:00,000"
+    return "2020-02-02 00:00:00,000"
 
 
 class TestColorFormatter:
-    # pylint: disable=broad-except
     def test_debug(self, caplog, dt):
         with caplog.at_level(logging.DEBUG, logger="dvc"):
             logger.debug("message")
 
-            expected = "{green}{datetime}{nc} {blue}DEBUG{nc}: message".format(
+            expected = "{blue}{datetime}{nc} {blue}DEBUG{nc}: message".format(
                 **colors, datetime=dt
             )
 
             assert expected == formatter.format(caplog.records[0])
 
     def test_info(self, caplog):
         with caplog.at_level(logging.INFO, logger="dvc"):
             logger.info("message")
 
-            assert "message" == formatter.format(caplog.records[0])
+            assert formatter.format(caplog.records[0]) == "message"
 
     def test_warning(self, caplog):
         with caplog.at_level(logging.INFO, logger="dvc"):
             logger.warning("message")
 
             expected = "{yellow}WARNING{nc}: message".format(**colors)
 
@@ -100,43 +99,37 @@
             try:
                 raise Exception("description")
             except Exception:
                 stack_trace = traceback.format_exc()
                 logger.exception("")
 
             expected = (
-                "{green}{datetime}{nc} "
+                "{red}{datetime}{nc} "
                 "{red}ERROR{nc}: description\n"
-                "{red}{line}{nc}\n"
-                "{stack_trace}"
-                "{red}{line}{nc}".format(
-                    line="-" * 60,
+                "{stack_trace}".format(
                     stack_trace=stack_trace,
                     **colors,
                     datetime=dt,
                 )
             )
 
             assert expected == formatter.format(caplog.records[0])
 
     def test_exc_info_on_other_record_types(self, caplog, dt):
         with caplog.at_level(logging.DEBUG, logger="dvc"):
             try:
                 raise Exception("description")
-            except Exception:
+            except Exception:  # noqa: BLE001
                 stack_trace = traceback.format_exc()
                 logger.debug("", exc_info=True)
 
             expected = (
-                "{green}{datetime}{nc} "
+                "{blue}{datetime}{nc} "
                 "{blue}DEBUG{nc}: description\n"
-                "{red}{line}{nc}\n"
-                "{stack_trace}"
-                "{red}{line}{nc}".format(
-                    line="-" * 60,
+                "{stack_trace}".format(
                     stack_trace=stack_trace,
                     datetime=dt,
                     **colors,
                 )
             )
 
             assert expected == formatter.format(caplog.records[0])
@@ -146,46 +139,40 @@
             try:
                 raise Exception("description")
             except Exception:
                 stack_trace = traceback.format_exc()
                 logger.exception("something", extra={"tb_only": True})
 
             expected = (
-                "{green}{datetime}{nc} "
+                "{red}{datetime}{nc} "
                 "{red}ERROR{nc}: something\n"
-                "{red}{line}{nc}\n"
-                "{stack_trace}"
-                "{red}{line}{nc}".format(
-                    line="-" * 60,
+                "{stack_trace}".format(
                     stack_trace=stack_trace,
                     **colors,
                     datetime=dt,
                 )
             )
 
             assert expected == formatter.format(caplog.records[0])
 
     def test_nested_exceptions(self, caplog, dt):
         with caplog.at_level(logging.DEBUG, logger="dvc"):
             try:
                 raise Exception("first")
-            except Exception as exc:
+            except Exception as exc:  # noqa: BLE001
                 try:
                     raise DvcException("second") from exc
                 except DvcException:
                     stack_trace = traceback.format_exc()
                     logger.exception("message")
 
             expected = (
-                "{green}{datetime}{nc} "
+                "{red}{datetime}{nc} "
                 "{red}ERROR{nc}: message - second: first\n"
-                "{red}{line}{nc}\n"
-                "{stack_trace}"
-                "{red}{line}{nc}".format(
-                    line="-" * 60,
+                "{stack_trace}".format(
                     stack_trace=stack_trace,
                     **colors,
                     datetime=dt,
                 )
             )
             assert expected == formatter.format(caplog.records[0])
             assert "Exception: first" in stack_trace
@@ -209,21 +196,21 @@
                     msg="debug",
                     args=(),
                     exc_info=None,
                 )
 
                 formatter.format(debug_record)
                 captured = capsys.readouterr()
-                assert captured.out == ""
+                assert not captured.out
 
             #  when the message is actually visible
             with caplog.at_level(logging.INFO, logger="dvc"):
                 logger.info("some info")
                 captured = capsys.readouterr()
-                assert captured.out == ""
+                assert not captured.out
 
 
 def test_handlers():
     out, deb, vrb, err = logger.handlers
 
     assert out.level == logging.INFO
     assert deb.level == logging.DEBUG
@@ -243,27 +230,27 @@
             assert record.levelname == record.message
 
 
 def test_info_with_debug_loglevel_shows_no_datetime(caplog, dt):
     with caplog.at_level(logging.DEBUG, logger="dvc"):
         logger.info("message")
 
-        assert "message" == formatter.format(caplog.records[0])
+        assert formatter.format(caplog.records[0]) == "message"
 
 
 def test_add_existing_level(caplog, dt):
     # Common pattern to configure logging level in external libraries
     # eg:
     # https://github.com/bokeh/bokeh/blob/04bb30fef2e72e64baaa8b2f330806d5bfdd3b11/
     # bokeh/util/logconfig.py#L79-L85
-    TRACE2 = 4
+    TRACE2 = 4  # noqa: N806
     logging.addLevelName(TRACE2, "TRACE2")
     logging.TRACE2 = TRACE2
 
-    dvc.logger.addLoggingLevel("TRACE2", 2)
+    dvc.logger.add_logging_level("TRACE2", 2)
 
     # DVC sets all expected entrypoints, but doesn't override the level
     assert logging.TRACE2 == 4
     assert hasattr(logging, "trace2")
     assert hasattr(logger, "trace2")
     assert logging.getLevelName("TRACE2") == 4
```

### Comparing `dvc-2.9.5/tests/unit/test_metrics.py` & `dvc-3.0.0a0/tests/unit/test_metrics.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/unit/test_params.py` & `dvc-3.0.0a0/tests/unit/test_params.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/unit/test_pathspec_math.py` & `dvc-3.0.0a0/tests/unit/test_pathspec_math.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/unit/test_progress.py` & `dvc-3.0.0a0/tests/unit/test_progress.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,44 +5,44 @@
 
 
 def test_quiet_logging(caplog, capsys):
     with caplog.at_level(logging.CRITICAL, logger="dvc"):
         for _ in Tqdm(range(10)):
             pass
         out_err = capsys.readouterr()
-        assert out_err.out == ""
-        assert out_err.err == ""
+        assert not out_err.out
+        assert not out_err.err
 
 
 def test_quiet_logging_disable_false(caplog, capsys, mocker):
     # simulate interactive terminal
     mocker.patch("sys.stdout.isatty", return_value=True)
     with caplog.at_level(logging.CRITICAL, logger="dvc"):
         for _ in Tqdm(range(10), disable=False):
             pass
         out_err = capsys.readouterr()
-        assert out_err.out == ""
-        assert out_err.err == ""
+        assert not out_err.out
+        assert not out_err.err
 
 
 def test_quiet_notty(caplog, capsys):
     with caplog.at_level(logging.INFO, logger="dvc"):
         for _ in Tqdm(range(10)):
             pass
         out_err = capsys.readouterr()
-        assert out_err.out == ""
+        assert not out_err.out
         if env2bool("DVC_IGNORE_ISATTY"):
             assert "0/10" in out_err.err
         else:
-            assert out_err.err == ""
+            assert not out_err.err
 
 
 def test_default(caplog, capsys, mocker):
     # simulate interactive terminal
     mocker.patch("sys.stdout.isatty", return_value=True)
     with caplog.at_level(logging.INFO, logger="dvc"):
         for _ in Tqdm(range(10)):
             pass
 
         out_err = capsys.readouterr()
-        assert out_err.out == ""
+        assert not out_err.out
         assert "0/10" in out_err.err
```

### Comparing `dvc-2.9.5/tests/unit/test_tabular_data.py` & `dvc-3.0.0a0/tests/unit/test_tabular_data.py`

 * *Files 18% similar despite different names*

```diff
@@ -273,17 +273,15 @@
             ],
             False,
         ),
     ],
 )
 def test_drop_duplicates(axis, expected, ignore_empty):
     td = TabularData(["col-1", "col-2", "col-3"], fill_value="-")
-    td.extend(
-        [["foo"], ["foo", "foo"], ["foo", "foo"], ["foo", "bar", "foobar"]]
-    )
+    td.extend([["foo"], ["foo", "foo"], ["foo", "foo"], ["foo", "bar", "foobar"]])
 
     assert list(td) == [
         ["foo", "-", "-"],
         ["foo", "foo", "-"],
         ["foo", "foo", "-"],
         ["foo", "bar", "foobar"],
     ]
@@ -372,25 +370,7 @@
 
 
 def test_drop_duplicates_invalid_axis():
     td = TabularData(["col-1", "col-2", "col-3"])
 
     with pytest.raises(ValueError, match="Invalid 'axis' value foo."):
         td.drop_duplicates("foo")
-
-
-def test_to_parallel_coordinates(tmp_dir, mocker):
-    (tmp_dir / "foo").mkdir()
-    td = TabularData(["categorical", "scalar"])
-    td.extend([["foo", "0.1"], ["bar", "2"]])
-
-    write = mocker.patch("dvc.render.html.write")
-    renderer_class = mocker.patch(
-        "dvc.render.plotly.ParallelCoordinatesRenderer"
-    )
-    renderer = renderer_class.return_value
-
-    td.to_parallel_coordinates(output_path="foo")
-
-    renderer_class.assert_called_with(td, None, td._fill_value)
-
-    write.assert_called_with("foo", renderers=[renderer])
```

### Comparing `dvc-2.9.5/tests/unit/test_updater.py` & `dvc-3.0.0a0/tests/unit/test_updater.py`

 * *Files 1% similar despite different names*

```diff
@@ -90,18 +90,16 @@
     updater.current = current
     with open(updater.updater_file, "w+", encoding="utf-8") as f:
         json.dump({"version": latest}, f)
 
     updater.check()
     out, err = capsys.readouterr()
     expected_message = (
-        (
-            f"You are using dvc version {current}; "
-            f"however, version {latest} is available.\n"
-        )
+        f"You are using dvc version {current}; "
+        f"however, version {latest} is available.\n"
         if notify
         else ""
     )
 
     assert expected_message in escape_ansi(err)
     assert not out
 
@@ -119,17 +117,15 @@
     with caplog.at_level(logging.INFO, logger="dvc.updater"):
         updater.check()
     assert not caplog.text
     fetch.assert_called_once()
     mock_time.assert_called()
 
 
-def test_check_fetches_on_invalid_data_format(
-    mock_tty, updater, caplog, mocker
-):
+def test_check_fetches_on_invalid_data_format(mock_tty, updater, caplog, mocker):
     updater.current = "0.0.5"
     with open(updater.updater_file, "w+", encoding="utf-8") as f:
         f.write('"{"version: "0.0.6"')
     fetch = mocker.patch.object(updater, "fetch")
     caplog.clear()
     with caplog.at_level(logging.INFO, logger="dvc.updater"):
         updater.check()
@@ -165,21 +161,25 @@
         ),
         (
             "binary",
             "To upgrade, uninstall dvc and reinstall from https://dvc.org.",
         ),
         (
             None,
-            "Find the latest release at "
-            "https://github.com/iterative/dvc/releases/latest.",
+            (
+                "Find the latest release at "
+                "https://github.com/iterative/dvc/releases/latest."
+            ),
         ),
         (
             "unknown",
-            "Find the latest release at "
-            "https://github.com/iterative/dvc/releases/latest.",
+            (
+                "Find the latest release at "
+                "https://github.com/iterative/dvc/releases/latest."
+            ),
         ),
     ],
 )
 def test_notify_message(updater, pkg, instruction):
     update_message = (
         "You are using dvc version 0.0.2; however, version 0.0.3 is available."
     )
```

### Comparing `dvc-2.9.5/tests/unit/ui/test_console.py` & `dvc-3.0.0a0/tests/unit/ui/test_console.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,18 +1,16 @@
 import datetime
 import textwrap
 
 import pytest
-from pytest import CaptureFixture
-from pytest_mock import MockerFixture
 
 from dvc.ui import Console
 
 
-def test_write(capsys: CaptureFixture[str]):
+def test_write(capsys):
     """Test that ui.write works."""
     console = Console(enable=True)
     message = "hello world"
     console.write(message)
     console.error_write(message)
 
     captured = capsys.readouterr()
@@ -40,28 +38,26 @@
                 """\
         {"hello": "world", "date": "1970-01-01 00:00:00"}
         """
             ),
         ),
     ],
 )
-def test_write_json(
-    capsys: CaptureFixture[str], mocker: MockerFixture, isatty, expected_output
-):
+def test_write_json(capsys, mocker, isatty, expected_output):
     """Test that ui.write_json works."""
 
     console = Console(enable=True)
     mocker.patch.object(console, "isatty", return_value=isatty)
     message = {"hello": "world", "date": datetime.datetime(1970, 1, 1)}
     console.write_json(message, default=str)
     captured = capsys.readouterr()
     assert captured.out == expected_output
 
 
-def test_capsys_works(capsys: CaptureFixture[str]):
+def test_capsys_works(capsys):
     """Sanity check that capsys can capture outputs from a global ui."""
     from dvc.ui import ui
 
     message = "hello world"
     ui.write(message)
     ui.error_write(message)
```

### Comparing `dvc-2.9.5/tests/unit/ui/test_pager.py` & `dvc-3.0.0a0/tests/unit/ui/test_pager.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,18 +1,11 @@
 import pytest
 
 from dvc.env import DVC_PAGER
-from dvc.ui.pager import (
-    DEFAULT_PAGER,
-    LESS,
-    PAGER_ENV,
-    find_pager,
-    make_pager,
-    pager,
-)
+from dvc.ui.pager import DEFAULT_PAGER, LESS, PAGER_ENV, find_pager, make_pager, pager
 
 
 @pytest.fixture(autouse=True)
 def clear_envs(monkeypatch):
     monkeypatch.delenv(DVC_PAGER, raising=False)
     monkeypatch.delenv(PAGER_ENV, raising=False)
     monkeypatch.delenv(LESS, raising=False)
@@ -28,17 +21,15 @@
 ):
     monkeypatch.setenv(DVC_PAGER, "my-pager")
     mocker.patch("sys.stdout.isatty", return_value=True)
 
     assert find_pager() == "my-pager"
 
 
-def test_find_pager_uses_custom_pager_when_pager_env_is_defined(
-    mocker, monkeypatch
-):
+def test_find_pager_uses_custom_pager_when_pager_env_is_defined(mocker, monkeypatch):
     monkeypatch.setenv(PAGER_ENV, "my-pager")
     mocker.patch("sys.stdout.isatty", return_value=True)
 
     assert find_pager() == "my-pager"
 
 
 def test_find_pager_uses_default_pager_when_found(mocker):
@@ -52,32 +43,28 @@
     mocker.patch("os.system", return_value=1)
     mocker.patch("sys.stdout.isatty", return_value=True)
 
     assert find_pager() is None
 
 
 @pytest.mark.parametrize("env", [DVC_PAGER, PAGER_ENV, None])
-def test_dvc_sets_default_options_on_less_without_less_env(
-    mocker, monkeypatch, env
-):
+def test_dvc_sets_default_options_on_less_without_less_env(mocker, monkeypatch, env):
     if env:
         monkeypatch.setenv(env, "less")
     mocker.patch("sys.stdout.isatty", return_value=True)
     mocker.patch("os.system", return_value=0)
 
-    assert find_pager() == (
-        "less --quit-if-one-screen --RAW-CONTROL-CHARS"
-        " --chop-long-lines --no-init"
+    assert (
+        find_pager()
+        == "less --quit-if-one-screen --RAW-CONTROL-CHARS --chop-long-lines --no-init"
     )
 
 
 @pytest.mark.parametrize("env", [DVC_PAGER, PAGER_ENV, None])
-def test_dvc_sets_some_options_on_less_if_less_env_defined(
-    mocker, monkeypatch, env
-):
+def test_dvc_sets_some_options_on_less_if_less_env_defined(mocker, monkeypatch, env):
     if env:
         monkeypatch.setenv(env, "less")
     mocker.patch("sys.stdout.isatty", return_value=True)
     mocker.patch("os.system", return_value=0)
     monkeypatch.setenv(LESS, "-R")
 
     assert find_pager() == "less --RAW-CONTROL-CHARS --chop-long-lines"
```

### Comparing `dvc-2.9.5/tests/unit/ui/test_table.py` & `dvc-3.0.0a0/tests/unit/ui/test_table.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,18 +1,16 @@
 import textwrap
 
 import pytest
-from pytest import CaptureFixture
-from pytest_mock import MockerFixture
 from rich.style import Style
 
 from dvc.ui import ui
 
 
-def test_plain(capsys: CaptureFixture[str]):
+def test_plain(capsys):
     ui.table(
         [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
         headers=["first", "second"],
     )
     captured = capsys.readouterr()
     assert captured.out == textwrap.dedent(
         """\
@@ -20,15 +18,15 @@
         foo      bar
         foo1     bar1
         foo2     bar2
     """
     )
 
 
-def test_plain_md(capsys: CaptureFixture[str]):
+def test_plain_md(capsys):
     ui.table(
         [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
         headers=["first", "second"],
         markdown=True,
     )
     captured = capsys.readouterr()
     assert captured.out == textwrap.dedent(
@@ -38,15 +36,15 @@
         | foo     | bar      |
         | foo1    | bar1     |
         | foo2    | bar2     |\n
     """
     )
 
 
-def test_plain_pager(mocker: MockerFixture):
+def test_plain_pager(mocker):
     pager_mock = mocker.patch("dvc.ui.pager.pager")
     ui.table(
         [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
         headers=["first", "second"],
         pager=True,
     )
 
@@ -58,60 +56,61 @@
             foo1     bar1
             foo2     bar2
             """
         )
     )
 
 
-def test_plain_headerless(capsys: CaptureFixture[str]):
+def test_plain_headerless(capsys):
     ui.table([("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")])
     captured = capsys.readouterr()
     assert captured.out == textwrap.dedent(
         """\
         foo   bar
         foo1  bar1
         foo2  bar2
     """
     )
 
 
-def test_rich_simple(capsys: CaptureFixture[str]):
+def test_rich_simple(capsys):
     ui.table(
         [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
         headers=["first", "second"],
         rich_table=True,
     )
     # not able to test the actual style for now
     captured = capsys.readouterr()
-    assert [
-        row.strip() for row in captured.out.splitlines() if row.strip()
-    ] == ["first  second", "foo    bar", "foo1   bar1", "foo2   bar2"]
+    assert [row.strip() for row in captured.out.splitlines() if row.strip()] == [
+        "first  second",
+        "foo    bar",
+        "foo1   bar1",
+        "foo2   bar2",
+    ]
 
 
-def test_rich_headerless(capsys: CaptureFixture[str]):
-    ui.table(
-        [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")], rich_table=True
-    )
+def test_rich_headerless(capsys):
+    ui.table([("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")], rich_table=True)
     captured = capsys.readouterr()
-    assert [
-        row.strip() for row in captured.out.splitlines() if row.strip()
-    ] == ["foo   bar", "foo1  bar1", "foo2  bar2"]
+    assert [row.strip() for row in captured.out.splitlines() if row.strip()] == [
+        "foo   bar",
+        "foo1  bar1",
+        "foo2  bar2",
+    ]
 
 
-def test_rich_border(capsys: CaptureFixture[str]):
+def test_rich_border(capsys):
     ui.table(
         [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
         headers=["first", "second"],
         rich_table=True,
         borders="simple",
     )
     captured = capsys.readouterr()
-    assert [
-        row.strip() for row in captured.out.splitlines() if row.strip()
-    ] == [
+    assert [row.strip() for row in captured.out.splitlines() if row.strip()] == [
         "first   second",
         "────────────────",
         "foo     bar",
         "foo1    bar1",
         "foo2    bar2",
     ]
 
@@ -120,47 +119,53 @@
     "extra_opts",
     [
         {"header_styles": [{"style": Style(bold=True)}]},
         {"header_styles": {"first": {"style": Style(bold=True)}}},
         {"row_styles": [{"style": Style(bold=True)}]},
     ],
 )
-def test_rich_styles(capsys: CaptureFixture[str], extra_opts):
+def test_rich_styles(capsys, extra_opts):
     ui.table(
         [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
         headers=["first", "second"],
         rich_table=True,
-        **extra_opts
+        **extra_opts,
     )
     # not able to test the actual style for now
     captured = capsys.readouterr()
-    assert [
-        row.strip() for row in captured.out.splitlines() if row.strip()
-    ] == ["first  second", "foo    bar", "foo1   bar1", "foo2   bar2"]
+    assert [row.strip() for row in captured.out.splitlines() if row.strip()] == [
+        "first  second",
+        "foo    bar",
+        "foo1   bar1",
+        "foo2   bar2",
+    ]
 
 
-def test_rich_pager(mocker: MockerFixture):
+def test_rich_pager(mocker):
     pager_mock = mocker.patch("dvc.ui.pager.pager")
 
     ui.table(
         [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
         headers=["first", "second"],
         rich_table=True,
         pager=True,
     )
     received_text = pager_mock.call_args[0][0]
-    assert [
-        row.strip() for row in received_text.splitlines() if row.strip()
-    ] == ["first  second", "foo    bar", "foo1   bar1", "foo2   bar2"]
+    assert [row.strip() for row in received_text.splitlines() if row.strip()] == [
+        "first  second",
+        "foo    bar",
+        "foo1   bar1",
+        "foo2   bar2",
+    ]
 
 
 @pytest.mark.parametrize("rich_table", [True, False])
-def test_empty(capsys: CaptureFixture[str], rich_table: str):
+def test_empty(capsys, rich_table):
     ui.table([], rich_table=rich_table)
     out, err = capsys.readouterr()
     assert (out, err) == ("", "")
 
 
-def test_empty_markdown(capsys: CaptureFixture[str]):
+def test_empty_markdown(capsys):
     ui.table([], headers=["Col1", "Col2"], markdown=True)
     out, err = capsys.readouterr()
     assert (out, err) == ("| Col1   | Col2   |\n|--------|--------|\n\n", "")
```

### Comparing `dvc-2.9.5/tests/unit/utils/serialize/test_python.py` & `dvc-3.0.0a0/tests/unit/utils/serialize/test_python.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/unit/utils/serialize/test_yaml.py` & `dvc-3.0.0a0/tests/unit/utils/serialize/test_yaml.py`

 * *Files identical despite different names*

### Comparing `dvc-2.9.5/tests/unit/utils/test_collections.py` & `dvc-3.0.0a0/tests/unit/utils/test_collections.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,18 +1,17 @@
-# pylint: disable=unidiomatic-typecheck
 import json
-from json import encoder
 
 import pytest
-from mock import create_autospec
 
 from dvc.utils.collections import (
     apply_diff,
     chunk_dict,
-    merge_params,
+    merge_dicts,
+    remove_missing_keys,
+    to_omegaconf,
     validate,
 )
 from dvc.utils.serialize import dumps_yaml
 
 
 class MyDict(dict):
     pass
@@ -20,15 +19,15 @@
 
 class MyInt(int):
     pass
 
 
 def test_apply_diff_is_inplace():
     dest = MyDict()
-    dest.attr = 42  # pylint: disable=attribute-defined-outside-init
+    dest.attr = 42
     apply_diff({}, dest)
 
     assert type(dest) is MyDict, "Preserves class"
     assert dest.attr == 42, "Preserves custom attrs"
 
 
 def test_apply_diff_mapping():
@@ -68,25 +67,22 @@
     d = {"a": 1, "b": 2, "c": 3}
     assert chunk_dict(d) == [{"a": 1}, {"b": 2}, {"c": 3}]
     assert chunk_dict(d, 2) == [{"a": 1, "b": 2}, {"c": 3}]
     assert chunk_dict(d, 3) == [d]
     assert chunk_dict(d, 4) == [d]
 
 
-# pylint: disable=unused-argument
-
-
 def _test_func(x, y, *args, j=3, k=5, **kwargs):
     pass
 
 
 def test_pre_validate_decorator_required_args(mocker):
     mock = mocker.MagicMock()
 
-    func_mock = create_autospec(_test_func)
+    func_mock = mocker.create_autospec(_test_func)
     func = validate(mock)(func_mock)
 
     func("x", "y")
 
     func_mock.assert_called_once_with("x", "y", j=3, k=5)
     mock.assert_called_once()
 
@@ -97,15 +93,15 @@
     assert args.j == 3
     assert args.k == 5
     assert args.kwargs == {}
 
 
 def test_pre_validate_decorator_kwargs_args(mocker):
     mock = mocker.MagicMock()
-    func_mock = create_autospec(_test_func)
+    func_mock = mocker.create_autospec(_test_func)
     func = validate(mock)(func_mock)
 
     func("x", "y", "part", "of", "args", j=1, k=10, m=100, n=1000)
 
     func_mock.assert_called_once_with(
         "x", "y", "part", "of", "args", j=1, k=10, m=100, n=1000
     )
@@ -115,24 +111,24 @@
     assert args.y == "y"
     assert args.args == ("part", "of", "args")
     assert args.j == 1
     assert args.k == 10
     assert args.kwargs == {"m": 100, "n": 1000}
 
 
-def test_pre_validate_update_args():
+def test_pre_validate_update_args(mocker):
     def test_validator(args):
         args.w += 50
         del args.x
         args.y = 100
 
     def test_func(w=1, x=5, y=10, z=15):
         pass
 
-    mock = create_autospec(test_func)
+    mock = mocker.create_autospec(test_func)
     func = validate(test_validator)(mock)
 
     func(100, 100)
     mock.assert_called_once_with(w=150, y=100, z=15)
 
 
 def test_post_validate_decorator(mocker):
@@ -149,86 +145,81 @@
 
 def is_serializable(d):
     json.dumps(d)
     dumps_yaml(d)
     return True
 
 
+def test_to_omegaconf():
+    class CustomDict(dict):
+        pass
+
+    class CustomList(list):
+        pass
+
+    data = {
+        "foo": CustomDict(bar=1, bag=CustomList([1, 2])),
+        "goo": CustomList([CustomDict(goobar=1)]),
+    }
+    new_data = to_omegaconf(data)
+    assert not isinstance(new_data["foo"], CustomDict)
+    assert not isinstance(new_data["foo"]["bag"], CustomList)
+    assert not isinstance(new_data["goo"], CustomList)
+    assert not isinstance(new_data["goo"][0], CustomDict)
+
+
 @pytest.mark.parametrize(
     "changes, expected",
     [
         [{"foo": "baz"}, {"foo": "baz", "goo": {"bag": 3}, "lorem": False}],
         [
             {"foo": "baz", "goo": "bar"},
             {"foo": "baz", "goo": "bar", "lorem": False},
         ],
         [
-            {"goo.bag": 4},
+            {"goo": {"bag": 4}},
             {"foo": {"bar": 1, "baz": 2}, "goo": {"bag": 4}, "lorem": False},
         ],
         [
-            {"foo[0]": "bar"},
+            {"foo": {"bar": 1, "baz": 2, 0: "bar"}},
             {
                 "foo": {"bar": 1, "baz": 2, 0: "bar"},
                 "goo": {"bag": 3},
                 "lorem": False,
             },
         ],
         [
-            {"foo[1].baz": 3},
-            {
-                "foo": {"bar": 1, "baz": 2, 1: {"baz": 3}},
-                "goo": {"bag": 3},
-                "lorem": False,
-            },
-        ],
-        [
-            {"foo[1]": ["baz", "goo"]},
-            {
-                "foo": {"bar": 1, "baz": 2, 1: ["baz", "goo"]},
-                "goo": {"bag": 3},
-                "lorem": False,
-            },
-        ],
-        [
-            {"lorem.ipsum": 3},
+            {"lorem": {"ipsum": 3}},
             {
                 "foo": {"bar": 1, "baz": 2},
                 "goo": {"bag": 3},
                 "lorem": {"ipsum": 3},
             },
         ],
         [{}, {"foo": {"bar": 1, "baz": 2}, "goo": {"bag": 3}, "lorem": False}],
     ],
 )
-def test_merge_params(changes, expected):
+def test_merge_dicts(changes, expected):
     params = {"foo": {"bar": 1, "baz": 2}, "goo": {"bag": 3}, "lorem": False}
-    merged = merge_params(params, changes)
+    merged = merge_dicts(params, changes)
     assert merged == expected == params
     assert params is merged  # references should be preserved
-    assert encoder.c_make_encoder
     assert is_serializable(params)
 
 
 @pytest.mark.parametrize(
     "changes, expected",
     [
-        [{"foo": "baz"}, {"foo": "baz"}],
-        [{"foo": "baz", "goo": "bar"}, {"foo": "baz", "goo": "bar"}],
-        [{"foo[1]": ["baz", "goo"]}, {"foo": [None, ["baz", "goo"]]}],
-        [{"foo.bar": "baz"}, {"foo": {"bar": "baz"}}],
+        [{"foo": "baz"}, {"foo": {"baz": 2}}],
+        [
+            {"foo": "baz", "goo": "bag"},
+            {"foo": {"baz": 2}, "goo": {"bag": 3}},
+        ],
+        [{}, {}],
     ],
 )
-def test_merge_params_on_empty_src(changes, expected):
-    params = {}
-    merged = merge_params(params, changes)
-    assert merged == expected == params
-    assert params is merged  # references should be preserved
-    assert encoder.c_make_encoder
+def test_remove_missing_keys(changes, expected):
+    params = {"foo": {"bar": 1, "baz": 2}, "goo": {"bag": 3}, "lorem": False}
+    removed = remove_missing_keys(params, changes)
+    assert removed == expected == params
+    assert params is removed  # references should be preserved
     assert is_serializable(params)
-
-
-def test_benedict_rollback_its_monkeypatch():
-    from dvc.utils._benedict import benedict
-
-    assert benedict({"foo": "foo"}) == {"foo": "foo"}
-    assert encoder.c_make_encoder
```

### Comparing `dvc-2.9.5/tests/unit/utils/test_humanize.py` & `dvc-3.0.0a0/tests/unit/utils/test_humanize.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,20 +12,17 @@
             ("fetched", 3),
             ("added", ["file1", "file2", "file3"]),
             ("deleted", ["file4", "file5"]),
             ("modified", ["file6", "file7"]),
         ]
     )
 
-    assert get_summary(stats.items()) == (
-        "3 files fetched, "
-        "3 files added, "
-        "2 files deleted "
-        "and "
-        "2 files modified"
+    assert (
+        get_summary(stats.items())
+        == "3 files fetched, 3 files added, 2 files deleted and 2 files modified"
     )
 
     del stats["fetched"]
     del stats["deleted"][1]
     assert (
         get_summary(stats.items())
         == "3 files added, 1 file deleted and 2 files modified"
@@ -33,16 +30,16 @@
 
     del stats["deleted"][0]
     assert get_summary(stats.items()) == "3 files added and 2 files modified"
 
     del stats["modified"]
     assert get_summary(stats.items()) == "3 files added"
 
-    assert get_summary([]) == ""
-    assert get_summary([("x", 0), ("y", [])]) == ""
+    assert not get_summary([])
+    assert not get_summary([("x", 0), ("y", [])])
     assert get_summary([("x", 1), ("y", [])]) == "1 file x"
 
 
 def test_truncate_text():
     text = "lorem ipsum"
     length = 5
```

### Comparing `dvc-2.9.5/tests/unit/utils/test_utils.py` & `dvc-3.0.0a0/tests/unit/utils/test_utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,72 +1,28 @@
 import os
-import re
 
 import pytest
 
-from dvc.fs.local import LocalFileSystem
-from dvc.utils import (
-    dict_sha256,
-    file_md5,
-    fix_env,
-    parse_target,
-    relpath,
-    resolve_output,
-    tmp_fname,
-    to_chunks,
-)
-
-
-@pytest.mark.parametrize(
-    "chunk_size, expected_chunks",
-    [(1, [[1], [2], [3], [4]]), (2, [[1, 2], [3, 4]]), (3, [[1, 2, 3], [4]])],
-)
-def test_to_chunks_chunk_size(chunk_size, expected_chunks):
-    list_to_chunk = [1, 2, 3, 4]
-    result = list(to_chunks(list_to_chunk, chunk_size=chunk_size))
-    assert result == expected_chunks
-
-
-@pytest.mark.parametrize("num_chunks, chunk_size", [(1, 2), (None, None)])
-def test_to_chunks_should_raise(num_chunks, chunk_size):
-    list_to_chunk = [1, 2, 3]
-    with pytest.raises(ValueError):
-        to_chunks(list_to_chunk, num_chunks, chunk_size)
-
-
-@pytest.mark.parametrize(
-    "num_chunks, expected_chunks",
-    [(4, [[1], [2], [3], [4]]), (3, [[1, 2], [3, 4]]), (2, [[1, 2], [3, 4]])],
-)
-def test_to_chunks_num_chunks(num_chunks, expected_chunks):
-    list_to_chunk = [1, 2, 3, 4]
-    result = to_chunks(list_to_chunk, num_chunks=num_chunks)
-    assert result == expected_chunks
+from dvc.utils import dict_sha256, fix_env, parse_target, relpath, resolve_output
 
 
 @pytest.mark.skipif(os.name == "nt", reason="pyenv-win is not supported")
 @pytest.mark.parametrize(
     "path, orig",
     [
         (
-            (
-                "/pyenv/bin:/pyenv/libexec:/pyenv/plugins/plugin:"
-                "/orig/path1:/orig/path2"
-            ),
+            "/pyenv/bin:/pyenv/libexec:/pyenv/plugins/plugin:/orig/path1:/orig/path2",
             "/orig/path1:/orig/path2",
         ),
         (
             "/pyenv/bin:/pyenv/libexec:/orig/path1:/orig/path2",
             "/orig/path1:/orig/path2",
         ),
         (
-            (
-                "/pyenv/bin:/some/libexec:/pyenv/plugins/plugin:"
-                "/orig/path1:/orig/path2"
-            ),
+            "/pyenv/bin:/some/libexec:/pyenv/plugins/plugin:/orig/path1:/orig/path2",
             "/orig/path1:/orig/path2",
         ),
         ("/orig/path1:/orig/path2", "/orig/path1:/orig/path2"),
         (
             "/orig/path1:/orig/path2:/pyenv/bin:/pyenv/libexec",
             "/orig/path1:/orig/path2:/pyenv/bin:/pyenv/libexec",
         ),
@@ -79,35 +35,14 @@
         "PYENV_VERSION": "3.7.2",
         "PYENV_DIR": "/some/dir",
         "PYENV_HOOK_PATH": "/some/hook/path",
     }
     assert fix_env(env)["PATH"] == orig
 
 
-def test_file_md5(tmp_dir):
-    tmp_dir.gen("foo", "foo content")
-
-    fs = LocalFileSystem()
-    assert file_md5("foo", fs) == file_md5("foo", fs)
-
-
-def test_tmp_fname():
-    file_path = os.path.join("path", "to", "file")
-
-    def pattern(path):
-        return r"^" + re.escape(path) + r"\.[a-z0-9]{22}\.tmp$"
-
-    assert re.search(pattern(file_path), tmp_fname(file_path), re.IGNORECASE)
-    assert re.search(
-        pattern(file_path),
-        tmp_fname(file_path),
-        re.IGNORECASE,
-    )
-
-
 @pytest.mark.skipif(os.name != "nt", reason="Windows specific")
 def test_relpath_windows(monkeypatch):
     """test that relpath correctly generated when run on a
     windows network share. The drive mapped path is mapped
     to a UNC path by os.path.realpath"""
 
     def dummy_realpath(path):
@@ -191,17 +126,17 @@
     ],
 )
 def test_parse_target(inp, out, default):
     assert parse_target(inp, default) == out
 
 
 def test_hint_on_lockfile():
-    with pytest.raises(Exception) as exc:
+    with pytest.raises(Exception, match="Did you mean: `dvc.yaml:name`?") as e:
         assert parse_target("dvc.lock:name")
-    assert "dvc.yaml:name" in str(exc.value)
+    assert "dvc.yaml:name" in str(e.value)
 
 
 @pytest.mark.parametrize(
     "d,sha",
     [
         (
             {
```

### Comparing `dvc-2.9.5/tests/utils/__init__.py` & `dvc-3.0.0a0/tests/utils/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -40,26 +40,14 @@
         writer.writerows(metrics)
     else:
         writer = csv.writer(stream)
         for d in metrics:
             writer.writerow(list(d.values()))
 
 
-def clean_staging():
-    from dvc.data.stage import _STAGING_MEMFS_PATH
-    from dvc.fs.memory import MemoryFileSystem
-
-    try:
-        MemoryFileSystem().fs.rm(
-            f"memory://{_STAGING_MEMFS_PATH}", recursive=True
-        )
-    except FileNotFoundError:
-        pass
-
-
 @contextmanager
 def console_width(console, width):
     console_options = console.options
     original = console_options.max_width
     con_width = console._width
 
     try:
```

### Comparing `dvc-2.9.5/tests/utils/asserts.py` & `dvc-3.0.0a0/tests/utils/asserts.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,18 +1,21 @@
-from typing import Any, Dict
-from unittest.mock import ANY, Mock
+from typing import TYPE_CHECKING, Any, Dict
+from unittest.mock import ANY
+
+if TYPE_CHECKING:
+    from unittest.mock import Mock
 
 
 def issubset(subset: Dict, superset: Dict) -> bool:
     assert {**superset, **subset} == superset
     return True
 
 
-def called_once_with_subset(m: Mock, *args: Any, **kwargs: Any) -> bool:
+def called_once_with_subset(m: "Mock", *args: Any, **kwargs: Any) -> bool:
     m.assert_called_once()
     m_args, m_kwargs = m.call_args
 
-    expected_args = m_args + (ANY,) * (len(m_args) - len(args))
+    expected_args = m_args + (ANY,) * (len(m_args) - len(args) - 1)
     expected_kwargs = {k: kwargs.get(k, ANY) for k in m_kwargs}
     m.assert_called_with(*expected_args, **expected_kwargs)
 
     return True
```

