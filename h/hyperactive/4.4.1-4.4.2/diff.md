# Comparing `tmp/hyperactive-4.4.1-py3-none-any.whl.zip` & `tmp/hyperactive-4.4.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,30 +1,30 @@
-Zip file size: 60112 bytes, number of entries: 73
--rw-rw-r--  2.0 unx      195 b- defN 23-Apr-11 05:43 hyperactive/__init__.py
+Zip file size: 61840 bytes, number of entries: 75
+-rw-rw-r--  2.0 unx      195 b- defN 23-May-22 15:49 hyperactive/__init__.py
 -rw-rw-r--  2.0 unx     1057 b- defN 23-Mar-18 07:16 hyperactive/distribution.py
--rw-rw-r--  2.0 unx     4980 b- defN 23-Apr-11 05:34 hyperactive/hyperactive.py
--rw-rw-r--  2.0 unx     3749 b- defN 22-Dec-26 17:59 hyperactive/print_results.py
--rw-rw-r--  2.0 unx      754 b- defN 23-Mar-26 10:22 hyperactive/process.py
--rw-rw-r--  2.0 unx     2505 b- defN 23-Apr-11 05:34 hyperactive/results.py
+-rw-rw-r--  2.0 unx     5285 b- defN 23-May-22 15:32 hyperactive/hyperactive.py
+-rw-rw-r--  2.0 unx     5516 b- defN 23-Apr-23 08:20 hyperactive/print_results.py
+-rw-rw-r--  2.0 unx      881 b- defN 23-May-22 15:47 hyperactive/process.py
+-rw-rw-r--  2.0 unx     2505 b- defN 23-May-22 15:32 hyperactive/results.py
 -rw-rw-r--  2.0 unx     1416 b- defN 22-Dec-26 17:59 hyperactive/run_search.py
--rw-rw-r--  2.0 unx     3981 b- defN 23-Apr-11 05:34 hyperactive/search_space.py
+-rw-rw-r--  2.0 unx     3981 b- defN 23-May-22 15:32 hyperactive/search_space.py
 -rw-rw-r--  2.0 unx     1292 b- defN 23-Feb-01 18:21 hyperactive/optimizers/__init__.py
 -rw-rw-r--  2.0 unx      337 b- defN 23-Feb-26 07:03 hyperactive/optimizers/dictionary.py
--rw-rw-r--  2.0 unx     4816 b- defN 23-Apr-11 05:34 hyperactive/optimizers/hyper_gradient_conv.py
--rw-rw-r--  2.0 unx     5107 b- defN 23-Apr-11 05:34 hyperactive/optimizers/hyper_optimizer.py
+-rw-rw-r--  2.0 unx     4816 b- defN 23-May-22 15:32 hyperactive/optimizers/hyper_gradient_conv.py
+-rw-rw-r--  2.0 unx     5215 b- defN 23-May-22 15:47 hyperactive/optimizers/hyper_optimizer.py
 -rw-rw-r--  2.0 unx     1456 b- defN 23-Mar-30 10:15 hyperactive/optimizers/objective_function.py
 -rw-rw-r--  2.0 unx      825 b- defN 23-Feb-01 18:21 hyperactive/optimizers/optimizer_attributes.py
 -rw-rw-r--  2.0 unx     5103 b- defN 23-Feb-01 18:21 hyperactive/optimizers/optimizers.py
 -rw-rw-r--  2.0 unx      199 b- defN 23-Feb-01 18:21 hyperactive/optimizers/strategies/__init__.py
--rw-rw-r--  2.0 unx      573 b- defN 23-Apr-11 05:34 hyperactive/optimizers/strategies/custom_optimization_strategy.py
--rw-rw-r--  2.0 unx     3581 b- defN 23-Apr-11 05:34 hyperactive/optimizers/strategies/optimization_strategy.py
+-rw-rw-r--  2.0 unx      640 b- defN 23-May-22 15:32 hyperactive/optimizers/strategies/custom_optimization_strategy.py
+-rw-rw-r--  2.0 unx     3820 b- defN 23-May-22 15:32 hyperactive/optimizers/strategies/optimization_strategy.py
 -rw-rw-r--  2.0 unx     1741 b- defN 23-Feb-01 18:21 hyperactive/optimizers/strategies/optimizer_attributes.py
 -rw-rw-r--  2.0 unx       79 b- defN 23-Mar-26 09:16 hyperactive/optimizers/strategies/layers/__init__.py
 -rw-rw-r--  2.0 unx     2328 b- defN 23-Mar-28 17:35 hyperactive/optimizers/strategies/layers/optimizer.py
--rw-rw-r--  2.0 unx     1945 b- defN 23-Mar-30 10:16 hyperactive/optimizers/strategies/layers/search_space_pruning.py
+-rw-rw-r--  2.0 unx     2120 b- defN 23-Apr-11 19:20 hyperactive/optimizers/strategies/layers/search_space_pruning.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 15:47 tests/__init__.py
 -rw-rw-r--  2.0 unx      686 b- defN 21-Dec-08 14:43 tests/_test_examples.py
 -rw-rw-r--  2.0 unx     1995 b- defN 22-May-03 10:56 tests/test_callbacks.py
 -rw-rw-r--  2.0 unx     2566 b- defN 22-May-03 10:56 tests/test_catch.py
 -rw-rw-r--  2.0 unx     4353 b- defN 23-Mar-01 11:28 tests/test_distribution.py
 -rw-rw-r--  2.0 unx     7035 b- defN 21-Dec-08 14:43 tests/test_early_stop.py
 -rw-rw-r--  2.0 unx     5196 b- defN 21-Dec-08 14:43 tests/test_hyper_gradient_trafo.py
@@ -52,24 +52,26 @@
 -rw-rw-r--  2.0 unx      743 b- defN 21-Dec-01 05:57 tests/_local_test_timings/_test_warm_start.py
 -rw-rw-r--  2.0 unx     3945 b- defN 21-Dec-01 05:57 tests/_local_test_timings/_test_warm_start_n_jobs.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Dec-01 05:57 tests/test_issues/__init__.py
 -rw-rw-r--  2.0 unx     1580 b- defN 21-Dec-01 05:57 tests/test_issues/test_issue_25.py
 -rw-rw-r--  2.0 unx      987 b- defN 21-Dec-08 14:43 tests/test_issues/test_issue_29.py
 -rw-rw-r--  2.0 unx     1711 b- defN 21-Dec-01 05:57 tests/test_issues/test_issue_34.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 15:47 tests/test_optimization_strategies/__init__.py
+-rw-rw-r--  2.0 unx     2891 b- defN 23-Feb-01 18:21 tests/test_optimization_strategies/_parametrize.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 15:47 tests/test_optimization_strategies/test_early_stopping.py
+-rw-rw-r--  2.0 unx     1339 b- defN 23-Apr-28 13:49 tests/test_optimization_strategies/test_search_space_pruning.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 15:47 tests/test_optimizers/__init__.py
 -rw-rw-r--  2.0 unx     1336 b- defN 23-Feb-01 18:21 tests/test_optimizers/_parametrize.py
 -rw-rw-r--  2.0 unx     2651 b- defN 21-Dec-08 14:43 tests/test_optimizers/test_best_results.py
--rw-rw-r--  2.0 unx     2615 b- defN 23-Apr-11 05:34 tests/test_optimizers/test_gfo_wrapper.py
+-rw-rw-r--  2.0 unx     2615 b- defN 23-May-22 15:32 tests/test_optimizers/test_gfo_wrapper.py
 -rw-rw-r--  2.0 unx      698 b- defN 23-Feb-01 18:21 tests/test_optimizers/test_memory.py
--rw-rw-r--  2.0 unx     1380 b- defN 23-Mar-01 11:28 tests/test_optimizers/test_optimization_strategies.py
+-rw-rw-r--  2.0 unx     1380 b- defN 23-May-22 15:32 tests/test_optimizers/test_optimization_strategies.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Dec-01 05:57 tests/test_warm_starts/__init__.py
 -rw-rw-r--  2.0 unx     2421 b- defN 21-Dec-08 14:43 tests/test_warm_starts/test_memory_warm_start.py
 -rw-rw-r--  2.0 unx     2413 b- defN 21-Dec-01 05:57 tests/test_warm_starts/test_warm_start.py
 -rw-rw-r--  2.0 unx     2896 b- defN 22-Dec-26 17:59 tests/test_warm_starts/test_warm_start_smbo.py
--rw-rw-r--  2.0 unx     1069 b- defN 23-Apr-11 05:43 hyperactive-4.4.1.dist-info/LICENSE
--rw-rw-r--  2.0 unx    40833 b- defN 23-Apr-11 05:43 hyperactive-4.4.1.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-11 05:43 hyperactive-4.4.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx       18 b- defN 23-Apr-11 05:43 hyperactive-4.4.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     6937 b- defN 23-Apr-11 05:43 hyperactive-4.4.1.dist-info/RECORD
-73 files, 213622 bytes uncompressed, 48800 bytes compressed:  77.2%
+-rw-rw-r--  2.0 unx     1069 b- defN 23-May-22 15:49 hyperactive-4.4.2.dist-info/LICENSE
+-rw-rw-r--  2.0 unx    40414 b- defN 23-May-22 15:49 hyperactive-4.4.2.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-May-22 15:49 hyperactive-4.4.2.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       18 b- defN 23-May-22 15:49 hyperactive-4.4.2.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     7164 b- defN 23-May-22 15:49 hyperactive-4.4.2.dist-info/RECORD
+75 files, 220448 bytes uncompressed, 50150 bytes compressed:  77.3%
```

## zipnote {}

```diff
@@ -165,17 +165,23 @@
 
 Filename: tests/test_issues/test_issue_34.py
 Comment: 
 
 Filename: tests/test_optimization_strategies/__init__.py
 Comment: 
 
+Filename: tests/test_optimization_strategies/_parametrize.py
+Comment: 
+
 Filename: tests/test_optimization_strategies/test_early_stopping.py
 Comment: 
 
+Filename: tests/test_optimization_strategies/test_search_space_pruning.py
+Comment: 
+
 Filename: tests/test_optimizers/__init__.py
 Comment: 
 
 Filename: tests/test_optimizers/_parametrize.py
 Comment: 
 
 Filename: tests/test_optimizers/test_best_results.py
@@ -198,23 +204,23 @@
 
 Filename: tests/test_warm_starts/test_warm_start.py
 Comment: 
 
 Filename: tests/test_warm_starts/test_warm_start_smbo.py
 Comment: 
 
-Filename: hyperactive-4.4.1.dist-info/LICENSE
+Filename: hyperactive-4.4.2.dist-info/LICENSE
 Comment: 
 
-Filename: hyperactive-4.4.1.dist-info/METADATA
+Filename: hyperactive-4.4.2.dist-info/METADATA
 Comment: 
 
-Filename: hyperactive-4.4.1.dist-info/WHEEL
+Filename: hyperactive-4.4.2.dist-info/WHEEL
 Comment: 
 
-Filename: hyperactive-4.4.1.dist-info/top_level.txt
+Filename: hyperactive-4.4.2.dist-info/top_level.txt
 Comment: 
 
-Filename: hyperactive-4.4.1.dist-info/RECORD
+Filename: hyperactive-4.4.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## hyperactive/__init__.py

```diff
@@ -1,12 +1,12 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
-__version__ = "4.4.1"
+__version__ = "4.4.2"
 __license__ = "MIT"
 
 
 from .hyperactive import Hyperactive
 
 
 __all__ = [
```

## hyperactive/hyperactive.py

```diff
@@ -1,44 +1,46 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 
 import copy
 import multiprocessing as mp
+import pandas as pd
+
+from typing import Union, List, Dict, Type
 
 from .optimizers import RandomSearchOptimizer
 from .run_search import run_search
 
 from .results import Results
 from .print_results import PrintResults
 from .search_space import SearchSpace
 
 
 class Hyperactive:
     def __init__(
         self,
-        verbosity=["progress_bar", "print_results", "print_times"],
-        distribution="multiprocessing",
-        n_processes="auto",
+        verbosity: list = ["progress_bar", "print_results", "print_times"],
+        distribution: str = "multiprocessing",
+        n_processes: Union[str, int] = "auto",
     ):
         super().__init__()
         if verbosity is False:
             verbosity = []
 
         self.verbosity = verbosity
         self.distribution = distribution
         self.n_processes = n_processes
 
         self.opt_pros = {}
 
     def _create_shared_memory(self, new_opt):
         if new_opt.memory == "share":
             if len(self.opt_pros) == 0:
-
                 manager = mp.Manager()
                 new_opt.memory = manager.dict()
 
             for opt in self.opt_pros.values():
                 same_obj_func = (
                     opt.objective_function.__name__
                     == new_opt.objective_function.__name__
@@ -76,29 +78,29 @@
             )
             if not isinstance(search_dim, list):
                 print("Warning", error_msg)
                 # raise ValueError(error_msg)
 
     def add_search(
         self,
-        objective_function,
-        search_space,
-        n_iter,
+        objective_function: callable,
+        search_space: Dict[str, list],
+        n_iter: int,
         search_id=None,
-        optimizer="default",
-        n_jobs=1,
-        initialize={"grid": 4, "random": 2, "vertices": 4},
-        pass_through=None,
-        callbacks={},
-        catch={},
-        max_score=None,
-        early_stopping=None,
-        random_state=None,
-        memory="share",
-        memory_warm_start=None,
+        optimizer: Union[str, type(RandomSearchOptimizer)] = "default",
+        n_jobs: int = 1,
+        initialize: Dict[str, int] = {"grid": 4, "random": 2, "vertices": 4},
+        pass_through: Dict = None,
+        callbacks: Dict[str, callable] = {},
+        catch: Dict = {},
+        max_score: float = None,
+        early_stopping: Dict = None,
+        random_state: int = None,
+        memory: Union[str, bool] = "share",
+        memory_warm_start: pd.DataFrame = None,
     ):
         self.check_list(search_space)
 
         if pass_through is None:
             pass_through = dict()
 
         optimizer = self._default_opt(optimizer)
@@ -137,15 +139,15 @@
         for _ in range(len(self.opt_pros)):
             print("")
 
         for results in self.results_list:
             nth_process = results["nth_process"]
             print_res.print_process(results, nth_process)
 
-    def run(self, max_time=None):
+    def run(self, max_time: float = None):
         for opt in self.opt_pros.values():
             opt.max_time = max_time
 
         self.results_list = run_search(
             self.opt_pros, self.distribution, self.n_processes
         )
```

## hyperactive/print_results.py

```diff
@@ -1,23 +1,23 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
+import logging
 import numpy as np
 
 indent = "  "
 
 
 class PrintResults:
     def __init__(self, opt_pros, verbosity):
         self.opt_pros = opt_pros
         self.verbosity = verbosity
 
     def _print_times(self, eval_time, iter_time, n_iter):
-
         opt_time = iter_time - eval_time
         iterPerSec = n_iter / iter_time
 
         print(
             indent,
             "Evaluation time   :",
             eval_time,
@@ -62,60 +62,108 @@
         for para_name, str_length in zip(para_names, str_lengths):
             added_spaces = max_length - str_length
             para_names_align[para_name] = " " * added_spaces
 
         return para_names_align
 
     def _print_results(
-        self, objective_function, best_score, best_para, best_iter, random_seed
+        self,
+        objective_function,
+        best_score,
+        best_para,
+        best_iter,
+        best_additional_results,
+        random_seed,
     ):
         print("\nResults: '{}'".format(objective_function.__name__), " ")
         if best_para is None:
             print(indent, "Best score:", best_score, " ")
             print(indent, "Best parameter set:", best_para, " ")
             print(indent, "Best iteration:", best_iter, " ")
 
         else:
-            para_names = list(best_para.keys())
-            para_names_align = self.align_para_names(para_names)
-
             print(indent, "Best score:", best_score, " ")
-            print(indent, "Best parameter set:")
 
-            for para_key in best_para.keys():
-                added_spaces = para_names_align[para_key]
-                print(
-                    indent,
-                    indent,
-                    "'{}'".format(para_key),
-                    "{}:".format(added_spaces),
-                    best_para[para_key],
-                    " ",
-                )
+            if best_additional_results:
+                print(indent, "Best additional results:")
+                add_results_names = list(best_additional_results.keys())
+                add_results_names_align = self.align_para_names(add_results_names)
+
+                for best_additional_result in best_additional_results.keys():
+                    added_spaces = add_results_names_align[best_additional_result]
+                    print(
+                        indent,
+                        indent,
+                        "'{}'".format(best_additional_result),
+                        "{}:".format(added_spaces),
+                        best_additional_results[best_additional_result],
+                        " ",
+                    )
+
+            if best_para:
+                print(indent, "Best parameter set:")
+                para_names = list(best_para.keys())
+                para_names_align = self.align_para_names(para_names)
+
+                for para_key in best_para.keys():
+                    added_spaces = para_names_align[para_key]
+                    print(
+                        indent,
+                        indent,
+                        "'{}'".format(para_key),
+                        "{}:".format(added_spaces),
+                        best_para[para_key],
+                        " ",
+                    )
+
             print(indent, "Best iteration:", best_iter, " ")
 
         print(" ")
         print(indent, "Random seed:", random_seed, " ")
         print(" ")
 
     def print_process(self, results, nth_process):
         verbosity = self.verbosity
         objective_function = self.opt_pros[nth_process].objective_function
-        best_score = results["best_score"]
-        best_para = results["best_para"]
-        best_iter = results["best_iter"]
-        eval_times = results["eval_times"]
-        iter_times = results["iter_times"]
-        random_seed = results["random_seed"]
-
-        n_iter = self.opt_pros[nth_process].n_iter
-
-        eval_time = np.array(eval_times).sum()
-        iter_time = np.array(iter_times).sum()
-
-        if "print_results" in verbosity:
-            self._print_results(
-                objective_function, best_score, best_para, best_iter, random_seed
+        search_space = self.opt_pros[nth_process].s_space.search_space
+
+        search_data = results["search_data"]
+
+        try:
+            best_sample = search_data.iloc[search_data["score"].idxmax()]
+
+        except TypeError:
+            logging.warning(
+                "Warning: Cannot index by location index with a non-integer key"
+            )
+
+        else:
+            best_score = best_sample["score"]
+            best_values = best_sample[list(search_space.keys())]
+            best_para = dict(zip(list(search_space.keys()), best_values))
+            best_additional_results_df = best_sample.drop(
+                ["score"] + list(search_space.keys())
             )
+            best_additional_results = best_additional_results_df.to_dict()
+
+            best_iter = results["best_iter"]
+            eval_times = results["eval_times"]
+            iter_times = results["iter_times"]
+            random_seed = results["random_seed"]
+
+            n_iter = self.opt_pros[nth_process].n_iter
+
+            eval_time = np.array(eval_times).sum()
+            iter_time = np.array(iter_times).sum()
+
+            if "print_results" in verbosity:
+                self._print_results(
+                    objective_function,
+                    best_score,
+                    best_para,
+                    best_iter,
+                    best_additional_results,
+                    random_seed,
+                )
 
-        if "print_times" in verbosity:
-            self._print_times(eval_time, iter_time, n_iter)
+            if "print_times" in verbosity:
+                self._print_times(eval_time, iter_time, n_iter)
```

## hyperactive/process.py

```diff
@@ -3,26 +3,30 @@
 # License: MIT License
 
 
 from tqdm import tqdm
 
 
 def _process_(nth_process, optimizer):
-    p_bar = tqdm(
-        position=nth_process,
-        total=optimizer.n_iter,
-        ascii=" ─",
-        colour="Yellow",
-    )
+    if "progress_bar" in optimizer.verbosity:
+        p_bar = tqdm(
+            position=nth_process,
+            total=optimizer.n_iter,
+            ascii=" ─",
+            colour="Yellow",
+        )
+    else:
+        p_bar = None
 
     optimizer.search(nth_process, p_bar)
 
-    p_bar.colour = "GREEN"
-    p_bar.refresh()
-    p_bar.close()
+    if p_bar:
+        p_bar.colour = "GREEN"
+        p_bar.refresh()
+        p_bar.close()
 
     return {
         "nth_process": nth_process,
         "best_para": optimizer.best_para,
         "best_score": optimizer.best_score,
         "best_iter": optimizer.best_since_iter,
         "eval_times": optimizer.eval_times,
```

## hyperactive/optimizers/hyper_optimizer.py

```diff
@@ -122,36 +122,38 @@
             self.max_score,
             self.early_stopping,
             self.memory,
             memory_warm_start,
             False,
         )
         for nth_iter in range(self.n_iter):
-            p_bar.set_description(
-                "["
-                + str(nth_process)
-                + "] "
-                + str(self.objective_function.__name__)
-                + " ("
-                + self.optimizer_class.name
-                + ")",
-            )
+            if p_bar:
+                p_bar.set_description(
+                    "["
+                    + str(nth_process)
+                    + "] "
+                    + str(self.objective_function.__name__)
+                    + " ("
+                    + self.optimizer_class.name
+                    + ")",
+                )
 
             self.gfo_optimizer.search_step(nth_iter)
             if self.gfo_optimizer.stop.check():
                 break
 
-            p_bar.set_postfix(
-                best_score=str(gfo_wrapper_model.optimizer.score_best),
-                best_pos=str(gfo_wrapper_model.optimizer.pos_best),
-                best_iter=str(gfo_wrapper_model.optimizer.p_bar._best_since_iter),
-            )
+            if p_bar:
+                p_bar.set_postfix(
+                    best_score=str(gfo_wrapper_model.optimizer.score_best),
+                    best_pos=str(gfo_wrapper_model.optimizer.pos_best),
+                    best_iter=str(gfo_wrapper_model.optimizer.p_bar._best_since_iter),
+                )
 
-            p_bar.update(1)
-            p_bar.refresh()
+                p_bar.update(1)
+                p_bar.refresh()
 
         self.gfo_optimizer.finish_search()
 
         self.convert_results2hyper()
 
         self._add_result_attributes(
             self.best_para,
```

## hyperactive/optimizers/strategies/custom_optimization_strategy.py

```diff
@@ -8,14 +8,15 @@
 class CustomOptimizationStrategy(BaseOptimizationStrategy):
     def __init__(self):
         super().__init__()
 
         self.optimizer_setup_l = []
         self.duration_sum = 0
 
-    def add_optimizer(self, optimizer, duration=1):
+    def add_optimizer(self, optimizer, duration=1, early_stopping=None):
         self.duration_sum += duration
         optimizer_setup = {
             "optimizer": optimizer,
             "duration": duration,
+            "early_stopping": early_stopping,
         }
         self.optimizer_setup_l.append(optimizer_setup)
```

## hyperactive/optimizers/strategies/optimization_strategy.py

```diff
@@ -59,14 +59,20 @@
         for optimizer_setup in self.optimizer_setup_l:
             optimizer_setup["optimizer"].max_time = value
 
     def search(self, nth_process, p_bar):
         for optimizer_setup in self.optimizer_setup_l:
             hyper_opt = optimizer_setup["optimizer"]
             duration = optimizer_setup["duration"]
+            opt_strat_early_stopping = optimizer_setup["early_stopping"]
+
+            if opt_strat_early_stopping:
+                early_stopping = opt_strat_early_stopping
+            else:
+                early_stopping = self.early_stopping
 
             n_iter = round(self.n_iter * duration / self.duration_sum)
 
             # initialize
             if self.best_para is not None:
                 initialize = {}
                 if "warm_start" in initialize:
@@ -94,15 +100,15 @@
                 s_space=self.s_space,
                 n_iter=n_iter,
                 initialize=initialize,
                 pass_through=self.pass_through,
                 callbacks=self.callbacks,
                 catch=self.catch,
                 max_score=self.max_score,
-                early_stopping=self.early_stopping,
+                early_stopping=early_stopping,
                 random_state=self.random_state,
                 memory=self.memory,
                 memory_warm_start=memory_warm_start,
                 verbosity=self.verbosity,
             )
 
             hyper_opt.search(nth_process, p_bar)
```

## hyperactive/optimizers/strategies/layers/search_space_pruning.py

```diff
@@ -37,14 +37,15 @@
                 margin_d = {}
                 for key in self.search_space.keys():
                     margin_d[key] = margin_1d
             else:
                 raise ValueError("2")
 
         pruned_search_space = {}
+        pruned_search_space_positions = {}
 
         for key in self.s_space.c_search_space.keys():
             best_value = self.best_para[key]
             values = self.s_space.c_search_space[key]
 
             idx = values.index(best_value)
             dim_margin = margin_d[key]
@@ -54,9 +55,11 @@
 
             if lower < 0:
                 lower = 0
             if upper > len(values) - 1:
                 upper = len(values) - 1
 
             pruned_search_space[key] = values[lower:upper]
+            pruned_search_space_positions[key] = range(lower, upper)
 
         self.s_space.c_search_space = pruned_search_space
+        self.s_space.positions = pruned_search_space_positions
```

## Comparing `hyperactive-4.4.1.dist-info/LICENSE` & `hyperactive-4.4.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `hyperactive-4.4.1.dist-info/METADATA` & `hyperactive-4.4.2.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: hyperactive
-Version: 4.4.1
+Version: 4.4.2
 Summary: A hyperparameter optimization toolbox for convenient and fast prototyping
 Home-page: https://github.com/SimonBlanke/Hyperactive
 Author: Simon Blanke
 Author-email: simon.blanke@yahoo.com
 License: MIT
 Keywords: machine learning,deep learning,optimization,data-science
 Classifier: Programming Language :: Python :: 3
@@ -982,45 +982,25 @@
   - [ ] add "prune_search_space"-method to custom optimization strategy class
   - [ ] display additional outputs from objective-function in results in command-line
   - [ ] add type hints to hyperactive-api
   
 </details>
 
 
-
-
-
-
-
-
 <details>
-<summary><b>Ideas for the Future</b></summary>
-   
-  - [ ] Experiment-tracking for search-data storage and usage
-  - [ ] Meta-Learning tool for hyperparameter optimization
+<summary><b>v4.6.0</b> </summary>
 
+  - [ ] add support for constrained optimization
+  
 </details>
 
 
 
 <br>
 
-
-## Experimental algorithms
-
-The following algorithms are of my own design and, to my knowledge, do not yet exist in the technical literature.
-If any of these algorithms already exist I would like you to share it with me in an issue.
-
-#### Random Annealing
-
-A combination between simulated annealing and random search.
-
-
-<br>
-
 ## FAQ
 
 #### Known Errors + Solutions
 
 <details>
 <summary><b> Read this before opening a bug-issue </b></summary>
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: hyperactive Version: 4.4.1 Summary: A
+Metadata-Version: 2.1 Name: hyperactive Version: 4.4.2 Summary: A
 hyperparameter optimization toolbox for convenient and fast prototyping Home-
 page: https://github.com/SimonBlanke/Hyperactive Author: Simon Blanke Author-
 email: simon.blanke@yahoo.com License: MIT Keywords: machine learning,deep
 learning,optimization,data-science Classifier: Programming Language :: Python
 :: 3 Classifier: Programming Language :: Python :: 3.5 Classifier: Programming
 Language :: Python :: 3.6 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8 Classifier: Programming
@@ -368,21 +368,16 @@
 :heavy_check_mark: - [x] add new features from GFO - [x] add Spiral
 Optimization - [x] add Lipschitz Optimizer - [x] add DIRECT Optimizer - [x]
 print the random seed for reproducibility   v4.4.0 :heavy_check_mark:  - [ ]
 add Optimization-Strategies - [ ] redesign progress-bar   v4.5.0  - [ ] add
 early stopping feature to custom optimization strategies - [ ] add
 "prune_search_space"-method to custom optimization strategy class - [ ] display
 additional outputs from objective-function in results in command-line - [ ] add
-type hints to hyperactive-api   Ideas for the Future - [ ] Experiment-tracking
-for search-data storage and usage - [ ] Meta-Learning tool for hyperparameter
+type hints to hyperactive-api   v4.6.0  - [ ] add support for constrained
 optimization
-## Experimental algorithms The following algorithms are of my own design and,
-to my knowledge, do not yet exist in the technical literature. If any of these
-algorithms already exist I would like you to share it with me in an issue. ####
-Random Annealing A combination between simulated annealing and random search.
 ## FAQ #### Known Errors + Solutions  Read this before opening a bug-issue
 - Are you sure the bug is located in Hyperactive? The error might be located in
 the optimization-backend. Look at the error message from the command line. If
 one of the last messages look like this: - File "/.../
 gradient_free_optimizers/...", line ... Then you should post the bug report in:
 - https://github.com/SimonBlanke/Gradient-Free-Optimizers
 Otherwise
```

## Comparing `hyperactive-4.4.1.dist-info/RECORD` & `hyperactive-4.4.2.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-hyperactive/__init__.py,sha256=g32vrxGIv8yeRjhs3QWrrOEbxXC_0IUJalMaTOQF0sU,195
+hyperactive/__init__.py,sha256=3VfSOq8HiwFiqfQTrmTFGjwS7EblyekecCITc3UyuUU,195
 hyperactive/distribution.py,sha256=pmiJcJ3MgzduAaPJq7ze3Jw2_Od1CHUUSY_24vhUqZY,1057
-hyperactive/hyperactive.py,sha256=JTSlzsLQWmNXpiIoB0bo4JV1fdE6ZST6pJhxg1XpAPg,4980
-hyperactive/print_results.py,sha256=bMHB9s3PIezDdeZbeLTDpwAn8oOTXKHOTXa7hvio23o,3749
-hyperactive/process.py,sha256=-eWKSKUJpPElc85r5gdHQ28MIMnfxLBw3QJvuFVHRok,754
+hyperactive/hyperactive.py,sha256=oH5IpQajmIGRlsp3aXLa2v-RtlPXklMtsUoQ2mP3q1o,5285
+hyperactive/print_results.py,sha256=a9vPkQXeZt-sQ6Ad3LIWYNjzeFAoPTyRBGs2R0ZFzEE,5516
+hyperactive/process.py,sha256=FxkRXyhrUaEr660zySCHBSXrsoldndGsMiunUT_5-iw,881
 hyperactive/results.py,sha256=Se2CQCVKUJ07UVdCu92vtUHmroDUclm4T2m8SEpDPhg,2505
 hyperactive/run_search.py,sha256=sECQFoX0tO3hby8u-febVrjo-Owuk2el_3klRusBFcQ,1416
 hyperactive/search_space.py,sha256=9j1acKdOy_TiIqa_BTlY9K4c9sdUOqVq8blC1ygBJCU,3981
 hyperactive/optimizers/__init__.py,sha256=tdppfOXjY0DcPpirI2kMAvJDdSFE7zX6oFaqT6FcIBQ,1292
 hyperactive/optimizers/dictionary.py,sha256=0SBoh_jzKTs-tf4uRk583Czd6usreQVnSh6wEq2-kMM,337
 hyperactive/optimizers/hyper_gradient_conv.py,sha256=LBrYm9pV3IDBY4RLiB_g1Ax9EEaIAlgatfRVDgYABMo,4816
-hyperactive/optimizers/hyper_optimizer.py,sha256=2tHCHJa0E6zuJYPdox99pQscgCXfdZ5x8vTa-J98L88,5107
+hyperactive/optimizers/hyper_optimizer.py,sha256=Rgm1WXh6m58LYwcsez7ulhZXMDbz6sBRkJGBXN9Ceb4,5215
 hyperactive/optimizers/objective_function.py,sha256=FWINdUMC4WFhKKV_xPN0N1EEcBlLktDVBRJrVk1y8Vo,1456
 hyperactive/optimizers/optimizer_attributes.py,sha256=YQiAsXk2Ag3ez9Fe0HZ1bq858Q21ADBRbnYpts8Bwg0,825
 hyperactive/optimizers/optimizers.py,sha256=SrSeTPi2t9AZMOfLx43J9s5a0Fb9dQvxJqfaHI9vZYE,5103
 hyperactive/optimizers/strategies/__init__.py,sha256=-XSIY9W45p81E6wLB98DkqZcUwZYK7aPFtFSK35-6Kw,199
-hyperactive/optimizers/strategies/custom_optimization_strategy.py,sha256=RCE-YM3OyNg37UFTnXLsKXdUyuZGXNMLtXnSMxczcgE,573
-hyperactive/optimizers/strategies/optimization_strategy.py,sha256=QSV2huEew7cEbXEj8jMLP5REXbFcPQ29ZoDNJlO2qCY,3581
+hyperactive/optimizers/strategies/custom_optimization_strategy.py,sha256=NiZ3G5kLXjDBnW01QAEYrFFjhaOASxpxJWFs7EEycDY,640
+hyperactive/optimizers/strategies/optimization_strategy.py,sha256=L4NXJouATu-GyU8kLA-Wn9cpaKEyv2zm5zQ-LifrC_c,3820
 hyperactive/optimizers/strategies/optimizer_attributes.py,sha256=4JQyZapCAKs6ruvEfknd0oh2v7WBgFmWf1FGREG3viY,1741
 hyperactive/optimizers/strategies/layers/__init__.py,sha256=5Ceb7BOEy-kdg2D98p_WeIJelkxT3CeboTA9ZqT-MPs,79
 hyperactive/optimizers/strategies/layers/optimizer.py,sha256=IHFu7opbSJ_XhBUjbG7F-uwK0ja7-Bnj13GOw6B03Xs,2328
-hyperactive/optimizers/strategies/layers/search_space_pruning.py,sha256=Z3XMRvWfD_2vtQ3aTyjdJ0l-rsT3hS7kI7DE5JuQKZk,1945
+hyperactive/optimizers/strategies/layers/search_space_pruning.py,sha256=NJkhYWoZp6jOV3rCuqcK1mbyaq6jEnC0b0BQJi89Ilk,2120
 tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/_test_examples.py,sha256=7QRuve6FWx7oiw39NU2Lzx1eAA3cN_Hn2POBDYpl2ig,686
 tests/test_callbacks.py,sha256=U_djPJTHztY-XtLEhmPtjOG-DDZEDs0vFYSywTLmdtM,1995
 tests/test_catch.py,sha256=Bsd_9tvSPzZU0TxVjhpVYY96_QhiDWjXQVZsbsLd5A4,2566
 tests/test_distribution.py,sha256=N0OXWPeMhz9C6yqEu6gDeWOtD_wTOqg5Mghqr8Vu8yM,4353
 tests/test_early_stop.py,sha256=apwDIOAFaczChs7M3c4YFlirX1oe1NbRjak1eTVTrbA,7035
 tests/test_hyper_gradient_trafo.py,sha256=U-So-BFqAkqEbEEljPgwduopZ-0y9Eye2p1CtVxnFJo,5196
@@ -51,23 +51,25 @@
 tests/_local_test_timings/_test_warm_start.py,sha256=YKmYVDjv8uH0RW156yDd7di2BYSdmawYtUK8sLrRI18,743
 tests/_local_test_timings/_test_warm_start_n_jobs.py,sha256=8buGbyc4AOjCoLdntjV0bJZQFXR3qjPySA2FSwWzB2c,3945
 tests/test_issues/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/test_issues/test_issue_25.py,sha256=TTOxFjqUWodHm9Fw3W72lwFGRLQjL4pIYtD2oPqa8z0,1580
 tests/test_issues/test_issue_29.py,sha256=Z5haa34Ja0y9iBkaOz6PYAqptWPu4ic8J-z0fbvWfUQ,987
 tests/test_issues/test_issue_34.py,sha256=7joD-gCmB77-xmDG2KFSiKc_aBAHCPdvi68QMcug4lU,1711
 tests/test_optimization_strategies/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+tests/test_optimization_strategies/_parametrize.py,sha256=eNeQTKW9CllItLdSXOLIBKM-Ox4UcG2tOse9voMNby0,2891
 tests/test_optimization_strategies/test_early_stopping.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+tests/test_optimization_strategies/test_search_space_pruning.py,sha256=uBrktsDuDgnew8YvIeVM5CALieBP-4uh-8Ygh1kHECk,1339
 tests/test_optimizers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/test_optimizers/_parametrize.py,sha256=_qYSVlOq_gSfx6e5U65e-UiqPZ5BKrBXW786yWkusS0,1336
 tests/test_optimizers/test_best_results.py,sha256=F8qSOY5VJp6KlntO9Qq36yib3Mfkky-_M6QBOXnbP3U,2651
 tests/test_optimizers/test_gfo_wrapper.py,sha256=fjxOTB6-mMJq6Bh2S5P1ZnnF6YW3QeaxZEs7XCPPPrU,2615
 tests/test_optimizers/test_memory.py,sha256=hjxotqtn29li7y__J1GbheEw01IYjhvq86YN-97y1jc,698
 tests/test_optimizers/test_optimization_strategies.py,sha256=qTVFU-5EdfxEYNMW7Rnsy611P82S4fVucQ54M-qaoHI,1380
 tests/test_warm_starts/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/test_warm_starts/test_memory_warm_start.py,sha256=CJLAKHAYFPUtuEQjHw8JtIpuxOvuYOuZQ596Z0ZUuT4,2421
 tests/test_warm_starts/test_warm_start.py,sha256=U7Digok8gyvKs0tuYIqCmkl2cLS3UA1sVm8a1SSH2ec,2413
 tests/test_warm_starts/test_warm_start_smbo.py,sha256=0ic7V-11QOVEplno_jQVdhmiw1DQGSU7rBLs-s5bOk0,2896
-hyperactive-4.4.1.dist-info/LICENSE,sha256=DA9r_H29OB7cmPO-wKWJ44gP2Zok54f08pBMqivlbwE,1069
-hyperactive-4.4.1.dist-info/METADATA,sha256=IxktZp-QKru7NF3vKcH3tJzy8T50-6iwGY8MgMuMZy4,40833
-hyperactive-4.4.1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-hyperactive-4.4.1.dist-info/top_level.txt,sha256=eKxBf3Mh-LzqGdk_eza4VBHt6My6tMtEljMI9hupE-8,18
-hyperactive-4.4.1.dist-info/RECORD,,
+hyperactive-4.4.2.dist-info/LICENSE,sha256=DA9r_H29OB7cmPO-wKWJ44gP2Zok54f08pBMqivlbwE,1069
+hyperactive-4.4.2.dist-info/METADATA,sha256=lQeIhLqw-VGgtEaLqiTmZ5JcpSrza6tmJh5LAWWevjI,40414
+hyperactive-4.4.2.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+hyperactive-4.4.2.dist-info/top_level.txt,sha256=eKxBf3Mh-LzqGdk_eza4VBHt6My6tMtEljMI9hupE-8,18
+hyperactive-4.4.2.dist-info/RECORD,,
```

