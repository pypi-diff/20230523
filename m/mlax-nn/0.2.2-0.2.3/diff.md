# Comparing `tmp/mlax_nn-0.2.2-py3-none-any.whl.zip` & `tmp/mlax_nn-0.2.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,12 +1,13 @@
-Zip file size: 39444 bytes, number of entries: 39
--rw-r--r--  2.0 unx      156 b- defN 23-Feb-19 08:42 mlax/__init__.py
--rw-r--r--  2.0 unx     3000 b- defN 23-Jan-16 00:22 mlax/_utils.py
+Zip file size: 41930 bytes, number of entries: 41
+-rw-r--r--  2.0 unx      125 b- defN 23-May-20 04:50 mlax/__init__.py
+-rw-r--r--  2.0 unx     1433 b- defN 23-May-23 05:09 mlax/_test_utils.py
+-rw-r--r--  2.0 unx     2717 b- defN 23-May-21 00:46 mlax/_utils.py
 -rw-r--r--  2.0 unx    15178 b- defN 22-Dec-29 06:01 mlax/functional.py
--rw-r--r--  2.0 unx     7029 b- defN 23-Feb-19 10:10 mlax/module.py
+-rw-r--r--  2.0 unx     9186 b- defN 23-May-23 05:41 mlax/module.py
 -rw-r--r--  2.0 unx     2140 b- defN 22-Dec-27 07:19 mlax/block/Parallel.py
 -rw-r--r--  2.0 unx     2645 b- defN 22-Dec-27 07:18 mlax/block/Parallel_rng.py
 -rw-r--r--  2.0 unx     2888 b- defN 22-Dec-27 07:16 mlax/block/Series.py
 -rw-r--r--  2.0 unx     3872 b- defN 22-Dec-27 07:16 mlax/block/Series_rng.py
 -rw-r--r--  2.0 unx       86 b- defN 22-Dec-06 07:42 mlax/block/__init__.py
 -rw-r--r--  2.0 unx     2519 b- defN 22-Nov-21 00:11 mlax/blocks/Linear.py
 -rw-r--r--  2.0 unx       39 b- defN 22-Nov-21 00:11 mlax/blocks/__init__.py
@@ -15,27 +16,28 @@
 -rw-r--r--  2.0 unx     4573 b- defN 22-Dec-28 02:58 mlax/nn/BatchNorm.py
 -rw-r--r--  2.0 unx     2457 b- defN 22-Dec-29 03:48 mlax/nn/Bias.py
 -rw-r--r--  2.0 unx     7145 b- defN 22-Dec-28 06:06 mlax/nn/Conv.py
 -rw-r--r--  2.0 unx     1805 b- defN 22-Dec-24 04:15 mlax/nn/F.py
 -rw-r--r--  2.0 unx     1497 b- defN 22-Dec-24 04:15 mlax/nn/F_rng.py
 -rw-r--r--  2.0 unx     3321 b- defN 22-Dec-29 05:23 mlax/nn/Linear.py
 -rw-r--r--  2.0 unx     2434 b- defN 22-Dec-29 03:48 mlax/nn/Scaler.py
--rw-r--r--  2.0 unx      326 b- defN 23-Jan-28 20:08 mlax/nn/__init__.py
+-rw-r--r--  2.0 unx      319 b- defN 23-May-12 01:14 mlax/nn/__init__.py
 -rw-r--r--  2.0 unx     4964 b- defN 23-Feb-14 07:17 mlax/nn/batchnorm.py
--rw-r--r--  2.0 unx     3171 b- defN 23-Feb-14 07:17 mlax/nn/bias.py
--rw-r--r--  2.0 unx     8117 b- defN 23-Feb-14 07:17 mlax/nn/conv.py
+-rw-r--r--  2.0 unx     2379 b- defN 23-May-23 05:07 mlax/nn/bias.py
+-rw-r--r--  2.0 unx     8635 b- defN 23-May-23 05:08 mlax/nn/conv.py
 -rw-r--r--  2.0 unx      922 b- defN 22-Nov-21 00:11 mlax/nn/dropout.py
--rw-r--r--  2.0 unx     1934 b- defN 23-Feb-14 07:17 mlax/nn/embed.py
--rw-r--r--  2.0 unx     3009 b- defN 23-Feb-14 07:17 mlax/nn/f.py
--rw-r--r--  2.0 unx    12771 b- defN 23-Feb-22 00:19 mlax/nn/functional.py
--rw-r--r--  2.0 unx     3778 b- defN 23-Feb-14 07:17 mlax/nn/linear.py
--rw-r--r--  2.0 unx     2641 b- defN 23-Feb-19 09:39 mlax/nn/parallel.py
--rw-r--r--  2.0 unx     3188 b- defN 23-Feb-14 07:17 mlax/nn/scaler.py
--rw-r--r--  2.0 unx     2404 b- defN 23-Feb-19 09:24 mlax/nn/series.py
+-rw-r--r--  2.0 unx     1691 b- defN 23-May-23 05:08 mlax/nn/embed.py
+-rw-r--r--  2.0 unx     3347 b- defN 23-May-23 05:08 mlax/nn/f.py
+-rw-r--r--  2.0 unx    13510 b- defN 23-May-22 00:22 mlax/nn/functional.py
+-rw-r--r--  2.0 unx     3126 b- defN 23-May-23 05:09 mlax/nn/linear.py
+-rw-r--r--  2.0 unx     2422 b- defN 23-May-23 05:09 mlax/nn/parallel.py
+-rw-r--r--  2.0 unx     2811 b- defN 23-May-23 05:09 mlax/nn/scaler.py
+-rw-r--r--  2.0 unx     2245 b- defN 23-May-23 05:09 mlax/nn/series.py
+-rw-r--r--  2.0 unx     4541 b- defN 23-May-23 05:09 mlax/nn/z_norm.py
 -rw-r--r--  2.0 unx      803 b- defN 22-Nov-21 00:11 mlax/optim/__init__.py
 -rw-r--r--  2.0 unx     2065 b- defN 22-Nov-21 00:11 mlax/optim/sgd.py
--rw-r--r--  2.0 unx     1065 b- defN 23-Feb-22 00:33 mlax_nn-0.2.2.dist-info/LICENSE
--rw-r--r--  2.0 unx     3530 b- defN 23-Feb-22 00:33 mlax_nn-0.2.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Feb-22 00:33 mlax_nn-0.2.2.dist-info/WHEEL
--rw-r--r--  2.0 unx        5 b- defN 23-Feb-22 00:33 mlax_nn-0.2.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2955 b- defN 23-Feb-22 00:33 mlax_nn-0.2.2.dist-info/RECORD
-39 files, 121048 bytes uncompressed, 34856 bytes compressed:  71.2%
+-rw-r--r--  2.0 unx     1065 b- defN 23-May-23 15:44 mlax_nn-0.2.3.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3964 b- defN 23-May-23 15:44 mlax_nn-0.2.3.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-23 15:44 mlax_nn-0.2.3.dist-info/WHEEL
+-rw-r--r--  2.0 unx        5 b- defN 23-May-23 15:44 mlax_nn-0.2.3.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3105 b- defN 23-May-23 15:44 mlax_nn-0.2.3.dist-info/RECORD
+41 files, 128595 bytes uncompressed, 37118 bytes compressed:  71.1%
```

## zipnote {}

```diff
@@ -1,10 +1,13 @@
 Filename: mlax/__init__.py
 Comment: 
 
+Filename: mlax/_test_utils.py
+Comment: 
+
 Filename: mlax/_utils.py
 Comment: 
 
 Filename: mlax/functional.py
 Comment: 
 
 Filename: mlax/module.py
@@ -90,29 +93,32 @@
 
 Filename: mlax/nn/scaler.py
 Comment: 
 
 Filename: mlax/nn/series.py
 Comment: 
 
+Filename: mlax/nn/z_norm.py
+Comment: 
+
 Filename: mlax/optim/__init__.py
 Comment: 
 
 Filename: mlax/optim/sgd.py
 Comment: 
 
-Filename: mlax_nn-0.2.2.dist-info/LICENSE
+Filename: mlax_nn-0.2.3.dist-info/LICENSE
 Comment: 
 
-Filename: mlax_nn-0.2.2.dist-info/METADATA
+Filename: mlax_nn-0.2.3.dist-info/METADATA
 Comment: 
 
-Filename: mlax_nn-0.2.2.dist-info/WHEEL
+Filename: mlax_nn-0.2.3.dist-info/WHEEL
 Comment: 
 
-Filename: mlax_nn-0.2.2.dist-info/top_level.txt
+Filename: mlax_nn-0.2.3.dist-info/top_level.txt
 Comment: 
 
-Filename: mlax_nn-0.2.2.dist-info/RECORD
+Filename: mlax_nn-0.2.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## mlax/__init__.py

```diff
@@ -1,10 +1,7 @@
 from mlax.module import (
     Parameter,
-    Module,
-    ModuleSeq,
-    fwd,
-    is_mlax_module,
-    is_parameter,
-    is_trainable,
-    is_non_trainable
+    is_trainable_param,
+    is_non_trainable_param,
+    is_leaf_param,
+    Module
 )
```

## mlax/_utils.py

```diff
@@ -1,106 +1,90 @@
-from jax import lax
+from jax import (
+    lax,
+    dtypes
+)
 from math import prod
 from inspect import signature
 
-def _canon_precision(precision):
-    return lax.Precision(precision)
-
 def _canon_precision_pair(precision):
     if isinstance(precision, tuple):
-        return _canon_precision(precision[0]), _canon_precision(precision[1])
+        return tuple(lax.Precision(p) for p in precision)
     else:
-        precision = _canon_precision(precision)
+        precision = lax.Precision(precision)
         return precision, precision
 
-def _canon_dtype(dtype):
-    return lax.dtype(dtype)
-
 def _canon_opt_dtype(dtype):
-    return None if dtype is None else _canon_dtype(dtype)
+    return None if dtype is None else dtypes.canonicalize_dtype(dtype)
 
-def _canon_int_sequence(int_or_seq, length):
-    return (
-        tuple([int_or_seq] * length) if isinstance(int_or_seq, int)
-        else tuple(int_or_seq)
-    )
+def _canon_int_sequence(int_or_seq, length=None):
+    if isinstance(int_or_seq, int):
+        if length is None:
+            return int(int_or_seq)
+        else:
+            return tuple([int(int_or_seq)] * length)
+    else:
+        return tuple(int(i) for i in int_or_seq)
 
-def _canon_opt_int_sequence(opt_int_or_seq, length):
+def _canon_opt_int_sequence(opt_int_or_seq, length=None):
     return (
         None if opt_int_or_seq is None else
         _canon_int_sequence(opt_int_or_seq, length)
     )
 
-def _canon_padding(padding, n_spatial_dims):
-    if padding == "VALID":
-        return tuple([(0, 0)] * n_spatial_dims)
-    elif isinstance(padding, str):
-        return padding
+def _canon_padding(padding, n_spatial_dims=None):
+    if isinstance(padding, str):
+        return str(padding)
     elif isinstance(padding, int):
-        return tuple([(padding, padding)] * n_spatial_dims)
+        if n_spatial_dims is None:
+            return int(padding)
+        else:
+            return tuple([(int(padding), int(padding))] * n_spatial_dims)
     else:
         return tuple(
-            (dim, dim) if isinstance(dim, int) else dim for dim in padding
+            (int(dim), int(dim)) if isinstance(dim, int)
+            else tuple(int(i) for i in dim)
+            for dim in padding
         )
 
-def _needs_rng(fwd):
-    # Raises exception if ``fwd`` does not have the ``rng`` keyword param
-    return signature(fwd).parameters["rng"].default is not None
-
-def _n_elems(x, dims):
-    return lax.convert_element_type(
-        prod(d for i, d in enumerate(x.shape) if i in dims),
-        x.dtype
-    )
+def _canon_norm_axis(axis):
+    if isinstance(axis, str):
+        return str(axis)
+    else:
+        return _canon_int_sequence(axis, 1)
+
+def _needs_rng(module):
+    return signature(module.apply).parameters["rng"].default is not None
+
+def _needs_axis_name(fn):
+    return "axis_name" in signature(fn).parameters.keys()
 
-def _mean(x, dims, n_elems=None):
-    if n_elems is None:
-        n_elems = _n_elems(x, dims)
-    return lax.div(
-        lax.reduce(x, 0, lax.add, dims),
+def _compute_std_stats(x, axis, norm_axis_name=()):
+    n_elems = lax.convert_element_type(lax.mul(
+        prod(d for i, d in enumerate(x.shape) if i in axis),
+        lax.psum(1, norm_axis_name)
+    ), x.dtype)
+    mean = lax.div(
+        lax.psum(lax.reduce(x, 0, lax.add, axis), norm_axis_name), n_elems
+    )
+    mean_of_squares = lax.div(
+        lax.psum(
+            lax.reduce(lax.integer_pow(x, 2), 0, lax.add, axis), norm_axis_name
+        ),
         n_elems
     )
-
-def _variance(x, dims, mean=None, mean_of_squares=None, n_elems=None):
-    if mean is None:
-        if n_elems is None:
-            n_elems = _n_elems(x, dims)
-        mean = _mean(x, dims, n_elems)
-
-    if mean_of_squares is None:
-        if n_elems is None:
-            n_elems = _n_elems(x, dims)
-        mean_of_squares = _mean(lax.integer_pow(x, 2), dims, n_elems)
-
-    return lax.sub(
-        mean_of_squares,
-        lax.integer_pow(mean, 2) # Square of means
+    variance = lax.max(
+        lax.convert_element_type(0, x.dtype),
+        lax.sub(mean_of_squares, lax.integer_pow(mean, 2))
     )
+    return mean, variance
 
-def _normalize(x, dims, eps=1e-5, mean=None, variance=None, n_elems=None):
-    if mean is None:
-        if n_elems is None:
-            n_elems = _n_elems(x, dims)
-        mean = _mean(x, dims, n_elems)
-
-    if variance is None:
-        variance = _variance(x, dims, mean=mean, n_elems=n_elems)
-
-    broadcast_dims = [i for i in range(x.ndim) if i not in dims]
-
+def _standadize(x, axis, mean, variance, epsilon=1e-05):
+    broadcast_dims = [i for i in range(x.ndim) if i not in axis]
     return lax.mul(
-        lax.sub(
-            x,
-            lax.broadcast_in_dim(mean, x.shape, broadcast_dims)
-        ),
+        lax.sub(x, lax.broadcast_in_dim(mean, x.shape, broadcast_dims)),
         lax.broadcast_in_dim(
             lax.rsqrt(
-                lax.add(
-                    variance,
-                    lax.convert_element_type(
-                        eps, x.dtype
-                    )
-                )
+                lax.add(variance, lax.convert_element_type(epsilon, x.dtype))
             ),
             x.shape, broadcast_dims 
         )
-    )
+    )
```

## mlax/module.py

```diff
@@ -1,215 +1,246 @@
-import jax
 from jax import (
-    tree_util as jtu
+    Array,
+    tree_util as jtu,
+    random
 )
-from typing import Any, Union, Hashable, Callable, Iterable
 from abc import ABCMeta
-from collections.abc import MutableSequence
+from typing import Any, Optional, Tuple, Union, Hashable
 
 @jtu.register_pytree_node_class
 class Parameter:
-    """PyTree wrapper around a valid JAX object with metadata."""
+    """PyTree wrapper around a valid JAX object and metadata."""
 
-    def __init__(self, trainable: bool, data: Any = None, name: Hashable = None) -> None:
+    def __init__(self, trainable: Optional[bool], data: Any=None):
         """Initialize parameter.
-        
+
         :param trainable: Whether the parameter is trainable or non-trainable.
+            If None, indicates that the data field contains nested parameters.
         :param data: The content of parameter. Must be a valid JAX type or a
-            PyTree of valid JAX types, but cannot contain submodules.
+            PyTree of valid JAX types.
             Default: None.
-        :param name: Additional metadata, must be hashable. Default: None.
         """
         super().__init__()
         self.trainable = trainable
         self.data = data
-        self.name = name
 
     def tree_flatten(self):
         """Flatten into a valid JAX object and auxiliary metadata."""
-        return (self.data,), (self.trainable, self.name)
+        meta_names = []
+        meta_values = []
+        for name, value in vars(self).items():
+            if name != "data" and name != "trainable":
+                meta_names.append(name)
+                meta_values.append(value)
+
+        return (self.data,), (
+            self.trainable, tuple(meta_names), tuple(meta_values)
+        )
 
     @classmethod
     def tree_unflatten(cls, aux, children):
         """Unflatten a valid JAX object and auxiliary metadata."""
-        trainable, name = aux
-        return cls(trainable, *children, name)
+        trainable, meta_names, meta_values = aux
+        self = cls(trainable, *children)
+        for name, value in zip(meta_names, meta_values):
+            object.__setattr__(self, name, value)
+        return self
 
     def __repr__(self) -> str:
-        return f"Parameter(trainable={self.trainable}, data={self.data}, name={self.name})"
+        return f"Parameter(trainable={self.trainable}, data={self.data})"
 
-def is_parameter(p) -> bool:
-    """Whether ``p`` is a Parameter."""
-    return isinstance(p, Parameter)
-
-def is_trainable(p) -> bool:
-    """Whether ``p`` is trainable."""
-    return p.trainable
-
-def is_non_trainable(p) -> bool:
-    """Whether ``p`` is non_trainable."""
-    return p.non_trainable
+def is_trainable_param(p):
+    """Whether ``p`` is a parameter whose ``trainable is True``."""
+    return isinstance(p, Parameter) and p.trainable is True
+
+def is_non_trainable_param(p):
+    """Whether ``p`` is a parameter whose ``trainable is False``."""
+    return isinstance(p, Parameter) and p.trainable is False
+
+def is_leaf_param(p):
+    """Whether ``p`` is a parameter whose ``trainable is not None``."""
+    return isinstance(p, Parameter) and p.trainable is not None
 
 class _ModuleMeta(ABCMeta):
     """Registers all modules as a PyTree"""
-    def __new__(mcs, name, bases, attrs):
-        cls = super().__new__(mcs, name, bases, attrs)
-        jtu.register_pytree_node_class(cls)
+    def __new__(mcs, name, bases, namespace, **kwargs):
+        cls = super().__new__(mcs, name, bases, namespace, **kwargs)
+        jtu.register_pytree_with_keys_class(cls)
         return cls
 
 class Module(metaclass=_ModuleMeta):
-    """MLAX layer base class. PyTree of parameters with hyperparameters as
-    auxiliary data.
+    """MLAX layer base class. PyTree of `mlax.Parameters`.
     """
 
-    def __init__(self)  -> None:
-        """Initialize module."""
-        super().__init__()
+    def __init__(self) -> None:
+        """Initialize module hyperparameters."""
+        self.initialized = False
 
-    def tree_flatten(self):
+    def tree_flatten_with_keys(self):
         """Flatten into parameters and auxiliary hyperparameters."""
         param_names = []
         param_values = []
         hyperparam_names = []
         hyperparam_values = []
         for name, value in vars(self).items():
-            if is_parameter(value) or is_mlax_module(value):
-                param_names.append(name)
-                param_values.append(value)
-            else:
-                hyperparam_names.append(name)
-                hyperparam_values.append(value)
+            if name != "initialized":
+                if isinstance(value, (Parameter, Module)):
+                    param_names.append(name)
+                    param_values.append((name, value))
+                else:
+                    hyperparam_names.append(name)
+                    hyperparam_values.append(value)
         return param_values, (
-            param_names,
-            hyperparam_names,
-            hyperparam_values
+            param_names, hyperparam_names, hyperparam_values, self.initialized
         )
 
     @classmethod
     def tree_unflatten(cls, aux, param_values):
         """Unflatten parameters and auxiliary hyperparameters."""
-        param_names, hyperparam_names, hyperparam_values = aux
+        param_names, hyperparam_names, hyperparam_values, initialized = aux
         self = cls.__new__(cls)
         for name, value in zip(param_names, param_values):
             object.__setattr__(self, name, value)
         for name, value in zip(hyperparam_names, hyperparam_values):
             object.__setattr__(self, name, value)
+        object.__setattr__(self, "initialized", initialized)
         return self
 
-    def __call__(self, x, rng, inference_mode=False):
-        """Forward pass."""
+    def init(self, x: Any) -> None:
+        """Initialize paramters and put ``self`` into a valid state for
+        ``apply``. ``self`` is not guaranteed to be fully initialized until
+        ``apply`` is called.
+
+        :param x: Compatible input features.
+        """
         raise NotImplementedError()
 
-    def __repr__(self) -> str:
-        repr = self.__class__.__name__ + "("
-        for name, value in vars(self).items():
-            repr += f"{name}={value}, "
-        return repr[:-1] + ")"
+    def apply(
+        self,
+        x: Any,
+        rng: Optional[Array],
+        inference_mode: bool = False,
+        batch_axis_name: Union[Hashable, Tuple[Hashable]] = ()
+    ) -> Tuple[Any, Any]:
+        """Perform the forward pass assuming ``init`` had been called.
+
+        :param x: Compatible input features.
+        :param rng: PRNG key. Only necessary for some modules.
+        :param inference_mode: Whether in inference or training mode. Default:
+            training mode.
+        :param batch_axis_name: Hashable or tuple of hashable representing
+            the batch axis name(s) when called in a `jax.vmap` or `jax.pmap`
+            context. Used by modules such as `ZNorm` to normalize along the
+            batch axis. Default: (), no batch axis.
+
+        :returns: Output features.
+
+        .. note::
+            When overriding ``rng``, set its default value to None if a key is
+            not required. MLAX uses this information to avoid splitting and
+            passing keys to modules that do not need them.
+        """
+        raise NotImplementedError()
     
-    def map(self, f: Callable, *rest):
-        """Apply a map function ``f`` on ``self``'s parameters. Equivalent to
-        ``jax.tree_utils.tree_map(self, *rest, is_leaf=is_parameter)``.
+    def __call__(
+        self,
+        x: Any,
+        rng: Optional[Array],
+        inference_mode: bool = False,
+        batch_axis_name: Union[Hashable, Tuple[Hashable]] = ()
+    ) -> Tuple[Any, Any]:
+        """Perform the forward pass, initializing ``self`` if needed.
+
+        :param x: Compatible input features.
+        :param rng: PRNG key. Only necessary for some modules.
+        :param inference_mode: Whether in inference or training mode. Default:
+            training mode.
+        :param batch_axis_name: Hashable or tuple of hashable representing
+            the batch axis name(s) when called in a `jax.vmap` or `jax.pmap`
+            context. Used by modules such as `ZNorm` to normalize along the
+            batch axis. Default: (), no batch axis.
+
+        :returns: Output features.
+        :returns: ``self``.
         """
-        return jtu.tree_map(f, self, *rest, is_leaf=is_parameter)
+        if self.initialized is False:
+            self.init(x)
+            self.initialized = True
+        return self.apply(x, rng, inference_mode, batch_axis_name), self
 
-    def filter(self, f: Callable[[Parameter], bool], *rest):
+    def filter(self, f=is_trainable_param, inverse=False) -> Any:
         """Apply a filter ``f`` on ``self``'s parameters. Filtered out
-        parameters are replaced with a Parameter whose ``trainable = None``.
+        parameters have their ``data`` field replaced with None.
         """
-        return jtu.tree_map(
-            _create_true_filter(f), self, *rest, is_leaf=is_parameter
-        )
 
-    def partition(self, f: Callable[[Parameter], bool] = is_trainable, *rest):
-        """Partition on ``self``'s parameters on filter ``f`` on ``self``'s
-        parameters. Unselected parameters are replaced with a Parameter whose
-        ``trainable = None``.
-        """
-        return (
-            jtu.tree_map(
-                _create_true_filter(f), self, *rest, is_leaf=is_parameter
-            ),
-            jtu.tree_map(
-                _create_false_filter(f), self, *rest, is_leaf=is_parameter
-            )
-        )
+        if self.initialized is False:
+            raise AttributeError("cannot filter an uninitialized module")
 
-    def combine(self, *rest):
-        """Combine ``rest``'s parameters with ``self``'s, with preceding args'
-        possibly overriding subsequent args' and ``self``'s.
+        def _filter(arg):
+            arg_copy = jtu.tree_map(lambda x: x, arg)
+            if (not f(arg_copy) if inverse else f(arg_copy)):
+                return arg_copy
+            else:
+                arg_copy.data = None
+                return arg_copy
+        return jtu.tree_map(_filter, self, is_leaf=is_leaf_param)
+
+
+    def partition(self, f=is_trainable_param) -> Tuple[Any, Any]:
+        """Partition on ``self``'s parameters on filter ``f``. Unselected
+        parameters have their ``data`` field replaced with None.
         """
-        def _combine(*args):
-            for arg in args:
-                if arg.trainable is not None:
-                    return arg
-            return args[-1]
+        if self.initialized is False:
+            raise AttributeError("cannot partition an uninitialized module")
 
-        return jtu.tree_map(
-            _combine, *rest, self, is_leaf=is_parameter
-        )
+        return (self.filter(f, inverse=False), self.filter(f, inverse=True))
 
-def fwd(
-    trainables,
-    non_trainables,
-    x: Any,
-    rng: jax.Array = None,
-    inference_mode: bool=False
-):
-    """Combine ``trainables`` with ``non_trainables`` and invoke ``call``."""
-    module = trainables.combine(non_trainables)
-    return module(x, rng, inference_mode)
-
-def is_mlax_module(m) -> bool:
-    """Whether ``m`` is a module"""
-    return isinstance(m, Module)
-
-EMPTY_PARAM = Parameter(trainable=None, data=None, name=None)
-
-def _create_true_filter(f):
-    def _filter_true(*args):
-        return args[0] if f(*args) else EMPTY_PARAM
-    return _filter_true
-
-def _create_false_filter(f):
-    def _filter_false(*args):
-        return EMPTY_PARAM if f(*args) else args[0]
-    return _filter_false
+    def filter_with_path(self, f, inverse=False) -> Any:
+        """``filter`` with path."""
 
-class ModuleSeq(Module, MutableSequence):
-    """A container containing a mutable sequence of submodules or parameters."""
+        if self.initialized is False:
+            raise AttributeError("cannot filter an uninitialized module")
 
-    def __init__(self, submodules: Iterable[Union[Module, Parameter]]) -> None:
-        """Initialize module."""
-        super().__init__()
-        self._submodules = list(submodules)
+        def _filter_w_path(path, arg):
+            arg_copy = jtu.tree_map(lambda x: x, arg)
+            if (not f(path, arg_copy) if inverse else f(path, arg_copy)):
+                return arg_copy
+            else:
+                arg_copy.data = None
+                return arg_copy
+        return jtu.tree_map_with_path(
+            _filter_w_path, self, is_leaf=is_leaf_param
+        )
 
-    def tree_flatten(self):
-        """Flatten into submodules."""
-        return self._submodules, ()
+    def partition_with_path(self, f) -> Tuple[Any, Any]:
+        """``partition`` with path."""
 
-    @classmethod
-    def tree_unflatten(cls, _, submodules):
-        """Unflatten submodules."""
-        self = cls.__new__(cls)
-        object.__setattr__(self, "_submodules", list(submodules))
-        return self
+        if self.initialized is False:
+            raise AttributeError("cannot partition an uninitialized module")
+
+        return (
+            self.filter_with_path(f, inverse=False),
+            self.filter_with_path(f, inverse=True)
+        )
+
+    def combine(self, *rest):
+        """Combine ``self``'s parameters with ``rest``'s."""
+        def _combine(*args):
+            arg_copy = jtu.tree_map(lambda x: x, args[0])
+            for arg in args[1:]:
+                if isinstance(arg, Parameter) and arg.data is not None:
+                    arg_copy.data = arg.data
+                    break
+            return arg_copy
+        return jtu.tree_map(_combine, self, *rest, is_leaf=is_leaf_param)
 
     def __repr__(self) -> str:
-        return f"{self.__class__}({self._submodules})"
+        string = self.__class__.__name__ + "("
+        for name, value in vars(self).items():
+            string += f"{name}={value}, "
+        return string[:-2] + ")"
 
-    def __len__(self):
-        return len(self._submodules)
-    
-    def __getitem__(self, i):
-        if isinstance(i, slice):
-            return self.__class__(self._submodules[i])
+    def __delattr__(self, __name: str) -> None:
+        if self.initialized is True:
+            raise AttributeError("cannot delete attribute of an initialized module")
         else:
-            return self._submodules[i]
-    
-    def __delitem__(self, i):
-        del self._submodules[i]
-    
-    def __setitem__(self, i, val):
-        self._submodules[i] = val
-
-    def insert(self, i, val):
-        self._submodules.insert(i, val)
+            super().__delattr__(__name)
```

## mlax/nn/__init__.py

```diff
@@ -1,9 +1,9 @@
 from mlax.nn.linear import Linear
 from mlax.nn.bias import Bias
 from mlax.nn.scaler import Scaler
 from mlax.nn.conv import Conv
-from mlax.nn.batchnorm import BatchNorm
+from mlax.nn.z_norm import ZNorm
 from mlax.nn.f import F, FRng
 from mlax.nn.series import Series, SeriesRng
 from mlax.nn.parallel import Parallel, ParallelRng
 from mlax.nn.embed import Embed
```

## mlax/nn/bias.py

```diff
@@ -1,95 +1,69 @@
-from mlax import Parameter, Module
 from jax import (
+    Array,
     numpy as jnp,
     nn,
-    lax
-)
-from typing import Any, Sequence, Optional, Union
-from mlax._utils import (
-    _canon_int_sequence,
-    _canon_dtype
+    lax,
+    dtypes
 )
-
+from typing import Sequence, Union, Tuple, Hashable, Any
+from mlax import Parameter, Module
+from mlax._utils import _canon_int_sequence
 
 class Bias(Module):
     """Bias addition layer."""
     def __init__(
         self,
-        rng: Any,
-        in_features: Union[int, Sequence[Optional[int]]],
+        rng: Array,
+        in_features: Union[int, Sequence[int]],
         bias_initializer=nn.initializers.zeros,
         dtype=jnp.float32
     ):
         """Initialize a bias layer.
 
-        :param rng: PRNG key for weight initialization.
-        :param in_features: Integer or sequence of optional integers indicating
-            the shape of the input features to add bias to. Empty sequence
-            indicates a scalar bias. For non-scaler biases, use ``None`` on axes
-            that do not require a bias, use ``1`` on axes that require a single
-            bias term, and ``-1`` or ``axis_length`` on axes that require a bias
-            term for each of their ``axis_length`` elements. A single integer
-            is interpreted as a sequence of one.
+        :param rng: PRNG key.
+        :param in_features: Integer or sequence of integers indicating the shape
+            of the input features to add bias to. Empty sequence indicates a
+            scalar bias. For non-scaler biases, use ``0`` on axes that do not
+            require a bias, use ``1`` on axes that require a single bias term,
+            and ``-1`` or ``axis_length`` on axes that require a bias term for
+            each of their ``axis_length`` elements. A single integer is
+            interpreted as a sequence of one.
         :param bias_initializer: Bias initializer as defined by
             `jax.nn.initalizers <https://jax.readthedocs.io/en/latest/jax.nn.initializers.html>`_.
             Default: zeros.
-        :param dtype: Dtype of initialized bias weight. Default: float32.
+        :param dtype: Type of initialized parameters. Default: float32.
         """
         super().__init__()
-        self.initialized = False
 
-        self._rng = Parameter(trainable=False, data=rng)
-        self._in_features = _canon_int_sequence(in_features, 1)
-        self._bias_initializer = bias_initializer
-        self._dtype = _canon_dtype(dtype)
-
-        self.bias_weight = Parameter(trainable=True)
-        self.bias_broadcast_dims = None
-
-    def _build(self, x):
-        """Initialize an uninitialized bias layer."""
-        self.bias_broadcast_dims = tuple(
-            i for i, axis in enumerate(self._in_features)
-            if axis is not None
-        )
+        self.rng = rng
+        self.in_features = _canon_int_sequence(in_features, 1)
+        self.bias_initializer = bias_initializer
+        self.dtype = dtypes.canonicalize_dtype(dtype)
 
+        self.bias_kernel = Parameter(trainable=True)
+
+    def init(self, x: Array) -> None:
         bias_shape = [
-            axis if axis != -1 else x.shape[axis]
-            for axis in self._in_features if axis is not None
+            axis if axis != -1 else x.shape[i]
+            for i, axis in enumerate(self.in_features) if axis != 0
         ]
-        self.bias_weight.data = self._bias_initializer(
-            self._rng.data,
-            bias_shape,
-            self._dtype
+        self.bias_kernel.data = self.bias_initializer(
+            self.rng, bias_shape, self.dtype
         )
 
-        del self._rng
-        del self._in_features
-        del self._bias_initializer
-        del self._dtype
-
-        self.initialized = True
-    
-    def __call__(self, x, rng=None, inference_mode=False):
-        """Add bias to input features.
-
-        :param x: Input features. Must be of the shape ``in_feature_shape``.
-        :param rng: PRNG key. Ignored. Default: None.
-        :param inference_mode: Whether in inference or training mode. Ignored.
-            Default: False.
-        
-        :returns y: ``x`` plus bias.
-        :returns: Bias layer with updated state. Possibly the same object as
-            ``self``.
-        """
-        if not self.initialized:
-            self._build(x)
-
+    def apply(
+        self,
+        x: Array,
+        rng: None=None,
+        inference_mode: bool=False,
+        batch_axis_name: Union[Hashable, Tuple[Hashable]]=()
+    ) -> Tuple[Array, Any]:
+        """Add bias to input features."""
         return lax.add(
             x,
             lax.broadcast_in_dim(
-                lax.convert_element_type(self.bias_weight.data, x.dtype),
+                lax.convert_element_type(self.bias_kernel.data, x.dtype),
                 x.shape,
-                self.bias_broadcast_dims
+                [i for i, axis in enumerate(self.in_features) if axis != 0]
             )
-        ), self
+        )
```

## mlax/nn/conv.py

```diff
@@ -1,49 +1,47 @@
-from mlax import Parameter, Module
 from jax import (
+    Array,
     numpy as jnp,
     nn,
-    lax
+    lax,
+    dtypes
 )
 from functools import reduce
-from typing import Tuple, Any, Sequence, Union, Optional
+from typing import Tuple, Sequence, Union, Optional, Hashable, Any
+from mlax import Parameter, Module
 from mlax._utils import (
     _canon_int_sequence,
     _canon_opt_int_sequence,
     _canon_padding,
-    _canon_dtype,
     _canon_opt_dtype,
     _canon_precision_pair
 )
 
-
 class Conv(Module):
     """Convolution transformation layer."""
     def __init__(
         self,
-        rng: Any,
-        n_spatial_dims: int,
+        rng: Array,
         out_channels: int,
         filter_shape: Union[int, Sequence[int]],
         strides: Union[int, Sequence[int]] = 1,
-        padding: Union[str, int, Sequence[Union[int, Tuple[int, int]]]] = "VALID",
-        input_dilation: Optional[Union[int, Sequence[int]]] = None,
-        filter_dilation: Optional[Union[int, Sequence[int]]] = None,
-        feature_group_count: int = 1,
-        batch_group_count: int = 1,
-        channel_last: bool=False,
+        padding: Union[str, int, Sequence[Union[int, Tuple[int, int]]]]="VALID",
+        input_dilation: Optional[Union[int, Sequence[int]]]=None,
+        filter_dilation: Optional[Union[int, Sequence[int]]]=None,
+        feature_group_count: int=1,
+        batch_group_count: int=1,
+        data_format: Union[str, Tuple[str, str, str]]="channel_last",
         precision=None,
         accum_dtype=None,
         kernel_initializer=nn.initializers.glorot_uniform(),
         dtype=jnp.float32
     ):
         """Initialize a Conv layer.
 
-        :param rng: PRNG key for weight initialization.
-        :param n_spatial_dims: Number of input spatial dimensions.
+        :param rng: PRNG key.
         :param out_channels: Number of desired output channels.
         :param filter_shape: An integer or a sequence of ``n_spatial_dims``
             integers, specifying the shape of the filters. A single integer
             specifies the same value for all spatial dimensions.
         :param strides: An integer or a sequence of ``n_spatial_dims`` integers,
             specifying the strides of the convolution along the spatial
             dimensions. A single integer specifies the same value for all
@@ -65,136 +63,142 @@
             dilation rate. See the ``rhs_dilation`` parameter of
             `jax.lax.conv_general_dilated`_. Default: None, no filter dilation.
         :param feature_group_count: See the ``feature_group_count`` parameter of
             `jax.lax.conv_general_dilated`_. Can be used to perform group and
             seperable convolutions. Default: 1.
         :param batch_group_count: See the ``batch_group_count`` parameter of
             `jax.lax.conv_general_dilated`_. Default: 1.
-        :param channel_last: Whether features are channel-last or first. Default:
-            False, channel-first.
+        :param data_format: "channel_last", "channel_first", or a 3-tuple of
+            strings as described in ``jax.lax.conv_general_dilated`` but without
+            the batch axis "N".
         :param precision: See the ``precision`` parameter of
             `jax.lax.conv_general_dilated`_. Default: None.
         :param accum_dtype: See the ``preferred_element_type`` parameter of
             `jax.lax.conv_general_dilated`_. Default: None.
         :param kernel_initializer: Initializer for kernel of format "N..IO" as
             defined by ``jax.nn.initalizers <https://jax.readthedocs.io/en/latest/jax.nn.initializers.html>``.
             Default:: glorot uniform.
-        :param dtype: Type of initialized kernel weight. Default: None.
-            ``kernel_initializer``'s default.
+        :param dtype: Type of initialized parameters. Default: float32.
         """
         super().__init__()
-        self.initialized = False
-
-        self._rng =  Parameter(trainable=False, data=rng)
-        self._n_spatial_dims = n_spatial_dims
-        self._out_channels = out_channels
-        self._filter_shape = _canon_int_sequence(
-            filter_shape, self._n_spatial_dims
-        )
-        self._kernel_initializer = kernel_initializer
-        self._dtype = _canon_dtype(dtype)
 
-        self.kernel_weight = Parameter(trainable=True)
-        self.strides = _canon_int_sequence(strides, self._n_spatial_dims)
-        self.padding = _canon_padding(padding, self._n_spatial_dims)
-        self.input_dilation = _canon_opt_int_sequence(
-            input_dilation, self._n_spatial_dims
+        self.rng = rng
+        self.out_channels = int(out_channels)
+        self.filter_shape = _canon_int_sequence(filter_shape)
+        self.strides = _canon_int_sequence(strides)
+        self.padding = _canon_padding(padding)
+        self.input_dilation = _canon_opt_int_sequence(input_dilation)
+        self.filter_dilation = _canon_opt_int_sequence(filter_dilation)
+        self.feature_group_count = int(feature_group_count)
+        self.batch_group_count = int(batch_group_count)
+        self.data_format = (
+            str(data_format) if isinstance(data_format, str)
+            else tuple(str(s) for s in data_format[:3])
         )
-        self.filter_dilation = _canon_opt_int_sequence(
-            filter_dilation, self._n_spatial_dims
-        )
-        self.feature_group_count = feature_group_count
-        self.batch_group_count = batch_group_count
-        self.channel_last = channel_last
         self.precision = _canon_precision_pair(precision)
         self.accum_dtype = _canon_opt_dtype(accum_dtype)
+        self.kernel_initializer = kernel_initializer
+        self.dtype = dtypes.canonicalize_dtype(dtype)
 
-        chars = reduce(
-            lambda a, b: a + chr(97 + b),
-            range(self._n_spatial_dims),
-            ""
-        ) # ab...
-        if self.channel_last:
-            io_spec = "N" + chars + "C" # Nab...C
-            kernel_spec = "O" + chars + "I" # Oab...I
-        else:
-            io_spec = "NC" + chars # NCab...
-            kernel_spec = "OI" + chars # OIab...
-        dummy_shape = [None] * (self._n_spatial_dims + 2)
-        self.dimension_numbers = lax.conv_dimension_numbers(
-            dummy_shape, dummy_shape,
-            (io_spec, kernel_spec, io_spec)
-        )
-        
-    def _build(self, x):
-        """Initialize an uninitialized conv layer."""
-        if self.channel_last:
-            kernel_weight = self._kernel_initializer(
-                self._rng.data,
-                (*self._filter_shape, x.shape[-1], self._out_channels),
-                self._dtype
+        self.conv_kernel = Parameter(trainable=True)
+        self.dimension_numbers = None
+
+    def init(self, x: Array) -> None:
+        n_spatial_dims = x.ndim - 1
+        filter_shape = _canon_int_sequence(self.filter_shape, n_spatial_dims)
+        if isinstance(self.data_format, tuple):
+            i_spec, kernel_spec, o_spec = self.data_format
+            dims_map = {}
+            dim = 0
+            for c in i_spec:
+                if c == "C":
+                    channel_dim = dim
+                else:
+                    dims_map[c] = dim
+                    dim += 1
+
+            self.conv_kernel.data = self.kernel_initializer(
+                self.rng,
+                [*filter_shape, x.shape[channel_dim], self.out_channels],
+                self.dtype
             )
-            self.kernel_weight.data = lax.transpose(
-                kernel_weight,
-                (
-                    self._n_spatial_dims + 1,
-                    *range(self._n_spatial_dims),
-                    self._n_spatial_dims
-                )
+            self.conv_kernel.data = lax.transpose(
+                self.conv_kernel.data,
+                [
+                    n_spatial_dims + 1 if c == "O" else
+                    n_spatial_dims if c == "I" else
+                    dims_map[c] for c in self.data_format[1]
+                ]
             )
         else:
-            kernel_weight = self._kernel_initializer(
-                self._rng.data,
-                (*self._filter_shape, x.shape[0], self._out_channels),
-                self._dtype
-            )
-            self.kernel_weight.data = lax.transpose(
-                kernel_weight,
-                (
-                    self._n_spatial_dims + 1,
-                    self._n_spatial_dims,
-                    *range(self._n_spatial_dims)
+            chars = reduce(
+                lambda a, b: a + chr(97 + b),
+                range(n_spatial_dims),
+                ""
+            ) # ab...
+
+            if self.data_format == "channel_last":
+                self.conv_kernel.data = self.kernel_initializer(
+                    self.rng,
+                    [*filter_shape, x.shape[-1], self.out_channels],
+                    self.dtype
                 )
-            )
+                self.conv_kernel.data = lax.transpose(
+                    self.conv_kernel.data,
+                    [
+                        n_spatial_dims + 1,
+                        *range(n_spatial_dims),
+                        n_spatial_dims
+                    ]
+                )
+                i_spec = chars + "C" # ab...C
+                kernel_spec = "O" + chars + "I" # Oab...I
+                o_spec = i_spec
+            elif self.data_format == "channel_first":
+                self.conv_kernel.data = self.kernel_initializer(
+                    self.rng,
+                    [*filter_shape, x.shape[0], self.out_channels],
+                    self.dtype
+                )
+                self.conv_kernel.data = lax.transpose(
+                    self.conv_kernel.data,
+                    [
+                        n_spatial_dims + 1,
+                        n_spatial_dims,
+                        *range(n_spatial_dims)
+                    ]
+                )
+                i_spec = "C" + chars # Cab...
+                kernel_spec = "OI" + chars # OIab...
+                o_spec = i_spec
 
-        del self._rng
-        del self._n_spatial_dims
-        del self._out_channels
-        del self._filter_shape
-        del self._kernel_initializer
-        del self._dtype
-
-        self.initialized = True
-
-    
-    def __call__(self, x, rng=None, inference_mode=False):
-        """Apply convolutions on input features.
-
-        :param x: Input features. Must be unbatched thus having
-            ``n_spatial_dims + 1`` dimensions and be compatible with
-            ``channel_last``.
-        :param rng: PRNG key. Ignored. Default: None.
-        :param inference_mode: Whether in inference or training mode. Ignored.
-            Default: False.
-        
-        :returns: Convolution on ``x``.
-        :returns: Conv layer with updated state. Possibly the same object as
-            ``self``.
-        """
-        if not self.initialized:
-            self._build(x)
+        i_spec = "N" + i_spec
+        o_spec = "N" + o_spec
+        self.dimension_numbers = lax.conv_dimension_numbers(
+            lax.broadcast(x, (1,)).shape, self.conv_kernel.data.shape,
+            (i_spec, kernel_spec, o_spec)
+        )
 
+    def apply(
+        self,
+        x: Array,
+        rng: None=None,
+        inference_mode: bool=False,
+        batch_axis_name: Union[Hashable, Tuple[Hashable]]=()
+    ) -> Tuple[Array, Any]:
+        """Apply convolutions on input features."""
+        n_spatial_dims = x.ndim - 1
         x = lax.broadcast(x, (1,))
         x = lax.conv_general_dilated(
             x,
-            lax.convert_element_type(self.kernel_weight.data, x.dtype),
-            self.strides,
-            self.padding,
-            self.input_dilation,
-            self.filter_dilation,
+            lax.convert_element_type(self.conv_kernel.data, x.dtype),
+            _canon_int_sequence(self.strides, n_spatial_dims),
+            _canon_padding(self.padding, n_spatial_dims),
+            _canon_opt_int_sequence(self.input_dilation, n_spatial_dims),
+            _canon_opt_int_sequence(self.filter_dilation, n_spatial_dims),
             self.dimension_numbers,
             self.feature_group_count,
             self.batch_group_count,
             self.precision,
             self.accum_dtype
         )
-        return lax.squeeze(x, (0,)), self
+        return lax.squeeze(x, (0,))
```

## mlax/nn/embed.py

```diff
@@ -1,53 +1,53 @@
 from mlax import Parameter, Module
 from jax import (
+    Array,
     numpy as jnp,
-    nn
+    nn,
+    dtypes
 )
+from typing import Any, Tuple, Union, Hashable
 
 class Embed(Module):
     """Embedding layer."""
     def __init__(
         self,
-        rng,
+        rng: Array,
         vocab_size: int,
         embed_dim: int,
         embed_initializer=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal', in_axis=1, out_axis=0),
         dtype=jnp.float32
     ):
         """Initialize an embedding layer.
  
-        :param rng: PRNG key for weight initialization.
+        :param rng: PRNG key.
         :vocab_size: Size of the vocabulary to embed.
         :embed_dim: Size of each embedding.
         :embed_inititializer: Initializer for embedding weight of shape
             ``(vocab_size, embed_dim)`` as defined by
             `jax.nn.initalizers <https://jax.readthedocs.io/en/latest/jax.nn.initializers.html>`_.
             Default: normal variance scaling.
-        :param dtype: Dtype of initialized embedding weight. Default: float32.
+        :param dtype: Type of initialized parameters. Default: float32.
         """
         super().__init__()
-        self.embed_weight = Parameter(
-            trainable=True,
-            data=embed_initializer(
-                rng,
-                (vocab_size, embed_dim),
-                dtype
-            )
-        )
-    
-    def __call__(self, x, rng=None, inference_mode=False):
-        """Convert sequence of tokens into embeddings.
 
-        :param x: Input tokens. Must be unbatched and of the shape
-            ``(sequence_length,)``. Must be of an integer-like dtype.
-        :param rng: PRNG key. Ignored. Default: None.
-        :param inference_mode: Whether in inference or training mode. Ignored.
-            Default: False.
+        self.rng = rng
+        self.vocab_size = int(vocab_size)
+        self.embed_dim = int(embed_dim)
+        self.embed_initializer = embed_initializer
+        self.dtype = dtypes.canonicalize_dtype(dtype)
 
-        :returns: Tokens converted to embeddings. Out-of-bound tokens are
-            converted to NaNs for inexact types and minimum values for exact
-            types.
-        :returns: Embed layer with updated state. Possibly the same object as
-            ``self``.    
-        """
-        return self.embed_weight.data.at[x].get(mode="fill"), self
+        self.embed_kernel = Parameter(trainable=True)
+
+    def init(self, x: Array) -> None:
+        self.embed_kernel.data=self.embed_initializer(
+            self.rng, (self.vocab_size, self.embed_dim), self.dtype
+        )
+
+    def apply(
+        self,
+        x: Array,
+        rng: None=None,
+        inference_mode: bool=False,
+        batch_axis_name: Union[Hashable, Tuple[Hashable]]=()
+    ) -> Tuple[Array, Any]:
+        return self.embed_kernel.data.at[x].get(mode="fill")
```

## mlax/nn/f.py

```diff
@@ -1,81 +1,89 @@
-import jax
+from jax import Array
+from typing import Any, Callable, Optional, Tuple, Union, Hashable
 from mlax import Module
-from typing import Callable, Optional, Any
+from mlax._utils import _needs_axis_name
 
 class F(Module):
     """Wrapper to create pure function layers."""
     def __init__(
         self,
-        train_fn: Callable[[jax.Array], jax.Array],
-        infer_fn: Optional[Callable[[jax.Array], jax.Array]] = None
+        train_fn: Union[Callable[[Any], Any], Callable[[Any], Any]],
+        infer_fn: Optional[Union[Callable[[Any], Any], Callable[[Any], Any]]]=None
     ):
         """Initialize a F layer.
 
-        :param train_fn: Pure function that takes in and returns a JAX array.
-            Called when ``inference_mode`` is False.
-        :param infer_fn: Optional pure function that takes in and returns a JAX
-            array. Called when ``inference_mode`` is True. If None, the
+        :param train_fn: Pure function that takes in a valid JAX type and
+            optionally a keyword argument `axis_name` and returns a valid JAX
+            type. Called when ``inference_mode`` is False.
+        :param infer_fn: Pure function that takes in a valid JAX type and
+            optionally a keyword argument `axis_name` and returns a valid JAX
+            type. Called when ``inference_mode`` is True. If None, the
             ``train_fn`` is called instead. Default: None.
         """
         super().__init__()
         self.train_fn = train_fn
         self.infer_fn = infer_fn
     
-    def __call__(self, x, rng=None, inference_mode=False):
-        """Apply an arbitrary pure functional transform.
+    def init(self, x: Any) -> None:
+        pass
 
-        :param x: Input features.
-        :param rng: PRNG key. Ignored. Default: None.
-        :param inference_mode: Whether in inference or training mode. Default:
-            False.
-        
-        :returns: ``x`` with the arbitrary transform applied.
-        :returns: F layer with updated state. Possibly the same object as
-            ``self``.
-        """
-        if inference_mode:
-            if self.infer_fn is None:
-                return self.train_fn(x), self
+    def apply(
+        self,
+        x: Any,
+        rng: None=None,
+        inference_mode: bool=False,
+        batch_axis_name: Union[Hashable, Tuple[Hashable]]=()
+    ) -> Tuple[Any, Any]:
+        if self.infer_fn is None or not inference_mode:
+            if _needs_axis_name(self.train_fn):
+                return self.train_fn(x, axis_name=batch_axis_name)
             else:
-                return self.infer_fn(x), self
+                return self.train_fn(x)
         else:
-            return self.train_fn(x), self
+            if _needs_axis_name(self.infer_fn):
+                return self.infer_fn(x, axis_name=batch_axis_name)
+            else:
+                return self.infer_fn(x)
 
 class FRng(Module):
     """Wrapper to create pure function layers that may require rng."""
     def __init__(
         self,
-        train_fn: Callable[[jax.Array, Any], jax.Array],
-        infer_fn: Optional[Callable[[jax.Array, Any], jax.Array]] = None
+        train_fn: Callable[[Any, Array], Any],
+        infer_fn: Optional[Callable[[Any, Array], Any]]=None
     ):
         """Initialize a FRng layer.
 
-        :param train_fn: Pure function that takes in a JAX array and a PRNGKey
-            and returns a JAX array. Called when ``inference_mode`` is False.
-        :param infer_fn: Optional pure function that takes in a JAX array and a
-            PRNGKey and and returns a JAX array. Called when ``inference_mode``
-            is True. If None, the ``train_fn`` is called instead. Default: None.
+        :param train_fn: Pure function that takes in a valid JAX type, a PRNG
+            key, and optionally a keyword argument `axis_name` and returns a
+            valid JAX type. Called when ``inference_mode`` is False.
+        :param infer_fn: Pure function that takes in a valid JAX type, a PRNG
+            key, and optionally a keyword argument `axis_name` and returns a
+            valid JAX type. Called when ``inference_mode`` is True. If None, the
+            ``train_fn`` is called instead. Default: None.
         """
         super().__init__()
         self.train_fn = train_fn
         self.infer_fn = infer_fn
     
-    def __call__(self, x, rng, inference_mode=False):
-        """Apply an arbitrary pure functional transform.
+    def init(self, x: Any) -> None:
+        pass
 
-        :param x: Input features.
-        :param rng: PRNG key.
-        :param inference_mode: Whether in inference or training mode. Default:
-            False.
-        
-        :returns: ``x`` with the arbitrary transform applied.
-        :returns: F layer with updated state. Possibly the same object as
-            ``self``.
-        """
-        if inference_mode:
-            if self.infer_fn is None:
-                return self.train_fn(x, rng), self
+    def apply(
+        self,
+        x: Any,
+        rng: Array,
+        inference_mode: bool=False,
+        batch_axis_name: Union[Hashable, Tuple[Hashable]]=()
+    ) -> Tuple[Any, Any]:
+        """Apply an arbitrary pure functional transform."""
+        if self.infer_fn is None or not inference_mode:
+            if _needs_axis_name(self.train_fn):
+                return self.train_fn(x, rng, axis_name=batch_axis_name)
             else:
-                return self.infer_fn(x, rng), self
+                return self.train_fn(x, rng)
         else:
-            return self.train_fn(x, rng), self
+            if _needs_axis_name(self.infer_fn):
+                return self.infer_fn(x, rng, axis_name=batch_axis_name)
+            else:
+                return self.infer_fn(x, rng)
```

## mlax/nn/functional.py

```diff
@@ -1,64 +1,70 @@
-import jax
 from jax import (
+    Array,
+    numpy as jnp,
     random,
     lax
 )
 from mlax._utils import (
     _canon_int_sequence,
     _canon_opt_int_sequence,
     _canon_padding,
-    _normalize
+    _compute_std_stats,
+    _standadize
 )
 from math import prod, sqrt
-from typing import Any, Tuple, Sequence, Union, Callable, Optional
+from typing import Any, Tuple, Sequence, Union, Callable, Optional, Hashable
 
-def identity(x: jax.Array) -> jax.Array:
+def identity(x: Any) -> Any:
     """Identity function.
     
     :param x: Input features.
 
     :returns y: ``x``.
     """
     return x
 
 def dropout(
-    x: jax.Array,
+    x: Array,
     rng: Any,
-    rate: float
-) -> jax.Array:
+    rate: float,
+    axis: Union[int, Sequence[int]]
+) -> Array:
     """Apply random dropouts to input features.
 
     :param x: Input features.
     :param rng: PRNG key for randomizing dropouts.
     :param rate: Probability at which each element is droped out. Must be in
         [0, 1).
+    :param axis: Axis or sequence of axes to drop features along.
 
     :returns y: ``x`` with dropouts applied.
     """
+    axis = _canon_int_sequence(axis, 1)
     prob = 1.0 - rate
-    mask = random.bernoulli(rng, prob, x.shape)
-    zeros = lax.full_like(x, 0)
+    mask = lax.broadcast_in_dim(
+        random.bernoulli(rng, prob, [x.shape[i] for i in axis]), x.shape, axis
+    )
     return lax.select(
         mask,
         lax.div(x, lax.convert_element_type(prob, x.dtype)),
-        zeros
+        lax.full_like(x, 0)
     )
 
 def pool(
-    x: jax.Array,
+    x: Array,
     init_value: Any,
     reduce_fn: Callable[[Any, Any], Any],
     window_shape: Union[int, Sequence[int]],
     strides: Union[int, Sequence[int]] = 1,
     padding: Union[str, int, Sequence[Union[int, Tuple[int, int]]]] = "VALID",
     input_dilation: Optional[Union[int, Sequence[int]]] = None,
     window_dilation: Optional[Union[int, Sequence[int]]] = None,
-    channel_last: bool=False
-) -> jax.Array:
+    data_format: str="channel_last"
+) -> Array:
     """Apply an arbitrary reduce function over poolings windows of input
         features.
 
     :param x: Input features. Must have be unbatched thus having
         ``n_spatial_dims + 1`` dimensions.
     :param init_value: Initial value of the reduce function over each pooling
         window.
@@ -82,285 +88,269 @@
         integers, specifying the input dilation rate in each spatial dimension.
         See the ``base_dilation`` parameter of `jax.lax.reduce_window`_.
         Default: None, no input dilation.
     :param window_dilation: None, an integer, or a sequence of ``n_spatial_dims``
         integers, specifying the window dilation rate in each spatial dimension.
         See the ``window_dilation`` parameter of `jax.lax.reduce_window`_.
         Default: None, no window dilation.
-    :param channel_last: Whether features are channel-last or first. Default:
-        False, channel-first.
-    
+    :param data_format: "channel_last", "channel_first", or a string
+        representing the kernel spec as described in
+        ``jax.lax.conv_general_dilated``, but  without `N` the batch dimension.
+        Default: "channel_last".
+
     :returns y: ``x`` with pooling applied.
-    
+
     .. _jax.lax.reduce_window:
         https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.reduce_window.html
     """
     n_spatial_dims = x.ndim - 1
     window_shape = _canon_int_sequence(window_shape, n_spatial_dims)
     strides = _canon_int_sequence(strides, n_spatial_dims)
     padding = _canon_padding(padding, n_spatial_dims)
     input_dilation = _canon_opt_int_sequence(input_dilation, n_spatial_dims)
     window_dilation = _canon_opt_int_sequence(window_dilation, n_spatial_dims)
 
-    if channel_last:
+    if data_format == "channel_last":
         window_shape = window_shape + (1,)
         strides = strides + (1,)
         padding = padding + ((0, 0),) if isinstance(padding, tuple) else padding
         input_dilation = (
             input_dilation + (1,) if isinstance(input_dilation, tuple)
             else input_dilation
         )
         window_dilation = (
             window_dilation + (1,) if isinstance(window_dilation, tuple)
             else window_dilation
         )
-    else:
+    elif data_format == "channel_first":
         window_shape = (1,) + window_shape
         strides = (1,) + strides
         padding = ((0, 0),) + padding if isinstance(padding, tuple) else padding
         input_dilation = (
             (1,) + input_dilation if isinstance(input_dilation, tuple) else
             input_dilation
         )
         window_dilation = (
             (1,) + window_dilation if isinstance(window_dilation, tuple) else
             window_dilation
         )
+    else:
+        channel_dim = data_format.index("C")
+        window_shape = (
+            window_shape[:channel_dim] + (1,) + window_shape[channel_dim:]
+        )
+        strides = (
+            strides[:channel_dim] + (1,) + strides[channel_dim:]
+        )
+        padding = (
+            padding[:channel_dim] + ((0, 0),) + padding[channel_dim:]
+            if isinstance(padding, tuple) else padding
+        )
+        input_dilation = (
+            input_dilation[:channel_dim] + (1,) + input_dilation[channel_dim:]
+            if isinstance(input_dilation, tuple) else input_dilation
+        )
+        window_dilation = (
+            window_dilation[:channel_dim] + (1,) + window_dilation[channel_dim:]
+            if isinstance(window_dilation, tuple) else window_dilation
+        )
 
     return lax.reduce_window(
         x,
         init_value,
         reduce_fn,
         window_shape,
         strides,
         padding,
         input_dilation,
         window_dilation
     )
 
 def max_pool(
-    x: jax.Array,
+    x: Array,
     window_shape: Union[int, Sequence[int]],
     strides: Union[int, Sequence[int]] = 1,
     padding: Union[str, int, Sequence[Union[int, Tuple[int, int]]]] = "VALID",
     input_dilation: Optional[Union[int, Sequence[int]]] = None,
     window_dilation: Optional[Union[int, Sequence[int]]] = None,
-    channel_last: bool=False
-) -> jax.Array:
+    data_format: str="channel_last"
+) -> Array:
     """Apply max pooling over input features.
 
     :param x: Input features. Must have be unbatched thus having
         ``n_spatial_dims + 1`` dimensions.
     :param window_shape: See the ``window_shape`` parameter of ``pooling``.
     :param strides: See the ``strides`` parameter of ``pooling``. Default: 1.
     :param padding: See the ``padding`` parameter of ``pooling``.
     :param input_dilation: See the ``input_dilation`` parameter of ``pooling``.
         Default: None, no input dilation.
     :param window_dilation: See the ``window_dilation`` parameter of
         ``pooling``. Default: None, no window dilation.
-    :param channel_last: Whether features are channel-last or first. Default:
-        False, channel-first.
+    :param data_format: "channel_last", "channel_first", or a string
+        representing the kernel spec as described in
+        ``jax.lax.conv_general_dilated``, but without `N` the batch dimension.
+        Default: "channel_last".
     
     :returns y: ``x`` with max pooling applied.
     """
     return pool(
         x,
-        -jax.numpy.inf,
+        -jnp.inf,
         lax.max,
         window_shape,
         strides,
         padding,
         input_dilation,
         window_dilation,
-        channel_last
+        data_format
     )
 
 def sum_pool(
-    x: jax.Array,
+    x: Array,
     window_shape: Union[int, Sequence[int]],
     strides: Union[int, Sequence[int]] = 1,
     padding: Union[str, int, Sequence[Union[int, Tuple[int, int]]]] = "VALID",
     input_dilation: Optional[Union[int, Sequence[int]]] = None,
     window_dilation: Optional[Union[int, Sequence[int]]] = None,
-    channel_last: bool=False
-) -> jax.Array:
+    data_format: str="channel_last"
+) -> Array:
     """Apply sum pooling over input features.
 
     :param x: Input features. Must have be unbatched thus having
         ``n_spatial_dims + 1`` dimensions.
     :param window_shape: See the ``window_shape`` parameter of ``pooling``.
     :param strides: See the ``strides`` parameter of ``pooling``. Default: 1.
     :param padding: See the ``padding`` parameter of ``pooling``.
     :param input_dilation: See the ``input_dilation`` parameter of ``pooling``.
         Default: None, no input dilation.
     :param window_dilation: See the ``window_dilation`` parameter of
         ``pooling``. Default: None, no window dilation.
-    :param channel_last: Whether features are channel-last or first. Default:
-        False, channel-first.
+    :param data_format: "channel_last", "channel_first", or a string
+        representing the kernel spec as described in
+        ``jax.lax.conv_general_dilated``, but without `N` the batch dimension.
+        Default: "channel_last".
     
     :returns y: ``x`` with sum pooling applied.
     """
     return pool(
         x,
         0,
         lax.add,
         window_shape,
         strides,
         padding,
         input_dilation,
         window_dilation,
-        channel_last
+        data_format
     )
 
 def avg_pool(
-    x: jax.Array,
+    x: Array,
     window_shape: Union[int, Sequence[int]],
     strides: Union[int, Sequence[int]] = 1,
     padding: Union[str, int, Sequence[Union[int, Tuple[int, int]]]] = "VALID",
     input_dilation: Optional[Union[int, Sequence[int]]] = None,
     window_dilation: Optional[Union[int, Sequence[int]]] = None,
-    channel_last: bool=False
-) -> jax.Array:
+    data_format: str="channel_last"
+) -> Array:
     """Apply average pooling over input features.
 
     :param x: Input features. Must have be unbatched thus having
         ``n_spatial_dims + 1`` dimensions.
     :param window_shape: See the ``window_shape`` parameter of ``pooling``.
     :param strides: See the ``strides`` parameter of ``pooling``. Default: 1.
     :param padding: See the ``padding`` parameter of ``pooling``.
     :param input_dilation: See the ``input_dilation`` parameter of ``pooling``.
         Default: None, no input dilation.
     :param window_dilation: See the ``window_dilation`` parameter of
         ``pooling``. Default: None, no window dilation.
-    :param channel_last: Whether features are channel-last or first. Default:
-        False, channel-first.
- 
+    :param data_format: "channel_last", "channel_first", or a string
+        representing the kernel spec as described in
+        ``jax.lax.conv_general_dilated``, but without `N` the batch dimension.
+        Default: "channel_last".
+
     :returns y: ``x`` with average pooling applied.
     """
     n_spatial_dims = x.ndim - 1
     window_shape = _canon_int_sequence(window_shape, n_spatial_dims)
-
     activations = pool(
         x,
         0,
         lax.add,
         window_shape,
         strides,
         padding,
         input_dilation,
         window_dilation,
-        channel_last
+        data_format
     )
-
     return lax.div(
         activations,
         lax.convert_element_type(prod(window_shape), activations.dtype)
     )
 
 def dot_product_attention_logits(
-    query: jax.Array,
-    key: jax.Array
-):
+    query: Array, key: Array
+) -> Array:
     """Compute scaled dot-product attention logits.
     
     :param query: Query array of shape
         ``(query_length, num_heads, query_key_depth)``.
     :param key: Key array of the same dtype as ``query`` and of shape
         ``(key_value_length, num_heads, query_key_depth)``.
 
     :returns: Attention logits of
         ``(num_heads, query_length, key_value_length)``.
     """
-    logits = lax.dot_general(
-        query, key,
-        (((2,), (2,)), ((1,), (1,)))
-    )
+    logits = lax.dot_general(query, key, (((2,), (2,)), ((1,), (1,))))
     return lax.div(
-        logits,
-        lax.convert_element_type(sqrt(query.shape[-1]), logits.dtype)
+        logits, lax.convert_element_type(sqrt(query.shape[2]), logits.dtype)
     )
 
 def apply_attention_weights(
-    value: jax.Array,
-    attention_weights: jax.Array
-):
+    value: Array, attention_weights: Array
+) -> Array:
     """Apply attention weights to values.
 
     :param value: Value array of shape
         ``(key_value_length, num_heads, value_depth)``.
     :param attention_weights: Attention weights of the same dtype as ``value``
         and of shape ``(num_heads, query_length, key_value_length)``.
 
     :returns activations: ``value`` with ``attention_weights`` applied, of shape
         ``(query_length, num_heads, value_depth)``.
     """
     activations = lax.dot_general(
-        value, attention_weights,
-        (((0,), (2,)), ((1,), (0,)))
+        value, attention_weights, (((0,), (2,)), ((1,), (0,)))
     )
     # activations: (num_heads, depth, value_length)
     return lax.transpose(activations, (2, 0, 1))
 
-def layer_norm(x, epsilon=1e-05):
-    """Apply layer normalization.
-
-    :param x: Input features.
-    :param epsilon: Small number added to variance to avoid divisions by zero.
-        Default: 1e-05.
-    
-    :returns: ``x`` with layer normalization applied.
-    """
-    return _normalize(x, range(x.ndim), epsilon)
-
-def instance_norm(x, channel_last=False, epsilon=1e-05):
-    """Apply instance normalization.
-
-    :param x: Input features. Must be compatible with ``channel_last``.
-    :param channel_last: Whether features are channel-last or first. Default:
-        False, channel-first.
-    :param epsilon: Small number added to variance to avoid divisions by zero.
-        Default: 1e-05.
-    
-    :returns: ``x`` with instance normalization applied.
-    """
-    dims = range(x.ndim - 1) if channel_last else range(1, x.ndim)
-    return _normalize(
-        x,
-        dims,
-        epsilon
-    )
-
-def group_norm(x, num_groups, channel_last=False, epsilon=1e-05):
-    """Apply group normalization.
+def z_norm(
+    x: Array,
+    axis: Union[str, int, Sequence[int]],
+    batch_axis_name: Union[Hashable, Tuple[Hashable]]=(),
+    epsilon: float=1e-05
+):
+    """Apply Z-score normalization.
 
-    :param x: Input features. Must be compatible with ``channel_last``.
-    :param num_groups: Number of groups to split the channels into. Must divide
-        the number of channels in ``x``.
-    :param channel_last: Whether features are channel-last or first. Default:
-        False, channel-first.
+    :param axis: "all", "channel_last", "channel_first", axis, or sequence of
+        axes to normalize input features along. "all" indicates normalization
+        along all axes (layer norm). "channel_last" and "channel_first" indicate
+        normalization along all but the channel axis, assumed to be the last or
+        first axis (instance norm).
     :param epsilon: Small number added to variance to avoid divisions by zero.
         Default: 1e-05.
-    
-    :returns: ``x`` with group normalization applied.
-    """
-    x_shape = x.shape
-    if channel_last:
-        num_channels = x_shape[-1]
-        x = lax.reshape(
-            x, (*x_shape[:-1], num_channels//num_groups, num_groups)
-        )
-        dims = range(x.ndim - 1)
-    else:
-        num_channels = x_shape[0]
-        x = lax.reshape(
-            x, (num_groups, num_channels//num_groups, *x_shape[1:])
-        )
-        dims = range(1, x.ndim)
-    
-    return lax.reshape(
-        _normalize(
-            x,
-            dims,
-            epsilon
-        ),
-        x_shape
-    )
+    :param batch_axis_name: Hashable or tuple of hashable representing
+        the batch axis name(s) to normalize along in addition to those in
+        ``axis``. Default: (), no normlization along any batch axis.
+
+    :returns: ``x`` with normalization applied.
+    """
+    if axis == "all":
+        axis = list(range(x.ndim))
+    elif axis == "channel_last":
+        axis = list(range(x.ndim - 1))
+    elif axis == "channel_first":
+        axis = list(range(1, x.ndim))
+    mean, variance = _compute_std_stats(x, axis, batch_axis_name)
+    return _standadize(x, axis, mean, variance, epsilon)
```

## mlax/nn/linear.py

```diff
@@ -1,103 +1,83 @@
-from mlax import Parameter, Module
 from jax import (
+    Array,
     numpy as jnp,
     nn,
-    lax
+    lax,
+    dtypes
 )
-from typing import Any
+from typing import Any, Tuple, Union, Hashable
+from mlax import Parameter, Module
 from mlax._utils import (
-    _canon_dtype,
     _canon_opt_dtype,
     _canon_precision_pair
 )
 
-
 class Linear(Module):
     """Linear transformation layer without bias with lazy kernel initialization."""
     def __init__(
         self,
-        rng: Any,
+        rng: Array,
         out_features: int,
         precision=None,
         accum_dtype=None,
-        transposed_kernel=False,
+        transposed_kernel: bool=False,
         kernel_initializer=nn.initializers.glorot_uniform(),
         dtype=jnp.float32
     ):
         """Initialize a linear layer.
 
-        :param rng: PRNG key for weight initialization.
+        :param rng: PRNG key.
         :param out_features: Number of output features.
         :param precision: See ``precision`` parameter of
             `jax.lax.dot <https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.dot_general.html>`_,
             which is used internally in the forward pass. Default: None.
         :param accum_dtype: See ``preferred_element_type`` parameter of
             `jax.lax.dot <https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.dot_general.html>`_,
             which is used internally in the forward pass. Default: None.
         :param transposed_kernel: Whether the kernel is of the shape
             ``(out_features, in_features)`` or ``(in_features, out_features)``.
             Default: False, the former.
         :param kernel_initializer: Initializer for kernel of shape
             ``(in_features, out_features)`` as defined by
             `jax.nn.initalizers <https://jax.readthedocs.io/en/latest/jax.nn.initializers.html>`_.
             Default: glorot uniform.
-        :param dtype: Dtype of initialized kernel weight. Default: float32.
+        :param dtype: Type of initialized parameters. Default: float32.
         """
         super().__init__()
-        self.initialized = False
 
-        self._rng =  Parameter(trainable=False, data=rng)
-        self._out_features = out_features
-        self._kernel_initializer = kernel_initializer
-        self._dtype = _canon_dtype(dtype)
-        
-        self.transposed_kernel = transposed_kernel
-        self.kernel_weight = Parameter(trainable=True)
+        self.rng = rng
+        self.out_features = int(out_features)
         self.precision = _canon_precision_pair(precision)
         self.accum_dtype = _canon_opt_dtype(accum_dtype)
+        self.kernel_initializer = kernel_initializer
+        self.transposed_kernel = transposed_kernel
+        self.dtype = dtypes.canonicalize_dtype(dtype)
+
+        self.linear_kernel = Parameter(trainable=True)
 
-    def _build(self, x):
+    def init(self, x: Array) -> None:
         """Initialize an uninitialized linear layer."""
-        self.kernel_weight.data = self._kernel_initializer(
-            self._rng.data,
-            (x.shape[-1], self._out_features),
-            self._dtype
+        self.linear_kernel.data = self.kernel_initializer(
+            self.rng, (x.shape[-1], self.out_features), self.dtype
         )
-
-        del self._rng      
-        del self._out_features
-        del self._kernel_initializer
-        del self._dtype
-
         if self.transposed_kernel:
-            self.kernel_weight.data = lax.transpose(
-                self.kernel_weight.data,
-                (1, 0)
+            self.linear_kernel.data = lax.transpose(
+                self.linear_kernel.data, (1, 0)
             )
-        
-        self.initialized = True
-    
-    def __call__(self, x, rng=None, inference_mode=False):
-        """Apply linear transformation to input features.
-
-        :param x: Input features. Must be of the shape ``(..., in_features,)``.
-        :param rng: PRNG key. Ignored. Default: None.
-        :param inference_mode: Whether in inference or training mode. Ignored.
-            Default: False.
-        
-        :returns: ``x`` with linear transformation applied. Shape
-            ``(..., out_features)``.
-        :returns: Linear layer with updated state. Possibly the same object as
-            ``self``.
-        """
-        if not self.initialized:
-            self._build(x)
 
+    def apply(
+        self,
+        x: Array,
+        rng: None=None,
+        inference_mode: bool=False,
+        batch_axis_name: Union[Hashable, Tuple[Hashable]]=()
+    ) -> Tuple[Array, Any]:
+        """Apply linear transformation to input features."""
         contracting_dims = (1,) if self.transposed_kernel else (0,)
         return lax.dot_general(
             x,
-            lax.convert_element_type(self.kernel_weight.data, x.dtype),
+            lax.convert_element_type(self.linear_kernel.data, x.dtype),
             (((x.ndim - 1,), contracting_dims), ((), ())),
             self.precision,
             self.accum_dtype
-        ), self
+        )
```

## mlax/nn/parallel.py

```diff
@@ -1,72 +1,79 @@
-from jax import random
-from typing import Sequence, Iterable
-from mlax import Module, ModuleSeq
+from jax import (
+    Array,
+    random
+)
+from typing import Any, Iterable, Tuple, List, Union, Hashable
+from mlax import Module, Parameter
 from mlax._utils import _needs_rng
 
 class Parallel(Module):
     """Combination of layers that do not require rng in parallel."""
     def __init__(self, layers: Iterable[Module]):
         """Initialize a Parallel layer.
 
         :param layers: Layers to combine in parallel.
         """
         super().__init__()
-        self.layers = ModuleSeq(submodules=layers)        
-    
-    def __call__(self, x: Sequence, rng=None, inference_mode=False):
-        """Apply layers that do not require rng in parallel.
-
-        :param x: Sequence of input features, one for each layer.
-        :param rng: PRNG key. Ignored. Default: None.
-        :param inference_mode: Whether in inference or training mode. Default:
-            False.
-        
-        :returns: List of outputs from each layer.
-        :returns: Parallel layer with updated state. Possibly the same object as
-            ``self``.
-        """
+        self.layers = Parameter(trainable=None, data=list(layers))
+
+    def init(self, x: Any) -> None:
+        pass
+
+    def apply(
+        self,
+        x: Iterable[Any],
+        rng: None=None,
+        inference_mode: bool=False,
+        batch_axis_name: Union[Hashable, Tuple[Hashable]]=()
+    ) -> Tuple[List[Any], Any]:
         res = []
-        for i, (_x, layer) in enumerate(zip(x, self.layers)):
-            _x, self.layers[i] = layer(_x, None, inference_mode)
-            res.append(_x)
-        return res, self
+        for i, (layer, _x) in enumerate(zip(self.layers.data, x)):
+            _y, self.layers.data[i] = layer(
+                _x, None, inference_mode, batch_axis_name
+            )
+            res.append(_y)
+        return res
 
 class ParallelRng(Module):
     """Combination of layers that may require rng in parallel."""
     def __init__(self, layers: Iterable[Module]):
         """Initialize a ParallelRng layer.
 
         :param layers: PyTree of layers to combine in parallel.
         """
         super().__init__()
-        self.layers = ModuleSeq(submodules=layers) 
-    
-    def __call__(self, x: Sequence, rng, inference_mode=False):
-        """Apply layers that may not require rng in parallel.
-
-        :param x: Sequence of input features, one for each layer.
-        :param rng: PRNG key.
-        :param inference_mode: Whether in inference or training mode. Default:
-            False.
-        
-        :returns: List of outputs from each layer.
-        :returns: ParallelRng layer with updated state. Possibly the same object
-            as ``self``.
-        """
-        needs_rngs = [_needs_rng(layer) for layer in self.layers]
-        num_rngs = sum(needs_rngs)
-        if num_rngs > 1:
-            rng_iter = iter(random.split(rng, num_rngs))
+        self.layers = Parameter(trainable=None, data=list(layers))
+
+    def init(self, x: Any) -> None:
+        pass
+
+    def apply(
+        self,
+        x: Iterable[Any],
+        rng: Array,
+        inference_mode: bool=False,
+        batch_axis_name: Union[Hashable, Tuple[Hashable]]=()
+    ) -> Tuple[Any, Any]:
+        needs_rngs = [_needs_rng(layer) for layer in self.layers.data]
+        n_needs_rng = sum(needs_rngs)
+        if n_needs_rng > 1:
+            keys_iter = iter(
+                [random.fold_in(rng, i) for i in range(n_needs_rng)]
+            )
         else:
-            rng_iter = iter([rng])
+            keys_iter = iter([rng])
 
         res = []
-        for i, (_x, needs_rng, layer) in enumerate(
-            zip(x, needs_rngs, self.layers)
+        for i, (needs_rng, layer, _x) in enumerate(
+            zip(needs_rngs, self.layers.data, x)
         ):
             if needs_rng:
-                _x, self.layers[i] = layer(_x, next(rng_iter), inference_mode)
+                _x, self.layers.data[i] = layer(
+                    _x, next(keys_iter), inference_mode, batch_axis_name
+                )
             else:
-                _x, self.layers[i] = layer(_x, None, inference_mode)
+                _x, self.layers.data[i] = layer(
+                    _x, None, inference_mode, batch_axis_name
+                )
             res.append(_x)
-        return res, self
+        return res
```

## mlax/nn/scaler.py

```diff
@@ -1,94 +1,79 @@
 from mlax import Parameter, Module
 from jax import (
+    Array,
     numpy as jnp,
     nn,
-    lax
-)
-from typing import Any, Sequence, Optional, Union
-from mlax._utils import (
-    _canon_int_sequence,
-    _canon_dtype
+    lax,
+    dtypes
 )
+from typing import Any, Sequence, Optional, Union, Tuple, Hashable
+from mlax._utils import _canon_int_sequence
 
 class Scaler(Module):
     """Scaler layer."""
     def __init__(
         self,
-        rng: Any,
+        rng: Array,
         in_features: Union[int, Sequence[Optional[int]]],
         scaler_initializer=nn.initializers.ones,
         dtype=jnp.float32
     ):
         """Initialize a bias layer.
 
-        :param rng: PRNG key for weight initialization.
+        :param rng: PRNG key.
         :param in_features: Integer or sequence of optional integers indicating
             the shape of the input features to scale. Empty sequence indicates a
-            single scalar. For per-axis scaling, use ``None`` on axes that do
-            not require scaling, use ``1`` on axes that require a single scaling
+            single scalar. For per-axis scaling, use ``0`` on axes that do not
+            require scaling, use ``1`` on axes that require a single scaling
             term, and ``-1`` or ``axis_length`` on axes that require a scaling
             term for each of their ``axis_length`` elements. A single integer
             is interpreted as a sequence of one.
         :param scaler_initializer: Scaler initializer as defined by
             `jax.nn.initalizers <https://jax.readthedocs.io/en/latest/jax.nn.initializers.html>`_.
             Default: ones.
-        :param dtype: Dtype of initialized bias weight. Default: float32.
+        :param dtype: Type of initialized parameters. Default: float32.
         """
         super().__init__()
-        self.initialized = False
 
-        self._rng = Parameter(trainable=False, data=rng)
-        self._in_features = _canon_int_sequence(in_features, 1)
-        self._scaler_initializer = scaler_initializer
-        self._dtype = _canon_dtype(dtype)
-
-        self.scaler_weight = Parameter(trainable=True)
-        self.scaler_broadcast_dims = None
-
-    def _build(self, x):
-        """Initialize an uninitialized scaler layer."""
-        self.scaler_broadcast_dims = tuple(
-            i for i, axis in enumerate(self._in_features)
-            if axis is not None
-        )
+        self.rng = rng
+        self.in_features = _canon_int_sequence(in_features, 1)
+        self.scaler_initializer = scaler_initializer
+        self.dtype = dtypes.canonicalize_dtype(dtype)
 
+        self.scaler_kernel = Parameter(trainable=True)
+
+    def init(self, x: Array) -> None:
         scaler_shape = [
-            axis if axis != -1 else x.shape[axis]
-            for axis in self._in_features if axis is not None
+            axis if axis != -1 else x.shape[i]
+            for i, axis in enumerate(self.in_features) if axis != 0
         ]
-        self.scaler_weight.data = self._scaler_initializer(
-            self._rng.data,
-            scaler_shape,
-            self._dtype
+        self.scaler_kernel.data=self.scaler_initializer(
+            self.rng, scaler_shape, self.dtype
         )
-
-        del self._rng
-        del self._in_features
-        del self._scaler_initializer
-        del self._dtype
-
-        self.initialized = True
     
-    def __call__(self, x, rng=None, inference_mode=False):
+    def apply(
+        self,
+        x: Array,
+        rng: None=None,
+        inference_mode: bool=False,
+        batch_axis_name: Union[Hashable, Tuple[Hashable]]=()
+    ) -> Tuple[Array, Any]:
         """Scale input features.
 
         :param x: Input features. Must be of the shape ``in_feature_shape``.
         :param rng: PRNG key. Ignored. Default: None.
         :param inference_mode: Whether in inference or training mode. Ignored.
             Default: False.
         
         :returns y: Scaled ``x``.
         :returns: Bias layer with updated state. Possibly the same object as
             ``self``.
         """
-        if not self.initialized:
-            self._build(x)
-
         return lax.mul(
             x,
             lax.broadcast_in_dim(
-                lax.convert_element_type(self.scaler_weight.data, x.dtype),
+                lax.convert_element_type(self.scaler_kernel.data, x.dtype),
                 x.shape,
-                self.scaler_broadcast_dims
+                [i for i, axis in enumerate(self.in_features) if axis != 0]
             )
-        ), self
+        )
```

## mlax/nn/series.py

```diff
@@ -1,67 +1,75 @@
-from jax import random
-from typing import Iterable
-from mlax import Module, ModuleSeq
+from jax import (
+    Array,
+    random
+)
+from typing import Any, Iterable, Tuple, Union, Hashable
+from mlax import Module, Parameter
 from mlax._utils import _needs_rng
 
 class Series(Module):
     """Combination of layers that do not require rng in series."""
     def __init__(self, layers: Iterable[Module]):
         """Initialize a Series layer.
 
         :param layers: Layers to combine in series.
         """
         super().__init__()
-        self.layers = ModuleSeq(submodules=layers)
-        
-    
-    def __call__(self, x, rng=None, inference_mode=False):
-        """Apply layers that do not require rng in series.
-
-        :param x: Input features.
-        :param rng: PRNG key. Ignored. Default: None.
-        :param inference_mode: Whether in inference or training mode. Default:
-            False.
-        
-        :returns: Output of ``layers`` applied in series on ``x``.
-        :returns: Series layer with updated state. Possibly the same object as
-            ``self``.
-        """
-        for i, layer in enumerate(self.layers):
-            x, self.layers[i] = layer(x, None, inference_mode)
-        return x, self
+        self.layers = Parameter(trainable=None, data=list(layers))
+
+    def init(self, x: Any) -> None:
+        pass
+
+    def apply(
+        self,
+        x: Any,
+        rng: None=None,
+        inference_mode: bool=False,
+        batch_axis_name: Union[Hashable, Tuple[Hashable]]=()
+    ) -> Tuple[Any, Any]:
+        for i, layer in enumerate(self.layers.data):
+            x, self.layers.data[i] = layer(
+                x, None, inference_mode, batch_axis_name
+            )
+        return x
 
 class SeriesRng(Module):
     """Combination of layers that may require rng in series."""
     def __init__(self, layers: Iterable[Module]):
         """Initialize a SeriesRNG layer.
 
         :param layers: Layers to combine in series.
         """
         super().__init__()
-        self.layers = ModuleSeq(submodules=layers)
-    
-    def __call__(self, x, rng, inference_mode=False):
-        """Apply layers that may not require rng in series.
-
-        :param x: Input features.
-        :param rng: PRNG key.
-        :param inference_mode: Whether in inference or training mode. Default:
-            False.
-        
-        :returns: Output of ``layers`` applied in series on ``x``.
-        :returns: SeriesRng layer with updated state. Possibly the same object
-            as ``self``.
-        """
-        needs_rngs = [_needs_rng(layer) for layer in self.layers]
-        num_rngs = sum(needs_rngs)
-        if num_rngs > 1:
-            rng_iter = iter(random.split(rng, num_rngs))
+        self.layers = Parameter(trainable=None, data=list(layers))
+
+    def init(self, x: Any) -> None:
+        pass
+
+    def apply(
+        self,
+        x: Any,
+        rng: Array,
+        inference_mode: bool=False,
+        batch_axis_name: Union[Hashable, Tuple[Hashable]]=()
+    ) -> Tuple[Any, Any]:
+        needs_rngs = [_needs_rng(layer) for layer in self.layers.data]
+        n_needs_rng = sum(needs_rngs)
+        if n_needs_rng > 1:
+            keys_iter = iter(
+                [random.fold_in(rng, i) for i in range(n_needs_rng)]
+            )
         else:
-            rng_iter = iter([rng])
+            keys_iter = iter([rng])
 
-        for i, (needs_rng, layer) in enumerate(zip(needs_rngs, self.layers)):
+        for i, (needs_rng, layer) in enumerate(
+            zip(needs_rngs, self.layers.data)
+        ):
             if needs_rng:
-                x, self.layers[i] = layer(x, next(rng_iter), inference_mode)
+                x, self.layers.data[i] = layer(
+                    x, next(keys_iter), inference_mode, batch_axis_name
+                )
             else:
-                x, self.layers[i] = layer(x, None, inference_mode)
-        return x, self
+                x, self.layers.data[i] = layer(
+                    x, None, inference_mode, batch_axis_name
+                )
+        return x
```

## Comparing `mlax_nn-0.2.2.dist-info/LICENSE` & `mlax_nn-0.2.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `mlax_nn-0.2.2.dist-info/METADATA` & `mlax_nn-0.2.3.dist-info/METADATA`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mlax-nn
-Version: 0.2.2
+Version: 0.2.3
 Summary: A pure functional machine learning library build on top of Google JAX.
 Home-page: https://github.com/zongyf02/mlax
 Author: Yifan Zong
 Author-email: y22zong@uwaterloo.ca
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.8
@@ -16,18 +16,18 @@
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Scientific/Engineering :: Mathematics
 Classifier: Topic :: Software Development
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: jax (>=0.4.1)
-Requires-Dist: jaxlib (>=0.4.1)
+Requires-Dist: jax (>=0.4.8)
+Requires-Dist: jaxlib (>=0.4.7)
 Provides-Extra: dev
-Requires-Dist: pytest (==7.2) ; extra == 'dev'
+Requires-Dist: pytest (==7.3.1) ; extra == 'dev'
 
 # MLAX: Pure functional ML library built on top of Google JAX
 
 [**Overview**](#overview)
 | [**Installation**](#installation)
 | [**Quickstart**](#quickstart)
 | [**Examples**](https://github.com/zongyf02/mlax/tree/main/examples)
@@ -44,58 +44,73 @@
 ## Installation<a id="installation"></a>
 [Install JAX](https://github.com/google/jax#installation) first if you have not
 already.
 
 ```pip install mlax-nn```
 
 ## Quickstart<a id="quickstart"></a>
-This is a simple linear layer defined using only the MLAX Module and Parameter.
+This is a simple lazy linear layer defined in MLAX.
 
 ``` Python
 import jax
 from jax import (
     numpy as jnp,
     nn,
     random
 )
 from mlax import Module, Parameter
 
 class Linear(Module):
-    def __init__(self, in_features, out_features, rng):
-        rng1, rng2 = random.split(rng)
-        self.kernel_weight = Parameter(
-            trainable=True,
-            data=nn.initializers.glorot_uniform()(rng1, (in_features, out_features))
-        )
-        self.bias_weight = Parameter(
-            trainable=True,
-            data=nn.initializers.zeros(rng2, (out_features,))
-        )
+    def __init__(self, rng, out_features):
+        super().__init__()
+        self.rng = rng
+        self.out_features = out_features
+        
+        self.kernel_weight = Parameter(trainable=True)
+        self.bias_weight = Parameter(trainable=True)
     
-    def __call__(self, x, rng=None, inference_mode=False):
+    # Define a ``init`` method for lazy initialziation of weights
+    def init(self, x):
+        rng1, rng2 = random.split(self.rng)
+        self.kernel_weight.data = nn.initializers.glorot_uniform()(
+            rng1, (x.shape[-1], self.out_features)
+        )
+        self.bias_weight.data=nn.initializers.zeros(rng2, (self.out_features,))
+
+    # Define an ``apply`` method for the forward pass
+    def apply(
+        self, x, rng = None, inference_mode = False, batch_axis_name = ()
+    ):
         return x @ self.kernel_weight.data + self.bias_weight.data, self
 ```
 
 It is fully compatible with native JAX transformations:
 
 ``` Python
-def loss_fn(model, x, y):
-    pred, model = model(x)
+def loss_fn(x, y, model):
+    pred, model = model(x, rng=None, inference_mode=True)
     return jnp.mean(y - pred) ** 2, model
 
-model = Linear(3, 4, random.PRNGKey(0))
 x = jnp.ones((4, 3), dtype=jnp.float32)
 y = jnp.ones((4, 4), dtype=jnp.float32)
+model = Linear(random.PRNGKey(0), 4)
+
+loss, updated_model = loss_fn(x, y, model)
+print(loss)
 
-(loss, model), grads = jax.jit(
+# Now let's apply `jax.jit` and `jax.value_and_grad`
+(loss, updated_model), grads = jax.jit(
     jax.value_and_grad(
         loss_fn,
         has_aux=True
     )
-)(model, x, y)
+)(x, y, model)
+
+print(loss)
+print(grads)
 ```
 
 For end-to-end examples with reference PyTorch implementations, visit MLAX's
 [GitHub](https://github.com/zongyf02/mlax/tree/main/examples).
 
 View the full documentation on [Read the Docs](https://mlax.readthedocs.io/en/latest/).
```

## Comparing `mlax_nn-0.2.2.dist-info/RECORD` & `mlax_nn-0.2.3.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,11 +1,12 @@
-mlax/__init__.py,sha256=FDayQ-_jislgELQ7AXrOUyrqlpHFfP80GCsMiWTQsOY,156
-mlax/_utils.py,sha256=Kclpgl3XZlT189M0iq7zU-BI3KxrgT0rcfk7rcqyFVk,3000
+mlax/__init__.py,sha256=CNQq1Kfd25CmKuXeNUS6WAgGadGzz_MhXmmNfmmT06I,125
+mlax/_test_utils.py,sha256=bKMjl9kKv73Dh6dr4Jrl29zphtbLOO_fJM9tcBMRnbk,1433
+mlax/_utils.py,sha256=h5NCBrSwtGHnlBo7TRINWESv_hbm53VGt3mtB671jC0,2717
 mlax/functional.py,sha256=Be3TYhbx4Xu-iVcrrOZxGBCo4k1FNNjiFEV2-DuaRaY,15178
-mlax/module.py,sha256=675XRM_u6qD10fvwCzeOPboa3iBM0cFtLXHjNG0UZZ0,7029
+mlax/module.py,sha256=NSJk9-sBRTLNpa364h5vEIzjjjLZzzXrRfR1t4bHIIk,9186
 mlax/block/Parallel.py,sha256=LH2JhEaKv9Ue1ahva2UfVyBq-Le11m23tbzd-5EDwqA,2140
 mlax/block/Parallel_rng.py,sha256=dbQSbDSXYXFS43f2HemyPisrzVFN6DFen5AKy1EyXGQ,2645
 mlax/block/Series.py,sha256=_Yk_WmuXPHD2or6ksVY-MLgsyeHrt_CV8jigynfhU68,2888
 mlax/block/Series_rng.py,sha256=pIRrA3VLdC8L4ibUFJVDJ5mNxQruCxpwjSA3GYip0EY,3872
 mlax/block/__init__.py,sha256=rST4UG45hdQ3ZvM-UQX2mlvkyZ4P3HkFJCDoO0j2BBM,86
 mlax/blocks/Linear.py,sha256=yANHsLCV5q0vq1ubXPoBRTXu99u2m69Fp0OeyItUDd8,2519
 mlax/blocks/__init__.py,sha256=QGYNBkHq_qL2fJxJrlPRW_hgkp7qsN7KgsB7dH7mZnQ,39
@@ -14,26 +15,27 @@
 mlax/nn/BatchNorm.py,sha256=h8KZU9wmGyB1ZODB7R4SfKhD567WxMLoc_mCHJKEjac,4573
 mlax/nn/Bias.py,sha256=UL5FEX1eXIEPl_jBK11HGypJ4oQwaXs6KTkvA-2DT5E,2457
 mlax/nn/Conv.py,sha256=X0e6xpOrLlFX_J_mtupj8_ZSvEdQrjEdK2d7bJTlOwg,7145
 mlax/nn/F.py,sha256=uHkq3yHe4in_LlOiAIc76WCapA1KgGwNi25Yr89XsHY,1805
 mlax/nn/F_rng.py,sha256=8SRdWsfgmLIYG5BFbR7YMGRZK2Mf7yZmilRAtJO8ujU,1497
 mlax/nn/Linear.py,sha256=eH5o8PXHA6dupqMCWieSqrmWRtRgWvYM0m977RCZhLs,3321
 mlax/nn/Scaler.py,sha256=YzKAKzTBbctBYVaLoop2Arlkyy3n26Ib5aS-kbjtB8s,2434
-mlax/nn/__init__.py,sha256=ECb9bgB6S45hgHu3inmW5FjPZnSUKTLQvjYdTXmCFEQ,326
+mlax/nn/__init__.py,sha256=J3lZ_I0Hxh01IGlLx4XtSq4L2JpbZZUrXdw30qDO-_U,319
 mlax/nn/batchnorm.py,sha256=2S2u-vl1JiansqYW6-l7_QdbsCoaoIlqn3KCPxbv0uA,4964
-mlax/nn/bias.py,sha256=7cQQS-MPTIVIX39i_QTUDc1TXN-T_zmfenm6tUoIJMU,3171
-mlax/nn/conv.py,sha256=3csCouX4dwXz65V67Jxz_jFdXMgQxxLgh3kVIAztkbY,8117
+mlax/nn/bias.py,sha256=e-rjzRuslAH3kQvHAaRvYl8j3fHzPwaiKCkemqxoweM,2379
+mlax/nn/conv.py,sha256=5EN3xiooiaA_vOVRDW6GROXMIF9N6qS0-eum_Uc1Ie0,8635
 mlax/nn/dropout.py,sha256=3w8B0v0DpfiOcK_hMGzrGuusmEMBADTZ1h2Xw924AX4,922
-mlax/nn/embed.py,sha256=yM3WXZgo67PToe5GogXLEQMKu7DpW5V-oQeSP2D1NhA,1934
-mlax/nn/f.py,sha256=AYiyK6VZ7fluLguKXVzYpnRS4yuKiBdHC-TXMs9avJQ,3009
-mlax/nn/functional.py,sha256=w1JBZLFYcctsTNOOAhB1pXJxb59yG3pytDMKljwIFgo,12771
-mlax/nn/linear.py,sha256=xx3WE2GH8UDf4oGCCp56Jb90ElJ-FBVxR91QLVqY4ts,3778
-mlax/nn/parallel.py,sha256=szFRxp-LoATxqp46H1gPZIVhnK8312PficTdzIqPLgM,2641
-mlax/nn/scaler.py,sha256=XBN76lsXsKh-VE7CBZeMcPiGmmxAMiL7WOOo1X8u7wo,3188
-mlax/nn/series.py,sha256=GB9o7DXlbifBDzHhs4hzFMgwhPSmSaYLTT3xHvWbAtM,2404
+mlax/nn/embed.py,sha256=4qk3lSfyGK_b4eLO9PSik378lZdB7Yji96nQNuDKn4U,1691
+mlax/nn/f.py,sha256=qW1MenEHR4nXkAQmN9tPZjwX-1MIEchs_Kh1Tf4Zhs4,3347
+mlax/nn/functional.py,sha256=y5fIwI14BBZXWfJlzFvwp9do0dT7y0n_Xxz2qbFbqk0,13510
+mlax/nn/linear.py,sha256=avpg5SXoJZgudUuVkT0nGQ8HqNqAdB1-5p28GvS_xBE,3126
+mlax/nn/parallel.py,sha256=X5lyBlB8n7nQMEJ0CuWLsv_FpjMKfg4rCmj1ioBHhtk,2422
+mlax/nn/scaler.py,sha256=gXf3SwAemCHWRD1BDWEtISZKO-_b_iWqEc1Tmp5XqOc,2811
+mlax/nn/series.py,sha256=VPDBXeycu8x9vLBkoHMSxzufYDPVNAWRZNZnCQMtSo4,2245
+mlax/nn/z_norm.py,sha256=VYkrzYJsZYOk6rrpq1Qp6SBM7UagvHlhitBrK3gf7jg,4541
 mlax/optim/__init__.py,sha256=ZzdykiH-2yK5S1RYfNT6wnroNaZrrVd8wNE3ZzpQEL4,803
 mlax/optim/sgd.py,sha256=302mURIvgNHm1fiH-4FjXYK5mIA0KNHU_QLDZAEv948,2065
-mlax_nn-0.2.2.dist-info/LICENSE,sha256=3UTijYdg95TqgQneU3wkBBxpkF4M1S7EOsxILhTdbxU,1065
-mlax_nn-0.2.2.dist-info/METADATA,sha256=kNtCBdZO3W-3IsD5VEplhAME2Fe3TmyxwOGb0dz8nrs,3530
-mlax_nn-0.2.2.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-mlax_nn-0.2.2.dist-info/top_level.txt,sha256=0Zs5Ly0aaeJGAQ5eKAg1CJRaIalWnuATMEBgbfa6nv0,5
-mlax_nn-0.2.2.dist-info/RECORD,,
+mlax_nn-0.2.3.dist-info/LICENSE,sha256=3UTijYdg95TqgQneU3wkBBxpkF4M1S7EOsxILhTdbxU,1065
+mlax_nn-0.2.3.dist-info/METADATA,sha256=uZ6FfdY-nE-OuigF9-igLGHry7-ws_gamuEhGZas218,3964
+mlax_nn-0.2.3.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+mlax_nn-0.2.3.dist-info/top_level.txt,sha256=0Zs5Ly0aaeJGAQ5eKAg1CJRaIalWnuATMEBgbfa6nv0,5
+mlax_nn-0.2.3.dist-info/RECORD,,
```

