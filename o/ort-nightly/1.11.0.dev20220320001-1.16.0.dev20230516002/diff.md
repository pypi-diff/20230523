# Comparing `tmp/ort_nightly-1.11.0.dev20220320001-cp39-cp39-win_amd64.whl.zip` & `tmp/ort_nightly-1.16.0.dev20230516002-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,170 +1,228 @@
-Zip file size: 5547168 bytes, number of entries: 168
--rw-rw-rw-  2.0 fat     1094 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/LICENSE
--rw-rw-rw-  2.0 fat     2490 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/Privacy.md
--rw-rw-rw-  2.0 fat   247049 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/ThirdPartyNotices.txt
--rw-rw-rw-  2.0 fat     2481 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/__init__.py
--rw-rw-rw-  2.0 fat      320 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/backend/__init__.py
--rw-rw-rw-  2.0 fat     8058 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/backend/backend.py
--rw-rw-rw-  2.0 fat     1812 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/backend/backend_rep.py
--rw-rw-rw-  2.0 fat      251 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/__init__.py
--rw-rw-rw-  2.0 fat      413 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/_ld_preload.py
--rw-rw-rw-  2.0 fat     1542 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/_pybind_state.py
--rw-rw-rw-  2.0 fat     3795 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_collect_build_info.py
--rw-rw-rw-  2.0 fat    36964 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_inference_collection.py
--rw-rw-rw-  2.0 fat    20904 b- defN 22-Mar-21 05:22 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_providers_shared.dll
--rw-rw-rw-  2.0 fat 16138664 b- defN 22-Mar-21 05:22 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_pybind11_state.pyd
--rw-rw-rw-  2.0 fat     6299 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_validation.py
--rw-rw-rw-  2.0 fat       33 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/version_info.py
--rw-rw-rw-  2.0 fat      326 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/training/__init__.py
--rw-rw-rw-  2.0 fat      480 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/datasets/__init__.py
--rw-rw-rw-  2.0 fat      670 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/datasets/logreg_iris.onnx
--rw-rw-rw-  2.0 fat      130 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/datasets/mul_1.onnx
--rw-rw-rw-  2.0 fat      103 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/datasets/sigmoid.onnx
--rw-rw-rw-  2.0 fat      340 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/__init__.py
--rw-rw-rw-  2.0 fat    36119 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/calibrate.py
--rw-rw-rw-  2.0 fat    14885 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/onnx_model.py
--rw-rw-rw-  2.0 fat    49942 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/onnx_quantizer.py
--rw-rw-rw-  2.0 fat    16788 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/qdq_quantizer.py
--rw-rw-rw-  2.0 fat    14926 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/quant_utils.py
--rw-rw-rw-  2.0 fat    20034 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/quantize.py
--rw-rw-rw-  2.0 fat     3163 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/registry.py
--rw-rw-rw-  2.0 fat     2018 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
--rw-rw-rw-  2.0 fat     2417 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/__init__.py
--rw-rw-rw-  2.0 fat       83 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/__init__.py
--rw-rw-rw-  2.0 fat     3878 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/activation.py
--rw-rw-rw-  2.0 fat      585 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/argmax.py
--rw-rw-rw-  2.0 fat     2067 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/attention.py
--rw-rw-rw-  2.0 fat      947 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/base_operator.py
--rw-rw-rw-  2.0 fat     2412 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/binary_op.py
--rw-rw-rw-  2.0 fat     2236 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/concat.py
--rw-rw-rw-  2.0 fat     8434 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/conv.py
--rw-rw-rw-  2.0 fat     2924 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/direct_q8.py
--rw-rw-rw-  2.0 fat     3869 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/embed_layernorm.py
--rw-rw-rw-  2.0 fat     1348 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/gather.py
--rw-rw-rw-  2.0 fat     2180 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/gavgpool.py
--rw-rw-rw-  2.0 fat     4917 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/gemm.py
--rw-rw-rw-  2.0 fat     4836 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/lstm.py
--rw-rw-rw-  2.0 fat     5595 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/matmul.py
--rw-rw-rw-  2.0 fat      965 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/maxpool.py
--rw-rw-rw-  2.0 fat     4135 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/pad.py
--rw-rw-rw-  2.0 fat     1940 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/pooling.py
--rw-rw-rw-  2.0 fat      847 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/qdq_base_operator.py
--rw-rw-rw-  2.0 fat      966 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/resize.py
--rw-rw-rw-  2.0 fat     1716 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/split.py
--rw-rw-rw-  2.0 fat      520 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/__init__.py
--rw-rw-rw-  2.0 fat     2963 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/check_onnx_model_mobile_usability.py
--rw-rw-rw-  2.0 fat    15398 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/convert_onnx_models_to_ort.py
--rw-rw-rw-  2.0 fat     1600 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/file_utils.py
--rw-rw-rw-  2.0 fat      305 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/logger.py
--rw-rw-rw-  2.0 fat     2619 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/make_dynamic_shape_fixed.py
--rw-rw-rw-  2.0 fat    14440 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/onnx_model_utils.py
--rw-rw-rw-  2.0 fat     3796 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/onnx_randomizer.py
--rw-rw-rw-  2.0 fat     5633 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/onnxruntime_test.py
--rw-rw-rw-  2.0 fat     2015 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/optimize_onnx_model.py
--rw-rw-rw-  2.0 fat     3417 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/pytorch_export_contrib_ops.py
--rw-rw-rw-  2.0 fat     5933 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/pytorch_export_helpers.py
--rw-rw-rw-  2.0 fat    10144 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/reduced_build_config_parser.py
--rw-rw-rw-  2.0 fat   104248 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/symbolic_shape_infer.py
--rw-rw-rw-  2.0 fat     1194 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/update_onnx_opset.py
--rw-rw-rw-  2.0 fat     1232 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/__init__.py
--rw-rw-rw-  2.0 fat    27738 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
--rw-rw-rw-  2.0 fat     4470 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_model_processor.py
--rw-rw-rw-  2.0 fat     4171 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/types.py
--rw-rw-rw-  2.0 fat     2588 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
--rw-rw-rw-  2.0 fat     9310 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py
--rw-rw-rw-  2.0 fat      348 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py
--rw-rw-rw-  2.0 fat     1792 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py
--rw-rw-rw-  2.0 fat     1988 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py
--rw-rw-rw-  2.0 fat      176 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValueType.py
--rw-rw-rw-  2.0 fat     1076 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py
--rw-rw-rw-  2.0 fat     9000 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py
--rw-rw-rw-  2.0 fat     2413 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py
--rw-rw-rw-  2.0 fat     3515 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelCreateInfos.py
--rw-rw-rw-  2.0 fat     1728 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py
--rw-rw-rw-  2.0 fat     6145 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py
--rw-rw-rw-  2.0 fat     8635 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py
--rw-rw-rw-  2.0 fat     3339 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py
--rw-rw-rw-  2.0 fat     1765 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeIndexAndKernelDefHash.py
--rw-rw-rw-  2.0 fat      153 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeType.py
--rw-rw-rw-  2.0 fat     4785 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py
--rw-rw-rw-  2.0 fat     1621 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py
--rw-rw-rw-  2.0 fat     3421 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py
--rw-rw-rw-  2.0 fat     2954 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py
--rw-rw-rw-  2.0 fat     2253 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py
--rw-rw-rw-  2.0 fat     1437 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py
--rw-rw-rw-  2.0 fat     2690 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SessionState.py
--rw-rw-rw-  2.0 fat     1889 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py
--rw-rw-rw-  2.0 fat     3133 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py
--rw-rw-rw-  2.0 fat     1673 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py
--rw-rw-rw-  2.0 fat     1923 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SubGraphSessionState.py
--rw-rw-rw-  2.0 fat     5144 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py
--rw-rw-rw-  2.0 fat      408 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py
--rw-rw-rw-  2.0 fat     1828 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py
--rw-rw-rw-  2.0 fat     2039 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py
--rw-rw-rw-  2.0 fat      200 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py
--rw-rw-rw-  2.0 fat     2118 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py
--rw-rw-rw-  2.0 fat      259 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py
--rw-rw-rw-  2.0 fat       69 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/__init__.py
--rw-rw-rw-  2.0 fat     1361 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/affinity_helper.py
--rw-rw-rw-  2.0 fat    28796 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/benchmark.py
--rw-rw-rw-  2.0 fat    20898 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/benchmark_gpt2.py
--rw-rw-rw-  2.0 fat    16823 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/benchmark_helper.py
--rw-rw-rw-  2.0 fat    18117 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/bert_perf_test.py
--rw-rw-rw-  2.0 fat    18567 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/bert_test_data.py
--rw-rw-rw-  2.0 fat     8014 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/compare_bert_results.py
--rw-rw-rw-  2.0 fat    28173 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/convert_beam_search.py
--rw-rw-rw-  2.0 fat     6313 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/convert_tf_models_to_pytorch.py
--rw-rw-rw-  2.0 fat    24811 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/convert_to_onnx.py
--rw-rw-rw-  2.0 fat    18338 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/float16.py
--rw-rw-rw-  2.0 fat    22992 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_attention.py
--rw-rw-rw-  2.0 fat     2538 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_base.py
--rw-rw-rw-  2.0 fat     2374 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_biasgelu.py
--rw-rw-rw-  2.0 fat    30105 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_embedlayer.py
--rw-rw-rw-  2.0 fat    13363 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_fastgelu.py
--rw-rw-rw-  2.0 fat    10211 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gelu.py
--rw-rw-rw-  2.0 fat     1105 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gelu_approximation.py
--rw-rw-rw-  2.0 fat    18814 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gpt_attention.py
--rw-rw-rw-  2.0 fat    11031 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gpt_attention_megatron.py
--rw-rw-rw-  2.0 fat     8325 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gpt_attention_no_past.py
--rw-rw-rw-  2.0 fat    11381 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_layernorm.py
--rw-rw-rw-  2.0 fat     5128 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_options.py
--rw-rw-rw-  2.0 fat     6352 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_reshape.py
--rw-rw-rw-  2.0 fat     3788 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_shape.py
--rw-rw-rw-  2.0 fat     6776 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_skiplayernorm.py
--rw-rw-rw-  2.0 fat     9471 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_utils.py
--rw-rw-rw-  2.0 fat    45792 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_beamsearch_helper.py
--rw-rw-rw-  2.0 fat    17517 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_beamsearch_tester.py
--rw-rw-rw-  2.0 fat    42274 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_helper.py
--rw-rw-rw-  2.0 fat    17067 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_parity.py
--rw-rw-rw-  2.0 fat    19875 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_tester.py
--rw-rw-rw-  2.0 fat     8532 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/huggingface_models.py
--rw-rw-rw-  2.0 fat     7304 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/io_binding_helper.py
--rw-rw-rw-  2.0 fat     6960 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/machine_info.py
--rw-rw-rw-  2.0 fat    24377 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_exporter.py
--rw-rw-rw-  2.0 fat    38855 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model.py
--rw-rw-rw-  2.0 fat    11567 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_bart.py
--rw-rw-rw-  2.0 fat    17784 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_bert.py
--rw-rw-rw-  2.0 fat    18311 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_bert_keras.py
--rw-rw-rw-  2.0 fat    23496 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_bert_tf.py
--rw-rw-rw-  2.0 fat     3693 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_gpt2.py
--rw-rw-rw-  2.0 fat     8549 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_tnlr.py
--rw-rw-rw-  2.0 fat    17609 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/optimizer.py
--rw-rw-rw-  2.0 fat     6098 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/parity_check_helper.py
--rw-rw-rw-  2.0 fat    25387 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/profiler.py
--rw-rw-rw-  2.0 fat     3409 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/quantize_helper.py
--rw-rw-rw-  2.0 fat     3814 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/shape_infer_helper.py
--rw-rw-rw-  2.0 fat    15456 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/shape_optimizer.py
--rw-rw-rw-  2.0 fat     2542 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/torch_onnx_export_helper.py
--rw-rw-rw-  2.0 fat      388 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/longformer/__init__.py
--rw-rw-rw-  2.0 fat    21396 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/longformer/benchmark_longformer.py
--rw-rw-rw-  2.0 fat    14058 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/longformer/convert_longformer_to_onnx.py
--rw-rw-rw-  2.0 fat     9294 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/longformer/generate_test_data.py
--rw-rw-rw-  2.0 fat     3359 b- defN 22-Mar-21 05:18 ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/longformer/longformer_helper.py
--rw-rw-rw-  2.0 fat     3394 b- defN 22-Mar-21 05:23 ort_nightly-1.11.0.dev20220320001.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 22-Mar-21 05:23 ort_nightly-1.11.0.dev20220320001.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       78 b- defN 22-Mar-21 05:23 ort_nightly-1.11.0.dev20220320001.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat       12 b- defN 22-Mar-21 05:23 ort_nightly-1.11.0.dev20220320001.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    25514 b- defN 22-Mar-21 05:23 ort_nightly-1.11.0.dev20220320001.dist-info/RECORD
-168 files, 17814867 bytes uncompressed, 5502416 bytes compressed:  69.1%
+Zip file size: 6821077 bytes, number of entries: 226
+-rw-rw-rw-  2.0 fat     1094 b- defN 23-May-16 15:30 onnxruntime/LICENSE
+-rw-rw-rw-  2.0 fat     2490 b- defN 23-May-16 15:30 onnxruntime/Privacy.md
+-rw-rw-rw-  2.0 fat   318315 b- defN 23-May-16 15:30 onnxruntime/ThirdPartyNotices.txt
+-rw-rw-rw-  2.0 fat     4246 b- defN 23-May-16 15:30 onnxruntime/__init__.py
+-rw-rw-rw-  2.0 fat      334 b- defN 23-May-16 15:30 onnxruntime/backend/__init__.py
+-rw-rw-rw-  2.0 fat     8141 b- defN 23-May-16 15:30 onnxruntime/backend/backend.py
+-rw-rw-rw-  2.0 fat     1821 b- defN 23-May-16 15:30 onnxruntime/backend/backend_rep.py
+-rw-rw-rw-  2.0 fat      251 b- defN 23-May-16 15:30 onnxruntime/capi/__init__.py
+-rw-rw-rw-  2.0 fat      413 b- defN 23-May-16 15:30 onnxruntime/capi/_ld_preload.py
+-rw-rw-rw-  2.0 fat     1544 b- defN 23-May-16 15:30 onnxruntime/capi/_pybind_state.py
+-rw-rw-rw-  2.0 fat     4068 b- defN 23-May-16 15:30 onnxruntime/capi/onnxruntime_collect_build_info.py
+-rw-rw-rw-  2.0 fat    39683 b- defN 23-May-16 15:30 onnxruntime/capi/onnxruntime_inference_collection.py
+-rw-rw-rw-  2.0 fat    21936 b- defN 23-May-16 15:37 onnxruntime/capi/onnxruntime_providers_shared.dll
+-rw-rw-rw-  2.0 fat 19718584 b- defN 23-May-16 15:36 onnxruntime/capi/onnxruntime_pybind11_state.pyd
+-rw-rw-rw-  2.0 fat     6382 b- defN 23-May-16 15:30 onnxruntime/capi/onnxruntime_validation.py
+-rw-rw-rw-  2.0 fat       33 b- defN 23-May-16 15:30 onnxruntime/capi/version_info.py
+-rw-rw-rw-  2.0 fat      326 b- defN 23-May-16 15:30 onnxruntime/capi/training/__init__.py
+-rw-rw-rw-  2.0 fat      471 b- defN 23-May-16 15:30 onnxruntime/datasets/__init__.py
+-rw-rw-rw-  2.0 fat      670 b- defN 23-May-16 15:30 onnxruntime/datasets/logreg_iris.onnx
+-rw-rw-rw-  2.0 fat      130 b- defN 23-May-16 15:30 onnxruntime/datasets/mul_1.onnx
+-rw-rw-rw-  2.0 fat      103 b- defN 23-May-16 15:30 onnxruntime/datasets/sigmoid.onnx
+-rw-rw-rw-  2.0 fat      686 b- defN 23-May-16 15:30 onnxruntime/quantization/__init__.py
+-rw-rw-rw-  2.0 fat    38011 b- defN 23-May-16 15:30 onnxruntime/quantization/calibrate.py
+-rw-rw-rw-  2.0 fat    18620 b- defN 23-May-16 15:30 onnxruntime/quantization/onnx_model.py
+-rw-rw-rw-  2.0 fat    46664 b- defN 23-May-16 15:30 onnxruntime/quantization/onnx_quantizer.py
+-rw-rw-rw-  2.0 fat     5045 b- defN 23-May-16 15:30 onnxruntime/quantization/preprocess.py
+-rw-rw-rw-  2.0 fat    15307 b- defN 23-May-16 15:30 onnxruntime/quantization/qdq_loss_debug.py
+-rw-rw-rw-  2.0 fat    19061 b- defN 23-May-16 15:30 onnxruntime/quantization/qdq_quantizer.py
+-rw-rw-rw-  2.0 fat    20102 b- defN 23-May-16 15:30 onnxruntime/quantization/quant_utils.py
+-rw-rw-rw-  2.0 fat    28359 b- defN 23-May-16 15:30 onnxruntime/quantization/quantize.py
+-rw-rw-rw-  2.0 fat     3671 b- defN 23-May-16 15:30 onnxruntime/quantization/registry.py
+-rw-rw-rw-  2.0 fat     6149 b- defN 23-May-16 15:30 onnxruntime/quantization/shape_inference.py
+-rw-rw-rw-  2.0 fat     2250 b- defN 23-May-16 15:30 onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
+-rw-rw-rw-  2.0 fat     2665 b- defN 23-May-16 15:30 onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-16 15:30 onnxruntime/quantization/CalTableFlatBuffers/__init__.py
+-rw-rw-rw-  2.0 fat       85 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/__init__.py
+-rw-rw-rw-  2.0 fat     4463 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/activation.py
+-rw-rw-rw-  2.0 fat      589 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/argmax.py
+-rw-rw-rw-  2.0 fat     2637 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/attention.py
+-rw-rw-rw-  2.0 fat     1118 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/base_operator.py
+-rw-rw-rw-  2.0 fat     2544 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/binary_op.py
+-rw-rw-rw-  2.0 fat     2149 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/concat.py
+-rw-rw-rw-  2.0 fat     9695 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/conv.py
+-rw-rw-rw-  2.0 fat     3350 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/direct_q8.py
+-rw-rw-rw-  2.0 fat     4058 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/embed_layernorm.py
+-rw-rw-rw-  2.0 fat     2166 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/gather.py
+-rw-rw-rw-  2.0 fat     2445 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/gavgpool.py
+-rw-rw-rw-  2.0 fat     5905 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/gemm.py
+-rw-rw-rw-  2.0 fat     1114 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/instnorm.py
+-rw-rw-rw-  2.0 fat     5050 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/lstm.py
+-rw-rw-rw-  2.0 fat     7762 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/matmul.py
+-rw-rw-rw-  2.0 fat      961 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/maxpool.py
+-rw-rw-rw-  2.0 fat     4277 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/pad.py
+-rw-rw-rw-  2.0 fat     2285 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/pooling.py
+-rw-rw-rw-  2.0 fat      823 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/qdq_base_operator.py
+-rw-rw-rw-  2.0 fat      962 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/resize.py
+-rw-rw-rw-  2.0 fat     3386 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/softmax.py
+-rw-rw-rw-  2.0 fat     2244 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/split.py
+-rw-rw-rw-  2.0 fat     3127 b- defN 23-May-16 15:30 onnxruntime/quantization/operators/where.py
+-rw-rw-rw-  2.0 fat      528 b- defN 23-May-16 15:30 onnxruntime/tools/__init__.py
+-rw-rw-rw-  2.0 fat     2871 b- defN 23-May-16 15:30 onnxruntime/tools/check_onnx_model_mobile_usability.py
+-rw-rw-rw-  2.0 fat    16064 b- defN 23-May-16 15:30 onnxruntime/tools/convert_onnx_models_to_ort.py
+-rw-rw-rw-  2.0 fat     1569 b- defN 23-May-16 15:30 onnxruntime/tools/file_utils.py
+-rw-rw-rw-  2.0 fat      286 b- defN 23-May-16 15:30 onnxruntime/tools/logger.py
+-rw-rw-rw-  2.0 fat     2608 b- defN 23-May-16 15:30 onnxruntime/tools/make_dynamic_shape_fixed.py
+-rw-rw-rw-  2.0 fat     6380 b- defN 23-May-16 15:30 onnxruntime/tools/offline_tuning.py
+-rw-rw-rw-  2.0 fat    14355 b- defN 23-May-16 15:30 onnxruntime/tools/onnx_model_utils.py
+-rw-rw-rw-  2.0 fat     3361 b- defN 23-May-16 15:30 onnxruntime/tools/onnx_randomizer.py
+-rw-rw-rw-  2.0 fat     5657 b- defN 23-May-16 15:30 onnxruntime/tools/onnxruntime_test.py
+-rw-rw-rw-  2.0 fat     1949 b- defN 23-May-16 15:30 onnxruntime/tools/optimize_onnx_model.py
+-rw-rw-rw-  2.0 fat     4091 b- defN 23-May-16 15:30 onnxruntime/tools/pytorch_export_contrib_ops.py
+-rw-rw-rw-  2.0 fat     5971 b- defN 23-May-16 15:30 onnxruntime/tools/pytorch_export_helpers.py
+-rw-rw-rw-  2.0 fat    10137 b- defN 23-May-16 15:30 onnxruntime/tools/reduced_build_config_parser.py
+-rw-rw-rw-  2.0 fat   129449 b- defN 23-May-16 15:30 onnxruntime/tools/symbolic_shape_infer.py
+-rw-rw-rw-  2.0 fat     1182 b- defN 23-May-16 15:30 onnxruntime/tools/update_onnx_opset.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-16 15:30 onnxruntime/tools/mobile_helpers/__init__.py
+-rw-rw-rw-  2.0 fat    12691 b- defN 23-May-16 15:30 onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py
+-rw-rw-rw-  2.0 fat     1319 b- defN 23-May-16 15:30 onnxruntime/tools/mobile_helpers/coreml_supported_ops.md
+-rw-rw-rw-  2.0 fat     3069 b- defN 23-May-16 15:30 onnxruntime/tools/mobile_helpers/mobile_package.required_operators.config
+-rw-rw-rw-  2.0 fat     2226 b- defN 23-May-16 15:30 onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md
+-rw-rw-rw-  2.0 fat    25768 b- defN 23-May-16 15:30 onnxruntime/tools/mobile_helpers/usability_checker.py
+-rw-rw-rw-  2.0 fat     1378 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/__init__.py
+-rw-rw-rw-  2.0 fat    27366 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
+-rw-rw-rw-  2.0 fat     4484 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_model_processor.py
+-rw-rw-rw-  2.0 fat     4154 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/types.py
+-rw-rw-rw-  2.0 fat     2604 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
+-rw-rw-rw-  2.0 fat      149 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgType.py
+-rw-rw-rw-  2.0 fat     1611 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgTypeAndIndex.py
+-rw-rw-rw-  2.0 fat     9310 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py
+-rw-rw-rw-  2.0 fat      348 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py
+-rw-rw-rw-  2.0 fat     3754 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedKernelCreateInfos.py
+-rw-rw-rw-  2.0 fat     1924 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedNodeIndexAndKernelDefHash.py
+-rw-rw-rw-  2.0 fat     2939 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSessionState.py
+-rw-rw-rw-  2.0 fat     2112 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSubGraphSessionState.py
+-rw-rw-rw-  2.0 fat     1792 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py
+-rw-rw-rw-  2.0 fat     1988 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py
+-rw-rw-rw-  2.0 fat      176 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValueType.py
+-rw-rw-rw-  2.0 fat     1076 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py
+-rw-rw-rw-  2.0 fat     9000 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py
+-rw-rw-rw-  2.0 fat     2477 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py
+-rw-rw-rw-  2.0 fat     2532 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrArgsEntry.py
+-rw-rw-rw-  2.0 fat     2244 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrResolver.py
+-rw-rw-rw-  2.0 fat     1728 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py
+-rw-rw-rw-  2.0 fat     6145 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py
+-rw-rw-rw-  2.0 fat     8635 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py
+-rw-rw-rw-  2.0 fat     3339 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py
+-rw-rw-rw-  2.0 fat      153 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeType.py
+-rw-rw-rw-  2.0 fat     4785 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py
+-rw-rw-rw-  2.0 fat     2664 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OpIdKernelTypeStrArgsEntry.py
+-rw-rw-rw-  2.0 fat     1621 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py
+-rw-rw-rw-  2.0 fat     3194 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py
+-rw-rw-rw-  2.0 fat     2954 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py
+-rw-rw-rw-  2.0 fat     2253 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py
+-rw-rw-rw-  2.0 fat     1437 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py
+-rw-rw-rw-  2.0 fat     1889 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py
+-rw-rw-rw-  2.0 fat     3133 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py
+-rw-rw-rw-  2.0 fat     1673 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py
+-rw-rw-rw-  2.0 fat     5144 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py
+-rw-rw-rw-  2.0 fat      408 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py
+-rw-rw-rw-  2.0 fat     1828 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py
+-rw-rw-rw-  2.0 fat     2039 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py
+-rw-rw-rw-  2.0 fat      200 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py
+-rw-rw-rw-  2.0 fat     2118 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py
+-rw-rw-rw-  2.0 fat      253 b- defN 23-May-16 15:30 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-16 15:30 onnxruntime/tools/qdq_helpers/__init__.py
+-rw-rw-rw-  2.0 fat     1279 b- defN 23-May-16 15:30 onnxruntime/tools/qdq_helpers/optimize_qdq_model.py
+-rw-rw-rw-  2.0 fat      552 b- defN 23-May-16 15:30 onnxruntime/transformers/__init__.py
+-rw-rw-rw-  2.0 fat     1442 b- defN 23-May-16 15:30 onnxruntime/transformers/affinity_helper.py
+-rw-rw-rw-  2.0 fat    33334 b- defN 23-May-16 15:30 onnxruntime/transformers/benchmark.py
+-rw-rw-rw-  2.0 fat    20969 b- defN 23-May-16 15:30 onnxruntime/transformers/benchmark_helper.py
+-rw-rw-rw-  2.0 fat    19728 b- defN 23-May-16 15:30 onnxruntime/transformers/bert_perf_test.py
+-rw-rw-rw-  2.0 fat    19039 b- defN 23-May-16 15:30 onnxruntime/transformers/bert_test_data.py
+-rw-rw-rw-  2.0 fat     7639 b- defN 23-May-16 15:30 onnxruntime/transformers/compare_bert_results.py
+-rw-rw-rw-  2.0 fat      740 b- defN 23-May-16 15:30 onnxruntime/transformers/constants.py
+-rw-rw-rw-  2.0 fat   111200 b- defN 23-May-16 15:30 onnxruntime/transformers/convert_generation.py
+-rw-rw-rw-  2.0 fat     6705 b- defN 23-May-16 15:30 onnxruntime/transformers/convert_tf_models_to_pytorch.py
+-rw-rw-rw-  2.0 fat    10057 b- defN 23-May-16 15:30 onnxruntime/transformers/convert_to_packing_mode.py
+-rw-rw-rw-  2.0 fat    22810 b- defN 23-May-16 15:30 onnxruntime/transformers/float16.py
+-rw-rw-rw-  2.0 fat    50487 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_attention.py
+-rw-rw-rw-  2.0 fat    18666 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_attention_unet.py
+-rw-rw-rw-  2.0 fat    12600 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_attention_vae.py
+-rw-rw-rw-  2.0 fat    18807 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_bart_attention.py
+-rw-rw-rw-  2.0 fat     2820 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_base.py
+-rw-rw-rw-  2.0 fat     2066 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_bias_add.py
+-rw-rw-rw-  2.0 fat     2300 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_biasgelu.py
+-rw-rw-rw-  2.0 fat     4516 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_biassplitgelu.py
+-rw-rw-rw-  2.0 fat    35646 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_embedlayer.py
+-rw-rw-rw-  2.0 fat    13324 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_fastgelu.py
+-rw-rw-rw-  2.0 fat    10180 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_gelu.py
+-rw-rw-rw-  2.0 fat     1076 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_gelu_approximation.py
+-rw-rw-rw-  2.0 fat     4262 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_gemmfastgelu.py
+-rw-rw-rw-  2.0 fat    22327 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_gpt_attention.py
+-rw-rw-rw-  2.0 fat    13887 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_gpt_attention_megatron.py
+-rw-rw-rw-  2.0 fat    11023 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_gpt_attention_no_past.py
+-rw-rw-rw-  2.0 fat     7693 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_group_norm.py
+-rw-rw-rw-  2.0 fat    12194 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_layernorm.py
+-rw-rw-rw-  2.0 fat     3592 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_nhwc_conv.py
+-rw-rw-rw-  2.0 fat    10607 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_options.py
+-rw-rw-rw-  2.0 fat    17163 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_qordered_attention.py
+-rw-rw-rw-  2.0 fat     4393 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_qordered_gelu.py
+-rw-rw-rw-  2.0 fat     4915 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_qordered_layernorm.py
+-rw-rw-rw-  2.0 fat     8566 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_qordered_matmul.py
+-rw-rw-rw-  2.0 fat     6463 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_reshape.py
+-rw-rw-rw-  2.0 fat     3845 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_shape.py
+-rw-rw-rw-  2.0 fat     8269 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_skiplayernorm.py
+-rw-rw-rw-  2.0 fat     6980 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_transpose.py
+-rw-rw-rw-  2.0 fat    12229 b- defN 23-May-16 15:30 onnxruntime/transformers/fusion_utils.py
+-rw-rw-rw-  2.0 fat     9130 b- defN 23-May-16 15:30 onnxruntime/transformers/huggingface_models.py
+-rw-rw-rw-  2.0 fat     7726 b- defN 23-May-16 15:30 onnxruntime/transformers/io_binding_helper.py
+-rw-rw-rw-  2.0 fat     7336 b- defN 23-May-16 15:30 onnxruntime/transformers/machine_info.py
+-rw-rw-rw-  2.0 fat    25364 b- defN 23-May-16 15:30 onnxruntime/transformers/onnx_exporter.py
+-rw-rw-rw-  2.0 fat    49231 b- defN 23-May-16 15:30 onnxruntime/transformers/onnx_model.py
+-rw-rw-rw-  2.0 fat     5397 b- defN 23-May-16 15:30 onnxruntime/transformers/onnx_model_bart.py
+-rw-rw-rw-  2.0 fat    20938 b- defN 23-May-16 15:30 onnxruntime/transformers/onnx_model_bert.py
+-rw-rw-rw-  2.0 fat    19132 b- defN 23-May-16 15:30 onnxruntime/transformers/onnx_model_bert_keras.py
+-rw-rw-rw-  2.0 fat    25561 b- defN 23-May-16 15:30 onnxruntime/transformers/onnx_model_bert_tf.py
+-rw-rw-rw-  2.0 fat     1067 b- defN 23-May-16 15:30 onnxruntime/transformers/onnx_model_clip.py
+-rw-rw-rw-  2.0 fat     3747 b- defN 23-May-16 15:30 onnxruntime/transformers/onnx_model_gpt2.py
+-rw-rw-rw-  2.0 fat    31346 b- defN 23-May-16 15:30 onnxruntime/transformers/onnx_model_t5.py
+-rw-rw-rw-  2.0 fat     8682 b- defN 23-May-16 15:30 onnxruntime/transformers/onnx_model_tnlr.py
+-rw-rw-rw-  2.0 fat     7100 b- defN 23-May-16 15:30 onnxruntime/transformers/onnx_model_unet.py
+-rw-rw-rw-  2.0 fat     1515 b- defN 23-May-16 15:30 onnxruntime/transformers/onnx_model_vae.py
+-rw-rw-rw-  2.0 fat    19209 b- defN 23-May-16 15:30 onnxruntime/transformers/optimizer.py
+-rw-rw-rw-  2.0 fat    25009 b- defN 23-May-16 15:30 onnxruntime/transformers/profiler.py
+-rw-rw-rw-  2.0 fat     2825 b- defN 23-May-16 15:30 onnxruntime/transformers/quantize_helper.py
+-rw-rw-rw-  2.0 fat     4590 b- defN 23-May-16 15:30 onnxruntime/transformers/shape_infer_helper.py
+-rw-rw-rw-  2.0 fat    15555 b- defN 23-May-16 15:30 onnxruntime/transformers/shape_optimizer.py
+-rw-rw-rw-  2.0 fat     2507 b- defN 23-May-16 15:30 onnxruntime/transformers/torch_onnx_export_helper.py
+-rw-rw-rw-  2.0 fat     4285 b- defN 23-May-16 15:30 onnxruntime/transformers/models/bart/export.py
+-rw-rw-rw-  2.0 fat      252 b- defN 23-May-16 15:30 onnxruntime/transformers/models/bert/__init__.py
+-rw-rw-rw-  2.0 fat    10783 b- defN 23-May-16 15:30 onnxruntime/transformers/models/bert/eval_squad.py
+-rw-rw-rw-  2.0 fat      252 b- defN 23-May-16 15:30 onnxruntime/transformers/models/gpt2/__init__.py
+-rw-rw-rw-  2.0 fat    16036 b- defN 23-May-16 15:30 onnxruntime/transformers/models/gpt2/benchmark_gpt2.py
+-rw-rw-rw-  2.0 fat    20751 b- defN 23-May-16 15:30 onnxruntime/transformers/models/gpt2/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat    41547 b- defN 23-May-16 15:30 onnxruntime/transformers/models/gpt2/gpt2_helper.py
+-rw-rw-rw-  2.0 fat    18322 b- defN 23-May-16 15:30 onnxruntime/transformers/models/gpt2/gpt2_parity.py
+-rw-rw-rw-  2.0 fat    20178 b- defN 23-May-16 15:30 onnxruntime/transformers/models/gpt2/gpt2_tester.py
+-rw-rw-rw-  2.0 fat     5906 b- defN 23-May-16 15:30 onnxruntime/transformers/models/gpt2/parity_check_helper.py
+-rw-rw-rw-  2.0 fat      252 b- defN 23-May-16 15:30 onnxruntime/transformers/models/longformer/__init__.py
+-rw-rw-rw-  2.0 fat    30370 b- defN 23-May-16 15:30 onnxruntime/transformers/models/longformer/benchmark_longformer.py
+-rw-rw-rw-  2.0 fat    15342 b- defN 23-May-16 15:30 onnxruntime/transformers/models/longformer/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat     9225 b- defN 23-May-16 15:30 onnxruntime/transformers/models/longformer/generate_test_data.py
+-rw-rw-rw-  2.0 fat     3180 b- defN 23-May-16 15:30 onnxruntime/transformers/models/longformer/longformer_helper.py
+-rw-rw-rw-  2.0 fat      252 b- defN 23-May-16 15:30 onnxruntime/transformers/models/stable_diffusion/__init__.py
+-rw-rw-rw-  2.0 fat    22218 b- defN 23-May-16 15:30 onnxruntime/transformers/models/stable_diffusion/benchmark.py
+-rw-rw-rw-  2.0 fat    12072 b- defN 23-May-16 15:30 onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py
+-rw-rw-rw-  2.0 fat      252 b- defN 23-May-16 15:30 onnxruntime/transformers/models/t5/__init__.py
+-rw-rw-rw-  2.0 fat     9108 b- defN 23-May-16 15:30 onnxruntime/transformers/models/t5/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat     6987 b- defN 23-May-16 15:30 onnxruntime/transformers/models/t5/past_helper.py
+-rw-rw-rw-  2.0 fat    17207 b- defN 23-May-16 15:30 onnxruntime/transformers/models/t5/t5_decoder.py
+-rw-rw-rw-  2.0 fat     6407 b- defN 23-May-16 15:30 onnxruntime/transformers/models/t5/t5_encoder.py
+-rw-rw-rw-  2.0 fat    12324 b- defN 23-May-16 15:30 onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py
+-rw-rw-rw-  2.0 fat    11158 b- defN 23-May-16 15:30 onnxruntime/transformers/models/t5/t5_helper.py
+-rw-rw-rw-  2.0 fat      321 b- defN 23-May-16 15:30 onnxruntime/transformers/models/whisper/__init__.py
+-rw-rw-rw-  2.0 fat    12006 b- defN 23-May-16 15:30 onnxruntime/transformers/models/whisper/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat     4209 b- defN 23-May-16 15:30 onnxruntime/transformers/models/whisper/whisper_chain.py
+-rw-rw-rw-  2.0 fat    15577 b- defN 23-May-16 15:30 onnxruntime/transformers/models/whisper/whisper_decoder.py
+-rw-rw-rw-  2.0 fat     5576 b- defN 23-May-16 15:30 onnxruntime/transformers/models/whisper/whisper_encoder.py
+-rw-rw-rw-  2.0 fat    12247 b- defN 23-May-16 15:30 onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py
+-rw-rw-rw-  2.0 fat    10529 b- defN 23-May-16 15:30 onnxruntime/transformers/models/whisper/whisper_helper.py
+-rw-rw-rw-  2.0 fat     4164 b- defN 23-May-16 15:37 ort_nightly-1.16.0.dev20230516002.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 23-May-16 15:37 ort_nightly-1.16.0.dev20230516002.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       78 b- defN 23-May-16 15:37 ort_nightly-1.16.0.dev20230516002.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat       12 b- defN 23-May-16 15:37 ort_nightly-1.16.0.dev20230516002.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat    24355 b- defN 23-May-16 15:37 ort_nightly-1.16.0.dev20230516002.dist-info/RECORD
+226 files, 22066559 bytes uncompressed, 6780865 bytes compressed:  69.3%
```

## zipnote {}

```diff
@@ -1,505 +1,679 @@
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/LICENSE
+Filename: onnxruntime/LICENSE
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/Privacy.md
+Filename: onnxruntime/Privacy.md
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/ThirdPartyNotices.txt
+Filename: onnxruntime/ThirdPartyNotices.txt
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/__init__.py
+Filename: onnxruntime/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/backend/__init__.py
+Filename: onnxruntime/backend/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/backend/backend.py
+Filename: onnxruntime/backend/backend.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/backend/backend_rep.py
+Filename: onnxruntime/backend/backend_rep.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/__init__.py
+Filename: onnxruntime/capi/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/_ld_preload.py
+Filename: onnxruntime/capi/_ld_preload.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/_pybind_state.py
+Filename: onnxruntime/capi/_pybind_state.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_collect_build_info.py
+Filename: onnxruntime/capi/onnxruntime_collect_build_info.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_inference_collection.py
+Filename: onnxruntime/capi/onnxruntime_inference_collection.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_providers_shared.dll
+Filename: onnxruntime/capi/onnxruntime_providers_shared.dll
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_pybind11_state.pyd
+Filename: onnxruntime/capi/onnxruntime_pybind11_state.pyd
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_validation.py
+Filename: onnxruntime/capi/onnxruntime_validation.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/version_info.py
+Filename: onnxruntime/capi/version_info.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/training/__init__.py
+Filename: onnxruntime/capi/training/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/datasets/__init__.py
+Filename: onnxruntime/datasets/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/datasets/logreg_iris.onnx
+Filename: onnxruntime/datasets/logreg_iris.onnx
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/datasets/mul_1.onnx
+Filename: onnxruntime/datasets/mul_1.onnx
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/datasets/sigmoid.onnx
+Filename: onnxruntime/datasets/sigmoid.onnx
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/__init__.py
+Filename: onnxruntime/quantization/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/calibrate.py
+Filename: onnxruntime/quantization/calibrate.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/onnx_model.py
+Filename: onnxruntime/quantization/onnx_model.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/onnx_quantizer.py
+Filename: onnxruntime/quantization/onnx_quantizer.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/qdq_quantizer.py
+Filename: onnxruntime/quantization/preprocess.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/quant_utils.py
+Filename: onnxruntime/quantization/qdq_loss_debug.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/quantize.py
+Filename: onnxruntime/quantization/qdq_quantizer.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/registry.py
+Filename: onnxruntime/quantization/quant_utils.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
+Filename: onnxruntime/quantization/quantize.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
+Filename: onnxruntime/quantization/registry.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/__init__.py
+Filename: onnxruntime/quantization/shape_inference.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/__init__.py
+Filename: onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/activation.py
+Filename: onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/argmax.py
+Filename: onnxruntime/quantization/CalTableFlatBuffers/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/attention.py
+Filename: onnxruntime/quantization/operators/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/base_operator.py
+Filename: onnxruntime/quantization/operators/activation.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/binary_op.py
+Filename: onnxruntime/quantization/operators/argmax.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/concat.py
+Filename: onnxruntime/quantization/operators/attention.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/conv.py
+Filename: onnxruntime/quantization/operators/base_operator.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/direct_q8.py
+Filename: onnxruntime/quantization/operators/binary_op.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/embed_layernorm.py
+Filename: onnxruntime/quantization/operators/concat.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/gather.py
+Filename: onnxruntime/quantization/operators/conv.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/gavgpool.py
+Filename: onnxruntime/quantization/operators/direct_q8.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/gemm.py
+Filename: onnxruntime/quantization/operators/embed_layernorm.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/lstm.py
+Filename: onnxruntime/quantization/operators/gather.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/matmul.py
+Filename: onnxruntime/quantization/operators/gavgpool.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/maxpool.py
+Filename: onnxruntime/quantization/operators/gemm.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/pad.py
+Filename: onnxruntime/quantization/operators/instnorm.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/pooling.py
+Filename: onnxruntime/quantization/operators/lstm.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/qdq_base_operator.py
+Filename: onnxruntime/quantization/operators/matmul.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/resize.py
+Filename: onnxruntime/quantization/operators/maxpool.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/split.py
+Filename: onnxruntime/quantization/operators/pad.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/__init__.py
+Filename: onnxruntime/quantization/operators/pooling.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/check_onnx_model_mobile_usability.py
+Filename: onnxruntime/quantization/operators/qdq_base_operator.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/convert_onnx_models_to_ort.py
+Filename: onnxruntime/quantization/operators/resize.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/file_utils.py
+Filename: onnxruntime/quantization/operators/softmax.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/logger.py
+Filename: onnxruntime/quantization/operators/split.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/make_dynamic_shape_fixed.py
+Filename: onnxruntime/quantization/operators/where.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/onnx_model_utils.py
+Filename: onnxruntime/tools/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/onnx_randomizer.py
+Filename: onnxruntime/tools/check_onnx_model_mobile_usability.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/onnxruntime_test.py
+Filename: onnxruntime/tools/convert_onnx_models_to_ort.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/optimize_onnx_model.py
+Filename: onnxruntime/tools/file_utils.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/pytorch_export_contrib_ops.py
+Filename: onnxruntime/tools/logger.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/pytorch_export_helpers.py
+Filename: onnxruntime/tools/make_dynamic_shape_fixed.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/reduced_build_config_parser.py
+Filename: onnxruntime/tools/offline_tuning.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/symbolic_shape_infer.py
+Filename: onnxruntime/tools/onnx_model_utils.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/update_onnx_opset.py
+Filename: onnxruntime/tools/onnx_randomizer.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/__init__.py
+Filename: onnxruntime/tools/onnxruntime_test.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
+Filename: onnxruntime/tools/optimize_onnx_model.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_model_processor.py
+Filename: onnxruntime/tools/pytorch_export_contrib_ops.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/types.py
+Filename: onnxruntime/tools/pytorch_export_helpers.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/utils.py
+Filename: onnxruntime/tools/reduced_build_config_parser.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
+Filename: onnxruntime/tools/symbolic_shape_infer.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py
+Filename: onnxruntime/tools/update_onnx_opset.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py
+Filename: onnxruntime/tools/mobile_helpers/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py
+Filename: onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py
+Filename: onnxruntime/tools/mobile_helpers/coreml_supported_ops.md
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValueType.py
+Filename: onnxruntime/tools/mobile_helpers/mobile_package.required_operators.config
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py
+Filename: onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py
+Filename: onnxruntime/tools/mobile_helpers/usability_checker.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py
+Filename: onnxruntime/tools/ort_format_model/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelCreateInfos.py
+Filename: onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py
+Filename: onnxruntime/tools/ort_format_model/ort_model_processor.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py
+Filename: onnxruntime/tools/ort_format_model/types.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py
+Filename: onnxruntime/tools/ort_format_model/utils.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeIndexAndKernelDefHash.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgType.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeType.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgTypeAndIndex.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedKernelCreateInfos.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedNodeIndexAndKernelDefHash.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSessionState.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSubGraphSessionState.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SessionState.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValueType.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SubGraphSessionState.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrArgsEntry.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrResolver.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/__init__.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeType.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/affinity_helper.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/benchmark.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OpIdKernelTypeStrArgsEntry.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/benchmark_gpt2.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/benchmark_helper.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/bert_perf_test.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/bert_test_data.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/compare_bert_results.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/convert_beam_search.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/convert_tf_models_to_pytorch.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/convert_to_onnx.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/float16.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_attention.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_base.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_biasgelu.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_embedlayer.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_fastgelu.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gelu.py
+Filename: onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gelu_approximation.py
+Filename: onnxruntime/tools/qdq_helpers/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gpt_attention.py
+Filename: onnxruntime/tools/qdq_helpers/optimize_qdq_model.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gpt_attention_megatron.py
+Filename: onnxruntime/transformers/__init__.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gpt_attention_no_past.py
+Filename: onnxruntime/transformers/affinity_helper.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_layernorm.py
+Filename: onnxruntime/transformers/benchmark.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_options.py
+Filename: onnxruntime/transformers/benchmark_helper.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_reshape.py
+Filename: onnxruntime/transformers/bert_perf_test.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_shape.py
+Filename: onnxruntime/transformers/bert_test_data.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_skiplayernorm.py
+Filename: onnxruntime/transformers/compare_bert_results.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_utils.py
+Filename: onnxruntime/transformers/constants.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_beamsearch_helper.py
+Filename: onnxruntime/transformers/convert_generation.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_beamsearch_tester.py
+Filename: onnxruntime/transformers/convert_tf_models_to_pytorch.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_helper.py
+Filename: onnxruntime/transformers/convert_to_packing_mode.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_parity.py
+Filename: onnxruntime/transformers/float16.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_tester.py
+Filename: onnxruntime/transformers/fusion_attention.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/huggingface_models.py
+Filename: onnxruntime/transformers/fusion_attention_unet.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/io_binding_helper.py
+Filename: onnxruntime/transformers/fusion_attention_vae.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/machine_info.py
+Filename: onnxruntime/transformers/fusion_bart_attention.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_exporter.py
+Filename: onnxruntime/transformers/fusion_base.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model.py
+Filename: onnxruntime/transformers/fusion_bias_add.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_bart.py
+Filename: onnxruntime/transformers/fusion_biasgelu.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_bert.py
+Filename: onnxruntime/transformers/fusion_biassplitgelu.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_bert_keras.py
+Filename: onnxruntime/transformers/fusion_embedlayer.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_bert_tf.py
+Filename: onnxruntime/transformers/fusion_fastgelu.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_gpt2.py
+Filename: onnxruntime/transformers/fusion_gelu.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_tnlr.py
+Filename: onnxruntime/transformers/fusion_gelu_approximation.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/optimizer.py
+Filename: onnxruntime/transformers/fusion_gemmfastgelu.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/parity_check_helper.py
+Filename: onnxruntime/transformers/fusion_gpt_attention.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/profiler.py
+Filename: onnxruntime/transformers/fusion_gpt_attention_megatron.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/quantize_helper.py
+Filename: onnxruntime/transformers/fusion_gpt_attention_no_past.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/shape_infer_helper.py
+Filename: onnxruntime/transformers/fusion_group_norm.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/shape_optimizer.py
+Filename: onnxruntime/transformers/fusion_layernorm.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/torch_onnx_export_helper.py
+Filename: onnxruntime/transformers/fusion_nhwc_conv.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/longformer/__init__.py
+Filename: onnxruntime/transformers/fusion_options.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/longformer/benchmark_longformer.py
+Filename: onnxruntime/transformers/fusion_qordered_attention.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/longformer/convert_longformer_to_onnx.py
+Filename: onnxruntime/transformers/fusion_qordered_gelu.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/longformer/generate_test_data.py
+Filename: onnxruntime/transformers/fusion_qordered_layernorm.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/longformer/longformer_helper.py
+Filename: onnxruntime/transformers/fusion_qordered_matmul.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.dist-info/METADATA
+Filename: onnxruntime/transformers/fusion_reshape.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.dist-info/WHEEL
+Filename: onnxruntime/transformers/fusion_shape.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.dist-info/entry_points.txt
+Filename: onnxruntime/transformers/fusion_skiplayernorm.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.dist-info/top_level.txt
+Filename: onnxruntime/transformers/fusion_transpose.py
 Comment: 
 
-Filename: ort_nightly-1.11.0.dev20220320001.dist-info/RECORD
+Filename: onnxruntime/transformers/fusion_utils.py
+Comment: 
+
+Filename: onnxruntime/transformers/huggingface_models.py
+Comment: 
+
+Filename: onnxruntime/transformers/io_binding_helper.py
+Comment: 
+
+Filename: onnxruntime/transformers/machine_info.py
+Comment: 
+
+Filename: onnxruntime/transformers/onnx_exporter.py
+Comment: 
+
+Filename: onnxruntime/transformers/onnx_model.py
+Comment: 
+
+Filename: onnxruntime/transformers/onnx_model_bart.py
+Comment: 
+
+Filename: onnxruntime/transformers/onnx_model_bert.py
+Comment: 
+
+Filename: onnxruntime/transformers/onnx_model_bert_keras.py
+Comment: 
+
+Filename: onnxruntime/transformers/onnx_model_bert_tf.py
+Comment: 
+
+Filename: onnxruntime/transformers/onnx_model_clip.py
+Comment: 
+
+Filename: onnxruntime/transformers/onnx_model_gpt2.py
+Comment: 
+
+Filename: onnxruntime/transformers/onnx_model_t5.py
+Comment: 
+
+Filename: onnxruntime/transformers/onnx_model_tnlr.py
+Comment: 
+
+Filename: onnxruntime/transformers/onnx_model_unet.py
+Comment: 
+
+Filename: onnxruntime/transformers/onnx_model_vae.py
+Comment: 
+
+Filename: onnxruntime/transformers/optimizer.py
+Comment: 
+
+Filename: onnxruntime/transformers/profiler.py
+Comment: 
+
+Filename: onnxruntime/transformers/quantize_helper.py
+Comment: 
+
+Filename: onnxruntime/transformers/shape_infer_helper.py
+Comment: 
+
+Filename: onnxruntime/transformers/shape_optimizer.py
+Comment: 
+
+Filename: onnxruntime/transformers/torch_onnx_export_helper.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/bart/export.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/bert/__init__.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/bert/eval_squad.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/gpt2/__init__.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/gpt2/benchmark_gpt2.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/gpt2/convert_to_onnx.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/gpt2/gpt2_helper.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/gpt2/gpt2_parity.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/gpt2/gpt2_tester.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/gpt2/parity_check_helper.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/longformer/__init__.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/longformer/benchmark_longformer.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/longformer/convert_to_onnx.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/longformer/generate_test_data.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/longformer/longformer_helper.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/__init__.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/benchmark.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/t5/__init__.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/t5/convert_to_onnx.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/t5/past_helper.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/t5/t5_decoder.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/t5/t5_encoder.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/t5/t5_helper.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/__init__.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/convert_to_onnx.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/whisper_chain.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/whisper_decoder.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/whisper_encoder.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/whisper_helper.py
+Comment: 
+
+Filename: ort_nightly-1.16.0.dev20230516002.dist-info/METADATA
+Comment: 
+
+Filename: ort_nightly-1.16.0.dev20230516002.dist-info/WHEEL
+Comment: 
+
+Filename: ort_nightly-1.16.0.dev20230516002.dist-info/entry_points.txt
+Comment: 
+
+Filename: ort_nightly-1.16.0.dev20230516002.dist-info/top_level.txt
+Comment: 
+
+Filename: ort_nightly-1.16.0.dev20230516002.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/LICENSE` & `onnxruntime/LICENSE`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/Privacy.md` & `onnxruntime/Privacy.md`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/ThirdPartyNotices.txt` & `onnxruntime/ThirdPartyNotices.txt`

 * *Files 10% similar despite different names*

```diff
@@ -206,26 +206,215 @@
 _____
 
 onnx
 Open Neural Network Exchange
 
 Copyright (c) Facebook, Inc. and Microsoft Corporation. All rights reserved.
 
-MIT License
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
 
-Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the
-Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,
-and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
+   1. Definitions.
 
-The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
 
-THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
-MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR
-ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH
-THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
 
 _____
 
 Eigen
 
 MPL v2.0
 Mozilla Public License Version 2.0
@@ -844,24 +1033,30 @@
 INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 THE POSSIBILITY OF SUCH DAMAGE.
 
 _____
 
-gsl-lite
-
-gsl-lite is based on GSL: Guidelines Support Library.
-For more information see https://github.com/martinmoene/gsl-lite
+Microsoft GSL
 
-Copyright (c) 2015 Martin Moene
 Copyright (c) 2015 Microsoft Corporation. All rights reserved.
 
 This code is licensed under the MIT License (MIT).
 
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
+of the Software, and to permit persons to whom the Software is furnished to do
+so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
 THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE SOFTWARE.
@@ -2555,14 +2750,45 @@
 FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 SOFTWARE.
 
 _____
+nvidia/cutlass
+
+Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+SPDX-License-Identifier: BSD-3-Clause
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+1. Redistributions of source code must retain the above copyright notice, this
+list of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright notice,
+this list of conditions and the following disclaimer in the documentation
+and/or other materials provided with the distribution.
+
+3. Neither the name of the copyright holder nor the names of its
+contributors may be used to endorse or promote products derived from
+this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+_____
 Boost
 
 Boost Software License - Version 1.0 - August 17th, 2003
 
 Permission is hereby granted, free of charge, to any person or organization
 obtaining a copy of the software and accompanying documentation covered by
 this license (the "Software") to use, reproduce, display, distribute,
@@ -3277,43 +3503,14 @@
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
 
 
 _____
 
-NVlabs/cub
-
-Copyright (c) 2010-2011, Duane Merrill.  All rights reserved.
-Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are met:
-   *  Redistributions of source code must retain the above copyright
-      notice, this list of conditions and the following disclaimer.
-   *  Redistributions in binary form must reproduce the above copyright
-      notice, this list of conditions and the following disclaimer in the
-      documentation and/or other materials provided with the distribution.
-   *  Neither the name of the NVIDIA CORPORATION nor the
-      names of its contributors may be used to endorse or promote products
-      derived from this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
-DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
-(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
-LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
-ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
-_____
-
 microsoft/wil
 
 MIT License
 
 Copyright (c) Microsoft Corporation. All rights reserved.
 
 Permission is hereby granted, free of charge, to any person obtaining a copy
@@ -4772,8 +4969,1056 @@
 WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR
 ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
 ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+_____
+
+google/sentencepiece, https://github.com/google/sentencepiece
+(included when statically linked with onnxruntime-extensions)
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+_____
+
+dlfcn-win32/dlfcn-win32 is licensed under the MIT License
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in
+all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
+THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+THE SOFTWARE.
+
+_____
+
+The Python Imaging Library (PIL) is
+
+    Copyright © 1997-2011 by Secret Labs AB
+    Copyright © 1995-2011 by Fredrik Lundh
+
+Pillow is the friendly PIL fork. It is
+
+    Copyright © 2010-2023 by Alex Clark and contributors
+
+Like PIL, Pillow is licensed under the open source HPND License:
+
+By obtaining, using, and/or copying this software and/or its associated
+documentation, you agree that you have read, understood, and will comply
+with the following terms and conditions:
+
+Permission to use, copy, modify, and distribute this software and its
+associated documentation for any purpose and without fee is hereby granted,
+provided that the above copyright notice appears in all copies, and that
+both that copyright notice and this permission notice appear in supporting
+documentation, and that the name of Secret Labs AB or the author not be
+used in advertising or publicity pertaining to distribution of the software
+without specific, written prior permission.
+
+SECRET LABS AB AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS
+SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS.
+IN NO EVENT SHALL SECRET LABS AB OR THE AUTHOR BE LIABLE FOR ANY SPECIAL,
+INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM
+LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE
+OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
+PERFORMANCE OF THIS SOFTWARE.
+
+_____
+
+openssl/openssl, https://github.com/openssl/openssl
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        https://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+_____
+
+Tencent/rapidjson, https://github.com/Tencent/rapidjson
+
+Tencent is pleased to support the open source community by making RapidJSON available.
+
+Copyright (C) 2015 THL A29 Limited, a Tencent company, and Milo Yip.  All rights reserved.
+
+If you have downloaded a copy of the RapidJSON binary from Tencent, please note that the RapidJSON binary is licensed under the MIT License.
+If you have downloaded a copy of the RapidJSON source code from Tencent, please note that RapidJSON source code is licensed under the MIT License, except for the third-party components listed below which are subject to different license terms.  Your integration of RapidJSON into your own projects may require compliance with the MIT License, as well as the other licenses applicable to the third-party components included within RapidJSON. To avoid the problematic JSON license in your own projects, it's sufficient to exclude the bin/jsonchecker/ directory, as it's the only code under the JSON license.
+A copy of the MIT License is included in this file.
+
+Other dependencies and licenses:
+
+Open Source Software Licensed Under the BSD License:
+--------------------------------------------------------------------
+
+The msinttypes r29
+Copyright (c) 2006-2013 Alexander Chemeris
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
+
+* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
+* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
+* Neither the name of  copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS AND CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+Open Source Software Licensed Under the JSON License:
+--------------------------------------------------------------------
+
+json.org
+Copyright (c) 2002 JSON.org
+All Rights Reserved.
+
+JSON_checker
+Copyright (c) 2002 JSON.org
+All Rights Reserved.
+
+
+Terms of the JSON License:
+---------------------------------------------------
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
+
+The Software shall be used for Good, not Evil.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+
+Terms of the MIT License:
+--------------------------------------------------------------------
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+_____
+
+boostorg/boost, https://github.com/boostorg/boost
+
+Boost Software License - Version 1.0 - August 17th, 2003
+
+Permission is hereby granted, free of charge, to any person or organization
+obtaining a copy of the software and accompanying documentation covered by
+this license (the "Software") to use, reproduce, display, distribute,
+execute, and transmit the Software, and to prepare derivative works of the
+Software, and to permit third-parties to whom the Software is furnished to
+do so, all subject to the following:
+
+The copyright notices in the Software and this entire statement, including
+the above license grant, this restriction and the following disclaimer,
+must be included in all copies of the Software, in whole or in part, and
+all derivative works of the Software, unless such copies or derivative
+works are solely in the form of machine-executable object code generated by
+a source language processor.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
+SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
+FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
+ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+DEALINGS IN THE SOFTWARE.
+
+_____
+
+libb64/libb64, https://github.com/libb64/libb64
+
+Copyright-Only Dedication (based on United States law) or Public Domain Certification
+
+The person or persons who have associated work with this document (the "Dedicator" or "Certifier") hereby either (a) certifies that, to the best of his knowledge, the work of authorship identified is in the public domain of the country from which the work is published, or (b) hereby dedicates whatever copyright the dedicators holds in the work of authorship identified below (the "Work") to the public domain. A certifier, moreover, dedicates any copyright interest he may have in the associated work, and for these purposes, is described as a "dedicator" below.
+
+A certifier has taken reasonable steps to verify the copyright status of this work. Certifier recognizes that his good faith efforts may not shield him from liability if in fact the work certified is not in the public domain.
+
+Dedicator makes this dedication for the benefit of the public at large and to the detriment of the Dedicator's heirs and successors. Dedicator intends this dedication to be an overt act of relinquishment in perpetuity of all present and future rights under copyright law, whether vested or contingent, in the Work. Dedicator understands that such relinquishment of all rights includes the relinquishment of all rights to enforce (by lawsuit or otherwise) those copyrights in the Work.
+
+Dedicator recognizes that, once placed in the public domain, the Work may be freely reproduced, distributed, transmitted, used, modified, built upon, or otherwise exploited by anyone for any purpose, commercial or non-commercial, and in any way, including by methods that have not yet been invented or conceived.
+
+_____
+
+posix pthread library, https://sourceforge.net/projects/pthreads4w
+
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+_____
+
+Triton Inference Server & Client, https://github.com/triton-inference-server
+
+Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+ * Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+ * Redistributions in binary form must reproduce the above copyright
+   notice, this list of conditions and the following disclaimer in the
+   documentation and/or other materials provided with the distribution.
+ * Neither the name of NVIDIA CORPORATION nor the names of its
+   contributors may be used to endorse or promote products derived
+   from this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+_____
+
+microsoft/mimalloc, https://github.com/microsoft/mimalloc
+
+MIT License
+
+Copyright (c) 2018-2021 Microsoft Corporation, Daan Leijen
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+_____
+
+TensorFlow.js
+
+https://github.com/tensorflow/tfjs
+
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+——
+
+curl/curl
+
+https://github.com/curl
+
+COPYRIGHT AND PERMISSION NOTICE
+
+Copyright (C) Daniel Stenberg, <daniel@haxx.se>, and many
+contributors, see the THANKS file.
+
+All rights reserved.
+
+Permission to use, copy, modify, and distribute this software for any purpose
+with or without fee is hereby granted, provided that the above copyright
+notice and this permission notice appear in all copies.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS. IN
+NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
+DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE
+OR OTHER DEALINGS IN THE SOFTWARE.
+
+Except as contained in this notice, the name of a copyright holder shall not
+be used in advertising or otherwise to promote the sale, use or other dealings
+in this Software without prior written authorization of the copyright holder.
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/backend/backend.py` & `onnxruntime/backend/backend.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,38 +1,39 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 """
 Implements ONNX's backend API.
 """
-from onnx import ModelProto
-from onnx import helper
-from onnx import version
-from onnx.checker import check_model
+import os
+import unittest
+
+import packaging.version
+from onnx import ModelProto, helper, version  # noqa: F401
 from onnx.backend.base import Backend
-from onnxruntime import InferenceSession, SessionOptions, get_device, get_available_providers
+from onnx.checker import check_model
+
+from onnxruntime import InferenceSession, SessionOptions, get_available_providers, get_device
 from onnxruntime.backend.backend_rep import OnnxRuntimeBackendRep
-import unittest
-import os
 
 
 class OnnxRuntimeBackend(Backend):
     """
     Implements
-    `ONNX's backend API <https://github.com/onnx/onnx/blob/master/docs/ImplementingAnOnnxBackend.md>`_
+    `ONNX's backend API <https://github.com/onnx/onnx/blob/main/docs/ImplementingAnOnnxBackend.md>`_
     with *ONNX Runtime*.
     The backend is mostly used when you need to switch between
     multiple runtimes with the same API.
     `Importing models from ONNX to Caffe2 <https://github.com/onnx/tutorials/blob/master/tutorials/OnnxCaffe2Import.ipynb>`_
     shows how to use *caffe2* as a backend for a converted model.
     Note: This is not the official Python API.
-    """  # noqa: E501
+    """
 
-    allowReleasedOpsetsOnly = bool(os.getenv('ALLOW_RELEASED_ONNX_OPSET_ONLY', '1') == '1')
+    allowReleasedOpsetsOnly = bool(os.getenv("ALLOW_RELEASED_ONNX_OPSET_ONLY", "1") == "1")  # noqa: N815
 
     @classmethod
     def is_compatible(cls, model, device=None, **kwargs):
         """
         Return whether the model is compatible with the backend.
 
         :param model: unused
@@ -51,41 +52,45 @@
         To test new opsets env variable ALLOW_RELEASED_ONNX_OPSET_ONLY should be set to 0
 
         :param model: Model whose opsets needed to be verified.
         :return: boolean and error message if opset is not supported.
         """
         if cls.allowReleasedOpsetsOnly:
             for opset in model.opset_import:
-                domain = opset.domain if opset.domain else 'ai.onnx'
+                domain = opset.domain if opset.domain else "ai.onnx"
                 try:
                     key = (domain, opset.version)
-                    if not (key in helper.OP_SET_ID_VERSION_MAP):
-                        error_message = ("Skipping this test as only released onnx opsets are supported."
-                                         "To run this test set env variable ALLOW_RELEASED_ONNX_OPSET_ONLY to 0."
-                                         " Got Domain '{0}' version '{1}'.".format(domain, opset.version))
+                    if key not in helper.OP_SET_ID_VERSION_MAP:
+                        error_message = (
+                            "Skipping this test as only released onnx opsets are supported."
+                            "To run this test set env variable ALLOW_RELEASED_ONNX_OPSET_ONLY to 0."
+                            " Got Domain '{}' version '{}'.".format(domain, opset.version)
+                        )
                         return False, error_message
                 except AttributeError:
                     # for some CI pipelines accessing helper.OP_SET_ID_VERSION_MAP
                     # is generating attribute error. TODO investigate the pipelines to
                     # fix this error. Falling back to a simple version check when this error is encountered
-                    if (domain == 'ai.onnx' and opset.version > 12) or (domain == 'ai.ommx.ml' and opset.version > 2):
-                        error_message = ("Skipping this test as only released onnx opsets are supported."
-                                         "To run this test set env variable ALLOW_RELEASED_ONNX_OPSET_ONLY to 0."
-                                         " Got Domain '{0}' version '{1}'.".format(domain, opset.version))
+                    if (domain == "ai.onnx" and opset.version > 12) or (domain == "ai.ommx.ml" and opset.version > 2):
+                        error_message = (
+                            "Skipping this test as only released onnx opsets are supported."
+                            "To run this test set env variable ALLOW_RELEASED_ONNX_OPSET_ONLY to 0."
+                            " Got Domain '{}' version '{}'.".format(domain, opset.version)
+                        )
                         return False, error_message
         return True, ""
 
     @classmethod
     def supports_device(cls, device):
         """
         Check whether the backend is compiled with particular device support.
         In particular it's used in the testing suite.
         """
-        if device == 'CUDA':
-            device = 'GPU'
+        if device == "CUDA":
+            device = "GPU"
         return device in get_device()
 
     @classmethod
     def prepare(cls, model, device=None, **kwargs):
         """
         Load the model and creates a :class:`onnxruntime.InferenceSession`
         ready to be used as a backend.
@@ -104,31 +109,31 @@
             return OnnxRuntimeBackendRep(model)
         elif isinstance(model, (str, bytes)):
             options = SessionOptions()
             for k, v in kwargs.items():
                 if hasattr(options, k):
                     setattr(options, k, v)
 
-            excluded_providers = os.getenv('ORT_ONNX_BACKEND_EXCLUDE_PROVIDERS', default="").split(',')
+            excluded_providers = os.getenv("ORT_ONNX_BACKEND_EXCLUDE_PROVIDERS", default="").split(",")
             providers = [x for x in get_available_providers() if (x not in excluded_providers)]
 
             inf = InferenceSession(model, sess_options=options, providers=providers)
             # backend API is primarily used for ONNX test/validation. As such, we should disable session.run() fallback
             # which may hide test failures.
             inf.disable_fallback()
             if device is not None and not cls.supports_device(device):
-                raise RuntimeError("Incompatible device expected '{0}', got '{1}'".format(device, get_device()))
+                raise RuntimeError(f"Incompatible device expected '{device}', got '{get_device()}'")
             return cls.prepare(inf, device, **kwargs)
         else:
             # type: ModelProto
             # check_model serializes the model anyways, so serialize the model once here
             # and reuse it below in the cls.prepare call to avoid an additional serialization
             # only works with onnx >= 1.10.0 hence the version check
-            onnx_version = tuple(map(int, (version.version.split(".")[:3])))
-            onnx_supports_serialized_model_check = onnx_version >= (1, 10, 0)
+            onnx_version = packaging.version.parse(version.version) or packaging.version.Version("0")
+            onnx_supports_serialized_model_check = onnx_version.release >= (1, 10, 0)
             bin_or_model = model.SerializeToString() if onnx_supports_serialized_model_check else model
             check_model(bin_or_model)
             opset_supported, error_message = cls.is_opset_supported(model)
             if not opset_supported:
                 raise unittest.SkipTest(error_message)
             # Now bin might be serialized, if it's not we need to serialize it otherwise we'll have
             # an infinite recursive call
@@ -152,18 +157,18 @@
         :return: predictions
         """
         rep = cls.prepare(model, device, **kwargs)
         return rep.run(inputs, **kwargs)
 
     @classmethod
     def run_node(cls, node, inputs, device=None, outputs_info=None, **kwargs):
-        '''
+        """
         This method is not implemented as it is much more efficient
         to run a whole model than every node independently.
-        '''
+        """
         raise NotImplementedError("It is much more efficient to run a whole model than every node independently.")
 
 
 is_compatible = OnnxRuntimeBackend.is_compatible
 prepare = OnnxRuntimeBackend.prepare
 run = OnnxRuntimeBackend.run_model
 supports_device = OnnxRuntimeBackend.supports_device
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/backend/backend_rep.py` & `onnxruntime/backend/backend_rep.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,19 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 """
 Implements ONNX's backend API.
 """
-from onnxruntime import RunOptions
+from typing import Any, Tuple  # noqa: F401
+
 from onnx.backend.base import BackendRep
-from typing import Any, Tuple
+
+from onnxruntime import RunOptions
 
 
 class OnnxRuntimeBackendRep(BackendRep):
     """
     Computes the prediction for a pipeline converted into
     an :class:`onnxruntime.InferenceSession` node.
     """
@@ -42,10 +44,10 @@
                 return outs
             else:
                 output_names = [o.name for o in self._session.get_outputs()]
                 return [outs[name] for name in output_names]
         else:
             inp = self._session.get_inputs()
             if len(inp) != 1:
-                raise RuntimeError("Model expect {0} inputs".format(len(inp)))
+                raise RuntimeError(f"Model expect {len(inp)} inputs")
             inps = {inp[0].name: inputs}
             return self._session.run(None, inps, options)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/_pybind_state.py` & `onnxruntime/capi/_pybind_state.py`

 * *Files 0% similar despite different names*

```diff
@@ -25,9 +25,10 @@
         if not os.path.isfile(os.path.join(system_root, "System32", "vcruntime140_1.dll")):
             warnings.warn("Please install the 2019 Visual C++ runtime and then try again. "
                           "If you've installed the runtime in a non-standard location "
                           "(other than %SystemRoot%\System32), "
                           "make sure it can be found by setting the correct path.")
 
 
+
 from .onnxruntime_pybind11_state import *  # noqa
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_collect_build_info.py` & `onnxruntime/capi/onnxruntime_collect_build_info.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,89 +1,103 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
-import warnings
 import ctypes
 import sys
+import warnings
 
 
 def find_cudart_versions(build_env=False, build_cuda_version=None):
     # ctypes.CDLL and ctypes.util.find_library load the latest installed library.
     # it may not the the library that would be loaded by onnxruntime.
     # for example, in an environment with Cuda 11.1 and subsequently
     # conda cudatoolkit 10.2.89 installed. ctypes will find cudart 10.2. however,
     # onnxruntime built with Cuda 11.1 will find and load cudart for Cuda 11.1.
     # for the above reason, we need find all versions in the environment and
     # only give warnings if the expected cuda version is not found.
     # in onnxruntime build environment, we expected only one Cuda version.
-    if not sys.platform.startswith('linux'):
-        warnings.warn('find_cudart_versions only works on Linux')
+    if not sys.platform.startswith("linux"):
+        warnings.warn("find_cudart_versions only works on Linux")
         return None
 
     cudart_possible_versions = {None, build_cuda_version}
 
     def get_cudart_version(find_cudart_version=None):
-        cudart_lib_filename = 'libcudart.so'
+        cudart_lib_filename = "libcudart.so"
         if find_cudart_version:
-            cudart_lib_filename = cudart_lib_filename + '.' + find_cudart_version
+            cudart_lib_filename = cudart_lib_filename + "." + find_cudart_version
 
         try:
             cudart = ctypes.CDLL(cudart_lib_filename)
             cudart.cudaRuntimeGetVersion.restype = int
             cudart.cudaRuntimeGetVersion.argtypes = [ctypes.POINTER(ctypes.c_int)]
             version = ctypes.c_int()
             status = cudart.cudaRuntimeGetVersion(ctypes.byref(version))
             if status != 0:
                 return None
-        except: # noqa
+        except Exception:
             return None
 
         return version.value
 
     # use set to avoid duplications
-    cudart_found_versions = {
-        get_cudart_version(cudart_version) for cudart_version in cudart_possible_versions}
+    cudart_found_versions = {get_cudart_version(cudart_version) for cudart_version in cudart_possible_versions}
 
     # convert to list and remove None
     return [ver for ver in cudart_found_versions if ver]
 
 
 def find_cudnn_supported_cuda_versions(build_env=False):
     # comments in get_cudart_version apply here
-    if not sys.platform.startswith('linux'):
-        warnings.warn('find_cudnn_versions only works on Linux')
+    if not sys.platform.startswith("linux"):
+        warnings.warn("find_cudnn_versions only works on Linux")
 
     cudnn_possible_versions = {None}
     if not build_env:
         # if not in a build environment, there may be more than one installed cudnn.
         # https://developer.nvidia.com/rdp/cudnn-archive to include all that may support Cuda 10+.
-        cudnn_possible_versions.update({
-            '8.2',
-            '8.1.1', '8.1.0',
-            '8.0.5', '8.0.4', '8.0.3', '8.0.2', '8.0.1',
-            '7.6.5', '7.6.4', '7.6.3', '7.6.2', '7.6.1', '7.6.0',
-            '7.5.1', '7.5.0',
-            '7.4.2', '7.4.1',
-            '7.3.1', '7.3.0',
-        })
+        cudnn_possible_versions.update(
+            {
+                "8.2",
+                "8.1.1",
+                "8.1.0",
+                "8.0.5",
+                "8.0.4",
+                "8.0.3",
+                "8.0.2",
+                "8.0.1",
+                "7.6.5",
+                "7.6.4",
+                "7.6.3",
+                "7.6.2",
+                "7.6.1",
+                "7.6.0",
+                "7.5.1",
+                "7.5.0",
+                "7.4.2",
+                "7.4.1",
+                "7.3.1",
+                "7.3.0",
+            }
+        )
 
     def get_cudnn_supported_cuda_version(find_cudnn_version=None):
-        cudnn_lib_filename = 'libcudnn.so'
+        cudnn_lib_filename = "libcudnn.so"
         if find_cudnn_version:
-            cudnn_lib_filename = cudnn_lib_filename + '.' + find_cudnn_version
+            cudnn_lib_filename = cudnn_lib_filename + "." + find_cudnn_version
 
         # in cudnn.h cudnn version are calculated as:
         # #define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)
         try:
             cudnn = ctypes.CDLL(cudnn_lib_filename)
             # cudnn_ver = cudnn.cudnnGetVersion()
             cuda_ver = cudnn.cudnnGetCudartVersion()
             return cuda_ver
-        except: # noqa
+        except Exception:
             return None
 
     # use set to avoid duplications
     cuda_found_versions = {get_cudnn_supported_cuda_version(cudnn_version) for cudnn_version in cudnn_possible_versions}
 
     # convert to list and remove None
     return [ver for ver in cuda_found_versions if ver]
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_inference_collection.py` & `onnxruntime/capi/onnxruntime_inference_collection.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,32 +1,42 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
+from __future__ import annotations
+
 import collections
 import collections.abc
 import os
+import typing
 import warnings
+from typing import Any, Sequence
 
 from onnxruntime.capi import _pybind_state as C
 
+if typing.TYPE_CHECKING:
+    import onnxruntime
+
 
-def get_ort_device_type(device):
-    device_type = device if type(device) is str else device.type.lower()
-    if device_type == 'cuda':
+def get_ort_device_type(device_type: str, device_index) -> C.OrtDevice:
+    if device_type == "cuda":
         return C.OrtDevice.cuda()
-    elif device_type == 'cpu':
+    elif device_type == "cpu":
         return C.OrtDevice.cpu()
-    elif device_type == 'ort':
-        return C.get_ort_device(device.index).device_type()
+    elif device_type == "ort":
+        return C.get_ort_device(device_index).device_type()
     else:
-        raise Exception('Unsupported device type: ' + device_type)
+        raise Exception("Unsupported device type: " + device_type)
 
 
-def check_and_normalize_provider_args(providers, provider_options, available_provider_names):
+def check_and_normalize_provider_args(
+    providers: Sequence[str, tuple[str, dict[Any, Any]]] | None,
+    provider_options: Sequence[dict[Any, Any]] | None,
+    available_provider_names: Sequence[str],
+):
     """
     Validates the 'providers' and 'provider_options' arguments and returns a
         normalized version.
 
     :param providers: Optional sequence of providers in order of decreasing
         precedence. Values can either be provider names or tuples of
         (provider name, options dict).
@@ -48,19 +58,21 @@
     if providers is None:
         return [], []
 
     provider_name_to_options = collections.OrderedDict()
 
     def set_provider_options(name, options):
         if name not in available_provider_names:
-            warnings.warn("Specified provider '{}' is not in available provider names."
-                          "Available providers: '{}'".format(name, ", ".join(available_provider_names)))
+            warnings.warn(
+                "Specified provider '{}' is not in available provider names."
+                "Available providers: '{}'".format(name, ", ".join(available_provider_names))
+            )
 
         if name in provider_name_to_options:
-            warnings.warn("Duplicate provider '{}' encountered, ignoring.".format(name))
+            warnings.warn(f"Duplicate provider '{name}' encountered, ignoring.")
             return
 
         normalized_options = {str(key): str(value) for key, value in options.items()}
         provider_name_to_options[name] = normalized_options
 
     if not isinstance(providers, collections.abc.Sequence):
         raise ValueError("'providers' should be a sequence.")
@@ -81,29 +93,33 @@
         for name, options in zip(providers, provider_options):
             set_provider_options(name, options)
 
     else:
         for provider in providers:
             if isinstance(provider, str):
                 set_provider_options(provider, dict())
-            elif isinstance(provider, tuple) and len(provider) == 2 and \
-                    isinstance(provider[0], str) and isinstance(provider[1], dict):
+            elif (
+                isinstance(provider, tuple)
+                and len(provider) == 2
+                and isinstance(provider[0], str)
+                and isinstance(provider[1], dict)
+            ):
                 set_provider_options(provider[0], provider[1])
             else:
                 raise ValueError("'providers' values must be either strings or (string, dict) tuples.")
 
     return list(provider_name_to_options.keys()), list(provider_name_to_options.values())
 
 
 class Session:
     """
     This is the main class used to run a model.
     """
-    def __init__(self):
 
+    def __init__(self):
         # self._sess is managed by the derived class and relies on bindings from C.InferenceSession
         self._sess = None
         self._enable_fallback = True
 
     def get_session_options(self):
         "Return the session options. See :class:`onnxruntime.SessionOptions`."
         return self._sess_options
@@ -165,45 +181,53 @@
         Enable session.Run() fallback mechanism. If session.Run() fails due to an internal Execution Provider failure,
         reset the Execution Providers enabled for this session.
         If GPU is enabled, fall back to CUDAExecutionProvider.
         otherwise fall back to CPUExecutionProvider.
         """
         self._enable_fallback = True
 
+    def _validate_input(self, feed_input_names):
+        # import pdb; pdb.set_trace()
+        missing_input_names = []
+        for input in self._inputs_meta:
+            if input.name not in feed_input_names and not input.type.startswith("optional"):
+                missing_input_names.append(input.name)
+        if missing_input_names:
+            raise ValueError(
+                f"Required inputs ({missing_input_names}) are missing from input feed ({feed_input_names})."
+            )
+
     def run(self, output_names, input_feed, run_options=None):
         """
         Compute the predictions.
 
         :param output_names: name of the outputs
         :param input_feed: dictionary ``{ input_name: input_value }``
         :param run_options: See :class:`onnxruntime.RunOptions`.
+        :return: list of results, every result is either a numpy array,
+            a sparse tensor, a list or a dictionary.
 
         ::
 
             sess.run([output_name], {input_name: x})
         """
-        num_required_inputs = len(self._inputs_meta)
-        num_inputs = len(input_feed)
-        # the graph may have optional inputs used to override initializers. allow for that.
-        if num_inputs < num_required_inputs:
-            raise ValueError("Model requires {} inputs. Input Feed contains {}".format(num_required_inputs, num_inputs))
+        self._validate_input(list(input_feed.keys()))
         if not output_names:
             output_names = [output.name for output in self._outputs_meta]
         try:
             return self._sess.run(output_names, input_feed, run_options)
         except C.EPFail as err:
             if self._enable_fallback:
-                print("EP Error: {} using {}".format(str(err), self._providers))
-                print("Falling back to {} and retrying.".format(self._fallback_providers))
+                print(f"EP Error: {str(err)} using {self._providers}")
+                print(f"Falling back to {self._fallback_providers} and retrying.")
                 self.set_providers(self._fallback_providers)
                 # Fallback only once.
                 self.disable_fallback()
                 return self._sess.run(output_names, input_feed, run_options)
-            else:
-                raise
+            raise
 
     def run_with_ort_values(self, output_names, input_dict_ort_values, run_options=None):
         """
         Compute the predictions.
 
         :param output_names: name of the outputs
         :param input_dict_ort_values: dictionary ``{ input_name: input_ort_value }``
@@ -212,41 +236,39 @@
         :param run_options: See :class:`onnxruntime.RunOptions`.
         :return: an array of `OrtValue`
 
         ::
 
             sess.run([output_name], {input_name: x})
         """
+
         def invoke(sess, output_names, input_dict_ort_values, run_options):
             input_dict = {}
             for n, v in input_dict_ort_values.items():
                 input_dict[n] = v._get_c_value()
             result = sess.run_with_ort_values(input_dict, output_names, run_options)
+            if not isinstance(result, C.OrtValueVector):
+                raise TypeError("run_with_ort_values() must return a instance of type 'OrtValueVector'.")
             ort_values = [OrtValue(v) for v in result]
             return ort_values
 
-        num_required_inputs = len(self._inputs_meta)
-        num_inputs = len(input_dict_ort_values)
-        # the graph may have optional inputs used to override initializers. allow for that.
-        if num_inputs < num_required_inputs:
-            raise ValueError("Model requires {} inputs. Input Feed contains {}".format(num_required_inputs, num_inputs))
+        self._validate_input(list(input_dict_ort_values.keys()))
         if not output_names:
             output_names = [output.name for output in self._outputs_meta]
         try:
             return invoke(self._sess, output_names, input_dict_ort_values, run_options)
         except C.EPFail as err:
             if self._enable_fallback:
-                print("EP Error: {} using {}".format(str(err), self._providers))
-                print("Falling back to {} and retrying.".format(self._fallback_providers))
+                print(f"EP Error: {str(err)} using {self._providers}")
+                print(f"Falling back to {self._fallback_providers} and retrying.")
                 self.set_providers(self._fallback_providers)
                 # Fallback only once.
                 self.disable_fallback()
                 return invoke(self._sess, output_names, input_dict_ort_values, run_options)
-            else:
-                raise
+            raise
 
     def end_profiling(self):
         """
         End profiling and return results in a file.
 
         The results are stored in a filename if the option
         :meth:`onnxruntime.SessionOptions.enable_profiling`.
@@ -264,30 +286,57 @@
 
     def io_binding(self):
         "Return an onnxruntime.IOBinding object`."
         return IOBinding(self)
 
     def run_with_iobinding(self, iobinding, run_options=None):
         """
-         Compute the predictions.
+        Compute the predictions.
 
-         :param iobinding: the iobinding object that has graph inputs/outputs bind.
-         :param run_options: See :class:`onnxruntime.RunOptions`.
+        :param iobinding: the iobinding object that has graph inputs/outputs bind.
+        :param run_options: See :class:`onnxruntime.RunOptions`.
         """
         self._sess.run_with_iobinding(iobinding._iobinding, run_options)
 
+    def get_tuning_results(self):
+        return self._sess.get_tuning_results()
+
+    def set_tuning_results(self, results, *, error_on_invalid=False):
+        return self._sess.set_tuning_results(results, error_on_invalid)
+
+    def run_with_ortvaluevector(self, run_options, feed_names, feeds, fetch_names, fetches, fetch_devices):
+        """
+        Compute the predictions similar to other run_*() methods but with minimal C++/Python conversion overhead.
+
+        :param run_options: See :class:`onnxruntime.RunOptions`.
+        :param feed_names: list of input names.
+        :param feeds: list of input OrtValue.
+        :param fetch_names: list of output names.
+        :param fetches: list of output OrtValue.
+        :param fetch_devices: list of output devices.
+        """
+        self._sess.run_with_ortvaluevector(run_options, feed_names, feeds, fetch_names, fetches, fetch_devices)
+
 
 class InferenceSession(Session):
     """
     This is the main class used to run a model.
     """
-    def __init__(self, path_or_bytes, sess_options=None, providers=None, provider_options=None, **kwargs):
+
+    def __init__(
+        self,
+        path_or_bytes: str | bytes | os.PathLike,
+        sess_options: Sequence[onnxruntime.SessionOptions] | None = None,
+        providers: Sequence[str, tuple[str, dict[Any, Any]]] | None = None,
+        provider_options: Sequence[dict[Any, Any]] | None = None,
+        **kwargs,
+    ) -> None:
         """
-        :param path_or_bytes: filename or serialized ONNX or ORT format model in a byte string
-        :param sess_options: session options
+        :param path_or_bytes: Filename or serialized ONNX or ORT format model in a byte string.
+        :param sess_options: Session options.
         :param providers: Optional sequence of providers in order of decreasing
             precedence. Values can either be provider names or tuples of
             (provider name, options dict). If not provided, then all available
             providers are used with the default precedence.
         :param provider_options: Optional sequence of options dicts corresponding
             to the providers listed in 'providers'.
 
@@ -307,67 +356,72 @@
         are given in 'providers', 'provider_options' should not be used.
 
         The list of providers is ordered by precedence. For example
         `['CUDAExecutionProvider', 'CPUExecutionProvider']`
         means execute a node using `CUDAExecutionProvider`
         if capable, otherwise execute using `CPUExecutionProvider`.
         """
+        super().__init__()
 
-        Session.__init__(self)
-
-        if isinstance(path_or_bytes, str):
-            self._model_path = path_or_bytes
+        if isinstance(path_or_bytes, (str, os.PathLike)):
+            self._model_path = os.fspath(path_or_bytes)
             self._model_bytes = None
         elif isinstance(path_or_bytes, bytes):
             self._model_path = None
             self._model_bytes = path_or_bytes  # TODO: This is bad as we're holding the memory indefinitely
         else:
-            raise TypeError("Unable to load from type '{0}'".format(type(path_or_bytes)))
+            raise TypeError(f"Unable to load from type '{type(path_or_bytes)}'")
 
         self._sess_options = sess_options
         self._sess_options_initial = sess_options
         self._enable_fallback = True
-        self._read_config_from_model = os.environ.get('ORT_LOAD_CONFIG_FROM_MODEL') == '1'
+        self._read_config_from_model = os.environ.get("ORT_LOAD_CONFIG_FROM_MODEL") == "1"
 
         # internal parameters that we don't expect to be used in general so aren't documented
-        disabled_optimizers = kwargs['disabled_optimizers'] if 'disabled_optimizers' in kwargs else None
+        disabled_optimizers = kwargs["disabled_optimizers"] if "disabled_optimizers" in kwargs else None
 
         try:
             self._create_inference_session(providers, provider_options, disabled_optimizers)
-        except ValueError:
+        except (ValueError, RuntimeError) as e:
             if self._enable_fallback:
-                print("EP Error using {}".format(providers))
-                print("Falling back to {} and retrying.".format(self._fallback_providers))
-                self._create_inference_session(self._fallback_providers, None)
-                # Fallback only once.
-                self.disable_fallback()
-            else:
-                raise
+                try:
+                    print(f"EP Error {e} when using {providers}")
+                    print(f"Falling back to {self._fallback_providers} and retrying.")
+                    self._create_inference_session(self._fallback_providers, None)
+                    # Fallback only once.
+                    self.disable_fallback()
+                    return
+                except Exception as fallback_error:
+                    raise fallback_error from e
+            # Fallback is disabled. Raise the original error.
+            raise e
 
     def _create_inference_session(self, providers, provider_options, disabled_optimizers=None):
         available_providers = C.get_available_providers()
 
         # Tensorrt can fall back to CUDA. All others fall back to CPU.
-        if 'TensorrtExecutionProvider' in available_providers:
-            self._fallback_providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
-        elif 'MIGraphXExecutionProvider' in available_providers:
-            self._fallback_providers = ['ROCMExecutionProvider', 'CPUExecutionProvider']
+        if "TensorrtExecutionProvider" in available_providers:
+            self._fallback_providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
+        elif "MIGraphXExecutionProvider" in available_providers:
+            self._fallback_providers = ["ROCMExecutionProvider", "CPUExecutionProvider"]
         else:
-            self._fallback_providers = ['CPUExecutionProvider']
+            self._fallback_providers = ["CPUExecutionProvider"]
 
         # validate providers and provider_options before other initialization
-        providers, provider_options = check_and_normalize_provider_args(providers,
-                                                                        provider_options,
-                                                                        available_providers)
-        if providers == [] and len(available_providers) > 1:
+        providers, provider_options = check_and_normalize_provider_args(
+            providers, provider_options, available_providers
+        )
+        if not providers and len(available_providers) > 1:
             self.disable_fallback()
-            raise ValueError("This ORT build has {} enabled. ".format(available_providers) +
-                             "Since ORT 1.9, you are required to explicitly set " +
-                             "the providers parameter when instantiating InferenceSession. For example, "
-                             "onnxruntime.InferenceSession(..., providers={}, ...)".format(available_providers))
+            raise ValueError(
+                f"This ORT build has {available_providers} enabled. "
+                "Since ORT 1.9, you are required to explicitly set "
+                "the providers parameter when instantiating InferenceSession. For example, "
+                f"onnxruntime.InferenceSession(..., providers={available_providers}, ...)"
+            )
 
         session_options = self._sess_options if self._sess_options else C.get_default_session_options()
         if self._model_path:
             sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
         else:
             sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
 
@@ -406,308 +460,373 @@
         # create a new C.InferenceSession
         self._sess = None
         self._sess_options = self._sess_options_initial
         self._create_inference_session(providers, provider_options)
 
 
 class IOBinding:
-    '''
+    """
     This class provides API to bind input/output to a specified device, e.g. GPU.
-    '''
-    def __init__(self, session):
+    """
+
+    def __init__(self, session: Session):
         self._iobinding = C.SessionIOBinding(session._sess)
         self._numpy_obj_references = {}
 
     def bind_cpu_input(self, name, arr_on_cpu):
-        '''
+        """
         bind an input to array on CPU
         :param name: input name
         :param arr_on_cpu: input values as a python array on CPU
-        '''
+        """
         # Hold a reference to the numpy object as the bound OrtValue is backed
         # directly by the data buffer of the numpy object and so the numpy object
         # must be around until this IOBinding instance is around
         self._numpy_obj_references[name] = arr_on_cpu
         self._iobinding.bind_input(name, arr_on_cpu)
 
     def bind_input(self, name, device_type, device_id, element_type, shape, buffer_ptr):
-        '''
+        """
         :param name: input name
         :param device_type: e.g. cpu, cuda
         :param device_id: device id, e.g. 0
         :param element_type: input element type
         :param shape: input shape
         :param buffer_ptr: memory pointer to input data
-        '''
-        self._iobinding.bind_input(name,
-                                   C.OrtDevice(get_ort_device_type(device_type), C.OrtDevice.default_memory(),
-                                               device_id),
-                                   element_type, shape, buffer_ptr)
+        """
+        self._iobinding.bind_input(
+            name,
+            C.OrtDevice(
+                get_ort_device_type(device_type, device_id),
+                C.OrtDevice.default_memory(),
+                device_id,
+            ),
+            element_type,
+            shape,
+            buffer_ptr,
+        )
 
     def bind_ortvalue_input(self, name, ortvalue):
-        '''
+        """
         :param name: input name
         :param ortvalue: OrtValue instance to bind
-        '''
+        """
         self._iobinding.bind_ortvalue_input(name, ortvalue._ortvalue)
 
     def synchronize_inputs(self):
         self._iobinding.synchronize_inputs()
 
-    def bind_output(self, name, device_type='cpu', device_id=0, element_type=None, shape=None, buffer_ptr=None):
-        '''
+    def bind_output(
+        self,
+        name,
+        device_type="cpu",
+        device_id=0,
+        element_type=None,
+        shape=None,
+        buffer_ptr=None,
+    ):
+        """
         :param name: output name
         :param device_type: e.g. cpu, cuda, cpu by default
         :param device_id: device id, e.g. 0
         :param element_type: output element type
         :param shape: output shape
         :param buffer_ptr: memory pointer to output data
-        '''
+        """
 
         # Follow the `if` path when the user has not provided any pre-allocated buffer but still
         # would like to bind an output to a specific device (e.g. cuda).
         # Pre-allocating an output buffer may not be an option for the user as :
         # (1) They may not want to use a custom allocator specific to the device they want to bind the output to,
         # in which case ORT will allocate the memory for the user
         # (2) The output has a dynamic shape and hence the size of the buffer may not be fixed across runs
         if buffer_ptr is None:
-            self._iobinding.bind_output(name,
-                                        C.OrtDevice(get_ort_device_type(device_type), C.OrtDevice.default_memory(),
-                                                    device_id))
+            self._iobinding.bind_output(
+                name,
+                C.OrtDevice(
+                    get_ort_device_type(device_type, device_id),
+                    C.OrtDevice.default_memory(),
+                    device_id,
+                ),
+            )
         else:
             if element_type is None or shape is None:
                 raise ValueError("`element_type` and `shape` are to be provided if pre-allocated memory is provided")
-            self._iobinding.bind_output(name,
-                                        C.OrtDevice(get_ort_device_type(device_type), C.OrtDevice.default_memory(),
-                                                    device_id),
-                                        element_type, shape, buffer_ptr)
+            self._iobinding.bind_output(
+                name,
+                C.OrtDevice(
+                    get_ort_device_type(device_type, device_id),
+                    C.OrtDevice.default_memory(),
+                    device_id,
+                ),
+                element_type,
+                shape,
+                buffer_ptr,
+            )
 
     def bind_ortvalue_output(self, name, ortvalue):
-        '''
+        """
         :param name: output name
         :param ortvalue: OrtValue instance to bind
-        '''
+        """
         self._iobinding.bind_ortvalue_output(name, ortvalue._ortvalue)
 
     def synchronize_outputs(self):
         self._iobinding.synchronize_outputs()
 
     def get_outputs(self):
-        '''
+        """
         Returns the output OrtValues from the Run() that preceded the call.
         The data buffer of the obtained OrtValues may not reside on CPU memory
-        '''
-        returned_ortvalues = []
-
-        for ortvalue in self._iobinding.get_outputs():
-            returned_ortvalues.append(OrtValue(ortvalue))
+        """
+        outputs = self._iobinding.get_outputs()
+        if not isinstance(outputs, C.OrtValueVector):
+            raise TypeError("get_outputs() must return an instance of type 'OrtValueVector'.")
+        return [OrtValue(ortvalue) for ortvalue in outputs]
 
-        return returned_ortvalues
+    def get_outputs_as_ortvaluevector(self):
+        return self._iobinding.get_outputs()
 
     def copy_outputs_to_cpu(self):
-        '''Copy output contents to CPU (if on another device). No-op if already on the CPU.'''
+        """Copy output contents to CPU (if on another device). No-op if already on the CPU."""
         return self._iobinding.copy_outputs_to_cpu()
 
     def clear_binding_inputs(self):
         self._iobinding.clear_binding_inputs()
 
     def clear_binding_outputs(self):
         self._iobinding.clear_binding_outputs()
 
 
 class OrtValue:
-    '''
+    """
     A data structure that supports all ONNX data formats (tensors and non-tensors) that allows users
     to place the data backing these on a device, for example, on a CUDA supported device.
     This class provides APIs to construct and deal with OrtValues.
-    '''
+    """
+
     def __init__(self, ortvalue, numpy_obj=None):
         if isinstance(ortvalue, C.OrtValue):
             self._ortvalue = ortvalue
             # Hold a ref count to the numpy object if the OrtValue is backed directly
             # by its data buffer so that it isn't destroyed when the OrtValue is in use
             self._numpy_obj = numpy_obj
         else:
             # An end user won't hit this error
-            raise ValueError("`Provided ortvalue` needs to be of type " +
-                             "`onnxruntime.capi.onnxruntime_pybind11_state.OrtValue`")
+            raise ValueError(
+                "`Provided ortvalue` needs to be of type `onnxruntime.capi.onnxruntime_pybind11_state.OrtValue`"
+            )
 
     def _get_c_value(self):
         return self._ortvalue
 
     @staticmethod
-    def ortvalue_from_numpy(numpy_obj, device_type='cpu', device_id=0):
-        '''
+    def ortvalue_from_numpy(numpy_obj, device_type="cpu", device_id=0):
+        """
         Factory method to construct an OrtValue (which holds a Tensor) from a given Numpy object
         A copy of the data in the Numpy object is held by the OrtValue only if the device is NOT cpu
 
         :param numpy_obj: The Numpy object to construct the OrtValue from
         :param device_type: e.g. cpu, cuda, cpu by default
         :param device_id: device id, e.g. 0
-        '''
+        """
         # Hold a reference to the numpy object (if device_type is 'cpu') as the OrtValue
         # is backed directly by the data buffer of the numpy object and so the numpy object
         # must be around until this OrtValue instance is around
-        return OrtValue(C.OrtValue.ortvalue_from_numpy(numpy_obj, C.OrtDevice(get_ort_device_type(device_type),
-                        C.OrtDevice.default_memory(), device_id)), numpy_obj if device_type.lower() == 'cpu' else None)
+        return OrtValue(
+            C.OrtValue.ortvalue_from_numpy(
+                numpy_obj,
+                C.OrtDevice(
+                    get_ort_device_type(device_type, device_id),
+                    C.OrtDevice.default_memory(),
+                    device_id,
+                ),
+            ),
+            numpy_obj if device_type.lower() == "cpu" else None,
+        )
 
     @staticmethod
-    def ortvalue_from_shape_and_type(shape=None, element_type=None, device_type='cpu', device_id=0):
-        '''
+    def ortvalue_from_shape_and_type(shape=None, element_type=None, device_type="cpu", device_id=0):
+        """
         Factory method to construct an OrtValue (which holds a Tensor) from given shape and element_type
 
         :param shape: List of integers indicating the shape of the OrtValue
         :param element_type: The data type of the elements in the OrtValue (numpy type)
         :param device_type: e.g. cpu, cuda, cpu by default
         :param device_id: device id, e.g. 0
-        '''
+        """
         if shape is None or element_type is None:
             raise ValueError("`element_type` and `shape` are to be provided if pre-allocated memory is provided")
 
-        return OrtValue(C.OrtValue.ortvalue_from_shape_and_type(shape, element_type,
-                        C.OrtDevice(get_ort_device_type(device_type), C.OrtDevice.default_memory(), device_id)))
+        return OrtValue(
+            C.OrtValue.ortvalue_from_shape_and_type(
+                shape,
+                element_type,
+                C.OrtDevice(
+                    get_ort_device_type(device_type, device_id),
+                    C.OrtDevice.default_memory(),
+                    device_id,
+                ),
+            )
+        )
 
     @staticmethod
     def ort_value_from_sparse_tensor(sparse_tensor):
-        '''
+        """
         The function will construct an OrtValue instance from a valid SparseTensor
         The new instance of OrtValue will assume the ownership of sparse_tensor
-        '''
+        """
         return OrtValue(C.OrtValue.ort_value_from_sparse_tensor(sparse_tensor._get_c_tensor()))
 
     def as_sparse_tensor(self):
-        '''
+        """
         The function will return SparseTensor contained in this OrtValue
-        '''
+        """
         return SparseTensor(self._ortvalue.as_sparse_tensor())
 
     def data_ptr(self):
-        '''
+        """
         Returns the address of the first element in the OrtValue's data buffer
-        '''
+        """
         return self._ortvalue.data_ptr()
 
     def device_name(self):
-        '''
+        """
         Returns the name of the device where the OrtValue's data buffer resides e.g. cpu, cuda
-        '''
+        """
         return self._ortvalue.device_name().lower()
 
     def shape(self):
-        '''
+        """
         Returns the shape of the data in the OrtValue
-        '''
+        """
         return self._ortvalue.shape()
 
     def data_type(self):
-        '''
+        """
         Returns the data type of the data in the OrtValue
-        '''
+        """
         return self._ortvalue.data_type()
 
+    def element_type(self):
+        """
+        Returns the proto type of the data in the OrtValue
+        if the OrtValue is a tensor.
+        """
+        return self._ortvalue.element_type()
+
     def has_value(self):
-        '''
+        """
         Returns True if the OrtValue corresponding to an
         optional type contains data, else returns False
-        '''
+        """
         return self._ortvalue.has_value()
 
     def is_tensor(self):
-        '''
+        """
         Returns True if the OrtValue contains a Tensor, else returns False
-        '''
+        """
         return self._ortvalue.is_tensor()
 
     def is_sparse_tensor(self):
-        '''
+        """
         Returns True if the OrtValue contains a SparseTensor, else returns False
-        '''
+        """
         return self._ortvalue.is_sparse_tensor()
 
     def is_tensor_sequence(self):
-        '''
+        """
         Returns True if the OrtValue contains a Tensor Sequence, else returns False
-        '''
+        """
         return self._ortvalue.is_tensor_sequence()
 
     def numpy(self):
-        '''
+        """
         Returns a Numpy object from the OrtValue.
         Valid only for OrtValues holding Tensors. Throws for OrtValues holding non-Tensors.
         Use accessors to gain a reference to non-Tensor objects such as SparseTensor
-        '''
+        """
         return self._ortvalue.numpy()
 
     def update_inplace(self, np_arr):
-        '''
+        """
         Update the OrtValue in place with a new Numpy array. The numpy contents
         are copied over to the device memory backing the OrtValue. It can be used
         to update the input valuess for an InferenceSession with CUDA graph
         enabled or other scenarios where the OrtValue needs to be updated while
         the memory address can not be changed.
-        '''
+        """
         self._ortvalue.update_inplace(np_arr)
 
 
 class OrtDevice:
-    '''
+    """
     A data structure that exposes the underlying C++ OrtDevice
-    '''
+    """
+
     def __init__(self, c_ort_device):
-        '''
+        """
         Internal constructor
-        '''
+        """
         if isinstance(c_ort_device, C.OrtDevice):
             self._ort_device = c_ort_device
         else:
-            raise ValueError("`Provided object` needs to be of type " +
-                             "`onnxruntime.capi.onnxruntime_pybind11_state.OrtDevice`")
+            raise ValueError(
+                "`Provided object` needs to be of type `onnxruntime.capi.onnxruntime_pybind11_state.OrtDevice`"
+            )
 
     def _get_c_device(self):
-        '''
+        """
         Internal accessor to underlying object
-        '''
+        """
         return self._ort_device
 
     @staticmethod
     def make(ort_device_name, device_id):
-        return OrtDevice(C.OrtDevice(get_ort_device_type(ort_device_name),
-                                     C.OrtDevice.default_memory(), device_id))
+        return OrtDevice(
+            C.OrtDevice(
+                get_ort_device_type(ort_device_name, device_id),
+                C.OrtDevice.default_memory(),
+                device_id,
+            )
+        )
 
     def device_id(self):
         return self._ort_device.device_id()
 
     def device_type(self):
         return self._ort_device.device_type()
 
 
 class SparseTensor:
-    '''
+    """
     A data structure that project the C++ SparseTensor object
     The class provides API to work with the object.
     Depending on the format, the class will hold more than one buffer
     depending on the format
-    '''
+    """
+
     def __init__(self, sparse_tensor):
-        '''
+        """
         Internal constructor
-        '''
+        """
         if isinstance(sparse_tensor, C.SparseTensor):
             self._tensor = sparse_tensor
         else:
             # An end user won't hit this error
-            raise ValueError("`Provided object` needs to be of type " +
-                             "`onnxruntime.capi.onnxruntime_pybind11_state.SparseTensor`")
+            raise ValueError(
+                "`Provided object` needs to be of type `onnxruntime.capi.onnxruntime_pybind11_state.SparseTensor`"
+            )
 
     def _get_c_tensor(self):
         return self._tensor
 
     @staticmethod
     def sparse_coo_from_numpy(dense_shape, values, coo_indices, ort_device):
-        '''
+        """
         Factory method to construct a SparseTensor in COO format from given arguments
 
         :param dense_shape: 1-D  numpy array(int64) or a python list that contains a dense_shape of the sparse tensor
             must be on cpu memory
         :param values: a homogeneous, contiguous 1-D numpy array that contains non-zero elements of the tensor
             of a type.
         :param coo_indices:  contiguous numpy array(int64) that contains COO indices for the tensor. coo_indices may
@@ -718,21 +837,22 @@
             suppored for non-numeric data types.
 
         For primitive types, the method will map values and coo_indices arrays into native memory and will use
         them as backing storage. It will increment the reference count for numpy arrays and it will decrement it
         on GC. The buffers may reside in any storage either CPU or GPU.
         For strings and objects, it will create a copy of the arrays in CPU memory as ORT does not support those
         on other devices and their memory can not be mapped.
-        '''
-        return SparseTensor(C.SparseTensor.sparse_coo_from_numpy(dense_shape, values, coo_indices,
-                            ort_device._get_c_device()))
+        """
+        return SparseTensor(
+            C.SparseTensor.sparse_coo_from_numpy(dense_shape, values, coo_indices, ort_device._get_c_device())
+        )
 
     @staticmethod
     def sparse_csr_from_numpy(dense_shape, values, inner_indices, outer_indices, ort_device):
-        '''
+        """
         Factory method to construct a SparseTensor in CSR format from given arguments
 
         :param dense_shape: 1-D numpy array(int64) or a python list that contains a dense_shape of the
             sparse tensor (rows, cols) must be on cpu memory
         :param values: a  contiguous, homogeneous 1-D numpy array that contains non-zero elements of the tensor
             of a type.
         :param inner_indices:  contiguous 1-D numpy array(int64) that contains CSR inner indices for the tensor.
@@ -743,100 +863,107 @@
             suppored for non-numeric data types.
 
         For primitive types, the method will map values and indices arrays into native memory and will use them as
         backing storage. It will increment the reference count and it will decrement then count when it is GCed.
         The buffers may reside in any storage either CPU or GPU.
         For strings and objects, it will create a copy of the arrays in CPU memory as ORT does not support those
         on other devices and their memory can not be mapped.
-        '''
-        return SparseTensor(C.SparseTensor.sparse_csr_from_numpy(dense_shape, values, inner_indices, outer_indices,
-                            ort_device._get_c_device()))
+        """
+        return SparseTensor(
+            C.SparseTensor.sparse_csr_from_numpy(
+                dense_shape,
+                values,
+                inner_indices,
+                outer_indices,
+                ort_device._get_c_device(),
+            )
+        )
 
     def values(self):
-        '''
+        """
         The method returns a numpy array that is backed by the native memory
         if the data type is numeric. Otherwise, the returned numpy array that contains
         copies of the strings.
-        '''
+        """
         return self._tensor.values()
 
     def as_coo_view(self):
-        '''
+        """
         The method will return coo representation of the sparse tensor which will enable
         querying COO indices. If the instance did not contain COO format, it would throw.
         You can query coo indices as:
 
         ::
 
             coo_indices = sparse_tensor.as_coo_view().indices()
 
         which will return a numpy array that is backed by the native memory.
-        '''
+        """
         return self._tensor.get_coo_data()
 
     def as_csrc_view(self):
-        '''
+        """
         The method will return CSR(C) representation of the sparse tensor which will enable
         querying CRS(C) indices. If the instance dit not contain CSR(C) format, it would throw.
         You can query indices as:
 
         ::
 
             inner_ndices = sparse_tensor.as_csrc_view().inner()
             outer_ndices = sparse_tensor.as_csrc_view().outer()
 
         returning numpy arrays backed by the native memory.
-        '''
+        """
         return self._tensor.get_csrc_data()
 
     def as_blocksparse_view(self):
-        '''
+        """
         The method will return coo representation of the sparse tensor which will enable
         querying BlockSparse indices. If the instance did not contain BlockSparse format, it would throw.
         You can query coo indices as:
 
         ::
 
             block_sparse_indices = sparse_tensor.as_blocksparse_view().indices()
 
         which will return a numpy array that is backed by the native memory
-        '''
+        """
         return self._tensor.get_blocksparse_data()
 
     def to_cuda(self, ort_device):
-        '''
+        """
         Returns a copy of this instance on the specified cuda device
 
         :param ort_device: with name 'cuda' and valid gpu device id
 
         The method will throw if:
 
         - this instance contains strings
         - this instance is already on GPU. Cross GPU copy is not supported
         - CUDA is not present in this build
         - if the specified device is not valid
-        '''
+        """
         return SparseTensor(self._tensor.to_cuda(ort_device._get_c_device()))
 
     def format(self):
-        '''
+        """
         Returns a OrtSparseFormat enumeration
-        '''
+        """
         return self._tensor.format
 
     def dense_shape(self):
-        '''
+        """
         Returns a numpy array(int64) containing a dense shape of a sparse tensor
-        '''
+        """
         return self._tensor.dense_shape()
 
     def data_type(self):
-        '''
+        """
         Returns a string data type of the data in the OrtValue
-        '''
+        """
         return self._tensor.data_type()
 
     def device_name(self):
-        '''
+        """
         Returns the name of the device where the SparseTensor data buffers reside e.g. cpu, cuda
-        '''
+        """
         return self._tensor.device_name().lower()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_providers_shared.dll` & `onnxruntime/capi/onnxruntime_providers_shared.dll`

 * *Files 12% similar despite different names*

### objdump

```diff
@@ -4,20 +4,20 @@
 start address 0x0000000180001370
 
 Characteristics 0x2022
 	executable
 	large address aware
 	DLL
 
-Time/Date		Mon Mar 21 05:11:29 2022
+Time/Date		Tue May 16 15:21:30 2023
 Magic			020b	(PE32+)
 MajorLinkerVersion	14
 MinorLinkerVersion	29
 SizeOfCode		0000000000001000
-SizeOfInitializedData	0000000000001c00
+SizeOfInitializedData	0000000000002000
 SizeOfUninitializedData	0000000000000000
 AddressOfEntryPoint	0000000000001370
 BaseOfCode		0000000000001000
 ImageBase		0000000180000000
 SectionAlignment	00001000
 FileAlignment		00000200
 MajorOSystemVersion	6
@@ -25,15 +25,15 @@
 MajorImageVersion	0
 MinorImageVersion	0
 MajorSubsystemVersion	6
 MinorSubsystemVersion	0
 Win32Version		00000000
 SizeOfImage		00007000
 SizeOfHeaders		00000400
-CheckSum		0000fc86
+CheckSum		00006e4d
 Subsystem		00000003	(Windows CUI)
 DllCharacteristics	00004160
 					HIGH_ENTROPY_VA
 					DYNAMIC_BASE
 					NX_COMPAT
 					GUARD_CF
 SizeOfStackReserve	0000000000100000
@@ -42,17 +42,17 @@
 SizeOfHeapCommit	0000000000001000
 LoaderFlags		00000000
 NumberOfRvaAndSizes	00000010
 
 The Data Directory
 Entry 0 0000000000002830 00000080 Export Directory [.edata (or where ever we found it)]
 Entry 1 00000000000028b0 00000050 Import Directory [parts of .idata]
-Entry 2 0000000000005000 000001e0 Resource Directory [.rsrc]
+Entry 2 0000000000005000 000005c8 Resource Directory [.rsrc]
 Entry 3 0000000000004000 000001b0 Exception Directory [.pdata]
-Entry 4 0000000000002a00 000027a8 Security Directory
+Entry 4 0000000000002e00 000027b0 Security Directory
 Entry 5 0000000000006000 00000028 Base Relocation Directory [.reloc]
 Entry 6 00000000000021a0 00000070 Debug Directory
 Entry 7 0000000000000000 00000000 Description Directory
 Entry 8 0000000000000000 00000000 Special Directory
 Entry 9 0000000000000000 00000000 Thread Storage Directory [.tls]
 Entry a 0000000000002210 00000138 Load Configuration Directory
 Entry b 0000000000000000 00000000 Bound Import Directory
@@ -87,29 +87,29 @@
 	2a48	   55  _initterm_e
 	2a3c	   54  _initterm
 
  000028d8	00002900 00000000 00000000 00002c6a 00002000
 
 	DLL Name: KERNEL32.dll
 	vma:  Hint/Ord Member-Name Bound-To
-	2bf4	  559  GetCurrentThreadId
-	2b32	 1272  RtlVirtualUnwind
-	2b18	 1265  RtlLookupFunctionEntry
-	2b04	 1257  RtlCaptureContext
-	2b62	 1431  SetUnhandledExceptionFilter
-	2c56	  919  IsDebuggerPresent
-	2c40	  897  InitializeSListHead
-	2c24	  300  DisableThreadLibraryCalls
-	2c0a	  769  GetSystemTimeAsFileTime
-	2b46	 1496  UnhandledExceptionFilter
-	2bde	  555  GetCurrentProcessId
-	2bc4	 1124  QueryPerformanceCounter
-	2ba8	  926  IsProcessorFeaturePresent
-	2b94	 1462  TerminateProcess
-	2b80	  554  GetCurrentProcess
+	2bf4	  567  GetCurrentThreadId
+	2b32	 1284  RtlVirtualUnwind
+	2b18	 1277  RtlLookupFunctionEntry
+	2b04	 1269  RtlCaptureContext
+	2b62	 1444  SetUnhandledExceptionFilter
+	2c56	  928  IsDebuggerPresent
+	2c40	  906  InitializeSListHead
+	2c24	  308  DisableThreadLibraryCalls
+	2c0a	  778  GetSystemTimeAsFileTime
+	2b46	 1510  UnhandledExceptionFilter
+	2bde	  563  GetCurrentProcessId
+	2bc4	 1136  QueryPerformanceCounter
+	2ba8	  936  IsProcessorFeaturePresent
+	2b94	 1476  TerminateProcess
+	2b80	  562  GetCurrentProcess
 
  000028ec	00000000 00000000 00000000 00000000 00000000
 
 There is an export table in .rdata at 0x180002830
 
 The Export Tables (interpreted .rdata section contents)
 
@@ -363,36 +363,36 @@
 	  260: 69 7a 65 5f 6e 61 72 72 6f 77 5f 65 6e 76 69 72
 	  270: 6f 6e 6d 65 6e 74 00 00 34 00 5f 69 6e 69 74 69
 	  280: 61 6c 69 7a 65 5f 6f 6e 65 78 69 74 5f 74 61 62
 	  290: 6c 65 00 00 22 00 5f 65 78 65 63 75 74 65 5f 6f
 	  2a0: 6e 65 78 69 74 5f 74 61 62 6c 65 00 16 00 5f 63
 	  2b0: 65 78 69 74 00 00 61 70 69 2d 6d 73 2d 77 69 6e
 	  2c0: 2d 63 72 74 2d 72 75 6e 74 69 6d 65 2d 6c 31 2d
-	  2d0: 31 2d 30 2e 64 6c 6c 00 e9 04 52 74 6c 43 61 70
-	  2e0: 74 75 72 65 43 6f 6e 74 65 78 74 00 f1 04 52 74
+	  2d0: 31 2d 30 2e 64 6c 6c 00 f5 04 52 74 6c 43 61 70
+	  2e0: 74 75 72 65 43 6f 6e 74 65 78 74 00 fd 04 52 74
 	  2f0: 6c 4c 6f 6f 6b 75 70 46 75 6e 63 74 69 6f 6e 45
-	  300: 6e 74 72 79 00 00 f8 04 52 74 6c 56 69 72 74 75
-	  310: 61 6c 55 6e 77 69 6e 64 00 00 d8 05 55 6e 68 61
+	  300: 6e 74 72 79 00 00 04 05 52 74 6c 56 69 72 74 75
+	  310: 61 6c 55 6e 77 69 6e 64 00 00 e6 05 55 6e 68 61
 	  320: 6e 64 6c 65 64 45 78 63 65 70 74 69 6f 6e 46 69
-	  330: 6c 74 65 72 00 00 97 05 53 65 74 55 6e 68 61 6e
+	  330: 6c 74 65 72 00 00 a4 05 53 65 74 55 6e 68 61 6e
 	  340: 64 6c 65 64 45 78 63 65 70 74 69 6f 6e 46 69 6c
-	  350: 74 65 72 00 2a 02 47 65 74 43 75 72 72 65 6e 74
-	  360: 50 72 6f 63 65 73 73 00 b6 05 54 65 72 6d 69 6e
-	  370: 61 74 65 50 72 6f 63 65 73 73 00 00 9e 03 49 73
+	  350: 74 65 72 00 32 02 47 65 74 43 75 72 72 65 6e 74
+	  360: 50 72 6f 63 65 73 73 00 c4 05 54 65 72 6d 69 6e
+	  370: 61 74 65 50 72 6f 63 65 73 73 00 00 a8 03 49 73
 	  380: 50 72 6f 63 65 73 73 6f 72 46 65 61 74 75 72 65
-	  390: 50 72 65 73 65 6e 74 00 64 04 51 75 65 72 79 50
+	  390: 50 72 65 73 65 6e 74 00 70 04 51 75 65 72 79 50
 	  3a0: 65 72 66 6f 72 6d 61 6e 63 65 43 6f 75 6e 74 65
-	  3b0: 72 00 2b 02 47 65 74 43 75 72 72 65 6e 74 50 72
-	  3c0: 6f 63 65 73 73 49 64 00 2f 02 47 65 74 43 75 72
-	  3d0: 72 65 6e 74 54 68 72 65 61 64 49 64 00 00 01 03
+	  3b0: 72 00 33 02 47 65 74 43 75 72 72 65 6e 74 50 72
+	  3c0: 6f 63 65 73 73 49 64 00 37 02 47 65 74 43 75 72
+	  3d0: 72 65 6e 74 54 68 72 65 61 64 49 64 00 00 0a 03
 	  3e0: 47 65 74 53 79 73 74 65 6d 54 69 6d 65 41 73 46
-	  3f0: 69 6c 65 54 69 6d 65 00 2c 01 44 69 73 61 62 6c
+	  3f0: 69 6c 65 54 69 6d 65 00 34 01 44 69 73 61 62 6c
 	  400: 65 54 68 72 65 61 64 4c 69 62 72 61 72 79 43 61
-	  410: 6c 6c 73 00 81 03 49 6e 69 74 69 61 6c 69 7a 65
-	  420: 53 4c 69 73 74 48 65 61 64 00 97 03 49 73 44 65
+	  410: 6c 6c 73 00 8a 03 49 6e 69 74 69 61 6c 69 7a 65
+	  420: 53 4c 69 73 74 48 65 61 64 00 a0 03 49 73 44 65
 	  430: 62 75 67 67 65 72 50 72 65 73 65 6e 74 00 4b 45
 	  440: 52 4e 45 4c 33 32 2e 64 6c 6c 00 00
  00000001800026e8 (rva: 000026e8): 0000000180001d96 - 0000000180001dad
 	Version: 1, Flags: none
 	Nbr codes: 2, Prologue size: 0x06, Frame offset: 0x0, Frame reg: none
 	  pc+0x06: alloc small area: rsp = rsp - 0x20
 	  pc+0x02: push rbp
@@ -429,42 +429,48 @@
 	reloc   14 offset  338 [2338] DIR64
 	reloc   15 offset  340 [2340] DIR64
 
 There is a debug directory in .rdata at 0x1800021a0
 
 Type                Size     Rva      Offset
   2        CodeView 00000066 000023a0 000017a0
-(format RSDS signature 3156bc2dfb754689abdbb39c6883208f age 1 pdb D:\a\_work\1\b\RelWithDebInfo\RelWithDebInfo\onnxruntime_providers_shared.pdb)
+(format RSDS signature 148799947b0c4d87a7bd6260cfb80c74 age 1 pdb C:\a\_work\1\b\RelWithDebInfo\RelWithDebInfo\onnxruntime_providers_shared.pdb)
  12         Feature 00000014 00002408 00001808
  13         CoffGrp 00000268 0000241c 0000181c
  20         Unknown 00000004 00002684 00001a84
 
 The .rsrc Resource Directory section:
-000  Type Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 1
-010   Entry: ID: 0x000018, Value: 0x80000018
-018    Name Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 1
-028     Entry: ID: 0x000002, Value: 0x80000030
-030      Language Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 1
-040       Entry: ID: 0x000409, Value: 0x000048
-048        Leaf: Addr: 0x005060, Size: 0x00017d, Codepage: 0
- Resources start at offset: 0x60
+000  Type Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 2
+010   Entry: ID: 0x000010, Value: 0x80000020
+020    Name Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 1
+030     Entry: ID: 0x000001, Value: 0x80000050
+050      Language Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 1
+060       Entry: ID: 0x000409, Value: 0x000080
+080        Leaf: Addr: 0x0050a0, Size: 0x0003a4, Codepage: 0
+018   Entry: ID: 0x000018, Value: 0x80000038
+038    Name Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 1
+048     Entry: ID: 0x000002, Value: 0x80000068
+068      Language Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 1
+078       Entry: ID: 0x000409, Value: 0x000090
+090        Leaf: Addr: 0x005448, Size: 0x00017d, Codepage: 0
+ Resources start at offset: 0xa0
 
 Sections:
 Idx Name          Size      VMA               LMA               File off  Algn
   0 .text         00000e28  0000000180001000  0000000180001000  00000400  2**4
                   CONTENTS, ALLOC, LOAD, READONLY, CODE
   1 .rdata        00000c78  0000000180002000  0000000180002000  00001400  2**4
                   CONTENTS, ALLOC, LOAD, READONLY, DATA
   2 .data         00000200  0000000180003000  0000000180003000  00002200  2**4
                   CONTENTS, ALLOC, LOAD, DATA
   3 .pdata        000001b0  0000000180004000  0000000180004000  00002400  2**2
                   CONTENTS, ALLOC, LOAD, READONLY, DATA
-  4 .rsrc         000001e0  0000000180005000  0000000180005000  00002600  2**2
+  4 .rsrc         000005c8  0000000180005000  0000000180005000  00002600  2**2
                   CONTENTS, ALLOC, LOAD, READONLY, DATA
-  5 .reloc        00000028  0000000180006000  0000000180006000  00002800  2**2
+  5 .reloc        00000028  0000000180006000  0000000180006000  00002c00  2**2
                   CONTENTS, ALLOC, LOAD, READONLY, DATA
 SYMBOL TABLE:
 no symbols
 
 
 
 Disassembly of section .text:
@@ -1675,56 +1681,52 @@
    18000219a:	(bad)
    18000219b:	(bad)
    18000219c:	(bad)
    18000219d:	(bad)
    18000219e:	(bad)
    18000219f:	incl   (%rax)
    1800021a1:	add    %al,(%rax)
-   1800021a3:	add    %al,(%rcx)
-   1800021a5:	or     %edi,(%rax)
-   1800021a7:	(bad)
-   1800021a8:	add    %al,(%rax)
+   1800021a3:	add    %bh,-0x61(%rdx)
+   1800021a6:	movsxd 0x0(%rax,%rax,1),%esp
    1800021aa:	add    %al,(%rax)
    1800021ac:	add    (%rax),%al
    1800021ae:	add    %al,(%rax)
    1800021b0:	data16 add %al,(%rax)
    1800021b3:	add    %ah,-0x5fffffdd(%rax)
    1800021b9:	(bad)
    1800021ba:	add    %al,(%rax)
    1800021bc:	add    %al,(%rax)
    1800021be:	add    %al,(%rax)
-   1800021c0:	add    %ecx,(%rcx)
-   1800021c2:	cmp    %ah,0x0(%rdx)
-   1800021c5:	add    %al,(%rax)
-   1800021c7:	add    %cl,(%rax,%rax,1)
+   1800021c0:	jp     0x180002161
+   1800021c2:	movsxd 0x0(%rax,%rax,1),%esp
+   1800021c6:	add    %al,(%rax)
+   1800021c8:	or     $0x0,%al
    1800021ca:	add    %al,(%rax)
    1800021cc:	adc    $0x0,%al
    1800021ce:	add    %al,(%rax)
    1800021d0:	or     %ah,(%rax,%rax,1)
    1800021d3:	add    %cl,(%rax)
    1800021d5:	sbb    %al,(%rax)
    1800021d7:	add    %al,(%rax)
    1800021d9:	add    %al,(%rax)
-   1800021db:	add    %al,(%rcx)
-   1800021dd:	or     %edi,(%rax)
-   1800021df:	(bad)
-   1800021e0:	add    %al,(%rax)
+   1800021db:	add    %bh,-0x61(%rdx)
+   1800021de:	movsxd 0x0(%rax,%rax,1),%esp
    1800021e2:	add    %al,(%rax)
    1800021e4:	or     $0x68000000,%eax
    1800021e9:	add    (%rax),%al
    1800021eb:	add    %bl,(%rsp)
    1800021ee:	add    %al,(%rax)
    1800021f0:	sbb    $0x18,%al
    1800021f2:	add    %al,(%rax)
    1800021f4:	add    %al,(%rax)
    1800021f6:	add    %al,(%rax)
-   1800021f8:	add    %ecx,(%rcx)
-   1800021fa:	cmp    %ah,0x0(%rdx)
-   1800021fd:	add    %al,(%rax)
-   1800021ff:	add    %dl,(%rax,%rax,1)
+   1800021f8:	jp     0x180002199
+   1800021fa:	movsxd 0x0(%rax,%rax,1),%esp
+   1800021fe:	add    %al,(%rax)
+   180002200:	adc    $0x0,%al
    180002202:	add    %al,(%rax)
    180002204:	add    $0x0,%al
    180002206:	add    %al,(%rax)
    180002208:	test   %ah,(%rsi)
    18000220a:	add    %al,(%rax)
    18000220c:	test   %bl,(%rdx)
    18000220e:	add    %al,(%rax)
@@ -1776,23 +1778,24 @@
    180002395:	add    %al,(%rax)
    180002397:	add    %al,(%rax)
    180002399:	adc    %al,(%rax)
    18000239b:	add    %ah,(%rax)
    18000239d:	add    %al,(%rax)
    18000239f:	add    %dl,0x53(%rdx)
    1800023a2:	rex.R push %rbx
-   1800023a4:	sub    $0x753156bc,%eax
-   1800023a9:	sti
-   1800023aa:	mov    %eax,-0x55(%rsi)
-   1800023ad:	(bad)  0x2083689c(%rbx)
-   1800023b3:	pop    (%rcx)
-   1800023b5:	add    %al,(%rax)
-   1800023b7:	add    %al,0x5c(%rdx,%rdi,1)
-   1800023bb:	(bad)
-   1800023bc:	pop    %rsp
+   1800023a4:	xchg   %eax,%esp
+   1800023a5:	cltd
+   1800023a6:	xchg   %edx,(%rsp,%rcx,1)
+   1800023a9:	jnp    0x180002332
+   1800023ab:	rex.WRB cmpsq %es:(%rdi),%ds:(%rsi)
+   1800023ad:	mov    $0xb8cf6062,%ebp
+   1800023b2:	or     $0x74,%al
+   1800023b4:	add    %eax,(%rax)
+   1800023b6:	add    %al,(%rax)
+   1800023b8:	cmp    0x5c(%r9,%r12,2),%bl
    1800023bd:	pop    %rdi
    1800023be:	ja     0x18000242f
    1800023c0:	jb     0x18000242d
    1800023c2:	pop    %rsp
    1800023c3:	xor    %ebx,0x5c(%rdx,%riz,2)
    1800023c7:	push   %rdx
    1800023c8:	gs insb (%dx),%es:(%rdi)
@@ -2086,28 +2089,24 @@
    180002650:	mov    $0x1,%al
    180002652:	add    %al,(%rax)
    180002654:	jo,pn  0x1800026bb
    180002657:	(bad)
    180002658:	je     0x1800026bb
    18000265a:	add    %al,(%rax)
    18000265c:	add    %dl,0x0(%rax)
-   18000265f:	add    %ah,0x0(%rax)
-   180002662:	add    %al,(%rax)
-   180002664:	jb,pn  0x1800026da
+   18000265f:	add    %ah,0x2e000000(%rax)
+   180002665:	jb     0x1800026da
    180002667:	jb     0x1800026cc
    180002669:	and    $0x30,%al
    18000266b:	xor    %eax,(%rax)
    18000266d:	add    %al,(%rax)
-   18000266f:	add    %ah,0x50(%rax)
-   180002672:	add    %al,(%rax)
-   180002674:	addb   $0x0,(%rcx)
-   180002677:	add    %ch,(%rsi)
-   180002679:	jb     0x1800026ee
-   18000267b:	jb     0x1800026e0
-   18000267d:	and    $0x30,%al
+   18000266f:	add    %ah,0x28000050(%rax)
+   180002675:	add    $0x722e0000,%eax
+   18000267a:	jae    0x1800026ee
+   18000267c:	movsxd (%rax,%rsi,1),%esp
    18000267f:	xor    (%rax),%al
    180002681:	add    %al,(%rax)
    180002683:	add    %al,(%rcx)
 	...
    1800026a5:	add    %al,(%rax)
    1800026a7:	add    %al,(%rcx)
    1800026a9:	add    %al,(%rax)
@@ -2551,146 +2550,145 @@
    180002ae1:	add    %ah,0x70(%rcx)
    180002ae4:	imul   $0x632d6e69,0x772d736d(%rip),%ebp        # 0x1f72d9e5b
    180002aee:	jb     0x180002b64
    180002af0:	sub    $0x746e7572,%eax
    180002af5:	imul   $0x2d316c2d,0x65(%rbp),%ebp
    180002afc:	xor    %ebp,0x6c642e30(%rip)        # 0x1ec645932
    180002b02:	insb   (%dx),%es:(%rdi)
-   180002b03:	add    %ch,%cl
+   180002b03:	add    %dh,%ch
    180002b05:	add    $0x52,%al
    180002b07:	je     0x180002b75
    180002b09:	rex.XB (bad)
    180002b0b:	jo     0x180002b81
    180002b0d:	jne    0x180002b81
    180002b0f:	rex.XB outsl %gs:(%rsi),(%dx)
    180002b12:	outsb  %ds:(%rsi),(%dx)
    180002b13:	je     0x180002b7a
    180002b15:	js     0x180002b8b
-   180002b17:	add    %dh,%cl
+   180002b17:	add    %bh,%ch
    180002b19:	add    $0x52,%al
    180002b1b:	je     0x180002b89
    180002b1d:	rex.WR outsl %ds:(%rsi),(%dx)
    180002b1f:	outsl  %ds:(%rsi),(%dx)
    180002b20:	imul   $0x46,0x70(%rbp),%esi
    180002b24:	jne    0x180002b94
    180002b26:	movsxd 0x6f(%rcx,%rbp,2),%esi
    180002b2a:	outsb  %ds:(%rsi),(%dx)
    180002b2b:	rex.RB outsb %ds:(%rsi),(%dx)
    180002b2d:	je     0x180002ba1
    180002b2f:	jns    0x180002b31
-   180002b31:	add    %bh,%al
-   180002b33:	add    $0x52,%al
-   180002b35:	je     0x180002ba3
-   180002b37:	push   %rsi
+   180002b31:	add    %al,0x566c7452(,%rax,1)
    180002b38:	imul   $0x556c6175,0x74(%rdx),%esi
    180002b3f:	outsb  %ds:(%rsi),(%dx)
    180002b40:	ja     0x180002bab
    180002b42:	outsb  %ds:(%rsi),(%dx)
    180002b43:	add    %al,%fs:(%rax)
-   180002b46:	fadds  0x61686e55(%rip)        # 0x1e16899a1
-   180002b4c:	outsb  %ds:(%rsi),(%dx)
-   180002b4d:	fs insb (%dx),%es:(%rdi)
+   180002b46:	out    %al,$0x5
+   180002b48:	push   %rbp
+   180002b49:	outsb  %ds:(%rsi),(%dx)
+   180002b4a:	push   $0x6c646e61
    180002b4f:	gs fs rex.RB js 0x180002bb7
    180002b54:	gs jo  0x180002bcb
    180002b57:	imul   $0x746c6946,0x6e(%rdi),%ebp
    180002b5e:	gs jb  0x180002b61
-   180002b61:	add    %dl,0x74655305(%rdi)
-   180002b67:	push   %rbp
+   180002b61:	add    %ah,0x55746553(%rbp,%rax,1)
    180002b68:	outsb  %ds:(%rsi),(%dx)
    180002b69:	push   $0x6c646e61
    180002b6e:	gs fs rex.RB js 0x180002bd6
    180002b73:	gs jo  0x180002bea
    180002b76:	imul   $0x746c6946,0x6e(%rdi),%ebp
    180002b7d:	gs jb  0x180002b80
-   180002b80:	sub    (%rdx),%al
+   180002b80:	xor    (%rdx),%al
    180002b82:	rex.RXB
    180002b83:	gs je  0x180002bc9
    180002b86:	jne    0x180002bfa
    180002b88:	jb     0x180002bef
    180002b8a:	outsb  %ds:(%rsi),(%dx)
    180002b8b:	je     0x180002bdd
    180002b8d:	jb     0x180002bfe
    180002b8f:	movsxd 0x73(%rbp),%esp
    180002b92:	jae    0x180002b94
-   180002b94:	mov    $0x5,%dh
-   180002b96:	push   %rsp
-   180002b97:	gs jb  0x180002c07
+   180002b94:	(bad)
+   180002b95:	add    $0x6d726554,%eax
    180002b9a:	imul   $0x72506574,0x61(%rsi),%ebp
    180002ba1:	outsl  %ds:(%rsi),(%dx)
    180002ba2:	movsxd 0x73(%rbp),%esp
    180002ba5:	jae    0x180002ba7
-   180002ba7:	add    %bl,0x50734903(%rsi)
+   180002ba7:	add    %ch,0x50734903(%rax)
    180002bad:	jb     0x180002c1e
    180002baf:	movsxd 0x73(%rbp),%esp
    180002bb2:	jae    0x180002c23
    180002bb4:	jb     0x180002bfc
    180002bb6:	gs (bad)
    180002bb8:	je     0x180002c2f
    180002bba:	jb     0x180002c21
    180002bbc:	push   %rax
    180002bbd:	jb     0x180002c24
    180002bbf:	jae    0x180002c26
    180002bc1:	outsb  %ds:(%rsi),(%dx)
    180002bc2:	je     0x180002bc4
-   180002bc4:	fs add $0x51,%al
+   180002bc4:	jo     0x180002bca
+   180002bc6:	push   %rcx
    180002bc7:	jne    0x180002c2e
    180002bc9:	jb     0x180002c44
    180002bcb:	push   %rax
    180002bcc:	gs jb  0x180002c35
    180002bcf:	outsl  %ds:(%rsi),(%dx)
    180002bd0:	jb     0x180002c3f
    180002bd2:	(bad)
    180002bd3:	outsb  %ds:(%rsi),(%dx)
    180002bd4:	movsxd 0x43(%rbp),%esp
    180002bd7:	outsl  %ds:(%rsi),(%dx)
    180002bd8:	jne    0x180002c48
    180002bda:	je     0x180002c41
    180002bdc:	jb     0x180002bde
-   180002bde:	sub    (%rdx),%eax
+   180002bde:	xor    (%rdx),%eax
    180002be0:	rex.RXB
    180002be1:	gs je  0x180002c27
    180002be4:	jne    0x180002c58
    180002be6:	jb     0x180002c4d
    180002be8:	outsb  %ds:(%rsi),(%dx)
    180002be9:	je     0x180002c3b
    180002beb:	jb     0x180002c5c
    180002bed:	movsxd 0x73(%rbp),%esp
    180002bf0:	jae    0x180002c3b
-   180002bf2:	add    %ch,%fs:(%rdi)
+   180002bf2:	add    %dh,%fs:(%rdi)
    180002bf5:	add    0x65(%rdi),%al
    180002bf8:	je     0x180002c3d
    180002bfa:	jne    0x180002c6e
    180002bfc:	jb     0x180002c63
    180002bfe:	outsb  %ds:(%rsi),(%dx)
    180002bff:	je     0x180002c55
    180002c01:	push   $0x64616572
    180002c06:	rex.WB
    180002c07:	add    %al,%fs:(%rax)
-   180002c0a:	add    %eax,(%rbx)
+   180002c0a:	or     (%rbx),%al
    180002c0c:	rex.RXB
    180002c0d:	gs je  0x180002c63
    180002c10:	jns    0x180002c85
    180002c12:	je     0x180002c79
    180002c14:	insl   (%dx),%es:(%rdi)
    180002c15:	push   %rsp
    180002c16:	imul   $0x69467341,0x65(%rbp),%ebp
    180002c1d:	insb   (%dx),%es:(%rdi)
    180002c1e:	gs push %rsp
-   180002c20:	imul   $0x44012c00,0x65(%rbp),%ebp
+   180002c20:	imul   $0x44013400,0x65(%rbp),%ebp
    180002c27:	imul   $0x54656c62,0x61(%rbx),%esi
    180002c2e:	push   $0x64616572
    180002c33:	imul   $0x43797261,0x72(%rdx),%r12
    180002c3b:	(bad)
    180002c3c:	insb   (%dx),%es:(%rdi)
    180002c3d:	insb   (%dx),%es:(%rdi)
    180002c3e:	jae    0x180002c40
-   180002c40:	addl   $0x74696e49,(%rbx)
-   180002c46:	imul   $0x53657a69,0x6c(%rcx),%esp
+   180002c40:	mov    (%rbx),%al
+   180002c42:	rex.WB outsb %ds:(%rsi),(%dx)
+   180002c44:	imul   $0x657a696c,0x61(%rcx,%rbp,2),%esi
+   180002c4c:	push   %rbx
    180002c4d:	imul   $0x64616548,0x74(%rbx),%r14
-   180002c55:	add    %dl,0x44734903(%rdi)
+   180002c55:	add    %ah,0x44734903(%rax)
    180002c5b:	(bad)
    180002c61:	jb     0x180002cb3
    180002c63:	jb     0x180002cca
    180002c65:	jae    0x180002ccc
    180002c67:	outsb  %ds:(%rsi),(%dx)
    180002c68:	je     0x180002c6a
    180002c6a:	rex.WXB
@@ -2909,177 +2907,559 @@
 	...
 
 Disassembly of section .rsrc:
 
 0000000180005000 <.rsrc>:
 	...
    18000500c:	add    %al,(%rax)
-   18000500e:	add    %eax,(%rax)
-   180005010:	sbb    %al,(%rax)
+   18000500e:	add    (%rax),%al
+   180005010:	adc    %al,(%rax)
    180005012:	add    %al,(%rax)
-   180005014:	sbb    %al,(%rax)
-   180005016:	add    %al,0x0(%rax)
+   180005014:	and    %al,(%rax)
+   180005016:	add    %al,0x18(%rax)
+   18000501c:	cmp    %al,(%rax)
+   18000501e:	add    %al,0x0(%rax)
+	...
+   18000502c:	add    %al,(%rax)
+   18000502e:	add    %eax,(%rax)
+   180005030:	add    %eax,(%rax)
+   180005032:	add    %al,(%rax)
+   180005034:	push   %rax
+   180005035:	add    %al,(%rax)
+   180005037:	addb   $0x0,(%rax)
 	...
-   180005024:	add    %al,(%rax)
-   180005026:	add    %eax,(%rax)
-   180005028:	add    (%rax),%al
-   18000502a:	add    %al,(%rax)
-   18000502c:	xor    %al,(%rax)
-   18000502e:	add    %al,0x0(%rax)
-	...
-   18000503c:	add    %al,(%rax)
-   18000503e:	add    %eax,(%rax)
-   180005040:	or     %eax,(%rax,%rax,1)
-   180005043:	add    %cl,0x0(%rax)
-   180005046:	add    %al,(%rax)
-   180005048:	(bad)
-   180005049:	push   %rax
+   180005046:	add    %eax,(%rax)
+   180005048:	add    (%rax),%al
    18000504a:	add    %al,(%rax)
-   18000504c:	jge    0x18000504f
+   18000504c:	push   $0x800000
 	...
-   18000505e:	add    %al,(%rax)
-   180005060:	cmp    $0x3f,%al
-   180005062:	js     0x1800050d1
-   180005064:	insb   (%dx),%es:(%rdi)
-   180005065:	and    %dh,0x65(%rsi)
-   180005068:	jb     0x1800050dd
-   18000506a:	imul   $0x2e31273d,0x6e(%rdi),%ebp
-   180005071:	xor    %ah,(%rdi)
-   180005073:	and    %ah,0x6e(%rbp)
-   180005076:	movsxd 0x64(%rdi),%ebp
-   180005079:	imul   $0x5455273d,0x67(%rsi),%ebp
-   180005080:	rex.RX sub $0x73202738,%eax
-   180005086:	je     0x1800050e9
-   180005088:	outsb  %ds:(%rsi),(%dx)
-   180005089:	fs (bad)
-   18000508b:	insb   (%dx),%es:(%rdi)
-   18000508c:	outsl  %ds:(%rsi),(%dx)
-   18000508d:	outsb  %ds:(%rsi),(%dx)
-   18000508e:	gs cmp $0x73657927,%eax
-   180005094:	(bad)
-   180005095:	(bad)
-   180005096:	ds or  $0x73613c0a,%eax
-   18000509c:	jae    0x180005103
-   18000509e:	insl   (%dx),%es:(%rdi)
-   18000509f:	(bad)
-   1800050a0:	insb   (%dx),%es:(%rdi)
-   1800050a1:	jns    0x1800050c3
-   1800050a3:	js     0x180005112
-   1800050a5:	insb   (%dx),%es:(%rdi)
-   1800050a6:	outsb  %ds:(%rsi),(%dx)
-   1800050a7:	jae    0x1800050e6
-   1800050a9:	(bad)
-   1800050aa:	jne    0x18000511e
-   1800050ac:	outsb  %ds:(%rsi),(%dx)
-   1800050ad:	cmp    0x63(%rbx),%dh
-   1800050b0:	push   $0x73616d65
-   1800050b5:	sub    $0x7263696d,%eax
-   1800050ba:	outsl  %ds:(%rsi),(%dx)
-   1800050bb:	jae    0x18000512c
-   1800050bd:	data16 je 0x1800050ed
-   1800050c0:	movsxd 0x6d(%rdi),%ebp
-   1800050c3:	cmp    0x73(%rcx),%ah
-   1800050c6:	insl   (%dx),%es:(%rdi)
-   1800050c7:	jbe,pn 0x1800050fb
-   1800050ca:	(bad)
-   1800050cb:	and    %ch,0x61(%rbp)
-   1800050ce:	outsb  %ds:(%rsi),(%dx)
-   1800050cf:	imul   $0x65567473,0x65(%rsi),%esp
-   1800050d6:	jb     0x18000514b
-   1800050d8:	imul   $0x2e31273d,0x6e(%rdi),%ebp
-   1800050df:	xor    %ah,(%rdi)
-   1800050e1:	ds or  $0x3c20200a,%eax
-   1800050e7:	je     0x18000515b
-   1800050e9:	jne    0x18000515e
-   1800050eb:	je     0x180005136
-   1800050ed:	outsb  %ds:(%rsi),(%dx)
-   1800050ee:	outsw  %ds:(%rsi),(%dx)
-   1800050f0:	and    %bh,0x6d(%rax)
-   1800050f3:	insb   (%dx),%es:(%rdi)
-   1800050f4:	outsb  %ds:(%rsi),(%dx)
-   1800050f5:	jae    0x180005134
-   1800050f7:	and    0x72(%rbp),%dh
-   1800050fa:	outsb  %ds:(%rsi),(%dx)
-   1800050fb:	cmp    0x63(%rbx),%dh
-   1800050fe:	push   $0x73616d65
-   180005103:	sub    $0x7263696d,%eax
-   180005108:	outsl  %ds:(%rsi),(%dx)
-   180005109:	jae    0x18000517a
-   18000510b:	data16 je 0x18000513b
-   18000510e:	movsxd 0x6d(%rdi),%ebp
-   180005111:	cmp    0x73(%rcx),%ah
-   180005114:	insl   (%dx),%es:(%rdi)
-   180005115:	jbe,pn 0x18000514b
-   180005118:	and    (%rsi),%bh
-   18000511a:	or     $0x2020200a,%eax
-   18000511f:	and    %bh,(%rbx,%rsi,2)
-   180005122:	movsxd %gs:0x72(%rbp),%esi
-   180005126:	imul   $0x20200a0d,0x3e(%rcx,%rdi,2),%esi
-   18000512e:	and    %ah,(%rax)
-   180005130:	and    %ah,(%rax)
-   180005132:	cmp    $0x72,%al
-   180005134:	gs jno 0x1800051ac
-   180005137:	gs jae 0x1800051ae
-   18000513a:	gs fs push %rax
-   18000513d:	jb     0x1800051a8
-   18000513f:	jbe    0x1800051aa
-   180005141:	insb   (%dx),%es:(%rdi)
-   180005142:	gs addr32 gs jae 0x180005185
-   180005147:	or     $0x2020200a,%eax
-   18000514c:	and    %ah,(%rax)
-   18000514e:	and    %ah,(%rax)
-   180005150:	and    %bh,(%rdx,%rsi,2)
-   180005153:	gs jno 0x1800051cb
-   180005156:	gs jae 0x1800051cd
-   180005159:	gs fs rex.RB js 0x1800051c3
-   18000515e:	movsxd 0x74(%rbp),%esi
-   180005161:	imul   $0x6576654c,0x6e(%rdi),%ebp
-   180005168:	insb   (%dx),%es:(%rdi)
-   180005169:	and    %ch,0x76(%rbp,%riz,2)
-   18000516d:	gs insb (%dx),%es:(%rdi)
-   18000516f:	cmp    $0x49736127,%eax
-   180005174:	outsb  %ds:(%rsi),(%dx)
-   180005175:	jbe    0x1800051e6
-   180005177:	imul   $0x27,0x72(%rbp),%esp
-   18000517b:	and    %dh,0x69(%rbp)
-   18000517e:	movsxd 0x65(%r11),%esp
-   180005182:	jae    0x1800051f7
-   180005184:	cmp    $0x6c616627,%eax
-   180005189:	jae    0x1800051f0
-   18000518b:	(bad)
-   18000518c:	and    %ch,(%rdi)
-   18000518e:	ds or  $0x2020200a,%eax
-   180005194:	and    %ah,(%rax)
-   180005196:	and    %bh,(%rdi,%rbp,1)
-   180005199:	jb     0x180005200
-   18000519b:	jno    0x180005212
-   18000519d:	gs jae 0x180005214
-   1800051a0:	gs fs push %rax
-   1800051a3:	jb     0x18000520e
-   1800051a5:	jbe    0x180005210
-   1800051a7:	insb   (%dx),%es:(%rdi)
-   1800051a8:	gs addr32 gs jae 0x1800051eb
-   1800051ad:	or     $0x2020200a,%eax
-   1800051b2:	and    %bh,(%rdi,%rbp,1)
-   1800051b5:	jae    0x18000521c
-   1800051b7:	movsxd 0x72(%rbp),%esi
-   1800051ba:	imul   $0x20200a0d,0x3e(%rcx,%rdi,2),%esi
-   1800051c2:	cmp    $0x2f,%al
-   1800051c4:	je     0x180005238
-   1800051c6:	jne    0x18000523b
-   1800051c8:	je     0x180005213
-   1800051ca:	outsb  %ds:(%rsi),(%dx)
-   1800051cb:	outsw  %ds:(%rsi),(%dx)
-   1800051cd:	ds or  $0x612f3c0a,%eax
-   1800051d3:	jae    0x180005248
-   1800051d5:	gs insl (%dx),%es:(%rdi)
-   1800051d7:	(bad)
-   1800051d8:	insb   (%dx),%es:(%rdi)
-   1800051d9:	jns    0x180005219
-   1800051db:	or     $0xa,%eax
+   18000505d:	add    %al,(%rcx)
+   18000505f:	add    %cl,(%rcx)
+   180005061:	add    $0x0,%al
+   180005063:	add    %al,0x0(%rax)
+	...
+   180005075:	add    %al,(%rcx)
+   180005077:	add    %cl,(%rcx)
+   180005079:	add    $0x0,%al
+   18000507b:	add    %dl,-0x60000000(%rax)
+   180005081:	push   %rax
+   180005082:	add    %al,(%rax)
+   180005084:	movsb  %ds:(%rsi),%es:(%rdi)
+   180005085:	add    (%rax),%eax
+	...
+   18000508f:	add    %cl,0x54(%rax)
+   180005092:	add    %al,(%rax)
+   180005094:	jge    0x180005097
+	...
+   18000509e:	add    %al,(%rax)
+   1800050a0:	movsb  %ds:(%rsi),%es:(%rdi)
+   1800050a1:	add    (%rax,%rax,1),%esi
+   1800050a4:	add    %al,(%rax)
+   1800050a6:	push   %rsi
+   1800050a7:	add    %dl,0x0(%rbx)
+   1800050aa:	pop    %rdi
+   1800050ab:	add    %dl,0x0(%rsi)
+   1800050ae:	add    %r10b,0x0(%r10)
+   1800050b2:	push   %rbx
+   1800050b3:	add    %cl,0x0(%rcx)
+   1800050b6:	rex.WRXB add %r9b,0x0(%r14)
+   1800050ba:	pop    %rdi
+   1800050bb:	add    %cl,0x0(%rcx)
+   1800050be:	rex.WRX add %r8b,0x0(%rsi)
+   1800050c2:	rex.WRXB add %r8b,(%r8)
+   1800050c5:	add    %al,(%rax)
+   1800050c7:	add    %bh,0xfeef04(%rbp)
+   1800050cd:	add    %al,(%rcx)
+   1800050cf:	add    %dl,(%rax)
+   1800050d1:	add    %al,(%rcx)
+   1800050d3:	add    %al,(%rdx,%rax,1)
+   1800050d6:	(bad)
+   1800050d7:	add    %dl,(%rax)
+   1800050d9:	add    %al,(%rcx)
+   1800050db:	add    %al,(%rdx,%rax,1)
+   1800050de:	(bad)
+   1800050df:	add    %bh,(%rdi)
+   1800050e1:	add    %al,(%rax)
+   1800050e3:	add    %al,(%rax)
+   1800050e5:	add    %al,(%rax)
+   1800050e7:	add    %al,(%rax,%rax,1)
+   1800050ea:	add    %al,(%rax)
+   1800050ec:	add    (%rax),%al
+	...
+   1800050fa:	add    %al,(%rax)
+   1800050fc:	add    $0x3,%al
+   1800050fe:	add    %al,(%rax)
+   180005100:	add    %eax,(%rax)
+   180005102:	push   %rbx
+   180005103:	add    %dh,0x72(%rax,%rax,1)
+   180005107:	add    %ch,0x0(%rcx)
+   18000510a:	outsb  %ds:(%rsi),(%dx)
+   18000510b:	add    %ah,0x0(%rdi)
+   18000510e:	rex.RX add %r13b,0x0(%rcx)
+   180005112:	insb   (%dx),%es:(%rdi)
+   180005113:	add    %ah,0x0(%rbp)
+   180005116:	rex.WB add %bpl,0x0(%r14)
+   18000511a:	data16 add %ch,0x0(%rdi)
+   18000511e:	add    %al,(%rax)
+   180005120:	loopne 0x180005124
+   180005122:	add    %al,(%rax)
+   180005124:	add    %eax,(%rax)
+   180005126:	xor    %al,(%rax)
+   180005128:	xor    $0x0,%al
+   18000512a:	xor    %al,(%rax)
+   18000512c:	cmp    %eax,(%rax)
+   18000512e:	xor    %al,(%rax)
+   180005130:	xor    $0x0,%al
+   180005132:	add    %r14b,(%r8,%rax,1)
+   180005136:	add    %al,(%rax)
+   180005138:	rex.WR add %r10b,(%rsi)
+   18000513b:	add    %al,(%rcx)
+   18000513d:	add    %al,0x0(%rbx)
+   180005140:	outsl  %ds:(%rsi),(%dx)
+   180005141:	add    %ch,0x0(%rbp)
+   180005144:	jo     0x180005146
+   180005146:	(bad)
+   180005147:	add    %ch,0x0(%rsi)
+   18000514a:	jns    0x18000514c
+   18000514c:	rex.WRX add %r12b,0x0(%rcx)
+   180005150:	insl   (%dx),%es:(%rdi)
+   180005151:	add    %ah,0x0(%rbp)
+   180005154:	add    %al,(%rax)
+   180005156:	add    %al,(%rax)
+   180005158:	rex.WRB add %r13b,0x0(%r9)
+   18000515c:	movsxd (%rax),%eax
+   18000515e:	jb     0x180005160
+   180005160:	outsl  %ds:(%rsi),(%dx)
+   180005161:	add    %dh,0x0(%rbx)
+   180005164:	outsl  %ds:(%rsi),(%dx)
+   180005165:	add    %ah,0x0(%rsi)
+   180005168:	je     0x18000516a
+   18000516a:	and    %al,(%rax)
+   18000516c:	rex.XB add %bpl,0x0(%r15)
+   180005170:	jb     0x180005172
+   180005172:	jo     0x180005174
+   180005174:	outsl  %ds:(%rsi),(%dx)
+   180005175:	add    %dh,0x0(%rdx)
+   180005178:	(bad)
+   180005179:	add    %dh,0x69(%rax,%rax,1)
+   18000517d:	add    %ch,0x0(%rdi)
+   180005180:	outsb  %ds:(%rsi),(%dx)
+   180005181:	add    %al,(%rax)
+   180005183:	add    %al,0x0(%rdx)
+   180005186:	or     $0x46000100,%eax
+   18000518b:	add    %ch,0x0(%rcx)
+   18000518e:	insb   (%dx),%es:(%rdi)
+   18000518f:	add    %ah,0x0(%rbp)
+   180005192:	add    %r12b,0x0(%rbp)
+   180005196:	jae    0x180005198
+   180005198:	movsxd (%rax),%eax
+   18000519a:	jb     0x18000519c
+   18000519c:	imul   $0x740070,(%rax),%eax
+   1800051a2:	imul   $0x6e006f,(%rax),%eax
+   1800051a8:	add    %al,(%rax)
+   1800051aa:	add    %al,(%rax)
+   1800051ac:	rex.WRXB add %r9b,0x0(%r14)
+   1800051b0:	rex.WRX add %r11b,0x0(%rax)
+   1800051b4:	and    %al,(%rax)
+   1800051b6:	push   %rdx
+   1800051b7:	add    %dh,0x0(%rbp)
+   1800051ba:	outsb  %ds:(%rsi),(%dx)
+   1800051bb:	add    %dh,0x69(%rax,%rax,1)
+   1800051bf:	add    %ch,0x0(%rbp)
+   1800051c2:	add    %al,%gs:(%rax)
+   1800051c5:	add    %al,(%rax)
+   1800051c7:	add    %dl,0x0(%rax)
+   1800051ca:	sbb    %al,(%rax)
+   1800051cc:	add    %eax,(%rax)
+   1800051ce:	rex.RX add %r13b,0x0(%rcx)
+   1800051d2:	insb   (%dx),%es:(%rdi)
+   1800051d3:	add    %ah,0x0(%rbp)
+   1800051d6:	push   %rsi
+   1800051d7:	add    %ah,0x0(%rbp)
+   1800051da:	jb     0x1800051dc
+   1800051dc:	jae    0x1800051de
+   1800051de:	imul   $0x6e006f,(%rax),%eax
+   1800051e4:	add    %al,(%rax)
+   1800051e6:	add    %al,(%rax)
+   1800051e8:	xor    %eax,(%rax)
+   1800051ea:	cs add %dh,(%rcx)
+   1800051ed:	add    %dh,(%rsi)
+   1800051ef:	add    %ch,(%rsi)
+   1800051f1:	add    %dh,(%rdx)
+   1800051f3:	add    %dh,(%rax)
+   1800051f5:	add    %dh,(%rdx)
+   1800051f7:	add    %dh,(%rbx)
+   1800051f9:	add    %dh,(%rax)
+   1800051fb:	add    %dh,0x36003100(%rip)        # 0x1b6008301
+   180005201:	add    %ch,(%rsi)
+   180005203:	add    %dh,(%rdx)
+   180005205:	add    %ch,(%rsi)
+   180005207:	add    %dh,(%rsi)
+   180005209:	add    %ah,0x0(%rdx)
+   18000520c:	(bad)
+   18000520d:	add    %dh,(%rcx)
+   18000520f:	add    %bh,(%rax)
+   180005211:	add    %dh,(%rcx)
+   180005213:	add    %ah,0x0(%rax,%rax,1)
+   180005217:	add    %bh,(%rdx)
+   180005219:	add    %cl,0x49000100(%rip)        # 0x1c900531f
+   18000521f:	add    %ch,0x0(%rsi)
+   180005222:	je     0x180005224
+   180005224:	add    %dh,%gs:0x0(%rdx)
+   180005228:	outsb  %ds:(%rsi),(%dx)
+   180005229:	add    %ah,0x0(%rcx)
+   18000522c:	insb   (%dx),%es:(%rdi)
+   18000522d:	add    %cl,0x0(%rsi)
+   180005230:	(bad)
+   180005231:	add    %ch,0x0(%rbp)
+   180005234:	add    %al,%gs:(%rax)
+   180005237:	add    %cl,0x0(%rdi)
+   18000523a:	rex.WRX add %r9b,0x0(%rsi)
+   18000523e:	pop    %rax
+   18000523f:	add    %ah,(%rax)
+   180005241:	add    %dl,0x0(%rdx)
+   180005244:	jne    0x180005246
+   180005246:	outsb  %ds:(%rsi),(%dx)
+   180005247:	add    %dh,0x69(%rax,%rax,1)
+   18000524b:	add    %ch,0x0(%rbp)
+   18000524e:	add    %al,%gs:(%rax)
+   180005251:	add    %al,(%rax)
+   180005253:	add    %al,0x1002e00(%rax)
+   180005259:	add    %cl,0x65(%rax,%rax,1)
+   18000525d:	add    %ah,0x0(%rdi)
+   180005260:	(bad)
+   180005261:	add    %ch,0x43(%rax,%rax,1)
+   180005265:	add    %ch,0x0(%rdi)
+   180005268:	jo     0x18000526a
+   18000526a:	jns    0x18000526c
+   18000526c:	jb     0x18000526e
+   18000526e:	imul   $0x680067,(%rax),%eax
+   180005274:	je     0x180005276
+   180005276:	add    %al,(%rax)
+   180005278:	test   $0x4d002000,%eax
+   18000527d:	add    %ch,0x0(%rcx)
+   180005280:	movsxd (%rax),%eax
+   180005282:	jb     0x180005284
+   180005284:	outsl  %ds:(%rsi),(%dx)
+   180005285:	add    %dh,0x0(%rbx)
+   180005288:	outsl  %ds:(%rsi),(%dx)
+   180005289:	add    %ah,0x0(%rsi)
+   18000528c:	je     0x18000528e
+   18000528e:	and    %al,(%rax)
+   180005290:	rex.XB add %bpl,0x0(%r15)
+   180005294:	jb     0x180005296
+   180005296:	jo     0x180005298
+   180005298:	outsl  %ds:(%rsi),(%dx)
+   180005299:	add    %dh,0x0(%rdx)
+   18000529c:	(bad)
+   18000529d:	add    %dh,0x69(%rax,%rax,1)
+   1800052a1:	add    %ch,0x0(%rdi)
+   1800052a4:	outsb  %ds:(%rsi),(%dx)
+   1800052a5:	add    %ch,(%rsi)
+   1800052a7:	add    %ah,(%rax)
+   1800052a9:	add    %al,0x0(%rcx)
+   1800052ac:	insb   (%dx),%es:(%rdi)
+   1800052ad:	add    %ch,0x20(%rax,%rax,1)
+   1800052b1:	add    %dh,0x0(%rdx)
+   1800052b4:	imul   $0x680067,(%rax),%eax
+   1800052ba:	je     0x1800052bc
+   1800052bc:	jae    0x1800052be
+   1800052be:	and    %al,(%rax)
+   1800052c0:	jb     0x1800052c2
+   1800052c2:	add    %dh,%gs:0x0(%rbx)
+   1800052c6:	add    %dh,%gs:0x0(%rdx)
+   1800052ca:	jbe    0x1800052cc
+   1800052cc:	add    %ah,%gs:0x2e(%rax,%rax,1)
+   1800052d1:	add    %al,(%rax)
+   1800052d3:	add    %ch,0x0(%rdx)
+   1800052d6:	and    %eax,(%rax)
+   1800052d8:	add    %eax,(%rax)
+   1800052da:	rex.WRXB add %r14b,0x0(%r10)
+   1800052de:	imul   $0x690067,(%rax),%eax
+   1800052e4:	outsb  %ds:(%rsi),(%dx)
+   1800052e5:	add    %ah,0x0(%rcx)
+   1800052e8:	insb   (%dx),%es:(%rdi)
+   1800052e9:	add    %al,0x0(%rsi)
+   1800052ec:	imul   $0x65006c,(%rax),%eax
+   1800052f2:	outsb  %ds:(%rsi),(%dx)
+   1800052f3:	add    %ah,0x0(%rcx)
+   1800052f6:	insl   (%dx),%es:(%rdi)
+   1800052f7:	add    %ah,0x0(%rbp)
+   1800052fa:	add    %al,(%rax)
+   1800052fc:	outsl  %ds:(%rsi),(%dx)
+   1800052fd:	add    %ch,0x0(%rsi)
+   180005300:	outsb  %ds:(%rsi),(%dx)
+   180005301:	add    %bh,0x0(%rax)
+   180005304:	jb     0x180005306
+   180005306:	jne    0x180005308
+   180005308:	outsb  %ds:(%rsi),(%dx)
+   180005309:	add    %dh,0x69(%rax,%rax,1)
+   18000530d:	add    %ch,0x0(%rbp)
+   180005310:	add    %bl,%gs:0x0(%rdi)
+   180005314:	jo     0x180005316
+   180005316:	jb     0x180005318
+   180005318:	outsl  %ds:(%rsi),(%dx)
+   180005319:	add    %dh,0x0(%rsi)
+   18000531c:	imul   $0x650064,(%rax),%eax
+   180005322:	jb     0x180005324
+   180005324:	jae    0x180005326
+   180005326:	pop    %rdi
+   180005327:	add    %dh,0x0(%rbx)
+   18000532a:	push   $0x72006100
+   18000532f:	add    %ah,0x0(%rbp)
+   180005332:	add    %ch,%fs:(%rsi)
+   180005335:	add    %ah,0x6c(%rax,%rax,1)
+   180005339:	add    %ch,0x0(%rax,%rax,1)
+   18000533d:	add    %al,(%rax)
+   18000533f:	add    %ch,0x0(%rdx)
+   180005342:	and    $0x50000100,%eax
+   180005347:	add    %dh,0x0(%rdx)
+   18000534a:	outsl  %ds:(%rsi),(%dx)
+   18000534b:	add    %ah,0x75(%rax,%rax,1)
+   18000534f:	add    %ah,0x0(%rbx)
+   180005352:	je     0x180005354
+   180005354:	rex.WRX add %r12b,0x0(%rcx)
+   180005358:	insl   (%dx),%es:(%rdi)
+   180005359:	add    %ah,0x0(%rbp)
+   18000535c:	add    %al,(%rax)
+   18000535e:	add    %al,(%rax)
+   180005360:	rex.WRB add %r13b,0x0(%r9)
+   180005364:	movsxd (%rax),%eax
+   180005366:	jb     0x180005368
+   180005368:	outsl  %ds:(%rsi),(%dx)
+   180005369:	add    %dh,0x0(%rbx)
+   18000536c:	outsl  %ds:(%rsi),(%dx)
+   18000536d:	add    %ah,0x0(%rsi)
+   180005370:	je     0x180005372
+   180005372:	scas   %es:(%rdi),%al
+   180005373:	add    %ah,(%rax)
+   180005375:	add    %dl,0x0(%rdi)
+   180005378:	imul   $0x64006e,(%rax),%eax
+   18000537e:	outsl  %ds:(%rsi),(%dx)
+   18000537f:	add    %dh,0x0(%rdi)
+   180005382:	jae    0x180005384
+   180005384:	scas   %es:(%rdi),%al
+   180005385:	add    %ah,(%rax)
+   180005387:	add    %cl,0x0(%rdi)
+   18000538a:	jo     0x18000538c
+   18000538c:	add    %dh,%gs:0x0(%rdx)
+   180005390:	(bad)
+   180005391:	add    %dh,0x69(%rax,%rax,1)
+   180005395:	add    %ch,0x0(%rsi)
+   180005398:	add    %ah,(%eax)
+   18000539b:	add    %dl,0x0(%rbx)
+   18000539e:	jns    0x1800053a0
+   1800053a0:	jae    0x1800053a2
+   1800053a2:	je     0x1800053a4
+   1800053a4:	add    %ch,%gs:0x0(%rbp)
+   1800053a8:	add    %al,(%rax)
+   1800053aa:	add    %al,(%rax)
+   1800053ac:	push   %rsp
+   1800053ad:	add    %bl,(%rax)
+   1800053af:	add    %al,(%rcx)
+   1800053b1:	add    %dl,0x0(%rax)
+   1800053b4:	jb     0x1800053b6
+   1800053b6:	outsl  %ds:(%rsi),(%dx)
+   1800053b7:	add    %ah,0x75(%rax,%rax,1)
+   1800053bb:	add    %ah,0x0(%rbx)
+   1800053be:	je     0x1800053c0
+   1800053c0:	push   %rsi
+   1800053c1:	add    %ah,0x0(%rbp)
+   1800053c4:	jb     0x1800053c6
+   1800053c6:	jae    0x1800053c8
+   1800053c8:	imul   $0x6e006f,(%rax),%eax
+   1800053ce:	add    %al,(%rax)
+   1800053d0:	xor    %eax,(%rax)
+   1800053d2:	cs add %dh,(%rcx)
+   1800053d5:	add    %dh,(%rsi)
+   1800053d7:	add    %ch,(%rsi)
+   1800053d9:	add    %dh,(%rdx)
+   1800053db:	add    %dh,(%rax)
+   1800053dd:	add    %dh,(%rdx)
+   1800053df:	add    %dh,(%rbx)
+   1800053e1:	add    %dh,(%rax)
+   1800053e3:	add    %dh,0x36003100(%rip)        # 0x1b60084e9
+   1800053e9:	add    %ch,(%rsi)
+   1800053eb:	add    %dh,(%rdx)
+   1800053ed:	add    %ch,(%rsi)
+   1800053ef:	add    %dh,(%rsi)
+   1800053f1:	add    %ah,0x0(%rdx)
+   1800053f4:	(bad)
+   1800053f5:	add    %dh,(%rcx)
+   1800053f7:	add    %bh,(%rax)
+   1800053f9:	add    %dh,(%rcx)
+   1800053fb:	add    %ah,0x0(%rax,%rax,1)
+   1800053ff:	add    %al,0x0(%rax,%rax,1)
+   180005403:	add    %al,(%rcx)
+   180005405:	add    %dl,0x0(%rsi)
+   180005408:	(bad)
+   180005409:	add    %dh,0x0(%rdx)
+   18000540c:	rex.RX add %r13b,0x0(%rcx)
+   180005410:	insb   (%dx),%es:(%rdi)
+   180005411:	add    %ah,0x0(%rbp)
+   180005414:	rex.WB add %bpl,0x0(%r14)
+   180005418:	data16 add %ch,0x0(%rdi)
+   18000541c:	add    %al,(%rax)
+   18000541e:	add    %al,(%rax)
+   180005420:	and    $0x0,%al
+   180005422:	add    $0x0,%al
+   180005424:	add    %al,(%rax)
+   180005426:	push   %rsp
+   180005427:	add    %dh,0x0(%rdx)
+   18000542a:	(bad)
+   18000542b:	add    %ch,0x0(%rsi)
+   18000542e:	jae    0x180005430
+   180005430:	insb   (%dx),%es:(%rdi)
+   180005431:	add    %ah,0x0(%rcx)
+   180005434:	je     0x180005436
+   180005436:	imul   $0x6e006f,(%rax),%eax
+   18000543c:	add    %al,(%rax)
+   18000543e:	add    %al,(%rax)
+   180005440:	or     %eax,(%rsp,%riz,8)
+   180005443:	add    $0x0,%al
+   180005445:	add    %al,(%rax)
+   180005447:	add    %bh,(%rdi,%rdi,1)
+   18000544a:	js     0x1800054b9
+   18000544c:	insb   (%dx),%es:(%rdi)
+   18000544d:	and    %dh,0x65(%rsi)
+   180005450:	jb     0x1800054c5
+   180005452:	imul   $0x2e31273d,0x6e(%rdi),%ebp
+   180005459:	xor    %ah,(%rdi)
+   18000545b:	and    %ah,0x6e(%rbp)
+   18000545e:	movsxd 0x64(%rdi),%ebp
+   180005461:	imul   $0x5455273d,0x67(%rsi),%ebp
+   180005468:	rex.RX sub $0x73202738,%eax
+   18000546e:	je     0x1800054d1
+   180005470:	outsb  %ds:(%rsi),(%dx)
+   180005471:	fs (bad)
+   180005473:	insb   (%dx),%es:(%rdi)
+   180005474:	outsl  %ds:(%rsi),(%dx)
+   180005475:	outsb  %ds:(%rsi),(%dx)
+   180005476:	gs cmp $0x73657927,%eax
+   18000547c:	(bad)
+   18000547d:	(bad)
+   18000547e:	ds or  $0x73613c0a,%eax
+   180005484:	jae    0x1800054eb
+   180005486:	insl   (%dx),%es:(%rdi)
+   180005487:	(bad)
+   180005488:	insb   (%dx),%es:(%rdi)
+   180005489:	jns    0x1800054ab
+   18000548b:	js     0x1800054fa
+   18000548d:	insb   (%dx),%es:(%rdi)
+   18000548e:	outsb  %ds:(%rsi),(%dx)
+   18000548f:	jae    0x1800054ce
+   180005491:	(bad)
+   180005492:	jne    0x180005506
+   180005494:	outsb  %ds:(%rsi),(%dx)
+   180005495:	cmp    0x63(%rbx),%dh
+   180005498:	push   $0x73616d65
+   18000549d:	sub    $0x7263696d,%eax
+   1800054a2:	outsl  %ds:(%rsi),(%dx)
+   1800054a3:	jae    0x180005514
+   1800054a5:	data16 je 0x1800054d5
+   1800054a8:	movsxd 0x6d(%rdi),%ebp
+   1800054ab:	cmp    0x73(%rcx),%ah
+   1800054ae:	insl   (%dx),%es:(%rdi)
+   1800054af:	jbe,pn 0x1800054e3
+   1800054b2:	(bad)
+   1800054b3:	and    %ch,0x61(%rbp)
+   1800054b6:	outsb  %ds:(%rsi),(%dx)
+   1800054b7:	imul   $0x65567473,0x65(%rsi),%esp
+   1800054be:	jb     0x180005533
+   1800054c0:	imul   $0x2e31273d,0x6e(%rdi),%ebp
+   1800054c7:	xor    %ah,(%rdi)
+   1800054c9:	ds or  $0x3c20200a,%eax
+   1800054cf:	je     0x180005543
+   1800054d1:	jne    0x180005546
+   1800054d3:	je     0x18000551e
+   1800054d5:	outsb  %ds:(%rsi),(%dx)
+   1800054d6:	outsw  %ds:(%rsi),(%dx)
+   1800054d8:	and    %bh,0x6d(%rax)
+   1800054db:	insb   (%dx),%es:(%rdi)
+   1800054dc:	outsb  %ds:(%rsi),(%dx)
+   1800054dd:	jae    0x18000551c
+   1800054df:	and    0x72(%rbp),%dh
+   1800054e2:	outsb  %ds:(%rsi),(%dx)
+   1800054e3:	cmp    0x63(%rbx),%dh
+   1800054e6:	push   $0x73616d65
+   1800054eb:	sub    $0x7263696d,%eax
+   1800054f0:	outsl  %ds:(%rsi),(%dx)
+   1800054f1:	jae    0x180005562
+   1800054f3:	data16 je 0x180005523
+   1800054f6:	movsxd 0x6d(%rdi),%ebp
+   1800054f9:	cmp    0x73(%rcx),%ah
+   1800054fc:	insl   (%dx),%es:(%rdi)
+   1800054fd:	jbe,pn 0x180005533
+   180005500:	and    (%rsi),%bh
+   180005502:	or     $0x2020200a,%eax
+   180005507:	and    %bh,(%rbx,%rsi,2)
+   18000550a:	movsxd %gs:0x72(%rbp),%esi
+   18000550e:	imul   $0x20200a0d,0x3e(%rcx,%rdi,2),%esi
+   180005516:	and    %ah,(%rax)
+   180005518:	and    %ah,(%rax)
+   18000551a:	cmp    $0x72,%al
+   18000551c:	gs jno 0x180005594
+   18000551f:	gs jae 0x180005596
+   180005522:	gs fs push %rax
+   180005525:	jb     0x180005590
+   180005527:	jbe    0x180005592
+   180005529:	insb   (%dx),%es:(%rdi)
+   18000552a:	gs addr32 gs jae 0x18000556d
+   18000552f:	or     $0x2020200a,%eax
+   180005534:	and    %ah,(%rax)
+   180005536:	and    %ah,(%rax)
+   180005538:	and    %bh,(%rdx,%rsi,2)
+   18000553b:	gs jno 0x1800055b3
+   18000553e:	gs jae 0x1800055b5
+   180005541:	gs fs rex.RB js 0x1800055ab
+   180005546:	movsxd 0x74(%rbp),%esi
+   180005549:	imul   $0x6576654c,0x6e(%rdi),%ebp
+   180005550:	insb   (%dx),%es:(%rdi)
+   180005551:	and    %ch,0x76(%rbp,%riz,2)
+   180005555:	gs insb (%dx),%es:(%rdi)
+   180005557:	cmp    $0x49736127,%eax
+   18000555c:	outsb  %ds:(%rsi),(%dx)
+   18000555d:	jbe    0x1800055ce
+   18000555f:	imul   $0x27,0x72(%rbp),%esp
+   180005563:	and    %dh,0x69(%rbp)
+   180005566:	movsxd 0x65(%r11),%esp
+   18000556a:	jae    0x1800055df
+   18000556c:	cmp    $0x6c616627,%eax
+   180005571:	jae    0x1800055d8
+   180005573:	(bad)
+   180005574:	and    %ch,(%rdi)
+   180005576:	ds or  $0x2020200a,%eax
+   18000557c:	and    %ah,(%rax)
+   18000557e:	and    %bh,(%rdi,%rbp,1)
+   180005581:	jb     0x1800055e8
+   180005583:	jno    0x1800055fa
+   180005585:	gs jae 0x1800055fc
+   180005588:	gs fs push %rax
+   18000558b:	jb     0x1800055f6
+   18000558d:	jbe    0x1800055f8
+   18000558f:	insb   (%dx),%es:(%rdi)
+   180005590:	gs addr32 gs jae 0x1800055d3
+   180005595:	or     $0x2020200a,%eax
+   18000559a:	and    %bh,(%rdi,%rbp,1)
+   18000559d:	jae    0x180005604
+   18000559f:	movsxd 0x72(%rbp),%esi
+   1800055a2:	imul   $0x20200a0d,0x3e(%rcx,%rdi,2),%esi
+   1800055aa:	cmp    $0x2f,%al
+   1800055ac:	je     0x180005620
+   1800055ae:	jne    0x180005623
+   1800055b0:	je     0x1800055fb
+   1800055b2:	outsb  %ds:(%rsi),(%dx)
+   1800055b3:	outsw  %ds:(%rsi),(%dx)
+   1800055b5:	ds or  $0x612f3c0a,%eax
+   1800055bb:	jae    0x180005630
+   1800055bd:	gs insl (%dx),%es:(%rdi)
+   1800055bf:	(bad)
+   1800055c0:	insb   (%dx),%es:(%rdi)
+   1800055c1:	jns    0x180005601
+   1800055c3:	or     $0xa,%eax
 
 Disassembly of section .reloc:
 
 0000000180006000 <.reloc>:
    180006000:	add    %ah,(%rax)
    180006002:	add    %al,(%rax)
    180006004:	sub    %al,(%rax)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/capi/onnxruntime_validation.py` & `onnxruntime/capi/onnxruntime_validation.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,136 +1,143 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 """
 Check OS requirements for ONNX Runtime Python Bindings.
 """
-import platform
 import linecache
+import platform
 import warnings
 
 
 def check_distro_info():
-    __my_distro__ = ''
-    __my_distro_ver__ = ''
+    __my_distro__ = ""
+    __my_distro_ver__ = ""
     __my_system__ = platform.system().lower()
 
-    __OS_RELEASE_FILE__ = '/etc/os-release'
-    __LSB_RELEASE_FILE__ = '/etc/lsb-release'
+    __OS_RELEASE_FILE__ = "/etc/os-release"  # noqa: N806
+    __LSB_RELEASE_FILE__ = "/etc/lsb-release"  # noqa: N806
 
-    if __my_system__ == 'windows':
+    if __my_system__ == "windows":
         __my_distro__ = __my_system__
         __my_distro_ver__ = platform.release().lower()
 
-        if __my_distro_ver__ != '10':
-            warnings.warn('Unsupported Windows version (%s). ONNX Runtime supports Windows 10 and above, only.' %
-                          __my_distro_ver__)
-    elif __my_system__ == 'linux':
-        ''' Although the 'platform' python module for getting Distro information works well on standard OS images
+        if __my_distro_ver__ != "10":
+            warnings.warn(
+                "Unsupported Windows version (%s). ONNX Runtime supports Windows 10 and above, only."
+                % __my_distro_ver__
+            )
+    elif __my_system__ == "linux":
+        """Although the 'platform' python module for getting Distro information works well on standard OS images
         running on real hardware, it is not accurate when running on Azure VMs, Git Bash, Cygwin, etc.
         The returned values for release and version are unpredictable for virtualized or emulated environments.
         /etc/os-release and /etc/lsb_release files, on the other hand, are guaranteed to exist and have standard values
         in all OSes supported by onnxruntime. The former is the current standard file to check OS info and the latter
         is its predecessor.
-        '''
+        """
         # Newer systems have /etc/os-release with relevant distro info
         __my_distro__ = linecache.getline(__OS_RELEASE_FILE__, 3)[3:-1]
         __my_distro_ver__ = linecache.getline(__OS_RELEASE_FILE__, 6)[12:-2]
 
         # Older systems may have /etc/os-release instead
         if not __my_distro__:
             __my_distro__ = linecache.getline(__LSB_RELEASE_FILE__, 1)[11:-1]
             __my_distro_ver__ = linecache.getline(__LSB_RELEASE_FILE__, 2)[16:-1]
 
         # Instead of trying to parse distro specific files,
         # warn the user ONNX Runtime may not work out of the box
         __my_distro__ = __my_distro__.lower()
         __my_distro_ver__ = __my_distro_ver__.lower()
-    elif __my_system__ == 'darwin':
+    elif __my_system__ == "darwin":
         __my_distro__ = __my_system__
         __my_distro_ver__ = platform.release().lower()
 
-        if int(__my_distro_ver__.split('.')[0]) < 11:
-            warnings.warn('Unsupported macOS version (%s). ONNX Runtime supports macOS 11.0 or later.' %
-                          (__my_distro_ver__))
+        if int(__my_distro_ver__.split(".")[0]) < 11:
+            warnings.warn(
+                "Unsupported macOS version (%s). ONNX Runtime supports macOS 11.0 or later." % (__my_distro_ver__)
+            )
     else:
-        warnings.warn('Unsupported platform (%s). ONNX Runtime supports Linux, macOS and Windows platforms, only.' %
-                      __my_system__)
+        warnings.warn(
+            "Unsupported platform (%s). ONNX Runtime supports Linux, macOS and Windows platforms, only." % __my_system__
+        )
 
 
 def validate_build_package_info():
     import_ortmodule_exception = None
 
     has_ortmodule = False
     try:
-        from onnxruntime.training.ortmodule import ORTModule # noqa
+        from onnxruntime.training.ortmodule import ORTModule  # noqa: F401
+
         has_ortmodule = True
     except ImportError:
         # ORTModule not present
         has_ortmodule = False
     except Exception as e:
         # this may happen if Cuda is not installed, we want to raise it after
         # for any exception other than not having ortmodule, we want to continue
         # device version validation and raise the exception after.
         try:
             from onnxruntime.training.ortmodule._fallback import ORTModuleInitException
+
             if isinstance(e, ORTModuleInitException):
                 # ORTModule is present but not ready to run yet
                 has_ortmodule = True
         except Exception:
             # ORTModule not present
             has_ortmodule = False
 
         if not has_ortmodule:
             import_ortmodule_exception = e
 
-    package_name = ''
-    version = ''
-    cuda_version = ''
+    package_name = ""
+    version = ""
+    cuda_version = ""
 
     if has_ortmodule:
         try:
             # collect onnxruntime package name, version, and cuda version
-            from .build_and_package_info import package_name
             from .build_and_package_info import __version__ as version
+            from .build_and_package_info import package_name
 
-            try:
+            try:  # noqa: SIM105
                 from .build_and_package_info import cuda_version
-            except: # noqa
+            except Exception:
                 pass
 
             if cuda_version:
                 # collect cuda library build info. the library info may not be available
                 # when the build environment has none or multiple libraries installed
                 try:
                     from .build_and_package_info import cudart_version
-                except: # noqa
-                    warnings.warn('WARNING: failed to get cudart_version from onnxruntime build info.')
+                except Exception:
+                    warnings.warn("WARNING: failed to get cudart_version from onnxruntime build info.")
                     cudart_version = None
 
                 def print_build_package_info():
-                    warnings.warn('onnxruntime training package info: package_name: %s' % package_name)
-                    warnings.warn('onnxruntime training package info: __version__: %s' % version)
-                    warnings.warn('onnxruntime training package info: cuda_version: %s' % cuda_version)
-                    warnings.warn('onnxruntime build info: cudart_version: %s' % cudart_version)
+                    warnings.warn("onnxruntime training package info: package_name: %s" % package_name)
+                    warnings.warn("onnxruntime training package info: __version__: %s" % version)
+                    warnings.warn("onnxruntime training package info: cuda_version: %s" % cuda_version)
+                    warnings.warn("onnxruntime build info: cudart_version: %s" % cudart_version)
 
                 # collection cuda library info from current environment.
                 from onnxruntime.capi.onnxruntime_collect_build_info import find_cudart_versions
+
                 local_cudart_versions = find_cudart_versions(build_env=False, build_cuda_version=cuda_version)
                 if cudart_version and local_cudart_versions and cudart_version not in local_cudart_versions:
                     print_build_package_info()
-                    warnings.warn('WARNING: failed to find cudart version that matches onnxruntime build info')
-                    warnings.warn('WARNING: found cudart versions: %s' % local_cudart_versions)
+                    warnings.warn("WARNING: failed to find cudart version that matches onnxruntime build info")
+                    warnings.warn("WARNING: found cudart versions: %s" % local_cudart_versions)
             else:
                 # TODO: rcom
                 pass
 
-        except Exception as e: # noqa
-            warnings.warn('WARNING: failed to collect onnxruntime version and build info')
+        except Exception as e:
+            warnings.warn("WARNING: failed to collect onnxruntime version and build info")
             print(e)
 
     if import_ortmodule_exception:
         raise import_ortmodule_exception
 
     return has_ortmodule, package_name, version, cuda_version
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/datasets/logreg_iris.onnx` & `onnxruntime/datasets/logreg_iris.onnx`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/calibrate.py` & `onnxruntime/quantization/calibrate.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,220 +1,244 @@
 #!/usr/bin/env python
-# coding: utf-8
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft, Intel Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
+import abc
+import itertools
+import uuid
+from enum import Enum
+from pathlib import Path
+from typing import Optional, Sequence
 
-import os
 import numpy as np
 import onnx
+from onnx import ModelProto, TensorProto, helper, numpy_helper
+
 import onnxruntime
-from onnx import helper, TensorProto, ModelProto
-from onnx import onnx_pb as onnx_proto
-from enum import Enum
 
-from .quant_utils import QuantType, smooth_distribution, apply_plot
-from .registry import QLinearOpsRegistry
-
-import abc
-import itertools
+from .quant_utils import apply_plot, clone_model_with_shape_infer, load_model, smooth_distribution
 
 
 class CalibrationMethod(Enum):
     MinMax = 0
     Entropy = 1
     Percentile = 2
 
+
 class CalibrationDataReader(metaclass=abc.ABCMeta):
     @classmethod
     def __subclasshook__(cls, subclass):
-        return (hasattr(subclass, 'get_next') and callable(subclass.get_next) or NotImplemented)
+        return hasattr(subclass, "get_next") and callable(subclass.get_next) or NotImplemented
 
     @abc.abstractmethod
     def get_next(self) -> dict:
         """generate the input data dict for ONNXinferenceSession run"""
         raise NotImplementedError
 
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        result = self.get_next()
+        if result is None:
+            raise StopIteration
+        return result
+
 
 class CalibraterBase:
-    def __init__(self, model, op_types_to_calibrate=[], augmented_model_path='augmented_model.onnx', symmetric=False, use_external_data_format=False):
-        '''
+    def __init__(
+        self,
+        model,
+        op_types_to_calibrate: Optional[Sequence[str]] = None,
+        augmented_model_path="augmented_model.onnx",
+        symmetric=False,
+        use_external_data_format=False,
+    ):
+        """
         :param model: ONNX model to calibrate. It can be a ModelProto or a model path
         :param op_types_to_calibrate: operator types to calibrate. By default, calibrate all the float32/float16 tensors.
         :param augmented_model_path: save augmented model to this path.
         :param symmetric: make range of tensor symmetric (central point is 0).
         :param use_external_data_format: use external data format to store model which size is >= 2Gb
-        '''
+        """
         if isinstance(model, str):
-            self.model = onnx.load(model)
+            self.model = load_model(Path(model), False)
+        elif isinstance(model, Path):
+            self.model = load_model(model, False)
         elif isinstance(model, ModelProto):
             self.model = model
         else:
-            raise ValueError('model should be either model path or onnx.ModelProto.')
+            raise ValueError("model should be either model path or onnx.ModelProto.")
 
         self.op_types_to_calibrate = op_types_to_calibrate
         self.augmented_model_path = augmented_model_path
         self.symmetric = symmetric
         self.use_external_data_format = use_external_data_format
 
-        # augment graph
         self.augment_model = None
-        self.augment_graph()
-
-        # Create InferenceSession
         self.infer_session = None
-        self.execution_providers = ['CPUExecutionProvider']
-        self._create_inference_session()
+        self.execution_providers = ["CPUExecutionProvider"]
 
-    def set_execution_providers(self, execution_providers=['CPUExecutionProvider']):
-        '''
+    def set_execution_providers(self, execution_providers=["CPUExecutionProvider"]):  # noqa: B006
+        """
         reset the execution providers to execute the collect_data. It triggers to re-creating inference session.
-        '''
+        """
         self.execution_providers = execution_providers
-        self._create_inference_session()
+        self.create_inference_session()
 
-    def _create_inference_session(self):
-        '''
+    def create_inference_session(self):
+        """
         create an OnnxRuntime InferenceSession.
-        '''
+        """
         sess_options = onnxruntime.SessionOptions()
         sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL
-        self.infer_session = onnxruntime.InferenceSession(self.augmented_model_path,
-                                                          sess_options=sess_options,
-                                                          providers=self.execution_providers)
+        self.infer_session = onnxruntime.InferenceSession(
+            self.augmented_model_path,
+            sess_options=sess_options,
+            providers=self.execution_providers,
+        )
 
     def select_tensors_to_calibrate(self, model):
-        '''
-        select all quantization_candidates op type nodes' input/output tensors. 
+        """
+        select all quantization_candidates op type nodes' input/output tensors.
         returns:
             tensors (set): set of tensor name.
             value_infos (dict): tensor name to value info.
-        '''
+        """
         value_infos = {vi.name: vi for vi in model.graph.value_info}
         value_infos.update({ot.name: ot for ot in model.graph.output})
         value_infos.update({it.name: it for it in model.graph.input})
-        initializer = set(init.name for init in model.graph.initializer)
+        initializer = {init.name for init in model.graph.initializer}
 
         tensors_to_calibrate = set()
-        tensor_type_to_calibrate = set([TensorProto.FLOAT, TensorProto.FLOAT16])
+        tensor_type_to_calibrate = {TensorProto.FLOAT, TensorProto.FLOAT16}
 
         for node in model.graph.node:
-            if len(self.op_types_to_calibrate) == 0 or node.op_type in self.op_types_to_calibrate:
+            if not self.op_types_to_calibrate or node.op_type in self.op_types_to_calibrate:
                 for tensor_name in itertools.chain(node.input, node.output):
-                    if tensor_name in value_infos.keys():
+                    if tensor_name in value_infos:
                         vi = value_infos[tensor_name]
-                        if vi.type.HasField('tensor_type') and (
-                                vi.type.tensor_type.elem_type in tensor_type_to_calibrate) and (
-                                    tensor_name not in initializer):
+                        if (
+                            vi.type.HasField("tensor_type")
+                            and (vi.type.tensor_type.elem_type in tensor_type_to_calibrate)
+                            and (tensor_name not in initializer)
+                        ):
                             tensors_to_calibrate.add(tensor_name)
 
         return tensors_to_calibrate, value_infos
 
     def get_augment_model(self):
-        '''
+        """
         return: augmented onnx model
-        '''
+        """
         return self.augment_model
 
     def augment_graph(self):
-        '''
+        """
         abstract method: augment the input model to prepare for collecting data. It will:
             1. save augmented model to augmented_model_path.
             2. set the self.augment_model
-        '''
+        """
         raise NotImplementedError
 
     def collect_data(self, data_reader: CalibrationDataReader):
-        '''
+        """
         abstract method: collect the tensors that will be used for range computation. It can be called multiple times.
-        '''
+        """
         raise NotImplementedError
 
     def compute_range(self, data_reader: CalibrationDataReader):
-        '''
+        """
         abstract method: compute the [min, max] range for the tensors to calibrate based on the collected data.
-        '''
+        """
         raise NotImplementedError
 
 
 class MinMaxCalibrater(CalibraterBase):
-    def __init__(self, 
-                model,
-                op_types_to_calibrate=[],
-                augmented_model_path='augmented_model.onnx',
-                symmetric=False,
-                use_external_data_format=False,
-                moving_average=False,
-                averaging_constant=0.01):
-        '''
+    def __init__(
+        self,
+        model,
+        op_types_to_calibrate: Optional[Sequence[str]] = None,
+        augmented_model_path="augmented_model.onnx",
+        symmetric=False,
+        use_external_data_format=False,
+        moving_average=False,
+        averaging_constant=0.01,
+    ):
+        """
         :param model: ONNX model to calibrate. It can be a ModelProto or a model path
         :param op_types_to_calibrate: operator types to calibrate. By default, calibrate all the float32/float16 tensors.
         :param augmented_model_path: save augmented model to this path.
         :param symmetric: make range of tensor symmetric (central point is 0).
         :param use_external_data_format: use external data format to store model which size is >= 2Gb
         :param moving_average: compute the moving average of the minimum and maximum values instead of the global minimum and maximum.
         :param averaging_constant: constant smoothing factor to use when computing the moving average.
-        '''
-        super(MinMaxCalibrater, self).__init__(model, op_types_to_calibrate, augmented_model_path, symmetric, use_external_data_format)
+        """
+        super().__init__(
+            model,
+            op_types_to_calibrate=op_types_to_calibrate,
+            augmented_model_path=augmented_model_path,
+            symmetric=symmetric,
+            use_external_data_format=use_external_data_format,
+        )
         self.intermediate_outputs = []
         self.calibrate_tensors_range = None
         self.num_model_outputs = len(self.model.graph.output)
-        self.model_original_outputs = set(output.name for output in self.model.graph.output)
+        self.model_original_outputs = {output.name for output in self.model.graph.output}
         self.moving_average = moving_average
         if moving_average and (averaging_constant < 0 or averaging_constant > 1):
             raise ValueError("Invalid averaging constant, which should not be < 0 or > 1.")
         self.averaging_constant = averaging_constant
 
     def augment_graph(self):
-        '''
+        """
         Adds ReduceMin and ReduceMax nodes to all quantization_candidates op type nodes in
         model and ensures their outputs are stored as part of the graph output
         :return: augmented ONNX model
-        '''
-        model = onnx_proto.ModelProto()
-        model.CopyFrom(self.model)
-        model = onnx.shape_inference.infer_shapes(model)
-
-        added_nodes = []
-        added_outputs = []
-        tensors, value_infos = self.select_tensors_to_calibrate(model) 
+        """
+        model = clone_model_with_shape_infer(self.model)
 
-        for tensor in tensors:
+        tensors, _ = self.select_tensors_to_calibrate(model)
+        reshape_shape_name = str(uuid.uuid4())
+        reshape_shape = numpy_helper.from_array(np.array([1], dtype=np.int64), reshape_shape_name)
+        model.graph.initializer.append(reshape_shape)
 
+        def add_reduce_min_max(tensor_name, reduce_op_name):
             # When doing ReduceMax/ReduceMin, ORT can't reduce on dim with value of 0 if 'keepdims' is false.
             # To make the code simple, we always let keepdims to be 1.
             keepdims = 1
 
-            # dim could be:
-            #   [dim_param: "batch_size", dim_value: 256, dim_value: 36, dim_value: 64],
-            #   [dim_value: 0],
-            #   ...
-            # Please see the definition of TensorShapeProto https://github.com/onnx/onnx/blob/master/onnx/onnx.proto#L651
-            dim = value_infos[tensor].type.tensor_type.shape.dim
-            shape = (1,) if len(dim) == 1 else tuple(1 for i in range(len(dim)))
-
-            # Adding ReduceMin nodes
-            reduce_min_name = tensor + '_ReduceMin'
-            reduce_min_node = onnx.helper.make_node('ReduceMin', [tensor], [tensor + '_ReduceMin'], reduce_min_name, keepdims=keepdims)
-
-            added_nodes.append(reduce_min_node)
-            added_outputs.append(helper.make_tensor_value_info(reduce_min_node.output[0], TensorProto.FLOAT, shape))
-
-            # Adding ReduceMax nodes
-            reduce_max_name = tensor + '_ReduceMax'
-            reduce_max_node = onnx.helper.make_node('ReduceMax', [tensor], [tensor + '_ReduceMax'], reduce_max_name, keepdims=keepdims)
-
-            added_nodes.append(reduce_max_node)
-            added_outputs.append(helper.make_tensor_value_info(reduce_max_node.output[0], TensorProto.FLOAT, shape))
-
-        model.graph.node.extend(added_nodes)
-        model.graph.output.extend(added_outputs)
-        onnx.save(model, self.augmented_model_path, save_as_external_data=self.use_external_data_format)
+            # Adding ReduceMin/ReduceMax nodes: ReduceMin/ReduceMax -> Reshape-> (output)
+            reduce_output = tensor_name + "_" + reduce_op_name
+            intermediate_output = reduce_output + "_Reshape"
+            reduce_node = onnx.helper.make_node(
+                reduce_op_name, [tensor_name], [intermediate_output], keepdims=keepdims, name=reduce_output
+            )
+
+            reshape_node = onnx.helper.make_node(
+                "Reshape",
+                inputs=[intermediate_output, reshape_shape_name],
+                outputs=[reduce_output],
+                name=intermediate_output,
+            )
+
+            model.graph.node.extend([reduce_node, reshape_node])
+            model.graph.output.append(helper.make_tensor_value_info(reduce_output, TensorProto.FLOAT, [1]))
+
+        for tensor in tensors:
+            add_reduce_min_max(tensor, "ReduceMin")
+            add_reduce_min_max(tensor, "ReduceMax")
+
+        onnx.save(
+            model,
+            self.augmented_model_path,
+            save_as_external_data=self.use_external_data_format,
+        )
         self.augment_model = model
 
     def clear_collected_data(self):
         self.intermediate_outputs = []
 
     def collect_data(self, data_reader: CalibrationDataReader):
         while True:
@@ -229,58 +253,59 @@
         self.compute_range()
         self.clear_collected_data()
 
     def merge_range(self, old_range, new_range):
         if not old_range:
             return new_range
 
-        for key, value in old_range.items(): 
+        for key, value in old_range.items():
             if self.moving_average:
                 min_value = value[0] + self.averaging_constant * (new_range[key][0] - value[0])
                 max_value = value[1] + self.averaging_constant * (new_range[key][1] - value[1])
             else:
                 min_value = min(value[0], new_range[key][0])
                 max_value = max(value[1], new_range[key][1])
             new_range[key] = (min_value, max_value)
 
         return new_range
 
     def compute_range(self):
-        ''' 
+        """
         Compute the min-max range of tensor
         :return: dictionary mapping: {added node names: (ReduceMin, ReduceMax) pairs }
-        '''
+        """
 
         if len(self.intermediate_outputs) == 0:
             return self.calibrate_tensors_range
 
         output_names = [self.infer_session.get_outputs()[i].name for i in range(len(self.intermediate_outputs[0]))]
         output_dicts_list = [
             dict(zip(output_names, intermediate_output)) for intermediate_output in self.intermediate_outputs
         ]
 
         merged_output_dict = {}
         for d in output_dicts_list:
             for k, v in d.items():
                 merged_output_dict.setdefault(k, []).append(v)
-        added_output_names = output_names[self.num_model_outputs:]
+        added_output_names = output_names[self.num_model_outputs :]
         calibrate_tensor_names = [
-            added_output_names[i].rpartition('_')[0] for i in range(0, len(added_output_names), 2)
-        ]  #output names
+            added_output_names[i].rpartition("_")[0] for i in range(0, len(added_output_names), 2)
+        ]  # output names
 
-        merged_added_output_dict = dict(
-            (i, merged_output_dict[i]) for i in merged_output_dict if i not in self.model_original_outputs)
+        merged_added_output_dict = {
+            i: merged_output_dict[i] for i in merged_output_dict if i not in self.model_original_outputs
+        }
 
         pairs = []
         for i in range(0, len(added_output_names), 2):
             min_value = 0
             max_value = 0
             if self.moving_average:
-                min_value_array = np.mean(merged_added_output_dict[added_output_names[i]], axis = 0)
-                max_value_array = np.mean(merged_added_output_dict[added_output_names[i + 1]], axis = 0)
+                min_value_array = np.mean(merged_added_output_dict[added_output_names[i]], axis=0)
+                max_value_array = np.mean(merged_added_output_dict[added_output_names[i + 1]], axis=0)
             else:
                 min_value_array = min(merged_added_output_dict[added_output_names[i]])
                 max_value_array = max(merged_added_output_dict[added_output_names[i + 1]])
             if type(min_value_array) == int or min_value_array.size > 0:
                 min_value = float(min_value_array)
             if type(max_value_array) == int or max_value_array.size > 0:
                 max_value = float(max_value_array)
@@ -291,80 +316,87 @@
             else:
                 pairs.append(tuple([min_value, max_value]))
 
         new_calibrate_tensors_range = dict(zip(calibrate_tensor_names, pairs))
         if self.calibrate_tensors_range:
             self.calibrate_tensors_range = self.merge_range(self.calibrate_tensors_range, new_calibrate_tensors_range)
         else:
-            self.calibrate_tensors_range = new_calibrate_tensors_range 
+            self.calibrate_tensors_range = new_calibrate_tensors_range
 
         return self.calibrate_tensors_range
 
+
 class HistogramCalibrater(CalibraterBase):
-    def __init__(self,
-                 model,
-                 op_types_to_calibrate=[],
-                 augmented_model_path='augmented_model.onnx',
-                 use_external_data_format=False,
-                 method='percentile',
-                 symmetric=False,
-                 num_bins=128,
-                 num_quantized_bins=2048,
-                 percentile=99.999):
-        '''
+    def __init__(
+        self,
+        model,
+        op_types_to_calibrate: Optional[Sequence[str]] = None,
+        augmented_model_path="augmented_model.onnx",
+        use_external_data_format=False,
+        method="percentile",
+        symmetric=False,
+        num_bins=128,
+        num_quantized_bins=2048,
+        percentile=99.999,
+    ):
+        """
         :param model: ONNX model to calibrate. It can be a ModelProto or a model path
         :param op_types_to_calibrate: operator types to calibrate. By default, calibrate all the float32/float16 tensors.
         :param augmented_model_path: save augmented model to this path.
         :param use_external_data_format: use external data format to store model which size is >= 2Gb
         :param method: A string. One of ['entropy', 'percentile'].
         :param symmetric: make range of tensor symmetric (central point is 0).
         :param num_bins: number of bins to create a new histogram for collecting tensor values.
         :param num_quantized_bins: number of quantized bins. Default 128.
         :param percentile: A float number between [0, 100]. Default 99.99.
-        '''
-        super(HistogramCalibrater, self).__init__(model, op_types_to_calibrate, augmented_model_path, use_external_data_format)
+        """
+        super().__init__(
+            model,
+            op_types_to_calibrate=op_types_to_calibrate,
+            augmented_model_path=augmented_model_path,
+            symmetric=symmetric,
+            use_external_data_format=use_external_data_format,
+        )
         self.intermediate_outputs = []
         self.calibrate_tensors_range = None
         self.num_model_outputs = len(self.model.graph.output)
-        self.model_original_outputs = set(output.name for output in self.model.graph.output)
+        self.model_original_outputs = {output.name for output in self.model.graph.output}
         self.collector = None
         self.method = method
-        self.symmetric = symmetric
         self.num_bins = num_bins
         self.num_quantized_bins = num_quantized_bins
         self.percentile = percentile
+        self.tensors_to_calibrate = None
 
     def augment_graph(self):
-        '''
+        """
         make all quantization_candidates op type nodes as part of the graph output.
         :return: augmented ONNX model
-        '''
-        model = onnx_proto.ModelProto()
-        model.CopyFrom(self.model)
-        model = onnx.shape_inference.infer_shapes(model)
-
-        added_nodes = []
-        added_outputs = []
-        tensors, value_infos = self.select_tensors_to_calibrate(model) 
-
-        for tensor in tensors:
-            added_outputs.append(value_infos[tensor])
+        """
+        model = clone_model_with_shape_infer(self.model)
 
-        model.graph.node.extend(added_nodes)
-        model.graph.output.extend(added_outputs)
-        onnx.save(model, self.augmented_model_path, save_as_external_data=self.use_external_data_format)
+        self.tensors_to_calibrate, value_infos = self.select_tensors_to_calibrate(model)
+        for tensor in self.tensors_to_calibrate:
+            if tensor not in self.model_original_outputs:
+                model.graph.output.append(value_infos[tensor])
+
+        onnx.save(
+            model,
+            self.augmented_model_path,
+            save_as_external_data=self.use_external_data_format,
+        )
         self.augment_model = model
 
     def clear_collected_data(self):
         self.intermediate_outputs = []
 
     def collect_data(self, data_reader: CalibrationDataReader):
-        '''
-        Entropy Calibrator collects operators' tensors as well as generates tensor histogram for each operator. 
-        '''
+        """
+        Entropy Calibrator collects operators' tensors as well as generates tensor histogram for each operator.
+        """
         while True:
             inputs = data_reader.get_next()
             if not inputs:
                 break
             self.intermediate_outputs.append(self.infer_session.run(None, inputs))
 
         if len(self.intermediate_outputs) == 0:
@@ -376,264 +408,335 @@
         ]
 
         merged_dict = {}
         for d in output_dicts_list:
             for k, v in d.items():
                 merged_dict.setdefault(k, []).append(v)
 
-        clean_merged_dict = dict((i, merged_dict[i]) for i in merged_dict if i not in self.model_original_outputs)
+        clean_merged_dict = {i: merged_dict[i] for i in merged_dict if i in self.tensors_to_calibrate}
 
         if not self.collector:
-            self.collector = HistogramCollector(method=self.method,
-                                                symmetric=self.symmetric,
-                                                num_bins=self.num_bins,
-                                                num_quantized_bins=self.num_quantized_bins,
-                                                percentile=self.percentile)
+            self.collector = HistogramCollector(
+                method=self.method,
+                symmetric=self.symmetric,
+                num_bins=self.num_bins,
+                num_quantized_bins=self.num_quantized_bins,
+                percentile=self.percentile,
+            )
         self.collector.collect(clean_merged_dict)
 
         self.clear_collected_data()
 
     def compute_range(self):
-        ''' 
+        """
         Compute the min-max range of tensor
         :return: dictionary mapping: {tensor name: (min value, max value)}
-        '''
+        """
         if not self.collector:
             raise ValueError("No collector created and can't generate calibration data.")
 
         return self.collector.compute_collection_result()
 
+
 class EntropyCalibrater(HistogramCalibrater):
-    def __init__(self,
-                 model,
-                 op_types_to_calibrate=[],
-                 augmented_model_path='augmented_model.onnx',
-                 use_external_data_format=False,
-                 method='entropy',
-                 symmetric=False,
-                 num_bins=128,
-                 num_quantized_bins=128):
-        '''
+    def __init__(
+        self,
+        model,
+        op_types_to_calibrate: Optional[Sequence[str]] = None,
+        augmented_model_path="augmented_model.onnx",
+        use_external_data_format=False,
+        method="entropy",
+        symmetric=False,
+        num_bins=128,
+        num_quantized_bins=128,
+    ):
+        """
         :param model: ONNX model to calibrate. It can be a ModelProto or a model path
         :param op_types_to_calibrate: operator types to calibrate. By default, calibrate all the float32/float16 tensors.
         :param augmented_model_path: save augmented model to this path.
         :param use_external_data_format: use external data format to store model which size is >= 2Gb
         :param method: A string. One of ['entropy', 'percentile'].
         :param symmetric: make range of tensor symmetric (central point is 0).
         :param num_bins: number of bins to create a new histogram for collecting tensor values.
         :param num_quantized_bins: number of quantized bins. Default 128.
-        '''
-        super(EntropyCalibrater, self).__init__(model, op_types_to_calibrate, augmented_model_path, use_external_data_format,
-                                                method=method, symmetric=symmetric, num_bins=num_bins, num_quantized_bins=num_quantized_bins)
+        """
+        super().__init__(
+            model,
+            op_types_to_calibrate,
+            augmented_model_path,
+            use_external_data_format,
+            method=method,
+            symmetric=symmetric,
+            num_bins=num_bins,
+            num_quantized_bins=num_quantized_bins,
+        )
+
 
 class PercentileCalibrater(HistogramCalibrater):
-    def __init__(self,
-                 model,
-                 op_types_to_calibrate=[],
-                 augmented_model_path='augmented_model.onnx',
-                 use_external_data_format=False,
-                 method='percentile',
-                 symmetric=False,
-                 num_bins=2048,
-                 percentile=99.999):
-        '''
+    def __init__(
+        self,
+        model,
+        op_types_to_calibrate: Optional[Sequence[str]] = None,
+        augmented_model_path="augmented_model.onnx",
+        use_external_data_format=False,
+        method="percentile",
+        symmetric=False,
+        num_bins=2048,
+        percentile=99.999,
+    ):
+        """
         :param model: ONNX model to calibrate. It can be a ModelProto or a model path
         :param op_types_to_calibrate: operator types to calibrate. By default, calibrate all the float32/float16 tensors.
         :param augmented_model_path: save augmented model to this path.
         :param use_external_data_format: use external data format to store model which size is >= 2Gb
         :param method: A string. One of ['entropy', 'percentile'].
         :param symmetric: make range of tensor symmetric (central point is 0).
         :param num_quantized_bins: number of quantized bins. Default 128.
         :param percentile: A float number between [0, 100]. Default 99.99.
-        '''
-        super(PercentileCalibrater, self).__init__(model, op_types_to_calibrate, augmented_model_path, use_external_data_format,
-                                                   method=method, symmetric=symmetric, num_bins=num_bins, percentile=percentile)
+        """
+        super().__init__(
+            model,
+            op_types_to_calibrate,
+            augmented_model_path,
+            use_external_data_format,
+            method=method,
+            symmetric=symmetric,
+            num_bins=num_bins,
+            percentile=percentile,
+        )
+
 
 class CalibrationDataCollector(metaclass=abc.ABCMeta):
     """
     Base class for collecting data for calibration-based quantization.
     """
 
     @abc.abstractmethod
     def collect(self, name_to_arr):
         """
         Generate informative data based on given data.
-            name_to_arr : dict 
-                tensor name to NDArray data 
+            name_to_arr : dict
+                tensor name to NDArray data
         """
         raise NotImplementedError
 
     @abc.abstractmethod
     def compute_collection_result(self):
         """
-        Get the optimal result among collection data.  
+        Get the optimal result among collection data.
         """
         raise NotImplementedError
 
+
 class HistogramCollector(CalibrationDataCollector):
     """
     Collecting histogram for each tensor. Percentile and Entropy method are supported.
 
     ref: https://github.com//apache/incubator-mxnet/blob/master/python/mxnet/contrib/quantization.py
     ref: https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/_modules/
                  pytorch_quantization/calib/histogram.html
     """
+
     def __init__(self, method, symmetric, num_bins, num_quantized_bins, percentile):
         self.histogram_dict = {}
         self.method = method
         self.symmetric = symmetric
         self.num_bins = num_bins
-        self.num_quantized_bins= num_quantized_bins
+        self.num_quantized_bins = num_quantized_bins
         self.percentile = percentile
 
     def get_histogram_dict(self):
         return self.histogram_dict
 
     def collect(self, name_to_arr):
         print("Collecting tensor data and making histogram ...")
 
         # TODO: Currently we have different collect() for entropy and percentile method respectively.
         #       Need unified collect in the future.
-        if self.method == 'entropy':
+        if self.method == "entropy":
             return self.collect_value(name_to_arr)
-        elif self.method == 'percentile':
+        elif self.method == "percentile":
             if self.symmetric:
                 return self.collect_absolute_value(name_to_arr)
             else:
                 return self.collect_value(name_to_arr)
         else:
-            raise ValueError('Only \'entropy\' or \'percentile\' method are supported')
+            raise ValueError("Only 'entropy' or 'percentile' method are supported")
 
     def collect_absolute_value(self, name_to_arr):
-        '''
+        """
         Collect histogram on absolute value
-        '''
+        """
         for tensor, data_arr in name_to_arr.items():
-            data_arr = np.asarray(data_arr)
-            data_arr = data_arr.flatten()
-            data_arr = np.absolute(data_arr) # only consider absolute value
+            data_arr = np.asarray(data_arr)  # noqa: PLW2901
+            data_arr = data_arr.flatten()  # noqa: PLW2901
+            if data_arr.size > 0:
+                min_value = np.min(data_arr)
+                max_value = np.max(data_arr)
+            else:
+                min_value = 0
+                max_value = 0
+
+            data_arr = np.absolute(data_arr)  # only consider absolute value  # noqa: PLW2901
 
             if tensor not in self.histogram_dict:
                 # first time it uses num_bins to compute histogram.
                 hist, hist_edges = np.histogram(data_arr, bins=self.num_bins)
-                self.histogram_dict[tensor] = (hist, hist_edges)
+                self.histogram_dict[tensor] = (hist, hist_edges, min_value, max_value)
             else:
                 old_histogram = self.histogram_dict[tensor]
+                old_min = old_histogram[2]
+                old_max = old_histogram[3]
                 old_hist = old_histogram[0]
                 old_hist_edges = old_histogram[1]
                 temp_amax = np.max(data_arr)
                 if temp_amax > old_hist_edges[-1]:
                     # increase the number of bins
                     width = old_hist_edges[1] - old_hist_edges[0]
                     # NOTE: np.arange may create an extra bin after the one containing temp_amax
                     new_bin_edges = np.arange(old_hist_edges[-1] + width, temp_amax + width, width)
                     old_hist_edges = np.hstack((old_hist_edges, new_bin_edges))
                 hist, hist_edges = np.histogram(data_arr, bins=old_hist_edges)
-                hist[:len(old_hist)] += old_hist
-                self.histogram_dict[tensor] = (hist, hist_edges)
+                hist[: len(old_hist)] += old_hist
+                self.histogram_dict[tensor] = (hist, hist_edges, min(old_min, min_value), max(old_max, max_value))
 
     def collect_value(self, name_to_arr):
-        '''
+        """
         Collect histogram on real value
-        '''
+        """
         for tensor, data_arr in name_to_arr.items():
-            data_arr = np.asarray(data_arr)
-            data_arr = data_arr.flatten()
+            data_arr = np.asarray(data_arr)  # noqa: PLW2901
+            data_arr = data_arr.flatten()  # noqa: PLW2901
 
             if data_arr.size > 0:
                 min_value = np.min(data_arr)
                 max_value = np.max(data_arr)
             else:
                 min_value = 0
                 max_value = 0
 
             threshold = max(abs(min_value), abs(max_value))
 
             if tensor in self.histogram_dict:
                 old_histogram = self.histogram_dict[tensor]
-                self.histogram_dict[tensor] = self.merge_histogram(old_histogram, data_arr, min_value, max_value, threshold)
+                self.histogram_dict[tensor] = self.merge_histogram(
+                    old_histogram, data_arr, min_value, max_value, threshold
+                )
             else:
                 hist, hist_edges = np.histogram(data_arr, self.num_bins, range=(-threshold, threshold))
-                self.histogram_dict[tensor] = (hist, hist_edges, min_value, max_value, threshold)
+                self.histogram_dict[tensor] = (
+                    hist,
+                    hist_edges,
+                    min_value,
+                    max_value,
+                    threshold,
+                )
 
     def merge_histogram(self, old_histogram, data_arr, new_min, new_max, new_threshold):
-
         (old_hist, old_hist_edges, old_min, old_max, old_threshold) = old_histogram
 
         if new_threshold <= old_threshold:
             new_hist, _ = np.histogram(data_arr, len(old_hist), range=(-old_threshold, old_threshold))
-            return (new_hist + old_hist, old_hist_edges, min(old_min, new_min), max(old_max, new_max), old_threshold)
+            return (
+                new_hist + old_hist,
+                old_hist_edges,
+                min(old_min, new_min),
+                max(old_max, new_max),
+                old_threshold,
+            )
         else:
             if old_threshold == 0:
                 hist, hist_edges = np.histogram(data_arr, len(old_hist), range=(-new_threshold, new_threshold))
                 hist += old_hist
             else:
                 old_num_bins = len(old_hist)
                 old_stride = 2 * old_threshold / old_num_bins
-                half_increased_bins = int((new_threshold - old_threshold) // old_stride + 1) 
+                half_increased_bins = int((new_threshold - old_threshold) // old_stride + 1)
                 new_num_bins = old_num_bins + 2 * half_increased_bins
                 new_threshold = half_increased_bins * old_stride + old_threshold
                 hist, hist_edges = np.histogram(data_arr, new_num_bins, range=(-new_threshold, new_threshold))
-                hist[half_increased_bins:new_num_bins-half_increased_bins] += old_hist
-            return (hist, hist_edges, min(old_min, new_min), max(old_max, new_max), new_threshold)
+                hist[half_increased_bins : new_num_bins - half_increased_bins] += old_hist
+            return (
+                hist,
+                hist_edges,
+                min(old_min, new_min),
+                max(old_max, new_max),
+                new_threshold,
+            )
 
     def compute_collection_result(self):
         if not self.histogram_dict or len(self.histogram_dict) == 0:
             raise ValueError("Histogram has not been collected. Please run collect() first.")
-        print("Finding optimal threshold for each tensor using {} algorithm ...".format(self.method))
+        print(f"Finding optimal threshold for each tensor using {self.method} algorithm ...")
 
-        if self.method == 'entropy':
+        if self.method == "entropy":
             return self.compute_entropy()
-        elif self.method == 'percentile':
+        elif self.method == "percentile":
             return self.compute_percentile()
         else:
-            raise ValueError('Only \'entropy\' or \'percentile\' method are supported')
+            raise ValueError("Only 'entropy' or 'percentile' method are supported")
 
     def compute_percentile(self):
         if self.percentile < 0 or self.percentile > 100:
             raise ValueError("Invalid percentile. Must be in range 0 <= percentile <= 100.")
 
         histogram_dict = self.histogram_dict
         percentile = self.percentile
 
-        thresholds_dict = {} # per tensor thresholds
+        thresholds_dict = {}  # per tensor thresholds
 
-        print("Number of tensors : {}".format(len(histogram_dict)))
-        print("Number of histogram bins : {}".format(self.num_bins))
-        print("Percentile : ({},{})".format(100.0 - percentile, percentile))
+        print(f"Number of tensors : {len(histogram_dict)}")
+        print(f"Number of histogram bins : {self.num_bins}")
+        print(f"Percentile : ({100.0 - percentile},{percentile})")
 
         for tensor, histogram in histogram_dict.items():
             hist = histogram[0]
             hist_edges = histogram[1]
             total = hist.sum()
-            cdf = np.cumsum(hist/total)
+            cdf = np.cumsum(hist / total)
             if self.symmetric:
                 idx_right = np.searchsorted(cdf, percentile / 100.0)
-                thresholds_dict[tensor] = (-float(hist_edges[idx_right]), float(hist_edges[idx_right]))
+
+                thresholds_dict[tensor] = (
+                    -float(hist_edges[idx_right]),
+                    float(hist_edges[idx_right]),
+                )
             else:
                 percent_to_cut_one_side = (100.0 - percentile) / 200.0
                 idx_right = np.searchsorted(cdf, 1.0 - percent_to_cut_one_side)
                 idx_left = np.searchsorted(cdf, percent_to_cut_one_side)
-                thresholds_dict[tensor] = (float(hist_edges[idx_left]), float(hist_edges[idx_right]))
-
+                thresholds_dict[tensor] = (
+                    float(hist_edges[idx_left]),
+                    float(hist_edges[idx_right]),
+                )
+            min_value = histogram[2]
+            max_value = histogram[3]
+            if thresholds_dict[tensor][0] < min_value:
+                thresholds_dict[tensor] = (min_value, thresholds_dict[tensor][1])
+            if thresholds_dict[tensor][1] > max_value:
+                thresholds_dict[tensor] = (thresholds_dict[tensor][0], max_value)
             # Plot histogram for debug only
             if False:
                 apply_plot(hist, hist_edges)
 
         return thresholds_dict
 
     def compute_entropy(self):
         histogram_dict = self.histogram_dict
         num_quantized_bins = self.num_quantized_bins
 
-        thresholds_dict = {} # per tensor thresholds
+        thresholds_dict = {}  # per tensor thresholds
 
-        print("Number of tensors : {}".format(len(histogram_dict)))
-        print("Number of histogram bins : {} (The number may increase depends on the data it collects)".format(self.num_bins))
-        print("Number of quantized bins : {}".format(self.num_quantized_bins))
+        print(f"Number of tensors : {len(histogram_dict)}")
+        print(
+            "Number of histogram bins : {} (The number may increase depends on the data it collects)".format(
+                self.num_bins
+            )
+        )
+        print(f"Number of quantized bins : {self.num_quantized_bins}")
 
         for tensor, histogram in histogram_dict.items():
             optimal_threshold = self.get_entropy_threshold(histogram, num_quantized_bins)
             thresholds_dict[tensor] = optimal_threshold
 
             # Plot histogram for debug only
             if False:
@@ -643,25 +746,26 @@
 
     def get_entropy_threshold(self, histogram, num_quantized_bins):
         """Given a dataset, find the optimal threshold for quantizing it.
         The reference distribution is `q`, and the candidate distribution is `p`.
         `q` is a truncated version of the original distribution.
         Ref: http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf
         """
-        from scipy.stats import entropy
         import copy
 
+        from scipy.stats import entropy
+
         hist = histogram[0]
         hist_edges = histogram[1]
         num_bins = hist.size
         zero_bin_index = num_bins // 2
         num_half_quantized_bin = num_quantized_bins // 2
-        
+
         kl_divergence = np.zeros(zero_bin_index - num_half_quantized_bin + 1)
-        thresholds = [(0, 0) for i in range(kl_divergence.size)] 
+        thresholds = [(0, 0) for i in range(kl_divergence.size)]
 
         # <------------ num bins ---------------->
         #        <--- quantized bins ---->
         # |======|===========|===========|=======|
         #              zero bin index
         #        ^                       ^
         #        |                       |
@@ -670,105 +774,126 @@
         #     |                             |
         #  start index                  end index               ...
         # ^                                      ^
         # |                                      |
         # start index                    end index       (end of iteration)
 
         for i in range(num_half_quantized_bin, zero_bin_index + 1, 1):
-            start_index = zero_bin_index - i 
+            start_index = zero_bin_index - i
             end_index = zero_bin_index + i + 1 if (zero_bin_index + i + 1) <= num_bins else num_bins
 
-            thresholds[i - num_half_quantized_bin] = (float(hist_edges[start_index]), float(hist_edges[end_index]))
+            thresholds[i - num_half_quantized_bin] = (
+                float(hist_edges[start_index]),
+                float(hist_edges[end_index]),
+            )
 
             sliced_distribution = copy.deepcopy(hist[start_index:end_index])
 
             # reference distribution p
-            p = sliced_distribution.copy() # a copy of np array
-            left_outliers_count = sum(hist[:start_index]) 
+            p = sliced_distribution.copy()  # a copy of np array
+            left_outliers_count = sum(hist[:start_index])
             right_outliers_count = sum(hist[end_index:])
             p[0] += left_outliers_count
             p[-1] += right_outliers_count
 
             # nonzeros[i] incidates whether p[i] is non-zero
             nonzeros = (p != 0).astype(np.int64)
-            
-            # quantize p.size bins into quantized bins (default 128 bins) 
+
+            # quantize p.size bins into quantized bins (default 128 bins)
             quantized_bins = np.zeros(num_quantized_bins, dtype=np.int64)
             num_merged_bins = sliced_distribution.size // num_quantized_bins
 
             # merge bins into quantized bins
             for index in range(num_quantized_bins):
-                start = index * num_merged_bins 
+                start = index * num_merged_bins
                 end = start + num_merged_bins
-                quantized_bins[index] = sum(sliced_distribution[start:end]) 
-            quantized_bins[-1] += sum(sliced_distribution[num_quantized_bins * num_merged_bins:])
+                quantized_bins[index] = sum(sliced_distribution[start:end])
+            quantized_bins[-1] += sum(sliced_distribution[num_quantized_bins * num_merged_bins :])
 
             # in order to compare p and q, we need to make length of q equals to length of p
             # expand quantized bins into p.size bins
             q = np.zeros(p.size, dtype=np.int64)
             for index in range(num_quantized_bins):
                 start = index * num_merged_bins
                 end = start + num_merged_bins
 
                 norm = sum(nonzeros[start:end])
                 if norm != 0:
                     q[start:end] = float(quantized_bins[index]) / float(norm)
-            
+
             p = smooth_distribution(p)
             q = smooth_distribution(q)
 
             if isinstance(q, np.ndarray):
                 kl_divergence[i - num_half_quantized_bin] = entropy(p, q)
             else:
-                kl_divergence[i - num_half_quantized_bin] = float('inf')
+                kl_divergence[i - num_half_quantized_bin] = float("inf")
 
         min_kl_divergence_idx = np.argmin(kl_divergence)
-        optimal_threshold = thresholds[min_kl_divergence_idx] 
-
+        optimal_threshold = thresholds[min_kl_divergence_idx]
+        min_value = histogram[2]
+        max_value = histogram[3]
+        if optimal_threshold[0] < min_value:
+            optimal_threshold = (min_value, optimal_threshold[1])
+        if optimal_threshold[1] > max_value:
+            optimal_threshold = (optimal_threshold[0], max_value)
         return optimal_threshold
 
 
-def create_calibrator(model,
-                      op_types_to_calibrate=[],
-                      augmented_model_path='augmented_model.onnx',
-                      calibrate_method=CalibrationMethod.MinMax,
-                      use_external_data_format=False,
-                      extra_options={}):
-
+def create_calibrator(
+    model,
+    op_types_to_calibrate: Optional[Sequence[str]] = None,
+    augmented_model_path="augmented_model.onnx",
+    calibrate_method=CalibrationMethod.MinMax,
+    use_external_data_format=False,
+    extra_options={},  # noqa: B006
+):
+    calibrator = None
     if calibrate_method == CalibrationMethod.MinMax:
         # default settings for min-max algorithm
-        symmetric = False if 'symmetric' not in extra_options else extra_options['symmetric']
-        moving_average = False if 'moving_average' not in extra_options else extra_options['moving_average']
-        averaging_constant = 0.01 if 'averaging_constant' not in extra_options else extra_options['averaging_constant']
-        return MinMaxCalibrater(
-            model, op_types_to_calibrate, augmented_model_path,
+        symmetric = False if "symmetric" not in extra_options else extra_options["symmetric"]
+        moving_average = False if "moving_average" not in extra_options else extra_options["moving_average"]
+        averaging_constant = 0.01 if "averaging_constant" not in extra_options else extra_options["averaging_constant"]
+        calibrator = MinMaxCalibrater(
+            model,
+            op_types_to_calibrate,
+            augmented_model_path,
             use_external_data_format=use_external_data_format,
             symmetric=symmetric,
             moving_average=moving_average,
-            averaging_constant=averaging_constant
+            averaging_constant=averaging_constant,
         )
     elif calibrate_method == CalibrationMethod.Entropy:
         # default settings for entropy algorithm
-        num_bins = 128 if 'num_bins' not in extra_options else extra_options['num_bins']
-        num_quantized_bins = 128 if 'num_quantized_bins' not in extra_options else extra_options['num_quantized_bins']
-        symmetric = False if 'symmetric' not in extra_options else extra_options['symmetric']
-        return EntropyCalibrater(
-            model, op_types_to_calibrate, augmented_model_path,
+        num_bins = 128 if "num_bins" not in extra_options else extra_options["num_bins"]
+        num_quantized_bins = 128 if "num_quantized_bins" not in extra_options else extra_options["num_quantized_bins"]
+        symmetric = False if "symmetric" not in extra_options else extra_options["symmetric"]
+        calibrator = EntropyCalibrater(
+            model,
+            op_types_to_calibrate,
+            augmented_model_path,
             use_external_data_format=use_external_data_format,
             symmetric=symmetric,
             num_bins=num_bins,
-            num_quantized_bins=num_quantized_bins
+            num_quantized_bins=num_quantized_bins,
         )
     elif calibrate_method == CalibrationMethod.Percentile:
         # default settings for percentile algorithm
-        num_bins = 2048 if 'num_bins' not in extra_options else extra_options['num_bins']
-        percentile = 99.999 if 'percentile' not in extra_options else extra_options['percentile']
-        symmetric = True if 'symmetric' not in extra_options else extra_options['symmetric']
-        return PercentileCalibrater(
-            model, op_types_to_calibrate, augmented_model_path,
+        num_bins = 2048 if "num_bins" not in extra_options else extra_options["num_bins"]
+        percentile = 99.999 if "percentile" not in extra_options else extra_options["percentile"]
+        symmetric = True if "symmetric" not in extra_options else extra_options["symmetric"]
+        calibrator = PercentileCalibrater(
+            model,
+            op_types_to_calibrate,
+            augmented_model_path,
             use_external_data_format=use_external_data_format,
             symmetric=symmetric,
             num_bins=num_bins,
-            percentile=percentile
+            percentile=percentile,
         )
 
-    raise ValueError('Unsupported calibration method {}'.format(calibrate_method))
+    if calibrator:
+        calibrator.augment_graph()
+        calibrator.create_inference_session()
+        return calibrator
+
+    raise ValueError(f"Unsupported calibration method {calibrate_method}")
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/onnx_model.py` & `onnxruntime/quantization/onnx_model.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,12 +1,94 @@
-import onnx
-import itertools
-from .quant_utils import find_by_name, attribute_to_kwarg
 from pathlib import Path
 
+import onnx
+import onnx.helper as onnx_helper
+import onnx.numpy_helper as onnx_numpy_helper
+
+from .quant_utils import attribute_to_kwarg, find_by_name
+
+
+def _clean_initializers_helper(graph, model):
+    """Clean unused initializers from graph.
+
+    Returns:
+        A cleaned graph without unused initializers
+        A list of tensor names, which are not produced by this graph and its subgraphes
+    """
+    requesting_tensor_names = set()
+    requesting_tensor_names.update(input_name for node in graph.node for input_name in node.input if input_name)
+    requesting_tensor_names.update(g_out.name for g_out in graph.output if g_out.name)
+
+    new_nodes = []
+    for node in graph.node:
+        new_node = node
+        graph_attrs = [
+            attr
+            for attr in node.attribute
+            if attr.type == onnx.AttributeProto.GRAPH or attr.type == onnx.AttributeProto.GRAPHS
+        ]
+        if graph_attrs:
+            kwargs = {}
+            for attr in node.attribute:
+                new_attribute = {}
+                if attr.type == onnx.AttributeProto.GRAPH:
+                    (
+                        cleaned_sub_graph,
+                        sub_requesting_tensor_names,
+                    ) = _clean_initializers_helper(attr.g, model)
+                    new_attribute = {attr.name: cleaned_sub_graph}
+                    requesting_tensor_names.update(sub_requesting_tensor_names)
+                elif attr.type == onnx.AttributeProto.GRAPHS:
+                    cleaned_graphes = []
+                    for subgraph in attr.graphs:
+                        (
+                            cleaned_sub_graph,
+                            sub_requesting_tensor_names,
+                        ) = _clean_initializers_helper(subgraph, model)
+                        cleaned_graphes.append(cleaned_sub_graph)
+                        requesting_tensor_names.update(sub_requesting_tensor_names)
+                    new_attribute = {attr.name: cleaned_graphes}
+                else:
+                    new_attribute = attribute_to_kwarg(attr)
+                kwargs.update(new_attribute)
+            new_node = onnx_helper.make_node(node.op_type, node.input, node.output, name=node.name, **kwargs)
+        new_nodes.append(new_node)
+
+    graph.ClearField("node")
+    graph.node.extend(new_nodes)
+
+    requesting_tensor_names.difference_update(output for node in graph.node for output in node.output)
+
+    unused_initializer = []
+    for initializer in graph.initializer:
+        if initializer.name in requesting_tensor_names:
+            requesting_tensor_names.remove(initializer.name)
+        else:
+            # mark it to remove, remove here directly will cause mis-behavier
+            unused_initializer.append(initializer)
+
+    name_to_input = {input.name: input for input in graph.input}
+    for initializer in unused_initializer:
+        graph.initializer.remove(initializer)
+        if initializer.name in name_to_input:
+            try:
+                graph.input.remove(name_to_input[initializer.name])
+            except StopIteration:
+                if model.ir_version < 4:
+                    print(
+                        "Warning: invalid weight name {} found in the graph (not a graph input)".format(
+                            initializer.name
+                        )
+                    )
+
+    requesting_tensor_names.difference_update(input.name for input in graph.input)
+
+    return graph, requesting_tensor_names
+
+
 class ONNXModel:
     def __init__(self, model):
         self.model = model
 
     def nodes(self):
         return self.model.graph.node
 
@@ -43,15 +125,15 @@
     def get_initializer(self, name):
         for tensor in self.model.graph.initializer:
             if tensor.name == name:
                 return tensor
         return None
 
     def get_initializer_name_set(self):
-        return set(initializer.name for initializer in self.model.graph.initializer)
+        return {initializer.name for initializer in self.model.graph.initializer}
 
     def remove_initializer(self, tensor):
         if tensor in self.model.graph.initializer:
             self.model.graph.initializer.remove(tensor)
             for input in self.model.graph.input:
                 if input.name == tensor.name:
                     self.model.graph.input.remove(input)
@@ -117,27 +199,29 @@
         input = node.input[idx]
         if input not in output_name_to_node:
             return None
 
         return output_name_to_node[input]
 
     def find_node_by_name(self, node_name, new_nodes_list, graph):
-        '''
-        Find out if a node exists in a graph or a node is in the
-        new set of nodes created during quantization. Return the node found.
-        '''
-        graph_nodes_list = list(graph.node)  #deep copy
+        """Find out if a node exists in a graph or a node is in the
+        new set of nodes created during quantization.
+
+        Returns:
+            The node found or None.
+        """
+        graph_nodes_list = list(graph.node)  # deep copy
         graph_nodes_list.extend(new_nodes_list)
         node = find_by_name(node_name, graph_nodes_list)
         return node
 
     def find_nodes_by_initializer(self, graph, initializer):
-        '''
+        """
         Find all nodes with given initializer as an input.
-        '''
+        """
         nodes = []
         for node in graph.node:
             for node_input in node.input:
                 if node_input == initializer.name:
                     nodes.append(node)
         return nodes
 
@@ -153,108 +237,116 @@
     @staticmethod
     def __replace_gemm_with_matmul(graph_path):
         new_nodes = []
         graph = graph_path[-1]
         for node in graph.node:
             graph_attrs = [attr for attr in node.attribute if attr.type == 5 or attr.type == 10]
             if len(graph_attrs):
-                node_name = node.name
                 kwargs = {}
                 for attr in node.attribute:
                     if attr.type == 5:
                         graph_path.append(attr.g)
                         kv = {attr.name: ONNXModel.__replace_gemm_with_matmul(graph_path)}
                     elif attr.type == 10:
                         value = []
                         for subgraph in attr.graphs:
                             graph_path.append(subgraph)
                             value.extend([ONNXModel.__replace_gemm_with_matmul(graph_path)])
                         kv = {attr.name: value}
                     else:
                         kv = attribute_to_kwarg(attr)
                     kwargs.update(kv)
-                node = onnx.helper.make_node(node.op_type, node.input, node.output, name=node.name, **kwargs)
+                node = onnx_helper.make_node(  # noqa: PLW2901
+                    node.op_type, node.input, node.output, name=node.name, **kwargs
+                )
 
-            if node.op_type == 'Gemm':
+            if node.op_type == "Gemm":
                 alpha = 1.0
                 beta = 1.0
-                transA = 0
-                transB = 0
+                transA = 0  # noqa: N806
+                transB = 0  # noqa: N806
                 for attr in node.attribute:
-                    if attr.name == 'alpha':
-                        alpha = onnx.helper.get_attribute_value(attr)
-                    elif attr.name == 'beta':
-                        beta = onnx.helper.get_attribute_value(attr)
-                    elif attr.name == 'transA':
-                        transA = onnx.helper.get_attribute_value(attr)
-                    elif attr.name == 'transB':
-                        transB = onnx.helper.get_attribute_value(attr)
+                    if attr.name == "alpha":
+                        alpha = onnx_helper.get_attribute_value(attr)
+                    elif attr.name == "beta":
+                        beta = onnx_helper.get_attribute_value(attr)
+                    elif attr.name == "transA":
+                        transA = onnx_helper.get_attribute_value(attr)  # noqa: N806
+                    elif attr.name == "transB":
+                        transB = onnx_helper.get_attribute_value(attr)  # noqa: N806
                 if alpha == 1.0 and beta == 1.0 and transA == 0:
-                    inputB = node.input[1]
+                    inputB = node.input[1]  # noqa: N806
                     if transB == 1:
-                        B, Bs_graph = ONNXModel.__get_initializer(node.input[1], graph_path)
+                        B, Bs_graph = ONNXModel.__get_initializer(node.input[1], graph_path)  # noqa: N806
                         if B:
                             # assume B is not used by any other node
-                            B_array = onnx.numpy_helper.to_array(B)
-                            B_trans = onnx.numpy_helper.from_array(B_array.T)
+                            B_array = onnx_numpy_helper.to_array(B)  # noqa: N806
+                            B_trans = onnx_numpy_helper.from_array(B_array.T)  # noqa: N806
                             B_trans.name = B.name
                             Bs_graph.initializer.remove(B)
                             for input in Bs_graph.input:
                                 if input.name == inputB:
                                     Bs_graph.input.remove(input)
                                     break
                             Bs_graph.initializer.extend([B_trans])
                         else:
-                            inputB += '_Transposed'
-                            transpose_node = onnx.helper.make_node('Transpose',
-                                                                   inputs=[node.input[1]],
-                                                                   outputs=[inputB],
-                                                                   name=node.name + '_Transpose' if node.name != "" else "")
+                            inputB += "_Transposed"  # noqa: N806
+                            transpose_node = onnx_helper.make_node(
+                                "Transpose",
+                                inputs=[node.input[1]],
+                                outputs=[inputB],
+                                name=node.name + "_Transpose" if node.name else "",
+                            )
                             new_nodes.append(transpose_node)
 
-                    matmul_node = onnx.helper.make_node(
-                        'MatMul',
+                    matmul_node = onnx_helper.make_node(
+                        "MatMul",
                         inputs=[node.input[0], inputB],
-                        outputs=[node.output[0] + ('_MatMul' if len(node.input) > 2 else '')],
-                        name=node.name + '_MatMul' if node.name != "" else "")
+                        outputs=[node.output[0] + ("_MatMul" if len(node.input) > 2 else "")],
+                        name=node.name + "_MatMul" if node.name else "",
+                    )
                     new_nodes.append(matmul_node)
 
                     if len(node.input) > 2:
-                        add_node = onnx.helper.make_node('Add',
-                                                         inputs=[node.output[0] + '_MatMul', node.input[2]],
-                                                         outputs=node.output,
-                                                         name=node.name + '_Add' if node.name != "" else "")
+                        add_node = onnx_helper.make_node(
+                            "Add",
+                            inputs=[node.output[0] + "_MatMul", node.input[2]],
+                            outputs=node.output,
+                            name=node.name + "_Add" if node.name else "",
+                        )
                         new_nodes.append(add_node)
 
                 # unsupported
                 else:
                     new_nodes.append(node)
 
             # not GEMM
             else:
                 new_nodes.append(node)
 
-        graph.ClearField('node')
+        graph.ClearField("node")
         graph.node.extend(new_nodes)
         graph_path.pop()
         return graph
 
     def replace_gemm_with_matmul(self):
         graph_path = [self.graph()]
         ONNXModel.__replace_gemm_with_matmul(graph_path)
 
     def save_model_to_file(self, output_path, use_external_data_format=False):
-        '''
+        """
         Save model to external data, which is needed for model size > 2GB
-        '''
+        """
         self.topological_sort()
         if use_external_data_format:
-            onnx.external_data_helper.convert_model_to_external_data(self.model,
-                                                                     all_tensors_to_one_file=True,
-                                                                     location=Path(output_path).name + ".data")
+            onnx.external_data_helper.convert_model_to_external_data(
+                self.model,
+                all_tensors_to_one_file=True,
+                location=Path(output_path).name + ".data",
+            )
         onnx.save_model(self.model, output_path)
 
     @staticmethod
     def replace_node_input(node, old_input_name, new_input_name):
         assert isinstance(old_input_name, str) and isinstance(new_input_name, str)
         for j in range(len(node.input)):
             if node.input[j] == old_input_name:
@@ -274,20 +366,23 @@
     def replace_output_of_all_nodes(self, old_output_name, new_output_name):
         for node in self.model.graph.node:
             ONNXModel.replace_node_output(node, old_output_name, new_output_name)
 
     def remove_unused_constant(self):
         input_name_to_nodes = self.input_name_to_nodes()
 
-        #remove unused constant
+        # remove unused constant
         unused_nodes = []
         nodes = self.nodes()
         for node in nodes:
-            if node.op_type == "Constant" and not self.is_graph_output(
-                    node.output[0]) and node.output[0] not in input_name_to_nodes:
+            if (
+                node.op_type == "Constant"
+                and not self.is_graph_output(node.output[0])
+                and node.output[0] not in input_name_to_nodes
+            ):
                 unused_nodes.append(node)
 
         self.remove_nodes(unused_nodes)
 
         ununsed_weights = []
         for w in self.initializer():
             if w.name not in input_name_to_nodes and not self.is_graph_output(w.name):
@@ -296,29 +391,29 @@
                 for graph_input in self.graph().input:
                     if graph_input.name == w.name:
                         self.graph().input.remove(graph_input)
 
         self.remove_initializers(ununsed_weights)
 
     def is_graph_output(self, output_name):
-        for output in self.model.graph.output:
-            if output.name == output_name:
-                return True
-        return False
+        return any(output.name == output_name for output in self.model.graph.output)
+
+    def is_graph_input(self, tensor_name: str) -> bool:
+        return any(input.name == tensor_name for input in self.model.graph.input)
 
     # TODO:use OnnxModel.graph_topological_sort(self.model.graph) from transformers.onnx_model
     # Currently it breaks Openvino/Linux training gpu pipeline so hold off for 1.8 release
     def topological_sort(self):
-        deps_count = [0]*len(self.nodes()) # dependency count of each node
-        deps_to_nodes = {} # input to node indice
+        deps_count = [0] * len(self.nodes())  # dependency count of each node
+        deps_to_nodes = {}  # input to node indice
         sorted_nodes = []  # initialize sorted_nodes
         for node_idx, node in enumerate(self.nodes()):
             # CANNOT use len(node.input) directly because input can be optional
-            deps_count[node_idx] = sum(1 for _ in node.input if _ )
-            if deps_count[node_idx] == 0: # Constant doesn't depend on any inputs
+            deps_count[node_idx] = sum(1 for _ in node.input if _)
+            if deps_count[node_idx] == 0:  # Constant doesn't depend on any inputs
                 sorted_nodes.append(self.nodes()[node_idx])
                 continue
 
             for input_name in node.input:
                 if input_name not in deps_to_nodes:
                     deps_to_nodes[input_name] = [node_idx]
                 else:
@@ -349,10 +444,13 @@
                     for node_idx in deps_to_nodes[output]:
                         deps_count[node_idx] = deps_count[node_idx] - 1
                         if deps_count[node_idx] == 0:
                             sorted_nodes.append(self.nodes()[node_idx])
                             end = end + 1
             start = start + 1
 
-        assert(end == len(self.graph().node)), "Graph is not a DAG"
-        self.graph().ClearField('node')
-        self.graph().node.extend(sorted_nodes)
+        assert end == len(self.graph().node), "Graph is not a DAG"
+        self.graph().ClearField("node")
+        self.graph().node.extend(sorted_nodes)
+
+    def clean_initializers(self):
+        return _clean_initializers_helper(self.graph(), self.model)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/onnx_quantizer.py` & `onnxruntime/quantization/onnx_quantizer.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,87 +1,124 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
-import os
-import struct
-from pathlib import Path
-import numpy as np
 import logging
 
+import numpy as np
 import onnx
 import onnx.numpy_helper
 from onnx import onnx_pb as onnx_proto
-from onnxruntime import SessionOptions, InferenceSession, GraphOptimizationLevel
-
-from .quant_utils import QuantizationMode, QuantizedValueType, QuantizedInitializer, QuantizedValue
-from .quant_utils import find_by_name, get_elem_index, get_mul_node, generate_identified_filename, attribute_to_kwarg, type_to_name
-from .quant_utils import quantize_nparray, quantize_data, compute_scale_zp, get_qrange_for_qType, get_qmin_qmax_for_qType
-from .quant_utils import QuantType, onnx_domain, __producer__, __version__
-
-from .registry import CreateOpQuantizer, CreateDefaultOpQuantizer
 
 from .onnx_model import ONNXModel
+from .quant_utils import (
+    TENSOR_NAME_QUANT_SUFFIX,
+    QuantizationMode,
+    QuantizedValue,
+    QuantizedValueType,
+    QuantType,
+    __producer__,
+    __version__,
+    add_infer_metadata,
+    attribute_to_kwarg,
+    compute_scale_zp,
+    find_by_name,
+    get_qmin_qmax_for_qType,
+    get_qrange_for_qType,
+    model_has_infer_metadata,
+    quantize_data,
+    save_and_reload_model,
+    tensor_proto_to_array,
+)
+from .registry import CreateOpQuantizer
 
-class ONNXQuantizer:
-    def __init__(self, model, per_channel, reduce_range, mode, static, weight_qType, input_qType, tensors_range,
-                 nodes_to_quantize, nodes_to_exclude, op_types_to_quantize, extra_options={}):
 
-        # run shape inference on the model (enabled by default)
-        self.extra_options = extra_options if extra_options is not None else {}
-        if not ('DisableShapeInference' in self.extra_options and self.extra_options['DisableShapeInference']):
-            model = onnx.shape_inference.infer_shapes(model)
+class ONNXQuantizer:
+    def __init__(
+        self,
+        model,
+        per_channel,
+        reduce_range,
+        mode,
+        static,
+        weight_qType,
+        activation_qType,
+        tensors_range,
+        nodes_to_quantize,
+        nodes_to_exclude,
+        op_types_to_quantize,
+        extra_options=None,
+    ):
+        if not model_has_infer_metadata(model):
+            model = save_and_reload_model(model)
         self.value_infos = {vi.name: vi for vi in model.graph.value_info}
         self.value_infos.update({ot.name: ot for ot in model.graph.output})
         self.value_infos.update({it.name: it for it in model.graph.input})
 
         self.model = ONNXModel(model)
+        if not static:
+            self.model.replace_gemm_with_matmul()
+
         self.per_channel = per_channel  # weight-pack per channel
         self.reduce_range = reduce_range
         self.mode = mode  # QuantizationMode.Value
         self.static = static  # use static quantization for inputs.
         self.fuse_dynamic_quant = False
-        self.enable_subgraph_quantization = 'EnableSubgraph' in self.extra_options and self.extra_options['EnableSubgraph']
-        self.force_quantize_no_input_check = 'ForceQuantizeNoInputCheck' in self.extra_options and self.extra_options['ForceQuantizeNoInputCheck']
-        self.q_matmul_const_b_only = 'MatMulConstBOnly' in self.extra_options and self.extra_options['MatMulConstBOnly']
-        is_weight_int8 = weight_qType == QuantType.QInt8
-        self.is_weight_symmetric = is_weight_int8 if 'WeightSymmetric' not in self.extra_options else self.extra_options['WeightSymmetric']
-        self.is_activation_symmetric = False if 'ActivationSymmetric' not in self.extra_options else self.extra_options['ActivationSymmetric']
 
-        self.input_qType = onnx_proto.TensorProto.INT8 if input_qType == QuantType.QInt8 else onnx_proto.TensorProto.UINT8
-        self.weight_qType = onnx_proto.TensorProto.INT8 if weight_qType == QuantType.QInt8 else onnx_proto.TensorProto.UINT8
-        '''
+        self.extra_options = extra_options if extra_options else {}
+        self.enable_subgraph_quantization = (
+            "EnableSubgraph" in self.extra_options and self.extra_options["EnableSubgraph"]
+        )
+        self.force_quantize_no_input_check = (
+            "ForceQuantizeNoInputCheck" in self.extra_options and self.extra_options["ForceQuantizeNoInputCheck"]
+        )
+        self.q_matmul_const_b_only = "MatMulConstBOnly" in self.extra_options and self.extra_options["MatMulConstBOnly"]
+        is_weight_int8 = weight_qType == QuantType.QInt8
+        self.is_weight_symmetric = (
+            is_weight_int8 if "WeightSymmetric" not in self.extra_options else self.extra_options["WeightSymmetric"]
+        )
+        self.is_activation_symmetric = (
+            False if "ActivationSymmetric" not in self.extra_options else self.extra_options["ActivationSymmetric"]
+        )
+
+        self.activation_qType = (
+            onnx_proto.TensorProto.INT8 if activation_qType == QuantType.QInt8 else onnx_proto.TensorProto.UINT8
+        )
+        self.weight_qType = (
+            onnx_proto.TensorProto.INT8 if weight_qType == QuantType.QInt8 else onnx_proto.TensorProto.UINT8
+        )
+        """
             Dictionary specifying the min and max values for tensors. It has following format:
                 {
                     "param_name": [min, max]
                 }
             example:
                 {
                     'Conv_3:0': [np.float32(0), np.float32(0.5)],
                     'Conv_4:0': [np.float32(1), np.float32(3.5)]
                 }
-        '''
+        """
         self.tensors_range = tensors_range
         self.nodes_to_quantize = nodes_to_quantize  # specific nodes to quantize
         self.nodes_to_exclude = nodes_to_exclude  # specific nodes to exclude
         self.op_types_to_quantize = op_types_to_quantize
         self.new_nodes = []
         self.parent = None
-        self.graph_scope = "/" # for human readable debug information
-        self.tensor_names = { } # in case the shape inference not totally working
+        self.graph_scope = "/"  # for human readable debug information
+        self.tensor_names = {}  # in case the shape inference not totally working
         self.tensor_names.update({ot.name: 1 for ot in model.graph.output})
         self.tensor_names.update({it.name: 1 for it in model.graph.input})
         for node in self.model.model.graph.node:
             self.tensor_names.update({output_name: 1 for output_name in node.output})
 
         self.opset_version = self.check_opset_version()
 
-        if not self.mode in QuantizationMode:
-            raise ValueError('unsupported quantization mode {}'.format(self.mode))
+        if self.mode not in QuantizationMode:
+            raise ValueError(f"unsupported quantization mode {self.mode}")
 
         self.quantization_params = self.calculate_quantization_params()
 
         # QuantizeRange tensor name and zero tensor name for scale and zero point calculation.
         # Used when static is False
         self.fixed_qrange_uint8_name = "fixed_quantization_range_uint8"
         self.fixed_qrange_int8_name = "fixed_quantization_range_int8"
@@ -91,510 +128,557 @@
         self.fixed_zero_zp_name = "fixed_zero_zp"
 
         # Map of all original value names to quantized value names
         self.quantized_value_map = {}
         # some output from nodes will be quantized, yet itself should be treat as existing so
         # no dequantized will be applied when needed later
         self.generated_value_names = self.model.get_non_initializer_inputs()
+        # to store specified scale and zeropoint instead of calculated value, tensor_name->(scale, zeropoint)
+        self.used_scale_zp_map = {}
 
     # routines for subgraph support
     def quantize_subgraph(self, subgraph, graph_key):
-        '''
-            generate submodel for the subgraph, so that we re-utilize current quantization implementation.
-            quantize the submodel
-            update subgraph and set it back to node
-        '''
-        warped_model = onnx.helper.make_model(subgraph, producer_name='onnx-quantizer',
-                                              opset_imports=self.model.model.opset_import)
-        sub_quanitzer = ONNXQuantizer(warped_model,
-                                      self.per_channel,
-                                      self.reduce_range,
-                                      self.mode,
-                                      self.static,
-                                      self.weight_qType,
-                                      self.input_qType,
-                                      self.tensors_range,
-                                      self.nodes_to_quantize,
-                                      self.nodes_to_exclude,
-                                      self.op_types_to_quantize,
-                                      self.extra_options)
+        """
+        generate submodel for the subgraph, so that we re-utilize current quantization implementation.
+        quantize the submodel
+        update subgraph and set it back to node
+        """
+        warped_model = onnx.helper.make_model(
+            subgraph,
+            producer_name="onnx-quantizer",
+            opset_imports=self.model.model.opset_import,
+        )
+        add_infer_metadata(warped_model)
+        sub_quanitzer = ONNXQuantizer(
+            warped_model,
+            self.per_channel,
+            self.reduce_range,
+            self.mode,
+            self.static,
+            self.weight_qType,
+            self.activation_qType,
+            self.tensors_range,
+            self.nodes_to_quantize,
+            self.nodes_to_exclude,
+            self.op_types_to_quantize,
+            self.extra_options,
+        )
         sub_quanitzer.parent = self
-        sub_quanitzer.graph_scope = "{}{}/".format(self.graph_scope, graph_key)
+        sub_quanitzer.graph_scope = f"{self.graph_scope}{graph_key}/"
         sub_quanitzer.quantize_model()
         return sub_quanitzer.model.model.graph
 
     def quantize_node_with_sub_graph(self, node):
-        '''
+        """
         Check subgraph, if any, quantize it and replace it.
         return new_nodes added for quantizing subgraph
-        '''
-        graph_attrs = [attr for attr in node.attribute if attr.type == onnx.AttributeProto.GRAPH or attr.type == onnx.AttributeProto.GRAPHS]
+        """
+        graph_attrs = [
+            attr
+            for attr in node.attribute
+            if attr.type == onnx.AttributeProto.GRAPH or attr.type == onnx.AttributeProto.GRAPHS
+        ]
         if len(graph_attrs) == 0:
             return node
-        node_name = node.name if node.name != "" else "{}_node_count_{}".format(node.op_type, len(self.new_nodes))
+        node_name = node.name if node.name else f"{node.op_type}_node_count_{len(self.new_nodes)}"
         kwargs = {}
         for attr in node.attribute:
             if attr.type == onnx.AttributeProto.GRAPH:
-                kv = {attr.name: self.quantize_subgraph(attr.g, "{}:{}".format(node_name, attr.name))}
+                kv = {attr.name: self.quantize_subgraph(attr.g, f"{node_name}:{attr.name}")}
             elif attr.type == onnx.AttributeProto.GRAPHS:
                 value = []
                 for subgraph in attr.graphs:
-                    value.extend([self.quantize_subgraph(subgraph, "{}:{}:{}".format(node_name, attr.name, len(value)))])
+                    value.extend(
+                        [
+                            self.quantize_subgraph(
+                                subgraph,
+                                f"{node_name}:{attr.name}:{len(value)}",
+                            )
+                        ]
+                    )
                 kv = {attr.name: value}
             else:
                 kv = attribute_to_kwarg(attr)
             kwargs.update(kv)
         return onnx.helper.make_node(node.op_type, node.input, node.output, name=node.name, **kwargs)
 
     def check_opset_version(self):
         ai_onnx_domain = [
             opset for opset in self.model.model.opset_import if not opset.domain or opset.domain == "ai.onnx"
         ]
-        if 1 != len(ai_onnx_domain):
-            raise ValueError('Failed to find proper ai.onnx domain')
+        if len(ai_onnx_domain) != 1:
+            raise ValueError("Failed to find proper ai.onnx domain")
         opset_version = ai_onnx_domain[0].version
 
         if opset_version == 10:
             logging.warning(
-                "The original model opset version is {}, which does not support node fusions. Please update the model to opset >= 11 for better performance."
-                .format(opset_version))
+                "The original model opset version is {}, which does not support node fusions. Please update the model to opset >= 11 for better performance.".format(
+                    opset_version
+                )
+            )
             return 10
 
         if opset_version < 10:
             logging.warning(
-                "The original model opset version is {}, which does not support quantization. Please update the model to opset >= 11. Updating the model automatically to opset 11. Please verify the quantized model."
-                .format(opset_version))
+                "The original model opset version is {}, which does not support quantization. Please update the model to opset >= 11. Updating the model automatically to opset 11. Please verify the quantized model.".format(
+                    opset_version
+                )
+            )
             self.model.model.opset_import.remove(ai_onnx_domain[0])
             self.model.model.opset_import.extend([onnx.helper.make_opsetid("", 11)])
             opset_version = 11
 
         self.fuse_dynamic_quant = True
         return opset_version
 
-    def has_QDQ_nodes(self):
-        '''
-            Detect if model already has QuantizeLinear or DequantizeLinear.
-        '''
-        return any(node.op_type == 'QuantizeLinear' or node.op_type == 'DequantizeLinear' for node in self.model.nodes())
-
-    def remove_fake_quantized_nodes(self):
-        '''
-            Detect and remove the quantize/dequantizelinear node pairs(fake quantized nodes in Quantization-Aware training)
-            and reconnect and update the nodes.
-        '''
-        nodes_to_remove = []
-        initializers_to_remove = []
-
-        for curr_node in self.model.nodes():
-            if curr_node.op_type == 'QuantizeLinear':
-                next_node, prev_node, succ_node = None, None, None
-                for child_node in self.model.get_children(curr_node):
-                    if child_node.op_type == 'DequantizeLinear':
-                        next_node = child_node
-                if next_node is None:
-                    raise ValueError(
-                        "Remove fake-quantized node pair Error: DequantizeLinear node is not found for {}.".format(
-                            curr_node.name))
-
-                prev_node = self.model.get_parent(curr_node, 0)
-                if prev_node is None:
-                    raise ValueError("Remove fake-quantized node pair Error: Parent node is not found for {}.".format(
-                        curr_node.name))
-
-                succ_nodes = self.model.get_children(next_node)
-                if len(succ_nodes) == 0:
-                    raise ValueError("Remove fake-quantized node pair Error: No successive nodes found for {}.".format(
-                        next_node.name))
-
-                # TODO: convert it to the specified input_type
-                scale_tensor_name = curr_node.input[1]
-                zp_tensor_name = curr_node.input[2]
-                initializer_scale = find_by_name(scale_tensor_name, self.model.initializer())
-                initializer_zp = find_by_name(zp_tensor_name, self.model.initializer())
-                zp_and_scale = [
-                    onnx.numpy_helper.to_array(initializer_zp),
-                    onnx.numpy_helper.to_array(initializer_scale)
-                ]
-
-                # connect the previous and successive node input and output
-                for succ_node in succ_nodes:
-                    succ_idx = get_elem_index(next_node.output[0], succ_node.input)
-                    if succ_idx != -1:
-                        succ_node.input[succ_idx] = curr_node.input[0]
-                    else:
-                        raise ValueError(
-                            "Remove fake-quantized node pair Error: Connection failed. No matched successive node input found for {}."
-                            .format(next_node.name))
-
-                param_name = curr_node.input[0]
-                if self.quantization_params is None:
-                    self.quantization_params = {}
-                self.quantization_params[param_name] = zp_and_scale
-
-                # remove fake-quantized nodes
-                nodes_to_remove.extend([curr_node])
-                nodes_to_remove.extend([next_node])
-
-                # remove unused initializers in graph
-                initializers_to_remove.extend([initializer_scale])
-                initializers_to_remove.extend([initializer_zp])
-
-        self.model.remove_nodes(nodes_to_remove)
-        self.model.remove_initializers(initializers_to_remove)
-
-        return self.model.model
+    def has_QDQ_nodes(self):  # noqa: N802
+        """
+        Detect if model already has QuantizeLinear or DequantizeLinear.
+        """
+        return any(
+            node.op_type == "QuantizeLinear" or node.op_type == "DequantizeLinear" for node in self.model.nodes()
+        )
 
     def find_initializer_in_path(self, initializer_name):
         if find_by_name(initializer_name, self.model.initializer()) is not None:
             return True
         if self.parent is not None:
             return self.parent.find_initializer_in_path(initializer_name)
         return False
 
-    def should_quantize(self, node):
-        if self.nodes_to_quantize is not None and len(
-                self.nodes_to_quantize) != 0 and node.name not in self.nodes_to_quantize:
-            return False
-
-        if (node.op_type not in self.op_types_to_quantize):
-            return False
-
-        if self.nodes_to_exclude is not None and node.name in self.nodes_to_exclude:
-            return False
-
-        # do not quantize non-constant B matrices for matmul
-        if self.q_matmul_const_b_only:
-            if node.op_type == "MatMul" and (not self.find_initializer_in_path(node.input[1])):
-                print("Ignore MatMul due to non constant B: {}[{}]".format(self.graph_scope, node.name))
-                return False
-
-        return True
-
     def add_new_nodes(self, nodes):
         self.new_nodes.extend(nodes)
         for node in nodes:
             for output_name in node.output:
                 self.generated_value_names.add(output_name)
 
     def quantize_model(self):
         if self.has_QDQ_nodes():
             logging.warning(
                 "Please check if the model is already quantized."
-                "Note you don't need to quantize a QAT model. OnnxRuntime support to run QAT model directly.")
+                "Note you don't need to quantize a QAT model. OnnxRuntime support to run QAT model directly."
+            )
 
         for node in self.model.nodes():
             # quantize subgraphes if have
             if self.enable_subgraph_quantization:
-                node = self.quantize_node_with_sub_graph(node)
+                node = self.quantize_node_with_sub_graph(node)  # noqa: PLW2901
 
             number_of_existing_new_nodes = len(self.new_nodes)
-            if self.should_quantize(node):
-                op_quantizer = CreateOpQuantizer(self, node)
-            else:
-                op_quantizer = CreateDefaultOpQuantizer(self, node)
-
+            op_quantizer = CreateOpQuantizer(self, node)
             op_quantizer.quantize()
             for i in range(number_of_existing_new_nodes, len(self.new_nodes)):
                 for output_name in self.new_nodes[i].output:
                     self.generated_value_names.add(output_name)
 
         self._dequantize_outputs()
 
         # extend is used to append to the list for a protobuf fields
         # https://developers.google.com/protocol-buffers/docs/reference/python-generated?csw=1#fields
-        self.model.graph().ClearField('node')
+        self.model.graph().ClearField("node")
         self.model.graph().node.extend(self.new_nodes)
 
         # Remove ununsed initializers from graph, starting from the top level graph.
         if self.parent is None:
-            _, initializers_not_found = ONNXQuantizer.CleanGraphInitializers(self.model.graph(), self.model.model)
+            _, initializers_not_found = self.model.clean_initializers()
             if len(initializers_not_found) > 0:
                 raise RuntimeError("Invalid model with unknown initializers/tensors." + str(initializers_not_found))
 
         self.model.model.producer_name = __producer__
         self.model.model.producer_version = __version__
 
         return self.model.model
 
-    @staticmethod
-    def tensor_proto_to_array(initializer):
-        if initializer.data_type == onnx_proto.TensorProto.FLOAT:
-            weights = onnx.numpy_helper.to_array(initializer)
-        else:
-            raise ValueError('Only float type quantization is supported. Weights {} is {}. '.format(
-                initializer.name, type_to_name[initializer.data_type]))
-        return weights
-
-    def is_input_a_weight(self, input_name):
+    def is_input_a_initializer(self, input_name):
         initializer = find_by_name(input_name, self.model.initializer())
         return initializer is not None
 
     def is_per_channel(self):
         return self.per_channel
 
     def is_valid_quantize_weight(self, weight_name):
         weight = find_by_name(weight_name, self.model.initializer())
         if weight is not None:
             return weight.data_type == onnx_proto.TensorProto.FLOAT
         if (not self.enable_subgraph_quantization) or (self.parent is None):
             return False
         return self.parent.is_valid_quantize_weight(weight_name)
 
+    def is_float_tensor(self, tensor_name):
+        if self.is_input_a_initializer(tensor_name):
+            return self.is_valid_quantize_weight(tensor_name)
+
+        if tensor_name in self.value_infos:
+            vi = self.value_infos[tensor_name]
+            if vi.type.HasField("tensor_type") and vi.type.tensor_type.elem_type == onnx_proto.TensorProto.FLOAT:
+                return True
+        elif self.enable_subgraph_quantization and self.parent:
+            return self.parent.is_float_tensor(tensor_name)
+        else:
+            logging.warning(
+                "Failed to infer data type of tensor: {}. Please add data type info for this tensor "
+                "if your model has customized operators.".format(tensor_name)
+            )
+
+        return False
+
+    def should_quantize_node(self, node):
+        if (
+            self.nodes_to_quantize is not None
+            and len(self.nodes_to_quantize) != 0
+            and node.name not in self.nodes_to_quantize
+        ):
+            return False
+
+        if node.op_type not in self.op_types_to_quantize:
+            return False
+
+        if self.nodes_to_exclude is not None and node.name in self.nodes_to_exclude:
+            return False
+
+        return True
+
     def _get_dynamic_input_quantization_params(self, input_name, nodes_list, qType):
-        '''
+        """
         Create nodes for dynamic quantization of input and add them to nodes_list.
             parameter input_name: Name of the input.
             parameter nodes_list: new nodes are appended to this list.
             parameter qType: type to quantize to.
             return: scale_name, zero_point_name, scale_shape, zero_point_shape.
-        '''
+        """
         if qType == onnx_proto.TensorProto.INT8:
             return self._get_dynamic_input_quantization_params_int8(input_name, nodes_list)
 
         return self._get_dynamic_input_quantization_params_uint8(input_name, nodes_list)
 
     def _get_dynamic_input_quantization_params_int8(self, input_name, nodes_list):
-        '''
+        """
         Create nodes for dynamic quantization of input to int8 and add them to nodes_list
             parameter input_name: Name of the input.
             parameter nodes_list: new nodes are appended to this list.
             return: scale_name, zero_point_name, scale_shape, zero_point_shape.
-        '''
-        qType = onnx_proto.TensorProto.INT8
+        """
+        qType = onnx_proto.TensorProto.INT8  # noqa: N806
 
         # Reduce min and Reduce max
         input_scale_name = input_name + "_scale"
 
         reduce_min_name = input_name + "_ReduceMin"
-        reduce_min_node = onnx.helper.make_node("ReduceMin", [input_name], [reduce_min_name + ":0"],
-                                                reduce_min_name,
-                                                keepdims=0)
+        reduce_min_node = onnx.helper.make_node(
+            "ReduceMin",
+            [input_name],
+            [reduce_min_name + ":0"],
+            reduce_min_name,
+            keepdims=0,
+        )
         nodes_list.append(reduce_min_node)
 
         reduce_max_name = input_name + "_ReduceMax"
-        reduce_max_node = onnx.helper.make_node("ReduceMax", [input_name], [reduce_max_name + ":0"],
-                                                reduce_max_name,
-                                                keepdims=0)
+        reduce_max_node = onnx.helper.make_node(
+            "ReduceMax",
+            [input_name],
+            [reduce_max_name + ":0"],
+            reduce_max_name,
+            keepdims=0,
+        )
         nodes_list.append(reduce_max_node)
 
         # Compute scale
         #   Find abs(rmin)
         reduce_min_abs_name = reduce_min_name + "_Abs"
-        reduce_min_abs_node = onnx.helper.make_node("Abs", [reduce_min_node.output[0]], [reduce_min_abs_name + ":0"],
-                                                    reduce_min_abs_name)
+        reduce_min_abs_node = onnx.helper.make_node(
+            "Abs",
+            [reduce_min_node.output[0]],
+            [reduce_min_abs_name + ":0"],
+            reduce_min_abs_name,
+        )
         nodes_list.append(reduce_min_abs_node)
         #   Find abs(rmax)
         reduce_max_abs_name = reduce_max_name + "_Abs"
-        reduce_max_abs_node = onnx.helper.make_node("Abs", [reduce_max_node.output[0]], [reduce_max_abs_name + ":0"],
-                                                    reduce_max_abs_name)
+        reduce_max_abs_node = onnx.helper.make_node(
+            "Abs",
+            [reduce_max_node.output[0]],
+            [reduce_max_abs_name + ":0"],
+            reduce_max_abs_name,
+        )
         nodes_list.append(reduce_max_abs_node)
         #   Compute max of abs(rmin) and abs(rmax)
         abs_max_name = input_name + "_Abs_Max"
-        abs_max_node = onnx.helper.make_node("Max", [reduce_min_abs_node.output[0], reduce_max_abs_node.output[0]],
-                                             [abs_max_name + ":0"], abs_max_name)
+        abs_max_node = onnx.helper.make_node(
+            "Max",
+            [reduce_min_abs_node.output[0], reduce_max_abs_node.output[0]],
+            [abs_max_name + ":0"],
+            abs_max_name,
+        )
         nodes_list.append(abs_max_node)
         #   and divide by (quantize_range/2.0) which will be equal to max(...)*2.0/quantize_range
-        initializer_div = onnx.helper.make_tensor(self.fixed_qrange_int8_name, onnx_proto.TensorProto.FLOAT, [],
-                                                  [get_qrange_for_qType(qType) / 2.0])
+        initializer_div = onnx.helper.make_tensor(
+            self.fixed_qrange_int8_name,
+            onnx_proto.TensorProto.FLOAT,
+            [],
+            [get_qrange_for_qType(qType) / 2.0],
+        )
         self.model.add_initializer(initializer_div)
         scale_div_name = input_name + "scale_Div"
-        scale_div_node = onnx.helper.make_node("Div", [abs_max_node.output[0], self.fixed_qrange_int8_name],
-                                               [input_scale_name], scale_div_name)
+        scale_div_node = onnx.helper.make_node(
+            "Div",
+            [abs_max_node.output[0], self.fixed_qrange_int8_name],
+            [input_scale_name],
+            scale_div_name,
+        )
         nodes_list.append(scale_div_node)
 
         # Zero point
         initializer_zp = onnx.helper.make_tensor(self.fixed_zero_zp_name, qType, [], [0])
         self.model.add_initializer(initializer_zp)
 
         return input_scale_name, self.fixed_zero_zp_name, [], []
 
     def _get_dynamic_input_quantization_params_uint8(self, input_name, nodes_list):
-        '''
+        """
         Create nodes for dynamic quantization of input to uint8 and add them to nodes_list
             parameter input_name: Name of the input.
             parameter nodes_list: new nodes are appended to this list.
             return: scale_name, zero_point_name, scale_shape, zero_point_shape.
-        '''
-        qType = onnx_proto.TensorProto.UINT8
+        """
+        qType = onnx_proto.TensorProto.UINT8  # noqa: N806
         # Reduce min and Reduce max
         input_scale_name = input_name + "_scale"
         input_zp_name = input_name + "_zero_point"
 
         reduce_min_name = input_name + "_ReduceMin"
-        reduce_min_node = onnx.helper.make_node("ReduceMin", [input_name], [reduce_min_name + ":0"],
-                                                reduce_min_name,
-                                                keepdims=0)
+        reduce_min_node = onnx.helper.make_node(
+            "ReduceMin",
+            [input_name],
+            [reduce_min_name + ":0"],
+            reduce_min_name,
+            keepdims=0,
+        )
         nodes_list.append(reduce_min_node)
 
         reduce_max_name = input_name + "_ReduceMax"
-        reduce_max_node = onnx.helper.make_node("ReduceMax", [input_name], [reduce_max_name + ":0"],
-                                                reduce_max_name,
-                                                keepdims=0)
+        reduce_max_node = onnx.helper.make_node(
+            "ReduceMax",
+            [input_name],
+            [reduce_max_name + ":0"],
+            reduce_max_name,
+            keepdims=0,
+        )
         nodes_list.append(reduce_max_node)
 
         # Add tensors for quantize range and zero value.
-        initializer_qrange = onnx.helper.make_tensor(self.fixed_qrange_uint8_name, onnx_proto.TensorProto.FLOAT, [],
-                                                     [get_qrange_for_qType(qType)])
+        initializer_qrange = onnx.helper.make_tensor(
+            self.fixed_qrange_uint8_name,
+            onnx_proto.TensorProto.FLOAT,
+            [],
+            [get_qrange_for_qType(qType)],
+        )
         self.model.add_initializer(initializer_qrange)
         initializer_qvalue = onnx.helper.make_tensor(self.fixed_zero_name, onnx_proto.TensorProto.FLOAT, [], [0.0])
         self.model.add_initializer(initializer_qvalue)
 
         # Compute Scale
         #   Subtract rmax and rmin
         scale_sub_name = input_name + "_scale_Sub"
-        scale_sub_node = onnx.helper.make_node("Sub", [reduce_max_node.output[0], reduce_min_node.output[0]],
-                                               [scale_sub_name + ":0"], scale_sub_name)
+        scale_sub_node = onnx.helper.make_node(
+            "Sub",
+            [reduce_max_node.output[0], reduce_min_node.output[0]],
+            [scale_sub_name + ":0"],
+            scale_sub_name,
+        )
         nodes_list.append(scale_sub_node)
         #   and divide by quantize range
         scale_div_name = input_name + "_scale_Div"
-        scale_div_node = onnx.helper.make_node("Div", [scale_sub_node.output[0], self.fixed_qrange_uint8_name],
-                                               [input_scale_name], scale_div_name)
+        scale_div_node = onnx.helper.make_node(
+            "Div",
+            [scale_sub_node.output[0], self.fixed_qrange_uint8_name],
+            [input_scale_name],
+            scale_div_name,
+        )
         nodes_list.append(scale_div_node)
 
         # Compute zero point
         #   Subtract zero and rmin
         zp_sub_name = input_name + "_zero_point_Sub"
-        zp_sub_node = onnx.helper.make_node("Sub", [self.fixed_zero_name, reduce_min_node.output[0]],
-                                            [zp_sub_name + ":0"], zp_sub_name)
+        zp_sub_node = onnx.helper.make_node(
+            "Sub",
+            [self.fixed_zero_name, reduce_min_node.output[0]],
+            [zp_sub_name + ":0"],
+            zp_sub_name,
+        )
         nodes_list.append(zp_sub_node)
         #   Divide by scale
         zp_div_name = input_name + "_zero_point_Div"
-        zp_div_node = onnx.helper.make_node("Div", [zp_sub_node.output[0], input_scale_name], [zp_div_name + ":0"],
-                                            zp_div_name)
+        zp_div_node = onnx.helper.make_node(
+            "Div",
+            [zp_sub_node.output[0], input_scale_name],
+            [zp_div_name + ":0"],
+            zp_div_name,
+        )
         nodes_list.append(zp_div_node)
         #   Compute floor
         zp_floor_name = input_name + "_zero_point_Floor"
         zp_floor_node = onnx.helper.make_node("Floor", zp_div_node.output, [zp_floor_name + ":0"], zp_floor_name)
         nodes_list.append(zp_floor_node)
         #   Cast to integer
         zp_cast_name = input_name + "_zero_point_Cast"
         zp_cast_node = onnx.helper.make_node("Cast", zp_floor_node.output, [input_zp_name], zp_cast_name, to=qType)
         nodes_list.append(zp_cast_node)
 
         return input_scale_name, input_zp_name, [], []
 
     def _get_quantization_params(self, param_name, use_scale=None, use_zeropoint=None):
-        '''
+        """
         Create initializers and inputs in the graph for zero point and scale of output.
         Zero point and scale values are obtained from self.quantization_params if specified.
             parameter param_name: Name of the quantization parameter.
             return: result, scale_name, zero_point_name, scale_shape, zero_point_shape.
-        '''
+        """
         if use_scale is None or use_zeropoint is None:
             if self.quantization_params is None or param_name not in self.quantization_params:
-                logging.info("Quantization parameters for tensor:\"{}\" not specified".format(param_name))
+                logging.info(f'Quantization parameters for tensor:"{param_name}" not specified')
                 return False, "", "", "", ""
 
             params = self.quantization_params[param_name]
             if params is None or len(params) != 2:
-                raise ValueError("Quantization parameters should contain zero point and scale. "
-                                 "Specified values for output {}: {}".format(param_name, params))
+                raise ValueError(
+                    "Quantization parameters should contain zero point and scale. "
+                    "Specified values for output {}: {}".format(param_name, params)
+                )
 
             zero_point_values = [params[0]]
             scale_values = [params[1]]
         else:
             zero_point_values = [use_zeropoint]
             scale_values = [use_scale]
 
         zero_point_shape = []
         zero_point_name = param_name + "_zero_point"
-        zero_point_type = self.input_qType
+        zero_point_type = self.activation_qType
         scale_shape = []
         scale_name = param_name + "_scale"
 
         # Add initializers
         init_zp = onnx.helper.make_tensor(zero_point_name, zero_point_type, zero_point_shape, zero_point_values)
         self.model.add_initializer(init_zp)
         init_scale = onnx.helper.make_tensor(scale_name, onnx_proto.TensorProto.FLOAT, scale_shape, scale_values)
         self.model.add_initializer(init_scale)
 
         return True, scale_name, zero_point_name, scale_shape, zero_point_shape
 
     def _get_quantize_input_nodes(self, node, input_index, qType, given_scale_name=None, given_zp_name=None):
-        '''
+        """
         Given an input for a node (which is not a initializer), this function
 
         - add nodes to compute zero point and scale for this input if they don't exist.
         - add new QuantizeLinear node to quantize the input.
 
         :param node: node being quantized in NodeProto format.
         :param input_index: index of input in node.input.
         :param qType: type to quantize to.
         :param given_scale_name: if those inputs need to be quanitzed using this scale tensor.
         :param given_zp_name: if those inputs to be quantized using this zeropoint tensor.
         :return: List of newly created nodes in NodeProto format.
-        '''
+        """
         input_name = node.input[input_index]
-        output_name = input_name + "_quantized"
+        output_name = input_name + TENSOR_NAME_QUANT_SUFFIX
         ql_node_name = input_name + "_QuantizeLinear"
 
         if (given_scale_name is not None) and (given_zp_name is not None):
             data_found, scale_name, zp_name = (True, given_scale_name, given_zp_name)
         else:
             data_found, scale_name, zp_name, _, _ = self._get_quantization_params(input_name)
 
         nodes = []
-        if data_found == True:
-            qlinear_node = onnx.helper.make_node("QuantizeLinear", [input_name, scale_name, zp_name],
-                                                 [output_name], ql_node_name)
+        if data_found:
+            qlinear_node = onnx.helper.make_node(
+                "QuantizeLinear",
+                [input_name, scale_name, zp_name],
+                [output_name],
+                ql_node_name,
+            )
         else:
             if self.static:
                 return None
             # dynamic mode
             # Scale and Zero Points not available for this input. Add nodes to dynamically compute it
             if self.fuse_dynamic_quant and qType == onnx_proto.TensorProto.UINT8:
                 scale_name = input_name + "_scale"
                 zp_name = input_name + "_zero_point"
-                qlinear_node = onnx.helper.make_node("DynamicQuantizeLinear", [input_name],
-                                                     [output_name, scale_name, zp_name], ql_node_name)
+                qlinear_node = onnx.helper.make_node(
+                    "DynamicQuantizeLinear",
+                    [input_name],
+                    [output_name, scale_name, zp_name],
+                    ql_node_name,
+                )
             else:
-                scale_name, zp_name, scale_shape, zp_shape = \
-                    self._get_dynamic_input_quantization_params(input_name, nodes, qType)
-                qlinear_node = onnx.helper.make_node("QuantizeLinear", [input_name, scale_name, zp_name],
-                                                     [output_name], ql_node_name)
+                (
+                    scale_name,
+                    zp_name,
+                    scale_shape,
+                    zp_shape,
+                ) = self._get_dynamic_input_quantization_params(input_name, nodes, qType)
+                qlinear_node = onnx.helper.make_node(
+                    "QuantizeLinear",
+                    [input_name, scale_name, zp_name],
+                    [output_name],
+                    ql_node_name,
+                )
 
         self.quantized_value_map[input_name] = QuantizedValue(input_name, output_name, scale_name, zp_name, qType)
-        return nodes + [qlinear_node]
+        return [*nodes, qlinear_node]
+
+    def set_quant_scale_zp(self, tensor_name, value):
+        assert isinstance(value, tuple) and len(value) == 2, "value must be scale(float) and zeropoint"
+        assert tensor_name not in self.used_scale_zp_map, f"{tensor_name} has been setted before"
+        self.used_scale_zp_map[tensor_name] = value
+
+    def find_quant_scale_zp(self, input_name):
+        if input_name in self.used_scale_zp_map:
+            return self.used_scale_zp_map[input_name]
+        if self.parent is not None:
+            return self.parent.find_quantized_value(input_name)
+        return (None, None)
 
     def find_quantized_value(self, input_name):
         if input_name in self.quantized_value_map:
             return self.quantized_value_map[input_name]
         if self.parent is not None:
             return self.parent.find_quantized_value(input_name)
         return None
 
-    def quantize_bias_static(self, bias_name, input_name, weight_name, beta = 1.0):
-        '''
+    def quantize_bias_static(self, bias_name, input_name, weight_name, beta=1.0):
+        """
         Quantized the bias. Zero Point == 0 and Scale == Input_Scale * Weight_Scale
-        '''
+        """
 
-        # Handle case where bias already in quantizatio map
+        # Handle case where bias already in quantization map
         if bias_name in self.quantized_value_map:
             return self.quantized_value_map[bias_name].q_name
 
         # get scale for weight
         weight_scale_name = self.quantized_value_map[weight_name].scale_name
         weight_initializer = find_by_name(weight_scale_name, self.model.initializer())
-        weight_scale = self.tensor_proto_to_array(weight_initializer)
+        weight_scale = tensor_proto_to_array(weight_initializer)
 
         # get bias
         bias_initializer = find_by_name(bias_name, self.model.initializer())
-        bias_data = self.tensor_proto_to_array(bias_initializer)
-        quantized_bias_name = bias_name + "_quantized"
+        bias_data = tensor_proto_to_array(bias_initializer)
+        quantized_bias_name = bias_name + TENSOR_NAME_QUANT_SUFFIX
 
         # get scale for input
         if input_name in self.quantized_value_map:
             input_scale_name = self.quantized_value_map[input_name].scale_name
         elif input_name in self.quantization_params:
             _, input_scale_name, _, _, _ = self._get_quantization_params(input_name)
         else:
-            raise ValueError("Expected {} to be in quantized value map for static quantization".format(input_name))
+            raise ValueError(f"Expected {input_name} to be in quantized value map for static quantization")
 
         inputscale_initializer = find_by_name(input_scale_name, self.model.initializer())
-        input_scale = self.tensor_proto_to_array(inputscale_initializer)
+        input_scale = tensor_proto_to_array(inputscale_initializer)
 
         # calcuate scale for bias
         bias_scale = input_scale * weight_scale * beta
 
         # quantize bias
         quantized_data = (np.asarray(bias_data) / bias_scale).round().astype(np.int32)
 
@@ -602,50 +686,110 @@
         bias_np_data = np.asarray(quantized_data, dtype=np.int32).reshape(bias_initializer.dims)
         packed_bias_initializer = onnx.numpy_helper.from_array(bias_np_data, quantized_bias_name)
         self.model.initializer().extend([packed_bias_initializer])
 
         # update scale initializer
         quantized_bias_scale_name = quantized_bias_name + "_scale"
         bias_scale_data = np.asarray(bias_scale, dtype=np.float32).reshape(-1)
-        packed_bias_scale_initializer = onnx.numpy_helper.from_array(bias_scale_data, quantized_bias_scale_name)
+        if self.is_per_channel():
+            packed_bias_scale_initializer = onnx.numpy_helper.from_array(bias_scale_data, quantized_bias_scale_name)
+        else:
+            packed_bias_scale_initializer = onnx.helper.make_tensor(
+                quantized_bias_scale_name, onnx_proto.TensorProto.FLOAT, [], bias_scale_data
+            )
         self.model.initializer().extend([packed_bias_scale_initializer])
 
         # update zero initializer
         quantized_bias_zp_name = quantized_bias_name + "_zero_point"
         bias_zp_data = np.zeros(bias_scale.shape, dtype=np.int32).reshape(-1)
-        packed_bias_zp_initializer = onnx.numpy_helper.from_array(bias_zp_data, quantized_bias_zp_name)
+        if self.is_per_channel():
+            packed_bias_zp_initializer = onnx.numpy_helper.from_array(bias_zp_data, quantized_bias_zp_name)
+        else:
+            packed_bias_zp_initializer = onnx.helper.make_tensor(
+                quantized_bias_zp_name, onnx_proto.TensorProto.INT32, [], bias_zp_data
+            )
         self.model.initializer().extend([packed_bias_zp_initializer])
 
-        assert (bias_name not in self.quantized_value_map)
-        quantized_value = QuantizedValue(bias_name, quantized_bias_name, quantized_bias_scale_name,
-                                         quantized_bias_zp_name, QuantizedValueType.Initializer,
-                                         0 if bias_scale_data.size > 1 else None)
+        assert bias_name not in self.quantized_value_map
+        quantized_value = QuantizedValue(
+            bias_name,
+            quantized_bias_name,
+            quantized_bias_scale_name,
+            quantized_bias_zp_name,
+            QuantizedValueType.Initializer,
+            0 if bias_scale_data.size > 1 else None,
+        )
         self.quantized_value_map[bias_name] = quantized_value
 
         return quantized_bias_name
 
     def contains_tensor(self, tensor_name):
-        '''
-        only check for value info and newly generated tensor names, initializers are checked seperately
-        '''
-        return (tensor_name in self.value_infos) or (tensor_name in self.tensor_names) or (tensor_name in self.generated_value_names)
-
-    def quantize_inputs(self, node, indices, initializer_use_weight_qType=True, reduce_range=False, op_level_per_channel=False, axis=-1, from_subgraph=False):
-        '''
+        """
+        only check for value info and newly generated tensor names, initializers are checked separately
+        """
+        return (
+            (tensor_name in self.value_infos)
+            or (tensor_name in self.tensor_names)
+            or (tensor_name in self.generated_value_names)
+        )
+
+    def quantize_activation(self, node, indices, from_subgraph=False):
+        return self.__quantize_inputs(
+            node=node,
+            indices=indices,
+            initializer_use_weight_qType=False,
+            reduce_range=False,
+            op_level_per_channel=False,
+            axis=-1,
+            from_subgraph=from_subgraph,
+        )
+
+    # In some circumstances a weight is not an initializer, for example of MatMul, if both A and B are not
+    # initializer, B can still be considered as Weight
+    def quantize_weight(
+        self,
+        node,
+        indices,
+        reduce_range=False,
+        op_level_per_channel=False,
+        axis=-1,
+        from_subgraph=False,
+    ):
+        return self.__quantize_inputs(
+            node=node,
+            indices=indices,
+            initializer_use_weight_qType=True,
+            reduce_range=reduce_range,
+            op_level_per_channel=op_level_per_channel,
+            axis=axis,
+            from_subgraph=from_subgraph,
+        )
+
+    def __quantize_inputs(
+        self,
+        node,
+        indices,
+        initializer_use_weight_qType=True,
+        reduce_range=False,
+        op_level_per_channel=False,
+        axis=-1,
+        from_subgraph=False,
+    ):
+        """
         Given a node, this function quantizes the inputs as follows:
             - If input is an initializer, quantize the initializer data, replace old initializer
               with new initializer
             - Else, add QuantizeLinear nodes to perform quantization
             parameter node: node being quantized in NodeProto format.
             parameter indices: input indices to quantize.
             return: (List of quantized input names,
                      List of zero point names used for input quantization,
                      List of scale names used for input quantization,
                      List of new QuantizeLinear nodes created)
-        '''
+        """
 
         scale_names = []
         zero_point_names = []
         quantized_input_names = []
         nodes = []
 
         for input_index in indices:
@@ -654,36 +798,51 @@
             # Find if this input is already quantized
             if node_input in self.quantized_value_map:
                 quantized_value = self.quantized_value_map[node_input]
                 scale_names.append(quantized_value.scale_name)
                 zero_point_names.append(quantized_value.zp_name)
                 quantized_input_names.append(quantized_value.q_name)
                 continue
-
+            # adding this for case embed_layernorm.py has optional segment_embedding
+            if not node_input:
+                quantized_input_names.append("")
+                scale_names.append("")
+                zero_point_names.append("")
+                continue
             # Quantize the input
             initializer = find_by_name(node_input, self.model.initializer())
             if initializer is not None:
                 if self.per_channel and op_level_per_channel:
-                    q_weight_name, zp_name, scale_name = self.quantize_weight_per_channel(
-                        initializer.name, self.weight_qType if initializer_use_weight_qType else self.input_qType,
-                        axis, reduce_range)
+                    (
+                        q_weight_name,
+                        zp_name,
+                        scale_name,
+                    ) = self.quantize_weight_per_channel(
+                        initializer.name,
+                        self.weight_qType if initializer_use_weight_qType else self.activation_qType,
+                        axis,
+                        reduce_range,
+                    )
                 else:
-                    q_weight_name, zp_name, scale_name = self.quantize_weight(
-                        initializer, self.weight_qType if initializer_use_weight_qType else self.input_qType,
-                        reduce_range)
+                    q_weight_name, zp_name, scale_name = self.quantize_initializer(
+                        initializer,
+                        self.weight_qType if initializer_use_weight_qType else self.activation_qType,
+                        reduce_range,
+                    )
 
                 quantized_input_names.append(q_weight_name)
                 zero_point_names.append(zp_name)
                 scale_names.append(scale_name)
             elif self.contains_tensor(node_input):
                 # Add QuantizeLinear node.
-                qlinear_node = self.model.find_node_by_name(node_input + "_QuantizeLinear", self.new_nodes,
-                                                            self.model.graph())
+                qlinear_node = self.model.find_node_by_name(
+                    node_input + "_QuantizeLinear", self.new_nodes, self.model.graph()
+                )
                 if qlinear_node is None:
-                    quantize_input_nodes = self._get_quantize_input_nodes(node, input_index, self.input_qType)
+                    quantize_input_nodes = self._get_quantize_input_nodes(node, input_index, self.activation_qType)
                     if quantize_input_nodes is None:
                         return (None, None, None, None)
                     if from_subgraph:
                         self.add_new_nodes(quantize_input_nodes)
                     else:
                         nodes.extend(quantize_input_nodes)
                     qlinear_node = quantize_input_nodes[-1]
@@ -693,258 +852,232 @@
                     scale_names.append(qlinear_node.input[1])
                     zero_point_names.append(qlinear_node.input[2])
                 else:
                     quantized_input_names.append(qlinear_node.output[0])
                     scale_names.append(qlinear_node.output[1])
                     zero_point_names.append(qlinear_node.output[2])
             elif self.parent is not None:
-                (parent_quantized_input_names, parent_zero_point_names, parent_scale_names, _) = self.parent.quantize_inputs(
+                (
+                    parent_quantized_input_names,
+                    parent_zero_point_names,
+                    parent_scale_names,
+                    _,
+                ) = self.parent.__quantize_inputs(
                     node,
                     [input_index],
                     initializer_use_weight_qType=initializer_use_weight_qType,
                     reduce_range=reduce_range,
                     op_level_per_channel=op_level_per_channel,
                     axis=axis,
-                    from_subgraph=True)
+                    from_subgraph=True,
+                )
                 quantized_input_names.append(parent_quantized_input_names[0])
                 scale_names.append(parent_scale_names[0])
                 zero_point_names.append(parent_zero_point_names[0])
                 # node should not be add this child level here
             else:
-                raise ValueError('Invalid tensor name to quantize: {} @graph scope{}'.format(node_input, self.graph_scope))
+                raise ValueError(f"Invalid tensor name to quantize: {node_input} @graph scope{self.graph_scope}")
 
-        return (quantized_input_names, zero_point_names, scale_names, nodes)
+        return quantized_input_names, zero_point_names, scale_names, nodes
 
-    def quantize_weight(self, weight, qType, reduce_range=False, keep_float_weight=False):
-        '''
-            :param weight: TensorProto initializer
-            :param qType: type to quantize to
-            :param keep_float_weight: Whether to quantize the weight. In some cases, we only want to qunatize scale and zero point.
-                                      If keep_float_weight is False, quantize the weight, or don't quantize the weight.
-            :return: quantized weight name, zero point name, scale name
-        '''
+    def quantize_initializer(self, weight, qType, reduce_range=False, keep_float_weight=False):
+        """
+        :param weight: TensorProto initializer
+        :param qType: type to quantize to
+        :param keep_float_weight: Whether to quantize the weight. In some cases, we only want to qunatize scale and zero point.
+                                  If keep_float_weight is False, quantize the weight, or don't quantize the weight.
+        :return: quantized weight name, zero point name, scale name
+        """
         # Find if this input is already quantized
         if weight.name in self.quantized_value_map:
             quantized_value = self.quantized_value_map[weight.name]
-            return (quantized_value.q_name, quantized_value.zp_name, quantized_value.scale_name)
+            return (
+                quantized_value.q_name,
+                quantized_value.zp_name,
+                quantized_value.scale_name,
+            )
 
-        q_weight_name = weight.name + "_quantized"
+        q_weight_name = weight.name + TENSOR_NAME_QUANT_SUFFIX
         zp_name = weight.name + "_zero_point"
         scale_name = weight.name + "_scale"
 
         # Update packed weight, zero point, and scale initializers
-        weight_data = self.tensor_proto_to_array(weight)
-        _, _, zero_point, scale, q_weight_data = quantize_data(weight_data.flatten().tolist(),
-                                                               qType, self.is_weight_symmetric,
-                                                               self.reduce_range and reduce_range)
+        weight_data = tensor_proto_to_array(weight)
+        _, _, zero_point, scale, q_weight_data = quantize_data(
+            weight_data.flatten().tolist(),
+            qType,
+            self.is_weight_symmetric,
+            self.reduce_range and reduce_range,
+        )
         scale_initializer = onnx.helper.make_tensor(scale_name, onnx_proto.TensorProto.FLOAT, [], [scale])
         zero_initializer = onnx.helper.make_tensor(zp_name, qType, [], [zero_point])
         self.model.initializer().extend([scale_initializer, zero_initializer])
 
         if not keep_float_weight:
-            q_weight_data = np.asarray(q_weight_data,
-                                       dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[qType]).reshape(weight.dims)
+            q_weight_data = np.asarray(q_weight_data, dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[qType]).reshape(
+                weight.dims
+            )
             q_weight_initializer = onnx.numpy_helper.from_array(q_weight_data, q_weight_name)
             self.model.initializer().extend([q_weight_initializer])
 
         # Log entry for this quantized weight
-        quantized_value = QuantizedValue(weight.name, q_weight_name, scale_name, zp_name,
-                                         QuantizedValueType.Initializer, None)
+        quantized_value = QuantizedValue(
+            weight.name,
+            q_weight_name,
+            scale_name,
+            zp_name,
+            QuantizedValueType.Initializer,
+            None,
+        )
         self.quantized_value_map[weight.name] = quantized_value
 
         return q_weight_name, zp_name, scale_name
 
-    def quantize_weight_per_channel(self, weight_name, weight_qType, channel_axis, reduce_range=True,
-                                    keep_float_weight=False):
+    def quantize_weight_per_channel(
+        self,
+        weight_name,
+        weight_qType,
+        channel_axis,
+        reduce_range=True,
+        keep_float_weight=False,
+    ):
         # Find if this input is already quantized
         if weight_name in self.quantized_value_map:
             quantized_value = self.quantized_value_map[weight_name]
-            return (quantized_value.q_name, quantized_value.zp_name, quantized_value.scale_name)
+            return (
+                quantized_value.q_name,
+                quantized_value.zp_name,
+                quantized_value.scale_name,
+            )
 
         initializer = find_by_name(weight_name, self.model.initializer())
         if initializer is None:
             raise ValueError("{} is not an initializer", weight_name)
 
-        weights = self.tensor_proto_to_array(initializer)
+        weights = tensor_proto_to_array(initializer)
         channel_count = weights.shape[channel_axis]
         rmin_list = []
         rmax_list = []
         zero_point_list = []
         scale_list = []
         quantized_per_channel_data_list = []
         for i in range(channel_count):
             per_channel_data = weights.take(i, channel_axis)
             rmin, rmax, zero_point, scale, quantized_per_channel_data = quantize_data(
-                per_channel_data.flatten().tolist(), weight_qType,
-                self.is_weight_symmetric or weight_qType == onnx_proto.TensorProto.INT8, self.reduce_range and reduce_range)
+                per_channel_data.flatten().tolist(),
+                weight_qType,
+                self.is_weight_symmetric or weight_qType == onnx_proto.TensorProto.INT8,
+                self.reduce_range and reduce_range,
+            )
             rmin_list.append(rmin)
             rmax_list.append(rmax)
             zero_point_list.append(zero_point)
             scale_list.append(scale)
             quantized_per_channel_data_list.append(quantized_per_channel_data)
 
         # combine per_channel_data into one
         reshape_dims = list(weights.shape)  # deep copy
         reshape_dims[channel_axis] = 1  # only one per channel for reshape
         quantized_weights = np.asarray(quantized_per_channel_data_list[0]).reshape(reshape_dims)
         for i in range(1, len(quantized_per_channel_data_list)):
             channel_weights = np.asarray(quantized_per_channel_data_list[i]).reshape(reshape_dims)
             quantized_weights = np.concatenate((quantized_weights, channel_weights), channel_axis)
 
-        q_weight_name = weight_name + "_quantized"
+        q_weight_name = weight_name + TENSOR_NAME_QUANT_SUFFIX
         zp_name = weight_name + "_zero_point"
         scale_name = weight_name + "_scale"
 
-        quantized_value = QuantizedValue(weight_name, q_weight_name, scale_name, zp_name,
-                                         QuantizedValueType.Initializer, None)
+        quantized_value = QuantizedValue(
+            weight_name,
+            q_weight_name,
+            scale_name,
+            zp_name,
+            QuantizedValueType.Initializer,
+            None,
+        )
         self.quantized_value_map[weight_name] = quantized_value
 
         # Update packed weight, zero point, and scale initializers
         zero_scale_shape = [initializer.dims[channel_axis]]
-        scale_initializer = onnx.helper.make_tensor(scale_name, onnx_proto.TensorProto.FLOAT, zero_scale_shape,
-                                                    scale_list)
+        scale_initializer = onnx.helper.make_tensor(
+            scale_name, onnx_proto.TensorProto.FLOAT, zero_scale_shape, scale_list
+        )
         zero_initializer = onnx.helper.make_tensor(zp_name, weight_qType, zero_scale_shape, zero_point_list)
 
         self.model.initializer().extend([scale_initializer, zero_initializer])
 
         if not keep_float_weight:
             quantized_weights = np.asarray(
-                quantized_weights, dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[weight_qType]).reshape(initializer.dims)
+                quantized_weights,
+                dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[weight_qType],
+            ).reshape(initializer.dims)
             q_weight_initializer = onnx.numpy_helper.from_array(quantized_weights, q_weight_name)
             self.model.initializer().extend([q_weight_initializer])
 
-        return (q_weight_name, zp_name, scale_name)
+        return q_weight_name, zp_name, scale_name
 
     def _dequantize_value(self, value_name):
-        '''
+        """
         Given a value (input/output) which is quantized, add a DequantizeLinear node to dequantize
         it back to float32
             parameter value_name: value to dequantize
             parameter new_nodes_list: List of new nodes created before processing current node
             return: None if there is already a DequantizeLinear node that dequantizes it
                     A DequantizeLinear node otherwise
-        '''
+        """
         if (value_name in self.quantized_value_map) and (value_name not in self.generated_value_names):
             quantized_value = self.quantized_value_map[value_name]
             # Add DequantizeLinear Node for this input
             dqlinear_name = value_name + "_DequantizeLinear"
             dqlinear_node = self.model.find_node_by_name(dqlinear_name, self.new_nodes, self.model.graph())
             if dqlinear_node is None:
-                dqlinear_inputs = [quantized_value.q_name, quantized_value.scale_name, quantized_value.zp_name]
-                dequantize_node = onnx.helper.make_node("DequantizeLinear", dqlinear_inputs, [value_name],
-                                                        dqlinear_name)
+                dqlinear_inputs = [
+                    quantized_value.q_name,
+                    quantized_value.scale_name,
+                    quantized_value.zp_name,
+                ]
+                dequantize_node = onnx.helper.make_node(
+                    "DequantizeLinear", dqlinear_inputs, [value_name], dqlinear_name
+                )
                 return dequantize_node
             else:
                 # DQ op is already present, assert it's output matches the input of current node
-                assert (value_name == dqlinear_node.output[0])
+                assert value_name == dqlinear_node.output[0]
         return None
 
     def _dequantize_outputs(self):
-        '''
+        """
         Dequantize output if it is quantized
             parameter new_nodes_list: List of new nodes created before processing current node
             return: List of new nodes created
-        '''
+        """
 
         for output in self.model.graph().output:
             dequantize_node = self._dequantize_value(output.name)
             if dequantize_node is not None:
                 self.new_nodes.append(dequantize_node)
 
     def calculate_quantization_params(self):
         if self.tensors_range is None:
             return
 
         # adjust tensor_ranges for input of Clip and Relu node
         for node in self.model.nodes():
-            if node.op_type not in ['Clip', 'Relu']:
+            if node.op_type not in ["Clip", "Relu"]:
+                continue
+            if self.is_activation_symmetric:
                 continue
-            if not self.should_quantize(node):
+            if not self.should_quantize_node(node):
                 continue
             if len(self.model.input_name_to_nodes()[node.input[0]]) != 1:
                 continue
             if node.input[0] not in self.tensors_range.keys() or node.output[0] not in self.tensors_range.keys():
                 continue
             self.tensors_range[node.input[0]] = self.tensors_range[node.output[0]]
-
         quantization_params = {}
-        for tensor_name in self.tensors_range.keys():
+        for tensor_name in self.tensors_range:
             rmin, rmax = self.tensors_range[tensor_name]
-            qmin, qmax = get_qmin_qmax_for_qType(self.input_qType, symmetric=self.is_activation_symmetric)
+            qmin, qmax = get_qmin_qmax_for_qType(self.activation_qType, symmetric=self.is_activation_symmetric)
 
-            quantization_params[tensor_name] = compute_scale_zp(rmin, rmax,
-                                                                qmin, qmax,
-                                                                self.is_activation_symmetric)
+            quantization_params[tensor_name] = compute_scale_zp(rmin, rmax, qmin, qmax, self.is_activation_symmetric)
 
         return quantization_params
-
-
-    # static method
-    def CleanGraphInitializers(graph, model):
-        '''
-        Clean unused initializers including which is caused by quantizing the model.
-            return cleaned graph, and list of tensor names from this graph and all its subgraphes
-            that can not be found in this graph and its subgraphes
-        '''
-        requesting_tensor_names = {}
-        requesting_tensor_names.update({input_name: 1 for node in graph.node for input_name in node.input if input_name})
-        requesting_tensor_names.update({g_out.name: 1 for g_out in graph.output if g_out.name})
-
-        new_nodes = []
-        for node in graph.node:
-            node_2_add = node
-            graph_attrs = [attr for attr in node.attribute if attr.type == onnx.AttributeProto.GRAPH or attr.type == onnx.AttributeProto.GRAPHS]
-            if len(graph_attrs) > 0:
-                kwargs = {}
-                for attr in node.attribute:
-                    kv = {}
-                    if attr.type == onnx.AttributeProto.GRAPH:
-                        cleaned_sub_graph, sub_requesting_tensor_names = ONNXQuantizer.CleanGraphInitializers(attr.g, model)
-                        kv = {attr.name: cleaned_sub_graph}
-                        requesting_tensor_names.update({gn: 1 for gn in sub_requesting_tensor_names})
-                    elif attr.type == onnx.AttributeProto.GRAPHS:
-                        cleaned_graphes = []
-                        for subgraph in attr.graphs:
-                            cleaned_sub_graph, sub_requesting_tensor_names = ONNXQuantizer.CleanGraphInitializers(subgraph, model)
-                            cleaned_graphes.extend([cleaned_sub_graph])
-                            requesting_tensor_names.update({gn: 1 for gn in sub_requesting_tensor_names})
-                        kv = {attr.name: cleaned_graphes}
-                    else:
-                        kv = attribute_to_kwarg(attr)
-                    kwargs.update(kv)
-                node_2_add = onnx.helper.make_node(node.op_type, node.input, node.output, name=node.name, **kwargs)
-            new_nodes.extend([node_2_add])
-
-        graph.ClearField('node')
-        graph.node.extend(new_nodes)
-
-        generated_names = {}
-        generated_names.update({output_name: 1 for node in graph.node for output_name in node.output if output_name})
-        for gn in generated_names:
-            requesting_tensor_names.pop(gn, None)
-
-        name_to_input = {}
-        for input in graph.input:
-            name_to_input[input.name] = input
-
-        unused_ini_tensors = []
-        for ini_tensor in graph.initializer:
-            if ini_tensor.name in requesting_tensor_names:
-                requesting_tensor_names.pop(ini_tensor.name, None)
-            else:
-                # mark it to remove, remove here directly will cause mis-behavier
-                unused_ini_tensors.append(ini_tensor)
-
-        for ini_tensor in unused_ini_tensors:
-            graph.initializer.remove(ini_tensor)
-            if ini_tensor.name in name_to_input:
-                try:
-                    graph.input.remove(name_to_input[ini_tensor.name])
-                except StopIteration:
-                    if model.ir_version < 4:
-                        print("Warning: invalid weight name {} found in the graph (not a graph input)".format(ini_tensor.name))
-
-        for input in graph.input:
-            if input.name in requesting_tensor_names:
-                requesting_tensor_names.pop(input.name, None)
-
-        return graph, requesting_tensor_names
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/qdq_quantizer.py` & `onnxruntime/quantization/qdq_quantizer.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,276 +1,436 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
-import os
-import struct
-from pathlib import Path
-import numpy as np
 import logging
+from enum import Enum
 
 import onnx
 import onnx.numpy_helper
-from onnx import onnx_pb as onnx_proto
 from onnx import TensorProto
-from onnxruntime import SessionOptions, InferenceSession, GraphOptimizationLevel
-
-from .quant_utils import QuantizationMode, QuantizedValueType, QuantizedInitializer, QuantizedValue
-from .quant_utils import find_by_name, get_elem_index, get_mul_node, generate_identified_filename, attribute_to_kwarg, type_to_name, quantize_nparray
-from .quant_utils import QuantType, onnx_domain, __producer__, __version__
+from onnx import onnx_pb as onnx_proto
 
+from .onnx_quantizer import ONNXQuantizer
+from .quant_utils import (
+    DEQUANT_OP_NAME,
+    QUANT_OP_NAME,
+    QuantizedValue,
+    QuantizedValueType,
+    __producer__,
+    __version__,
+    add_dequant_output_suffix,
+    add_dequant_suffix,
+    add_quant_input_suffix,
+    add_quant_output_suffix,
+    add_quant_suffix,
+    find_by_name,
+)
 from .registry import CreateQDQQuantizer
 
-from .onnx_model import ONNXModel
-from .onnx_quantizer import ONNXQuantizer
+
+class QDQQuantTensorType(Enum):
+    ACTIVATION = 0
+    WEIGHT = 1
+    BIAS = 2
+
+
+class QDQTensorQuantInfo:
+    def __init__(self, tensor_type=QDQQuantTensorType.ACTIVATION, quant_para_provider=None, axis=None):
+        self.tensor_type = tensor_type
+        self.quant_para_provider = quant_para_provider
+        self.axis = axis
+        self.is_shared = quant_para_provider is not None
 
 
 class QDQQuantizer(ONNXQuantizer):
-    def __init__(self, model, per_channel, reduce_range, mode, static, weight_qType, input_qType, tensors_range,
-                 nodes_to_quantize, nodes_to_exclude, op_types_to_quantize, extra_options={}):
-        ONNXQuantizer.__init__(self, model, per_channel, reduce_range, mode, static, weight_qType, input_qType,
-                               tensors_range, nodes_to_quantize, nodes_to_exclude, op_types_to_quantize, extra_options)
-        self.tensors_to_quantize = []
-        self.tensors_to_quantize_per_channel = []
+    def __init__(
+        self,
+        model,
+        per_channel,
+        reduce_range,
+        mode,
+        static,
+        weight_qType,
+        activation_qType,
+        tensors_range,
+        nodes_to_quantize,
+        nodes_to_exclude,
+        op_types_to_quantize,
+        extra_options=None,
+    ):
+        ONNXQuantizer.__init__(
+            self,
+            model,
+            per_channel,
+            reduce_range,
+            mode,
+            static,
+            weight_qType,
+            activation_qType,
+            tensors_range,
+            nodes_to_quantize,
+            nodes_to_exclude,
+            op_types_to_quantize,
+            extra_options,
+        )
+        self.tensors_to_quantize = {}
         self.bias_to_quantize = []
+
         self.nodes_to_remove = []
 
         # Specific op types to exclude qdq quantization for their outputs.
         # In TRT, it's not recommended to quantize outputs for weighted ops such as Conv, Matmul, Gemm
         # because those ops may be followed by nodes that require high resolution inputs.
         # Adding QDQ for those ops' output may end up with worse accuracy.
         # So, we don't recommend to add QDQ to node's output under such condition.
-        self.op_types_to_exclude_output_quantization = [] if 'OpTypesToExcludeOutputQuantizatioin' not in extra_options \
-                                                        else extra_options['OpTypesToExcludeOutputQuantizatioin']
+        self.op_types_to_exclude_output_quantization = (
+            []
+            if "OpTypesToExcludeOutputQuantization" not in extra_options
+            else extra_options["OpTypesToExcludeOutputQuantization"]
+        )
 
         # We do quantization on Dequantizelinear's input to remove Quantizelinear for weight as an optimization.
         # In some cases, for example QDQ BERT model for TensorRT, QDQ should always appear as a pair.
         # Therefore, we need to disable this optimization and add qdq pair to weight.
-        self.add_qdq_pair_to_weight = False if 'AddQDQPairToWeight' not in extra_options \
-                                        else extra_options['AddQDQPairToWeight'] 
-
-        # The default behavior is that multiple nodes can share a QDQ pair as their inputs. 
-        # In TRT, QDQ pair can’t be shared between nodes, so it will create dedicated QDQ pairs for each node. 
-        self.dedicated_qdq_pair = False if 'DedicatedQDQPair' not in extra_options else extra_options['DedicatedQDQPair'] 
+        self.add_qdq_pair_to_weight = (
+            False if "AddQDQPairToWeight" not in extra_options else extra_options["AddQDQPairToWeight"]
+        )
+
+        # Some scenarios do not need the bias quantized. For example, in the case of Quantization Aware Training,
+        # quantizing the bias is not needed. This is because in QAT, all model parameters are expected to be in
+        # floating point format. To that end, we can use the FakeQuant operator for weights and activations that
+        # can always have QDQ pairs (by using AddQDQPairToWeight). But for biases in a quantized model, we can't use
+        # FakeQuant because it only ever appears before a DQ (since it is quantized as int32).
+        self.quantize_bias = True if "QuantizeBias" not in extra_options else extra_options["QuantizeBias"]
+
+        # The default behavior is that multiple nodes can share a QDQ pair as their inputs.
+        # In TRT, QDQ pair can`t be shared between nodes, so it will create dedicated QDQ pairs for each node.
+        self.dedicated_qdq_pair = (
+            False if "DedicatedQDQPair" not in extra_options else extra_options["DedicatedQDQPair"]
+        )
         if self.dedicated_qdq_pair:
             self.tensor_to_its_receiving_nodes = {}
 
         # Let user set channel axis for specific op type and it's effective only when per channel quantization is supported and per_channel is True.
-        self.qdq_op_type_per_channel_support_to_axis = {} if 'QDQOpTypePerChannelSupportToAxis' not in extra_options else extra_options['QDQOpTypePerChannelSupportToAxis']
-
-    def quantize_tensor(self, tensor_name):
+        self.qdq_op_type_per_channel_support_to_axis = (
+            {}
+            if "QDQOpTypePerChannelSupportToAxis" not in extra_options
+            else extra_options["QDQOpTypePerChannelSupportToAxis"]
+        )
+
+    def _is_tensor_quantizable(self, tensor_name):
+        """
+        Check if tensor can be quantized
+        """
         weight = find_by_name(tensor_name, self.model.initializer())
         if weight is not None:
             if weight.data_type == onnx_proto.TensorProto.FLOAT:
-                self.tensors_to_quantize.append(tensor_name)
-        elif tensor_name in self.value_infos.keys():
+                return True
+        elif tensor_name in self.value_infos:
             vi = self.value_infos[tensor_name]
-            if vi.type.HasField('tensor_type') and vi.type.tensor_type.elem_type == TensorProto.FLOAT:
-                self.tensors_to_quantize.append(tensor_name)
+            if vi.type.HasField("tensor_type") and vi.type.tensor_type.elem_type == TensorProto.FLOAT:
+                return True
         else:
             logging.warning(
                 "failed to infer the type of tensor: {}. Skip to quantize it. Please check if it is expected.".format(
-                    tensor_name))
+                    tensor_name
+                )
+            )
 
-    def quantize_tensor_per_channel(self, tensor_name, axis):
+        return False
+
+    def __quantize_tensor(self, tensor_name, quant_sharing_param=None, tensor_type=QDQQuantTensorType.ACTIVATION):
+        """
+        Quantize tensors. If quant_param_tensor is not None, tensor with name tensor_name will be quantized with same
+        quantization parameters as tensor quant_param_tensor
+
+        Args:
+            tensor_name: name of the tensor to quantize
+            quant_sharing_param: name of the tensor that provides quantization parameter
+            tensor_type: QDQQuantTensorType default ACTIVATION
+        """
+        if self._is_tensor_quantizable(tensor_name):
+            if quant_sharing_param:
+                self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(
+                    tensor_type=tensor_type, quant_para_provider=quant_sharing_param
+                )
+            elif tensor_name not in self.tensors_to_quantize:
+                self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(tensor_type=tensor_type)
+
+    def quantize_activation_tensor(self, tensor_name, quant_sharing_param=None):
+        """
+        Quantize Activation Tensor
+        Args:
+            tensor_name: name of the tensor to quantize
+            quant_sharing_param: name of the tensor that provides quantization parameter
+
+        """
+        return self.__quantize_tensor(tensor_name, quant_sharing_param, QDQQuantTensorType.ACTIVATION)
+
+    def quantize_weight_tensor(self, tensor_name, quant_sharing_param=None):
+        """
+        Quantize Weight Tensor
+        Args:
+            tensor_name: name of the tensor to quantize
+            quant_sharing_param: name of the tensor that provides quantization parameter
+
+        """
+        return self.__quantize_tensor(tensor_name, quant_sharing_param, QDQQuantTensorType.WEIGHT)
+
+    def quantize_weight_tensor_per_channel(self, tensor_name, axis):
         weight = find_by_name(tensor_name, self.model.initializer())
-        if weight is not None:
+        if weight:
             if weight.data_type == onnx_proto.TensorProto.FLOAT:
-                self.tensors_to_quantize_per_channel.append((tensor_name, axis))
+                self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(
+                    tensor_type=QDQQuantTensorType.WEIGHT, axis=axis
+                )
         else:
-            logging.warning(
-                "only support per-channel quantization on weight. Quantize tensor: {} with per-tensor instead.".format(
-                    tensor_name))
-            self.quantize_tensor(tensor_name)
+            logging.warning(f"only support per-channel quantization on weight. Tensor: {tensor_name} is not quantized.")
 
-    def quantize_bias_tensor(self, bias_name, input_name, weight_name, beta = 1.0):
+    def quantize_bias_tensor(self, bias_name, input_name, weight_name, beta=1.0):
         weight = find_by_name(bias_name, self.model.initializer())
         if weight is not None:
             if weight.data_type == onnx_proto.TensorProto.FLOAT:
                 self.bias_to_quantize.append((bias_name, input_name, weight_name, beta))
         else:
-            logging.warning("Expected {} to be a weight".format(bias_name))
+            logging.warning(f"Expected {bias_name} to be a weight")
 
     def remove_node(self, node):
         self.nodes_to_remove.append(node)
 
     def remove_nodes(self):
         self.model.remove_nodes(self.nodes_to_remove)
 
     def quantize_model(self):
-        if self.dedicated_qdq_pair:
-            for node in self.model.nodes():
-                if self.should_quantize(node):
+        for node in self.model.nodes():
+            if self.should_quantize_node(node):
+                op_quantizer = CreateQDQQuantizer(self, node)
+                op_quantizer.quantize()
+
+                if self.dedicated_qdq_pair:
                     for tensor_name in node.input:
                         if tensor_name not in self.tensor_to_its_receiving_nodes:
                             self.tensor_to_its_receiving_nodes[tensor_name] = []
                         self.tensor_to_its_receiving_nodes[tensor_name].append(node)
 
-        for node in self.model.nodes():
-            if self.should_quantize(node):
-                op_quantizer = CreateQDQQuantizer(self, node)
-                op_quantizer.quantize()
-
-        self.quantize_tensors()
-        self.quantize_weights_per_channel()
-        self.quantize_bias_tensors()
+        self._quantize_normal_tensors()
+        self._quantize_sharing_param_tensors()
+        if self.quantize_bias:
+            self._quantize_bias_tensors()
         self.remove_nodes()
         if not self.add_qdq_pair_to_weight:
-            ONNXQuantizer.CleanGraphInitializers(self.model.graph(), self.model.model)
+            self.model.clean_initializers()
 
         self.model.model.producer_name = __producer__
         self.model.model.producer_version = __version__
 
         return self.model.model
 
     def try_replacing_upstream_output(self, upstream_output_name, output_name):
-        if output_name in self.quantization_params.keys() and \
-           len(self.model.input_name_to_nodes()[upstream_output_name]) == 1 and \
-           not self.model.is_graph_output(upstream_output_name):
+        if (
+            output_name in self.quantization_params
+            and len(self.model.input_name_to_nodes()[upstream_output_name]) == 1
+            and not self.model.is_graph_output(upstream_output_name)
+            and not self.model.is_graph_input(upstream_output_name)
+        ):
             self.model.replace_output_of_all_nodes(upstream_output_name, output_name)
-            self.tensors_to_quantize.remove(upstream_output_name)
+            if upstream_output_name in self.tensors_to_quantize:
+                del self.tensors_to_quantize[upstream_output_name]
             return True
         return False
 
-    def quantize_tensors(self):
-        for tensor_name in self.tensors_to_quantize:
-            if tensor_name in self.quantized_value_map.keys():
-                continue
-            # Quantize the input
-            initializer = find_by_name(tensor_name, self.model.initializer())
-            if initializer is not None:
-
-                if self.add_qdq_pair_to_weight:
-                    q_weight_name, zp_name, scale_name = self.quantize_weight(initializer,
-                                                                              self.weight_qType,
-                                                                              keep_float_weight=True)
-                    qlinear_node = onnx.helper.make_node("QuantizeLinear", [tensor_name, scale_name, zp_name],
-                                                         [tensor_name + "_QuantizeLinear"],
-                                                         tensor_name + "_QuantizeLinear")
-                    dequant_node = onnx.helper.make_node("DequantizeLinear",
-                                                         [tensor_name + "_QuantizeLinear", scale_name, zp_name],
-                                                         [tensor_name + "_DequantizeLinear"],
-                                                         tensor_name + "_DequantizeLinear")
-                    self.model.replace_input_of_all_nodes(tensor_name, tensor_name + "_DequantizeLinear")
+    def _create_qdq_nodes(
+        self, q_input, q_output, quant_node_name, dq_input, dq_output, dequant_node_name, scale_name, zp_name, axis=None
+    ):
+        qlinear_node = onnx.helper.make_node(
+            QUANT_OP_NAME,
+            [q_input, scale_name, zp_name],
+            [q_output],
+            quant_node_name,
+            axis=axis,
+        )
+        dequant_node = onnx.helper.make_node(
+            DEQUANT_OP_NAME,
+            [dq_input, scale_name, zp_name],
+            [dq_output],
+            dequant_node_name,
+            axis=axis,
+        )
+        self.model.add_nodes([qlinear_node, dequant_node])
+
+    def _add_qdq_pair_for_initializer(self, weight_proto, tensor_type, axis=None):
+        weight_name = weight_proto.name
+        if axis is not None:
+            if self.opset_version < 13:
+                raise ValueError("Per-Channel support with QDQ format requires onnx opset version 13 or above.")
+            q_weight_name, zp_name, scale_name = self.quantize_weight_per_channel(
+                weight_name, onnx_proto.TensorProto.INT8, axis, keep_float_weight=self.add_qdq_pair_to_weight
+            )
+        else:
+            q_weight_name, zp_name, scale_name = self.quantize_initializer(
+                weight_proto,
+                self.weight_qType if tensor_type is QDQQuantTensorType.WEIGHT else self.activation_qType,
+                keep_float_weight=self.add_qdq_pair_to_weight,
+            )
+
+        weight_dequant_output = add_dequant_output_suffix(weight_name)
+        self.model.replace_input_of_all_nodes(weight_name, weight_dequant_output)
+        if self.add_qdq_pair_to_weight:
+            weight_quant_output = add_quant_output_suffix(weight_name)
+
+            self._create_qdq_nodes(
+                weight_name,
+                weight_quant_output,
+                add_quant_suffix(weight_name),
+                weight_quant_output,
+                weight_dequant_output,
+                add_dequant_suffix(weight_name),
+                scale_name,
+                zp_name,
+                axis,
+            )
+        else:
+            dequant_node = onnx.helper.make_node(
+                DEQUANT_OP_NAME,
+                [q_weight_name, scale_name, zp_name],
+                [weight_dequant_output],
+                add_dequant_suffix(weight_name),
+                axis=axis,
+            )
+            self.model.add_node(dequant_node)
 
-                    self.model.add_nodes([qlinear_node, dequant_node])
-                else:
-                    q_weight_name, zp_name, scale_name = self.quantize_weight(initializer, self.weight_qType)
-                    inputs = [q_weight_name, scale_name, zp_name]
-                    output_name = tensor_name + '_DequantizeLinear'
-                    node = onnx.helper.make_node("DequantizeLinear", inputs, [output_name],
-                                                 tensor_name + '_DequantizeLinear')
-                    self.model.add_node(node)
-                    self.model.replace_input_of_all_nodes(tensor_name, tensor_name + "_DequantizeLinear")
+    def _add_qdq_pair_for_activation(self, tensor_name, scale_name, zp_name):
+        if (
+            self.dedicated_qdq_pair
+            and tensor_name in self.tensor_to_its_receiving_nodes
+            and len(self.tensor_to_its_receiving_nodes[tensor_name]) > 1
+        ):
+            num_dedicated_qdq_pair = len(self.tensor_to_its_receiving_nodes[tensor_name])
+            for i in range(num_dedicated_qdq_pair):
+                postfix = f"_{i + 1}"
+                tensor_name_quant_output_postfix = add_quant_output_suffix(tensor_name) + postfix
+                tensor_name_dequant_output_postfix = add_dequant_output_suffix(tensor_name) + postfix
+                quant_node_name_postfix = add_quant_suffix(tensor_name) + postfix
+                dequant_node_name_postfix = add_dequant_suffix(tensor_name) + postfix
+                self._create_qdq_nodes(
+                    tensor_name,
+                    tensor_name_quant_output_postfix,
+                    quant_node_name_postfix,
+                    tensor_name_quant_output_postfix,
+                    tensor_name_dequant_output_postfix,
+                    dequant_node_name_postfix,
+                    scale_name,
+                    zp_name,
+                )
+
+                node = self.tensor_to_its_receiving_nodes[tensor_name][i]
+                self.model.replace_node_input(node, tensor_name, tensor_name_dequant_output_postfix)
+                if i == 0:
+                    quantized_value = QuantizedValue(
+                        tensor_name,
+                        tensor_name_dequant_output_postfix,
+                        scale_name,
+                        zp_name,
+                        QuantizedValueType.Input,
+                    )
+                    self.quantized_value_map[tensor_name] = quantized_value
+        else:
+            q_input = tensor_name
+            dq_output = add_dequant_output_suffix(tensor_name)
+            if self.model.is_graph_output(tensor_name):
+                q_input = add_quant_input_suffix(tensor_name)
+                dq_output = tensor_name
+                self.model.replace_output_of_all_nodes(tensor_name, q_input)
             else:
-                data_found, scale_name, zp_name, _, _ = self._get_quantization_params(tensor_name)
-
-                if data_found == False:
-                    raise ValueError(
-                        "Quantization parameters are not specified for param {}."
-                        "In static mode quantization params for inputs and outputs of nodes to be quantized are required."
-                        .format(tensor_name))
-
-                if self.dedicated_qdq_pair and tensor_name in self.tensor_to_its_receiving_nodes and len(self.tensor_to_its_receiving_nodes[tensor_name]) > 1:
-                    num_dedicated_qdq_pair = len(self.tensor_to_its_receiving_nodes[tensor_name])
-                    for i in range(num_dedicated_qdq_pair):
-                        postfix = str(i+1)
-                        q_input = tensor_name
-                        q_output = tensor_name + "_QuantizeLinear_" + postfix 
-                        dq_input = q_output
-                        dq_output = tensor_name + "_DequantizeLinear_" + postfix
-                        quant_node_name = tensor_name + "_QuantizeLinear_" + postfix
-                        dequant_node_name = tensor_name + "_DequantizeLinear_" + postfix
-                        qlinear_node = onnx.helper.make_node("QuantizeLinear", [q_input, scale_name, zp_name],
-                                                             [q_output], quant_node_name)
-                        dequant_node = onnx.helper.make_node("DequantizeLinear",
-                                                             [dq_input, scale_name, zp_name],
-                                                             [dq_output],
-                                                             dequant_node_name)
-                        self.model.add_nodes([qlinear_node, dequant_node])
+                self.model.replace_input_of_all_nodes(tensor_name, dq_output)
 
-                        node = self.tensor_to_its_receiving_nodes[tensor_name][i]
-                        self.model.replace_node_input(node, tensor_name, dq_output)
+            self._create_qdq_nodes(
+                q_input,
+                add_quant_output_suffix(tensor_name),
+                add_quant_suffix(tensor_name),
+                add_quant_output_suffix(tensor_name),
+                dq_output,
+                add_dequant_suffix(tensor_name),
+                scale_name,
+                zp_name,
+            )
+
+            quantized_value = QuantizedValue(
+                tensor_name,
+                dq_output,
+                scale_name,
+                zp_name,
+                QuantizedValueType.Input,
+            )
+            self.quantized_value_map[tensor_name] = quantized_value
+
+    def _quantize_normal_tensors(self):
+        for tensor_name, tensor_info in self.tensors_to_quantize.copy().items():
+            if tensor_name in self.quantized_value_map:
+                continue
 
-                    quantized_value = QuantizedValue(tensor_name, dq_output, scale_name, zp_name,
-                                                     QuantizedValueType.Input)
-                    self.quantized_value_map[tensor_name] = quantized_value
+            if not tensor_info.is_shared:
+                # Quantize the input
+                initializer = find_by_name(tensor_name, self.model.initializer())
+                if initializer:
+                    self._add_qdq_pair_for_initializer(initializer, tensor_info.tensor_type, tensor_info.axis)
                 else:
-                    q_input = tensor_name
-                    q_output = tensor_name + "_QuantizeLinear"
-                    dq_input = q_output
-                    dq_output = tensor_name + "_DequantizeLinear"
-                    if self.model.is_graph_output(tensor_name):
-                        q_input = tensor_name + "_QuantizeLinearInput"
-                        dq_output = tensor_name
-                        self.model.replace_output_of_all_nodes(tensor_name, q_input)
-                    else:
-                        self.model.replace_input_of_all_nodes(tensor_name, dq_output)
-
-                    quant_node_name = tensor_name + "_QuantizeLinear"
-                    dequant_node_name = tensor_name + "_DequantizeLinear"
-                    qlinear_node = onnx.helper.make_node("QuantizeLinear", [q_input, scale_name, zp_name],
-                                                         [q_output], quant_node_name)
-                    dequant_node = onnx.helper.make_node("DequantizeLinear",
-                                                         [dq_input, scale_name, zp_name],
-                                                         [dq_output],
-                                                         dequant_node_name)
-                    self.model.add_nodes([qlinear_node, dequant_node])
+                    used_scale, used_zp = self.find_quant_scale_zp(tensor_name)
+                    data_found, scale_name, zp_name, _, _ = self._get_quantization_params(
+                        tensor_name, used_scale, used_zp
+                    )
+
+                    if not data_found:
+                        raise ValueError(
+                            f"Quantization parameters are not specified for param {tensor_name}. "
+                            "In static mode quantization params for inputs and outputs of nodes to be quantized are required."
+                        )
+
+                    self._add_qdq_pair_for_activation(tensor_name, scale_name, zp_name)
+
+                del self.tensors_to_quantize[tensor_name]
+
+    def _quantize_sharing_param_tensors(self):
+        while self.tensors_to_quantize:
+            for tensor_name, tensor_info in self.tensors_to_quantize.copy().items():
+                tensor_provider_name = tensor_info.quant_para_provider
+                if tensor_provider_name in self.quantized_value_map:
+                    del self.tensors_to_quantize[tensor_name]
+
+                    quantized_value = self.quantized_value_map[tensor_provider_name]
+                    # Quantize the input
+                    initializer = find_by_name(tensor_name, self.model.initializer())
+                    if initializer is not None:
+                        raise ValueError("Quantization parameter shared mode is not supported for weight yet")
+                    self._add_qdq_pair_for_activation(tensor_name, quantized_value.scale_name, quantized_value.zp_name)
 
-                    quantized_value = QuantizedValue(tensor_name, dq_output, scale_name, zp_name,
-                                                     QuantizedValueType.Input)
-                    self.quantized_value_map[tensor_name] = quantized_value
-
-    def quantize_bias_tensors(self):
+    def _quantize_bias_tensors(self):
         for bias_name, input_name, weight_name, beta in self.bias_to_quantize:
-            if bias_name in self.quantized_value_map.keys():
+            if bias_name in self.quantized_value_map:
                 continue
             # Quantize the input
             self.quantize_bias_static(bias_name, input_name, weight_name, beta)
             self.model.remove_initializer(find_by_name(bias_name, self.model.initializer()))
             quant_value = self.quantized_value_map[bias_name]
             inputs = [quant_value.q_name, quant_value.scale_name, quant_value.zp_name]
+            node_name = add_dequant_suffix(bias_name)
             if quant_value.axis is not None:
-                dequant_node = onnx.helper.make_node("DequantizeLinear",
-                                                     inputs, [bias_name],
-                                                     bias_name + '_DequantizeLinear',
-                                                     axis=quant_value.axis)
+                dequant_node = onnx.helper.make_node(
+                    "DequantizeLinear",
+                    inputs,
+                    [bias_name],
+                    node_name,
+                    axis=quant_value.axis,
+                )
             else:
-                dequant_node = onnx.helper.make_node("DequantizeLinear", inputs, [bias_name],
-                                                     bias_name + '_DequantizeLinear')
+                dequant_node = onnx.helper.make_node(
+                    "DequantizeLinear",
+                    inputs,
+                    [bias_name],
+                    node_name,
+                )
             self.model.add_node(dequant_node)
 
-    def quantize_weights_per_channel(self):
-        if self.opset_version < 13 and len(self.tensors_to_quantize_per_channel) > 0:
-            raise ValueError("Per-Channel support with QDQ format requires onnx opset version 13 or above.")
-        for weight_name, axis in self.tensors_to_quantize_per_channel:
-            if self.add_qdq_pair_to_weight:
-                q_name, zp_name, scale_name = self.quantize_weight_per_channel(weight_name, onnx_proto.TensorProto.INT8,
-                                                                               axis, keep_float_weight=True)
-                qlinear_node = onnx.helper.make_node("QuantizeLinear", [weight_name, scale_name, zp_name],
-                                                     [weight_name + "_QuantizeLinear"],
-                                                     weight_name + "_QuantizeLinear",
-                                                     axis=axis)
-                dequant_node = onnx.helper.make_node("DequantizeLinear",
-                                                     [weight_name + "_QuantizeLinear", scale_name, zp_name],
-                                                     [weight_name + "_DequantizeLinear"],
-                                                     weight_name + "_DequantizeLinear",
-                                                     axis=axis)
-                self.model.replace_input_of_all_nodes(weight_name, weight_name + "_DequantizeLinear")
-
-                self.model.add_nodes([qlinear_node, dequant_node])
-            else:
-                #q_name, zp_name, scale_name = self.quantize_weight_per_channel(weight_name, self.weight_qType, axis)
-                q_name, zp_name, scale_name = self.quantize_weight_per_channel(weight_name, onnx_proto.TensorProto.INT8,
-                                                                               axis)
-
-                inputs = [q_name, scale_name, zp_name]
-                output_name = weight_name + "_DequantizeLinear"
-                node = onnx.helper.make_node("DequantizeLinear",
-                                             inputs, [output_name],
-                                             weight_name + '_DequantizeLinear',
-                                             axis=axis)
-                self.model.add_node(node)
-
-                # Replace weight_name with output of DequantizeLinear
-                self.model.replace_input_of_all_nodes(weight_name, output_name)
+    def is_tensor_quantized(self, tensor_name):
+        return tensor_name in self.tensors_to_quantize or tensor_name in self.bias_to_quantize
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/quant_utils.py` & `onnxruntime/quantization/quant_utils.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,19 +1,29 @@
 import logging
+import tempfile
+from enum import Enum
+from pathlib import Path
+
 import numpy
 import onnx
-
-from enum import Enum
+from onnx import external_data_helper
 from onnx import onnx_pb as onnx_proto
-from pathlib import Path
+
+from onnxruntime import GraphOptimizationLevel, InferenceSession, SessionOptions
 
 __producer__ = "onnx.quantize"
 __version__ = "0.1.0"
 onnx_domain = "ai.onnx"
 ms_domain = "com.microsoft"
+QUANT_OP_NAME = "QuantizeLinear"
+QUANT_INPUT_SUFFIX = "_QuantizeLinear_Input"
+DEQUANT_OP_NAME = "DequantizeLinear"
+DEQUANT_OUTPUT_SUFFIX = "_DequantizeLinear_Output"
+TENSOR_NAME_QUANT_SUFFIX = "_quantized"
+
 
 type_to_name = {
     1: "FLOAT",
     2: "UINT8",
     3: "INT8",
     4: "UINT16",
     5: "INT16",
@@ -42,136 +52,143 @@
         return self.name
 
     @staticmethod
     def from_string(mode):
         try:
             return QuantizationMode[mode]
         except KeyError:
-            raise ValueError()
+            raise ValueError()  # noqa: B904
 
 
 class QuantizedValueType(Enum):
     Input = 0
     Initializer = 1
 
     def __str__(self):
         return self.name
 
     @staticmethod
     def from_string(v):
         try:
             return QuantizedValueType[v]
         except KeyError:
-            raise ValueError()
+            raise ValueError()  # noqa: B904
 
 
 class QuantType(Enum):
     QInt8 = 0
     QUInt8 = 1
 
     def __str__(self):
         return self.name
 
     @staticmethod
     def from_string(t):
         try:
             return QuantType[t]
         except KeyError:
-            raise ValueError()
+            raise ValueError()  # noqa: B904
 
 
 class QuantFormat(Enum):
     QOperator = 0
     QDQ = 1
 
     def __str__(self):
         return self.name
 
     @staticmethod
     def from_string(format):
         try:
             return QuantFormat[format]
         except KeyError:
-            raise ValueError()
+            raise ValueError()  # noqa: B904
+
 
 ONNX_TYPE_TO_NP_TYPE = {
-    onnx_proto.TensorProto.INT8: numpy.dtype('int8'),
-    onnx_proto.TensorProto.UINT8:  numpy.dtype('uint8')
+    onnx_proto.TensorProto.INT8: numpy.dtype("int8"),
+    onnx_proto.TensorProto.UINT8: numpy.dtype("uint8"),
 }
 
+
 def quantize_nparray(qType, arr, scale, zero_point, low=None, high=None):
-    assert qType in ONNX_TYPE_TO_NP_TYPE, \
-        "Unexpected data type {} requested. Only INT8 and UINT8 are supported.".format(qType)
+    assert qType in ONNX_TYPE_TO_NP_TYPE, f"Unexpected data type {qType} requested. Only INT8 and UINT8 are supported."
     dtype = ONNX_TYPE_TO_NP_TYPE[qType]
     cliplow = max(0 if dtype == numpy.uint8 else -127, -127 if low is None else low)
     cliphigh = min(255 if dtype == numpy.uint8 else 127, 255 if high is None else high)
     arr_fp32 = numpy.asarray((arr.astype(numpy.float32) / scale).round() + zero_point)
     numpy.clip(arr_fp32, cliplow, cliphigh, out=arr_fp32)
     return arr_fp32.astype(dtype)
 
 
 def compute_scale_zp(rmin, rmax, qmin, qmax, symmetric=False):
-    '''
-    Calculate the scale s and zero point z for the quantization relation 
+    """Calculate the scale s and zero point z for the quantization relation
     r = s(q-z), where r are the original values and q are the corresponding
-    quantized values. 
+    quantized values.
 
     r and z are calculated such that every value within [rmin,rmax] has an
     approximate representation within [qmin,qmax]. In addition, qmin <= z <=
     qmax is enforced. If the symmetric flag is set to True, the interval
     [rmin,rmax] is symmetrized to [-absmax, +absmax], where
     absmax = max(abs(rmin), abs(rmax)).
 
     :parameter rmin: minimum value of r
     :parameter rmax: maximum value of r
     :parameter qmin: minimum value representable by the target quantization data type
     :parameter qmax: maximum value representable by the target quantization data type
     :return: zero and scale [z, s]
 
-    '''
-    
+    """
+
+    if qmin > 0 or qmax < 0:
+        raise ValueError(f"qmin and qmax must meet requirement: qmin <= 0 <= qmax while qmin:{qmin}, qmmax:{qmax}")
+
     # Adjust rmin and rmax such that 0 is included in the range. This is
     # required to make sure zero can be represented by the quantization data
     # type (i.e. to make sure qmin <= zero_point <= qmax)
     rmin = min(rmin, 0)
     rmax = max(rmax, 0)
 
     if symmetric:
         absmax = max(abs(rmin), abs(rmax))
         rmin = -absmax
         rmax = +absmax
 
-    scale = (rmax - rmin) / float(qmax-qmin) if rmax!=rmin else 1.0
-    zero_point = round(qmin - rmin/scale)
+    scale = (rmax - rmin) / float(qmax - qmin)
+    if scale < numpy.finfo(numpy.float32).tiny:
+        scale = 1.0
+        zero_point = 0
+    else:
+        zero_point = round(qmin - rmin / scale)
 
     return [zero_point, scale]
 
 
 def quantize_data(data, qType, symmetric, reduce_range=False):
-    '''
+    """
     :param data: data to quantize
     :param qType: data type to quantize to. Supported types UINT8 and INT8
     :param symmetric: whether symmetric quantization is used or not. This is applied to INT8.
     :return: minimum, maximum, zero point, scale, and quantized weights
 
     To pack weights, we compute a linear transformation
-    
+
     - when data `type == uint8` mode, from `[rmin, rmax]` -> :math:`[0, 2^{b-1}]` and
     - when data `type == int8`, from `[-m , m]` -> :math:`[-(2^{b-1}-1), 2^{b-1}-1]` where
         `m = max(abs(rmin), abs(rmax))`
 
     and add necessary intermediate nodes to trasnform quantized weight to full weight using the equation
 
     :math:`r = S(q-z)`, where
-    
+
     - *r*: real original value
     - *q*: quantized value
     - *S*: scale
     - *z*: zero point
-    '''
+    """
 
     rmin = 0
     rmax = 0
     zero_point = 0
     scale = 1.0
     if len(data):
         rmin = min(data)
@@ -180,54 +197,60 @@
 
         zero_point, scale = compute_scale_zp(rmin, rmax, qmin, qmax, symmetric)
 
     quantized_data = quantize_nparray(qType, numpy.asarray(data), scale, zero_point)
 
     return rmin, rmax, zero_point, scale, quantized_data
 
-def get_qmin_qmax_for_qType(qType, reduce_range=False, symmetric=False):
-    '''
+
+def get_qmin_qmax_for_qType(qType, reduce_range=False, symmetric=False):  # noqa: N802
+    """
     Return qmin and qmax, the minimum and maximum value representable by the given qType
     :parameter qType: onnx.onnx_pb.TensorProto.UINT8 or onnx.onnx_pb.TensorProto.UINT8
     :return: qmin, qmax
-    '''
+    """
     if qType == onnx_proto.TensorProto.UINT8:
-        (qmin, qmax) = (0,127) if reduce_range else (0,255)
+        (qmin, qmax) = (0, 127) if reduce_range else (0, 255)
     elif qType == onnx_proto.TensorProto.INT8:
         if symmetric:
-            (qmin, qmax) = (-64,64) if reduce_range else (-127,127)
+            (qmin, qmax) = (-64, 64) if reduce_range else (-127, 127)
         else:
-            (qmin, qmax) = (-64,64) if reduce_range else (-128,127)
+            (qmin, qmax) = (-64, 64) if reduce_range else (-128, 127)
     else:
-        raise ValueError("Unexpected data type {} requested. Only INT8 and UINT8 are supported.".format(qType))
+        raise ValueError(f"Unexpected data type {qType} requested. Only INT8 and UINT8 are supported.")
     return qmin, qmax
 
-def get_qrange_for_qType(qType, reduce_range=False, symmetric=False):
-    '''
+
+def get_qrange_for_qType(qType, reduce_range=False, symmetric=False):  # noqa: N802
+    """
     Helper function to get the quantization range for a type.
         parameter qType: quantization type.
         return: quantization range.
-    '''
+    """
     qmin, qmax = get_qmin_qmax_for_qType(qType, reduce_range, symmetric=symmetric)
-    return  qmax - qmin
+    return qmax - qmin
+
 
 class QuantizedInitializer:
-    '''
-        Represents a linearly quantized weight input from ONNX operators
-    '''
-    def __init__(self,
-                 name,
-                 initializer,
-                 rmins,
-                 rmaxs,
-                 zero_points,
-                 scales,
-                 data=[],
-                 quantized_data=[],
-                 axis=None):
+    """
+    Represents a linearly quantized weight input from ONNX operators
+    """
+
+    def __init__(
+        self,
+        name,
+        initializer,
+        rmins,
+        rmaxs,
+        zero_points,
+        scales,
+        data=[],  # noqa: B006
+        quantized_data=[],  # noqa: B006
+        axis=None,
+    ):
         self.name = name
         self.initializer = initializer  # TensorProto initializer in ONNX graph
         self.rmins = rmins  # List of minimum range for each axis
         self.rmaxs = rmaxs  # List of maximum range for each axis
         # 1D tensor of zero points computed for each axis. scalar if axis is empty
         self.zero_points = zero_points
         self.scales = scales  # 1D tensor of scales computed for each axis. scalar if axis is empty
@@ -235,146 +258,156 @@
         self.quantized_data = quantized_data  # weight-packed data from data
         # Scalar to specify which dimension in the initializer to weight pack.
         self.axis = axis
         # If empty, single zero point and scales computed from a single rmin and rmax
 
 
 class QuantizedValue:
-    '''
-    Represents a linearly quantized value (input\output\intializer)
-    '''
-    def __init__(self,
-                 name,
-                 new_quantized_name,
-                 scale_name,
-                 zero_point_name,
-                 quantized_value_type,
-                 axis=None):
+    """
+    Represents a linearly quantized value (input\\output\\intializer)
+    """
+
+    def __init__(
+        self,
+        name,
+        new_quantized_name,
+        scale_name,
+        zero_point_name,
+        quantized_value_type,
+        axis=None,
+    ):
         self.original_name = name
         self.q_name = new_quantized_name
         self.scale_name = scale_name
         self.zp_name = zero_point_name
         self.value_type = quantized_value_type
         self.axis = axis
 
 
 class BiasToQuantize:
-    '''
+    """
     Represents a bias to be quantized
-    '''
+    """
+
     def __init__(self, bias_name, input_name, weight_name):
         self.bias_name = bias_name
         self.input_name = input_name
         self.weight_name = weight_name
 
 
 def attribute_to_kwarg(attribute):
-    '''
+    """
     Convert attribute to kwarg format for use with onnx.helper.make_node.
         :parameter attribute: attribute in AttributeProto format.
         :return: attribute in {key: value} format.
-    '''
-    if (attribute.type == 0):
-        raise ValueError('attribute {} does not have type specified.'.format(attribute.name))
+    """
+    if attribute.type == 0:
+        raise ValueError(f"attribute {attribute.name} does not have type specified.")
 
     # Based on attribute type definitions from AttributeProto
-    # definition in https://github.com/onnx/onnx/blob/master/onnx/onnx.proto
-    if (attribute.type == 1):
+    # definition in https://github.com/onnx/onnx/blob/main/onnx/onnx.proto
+    if attribute.type == 1:
         value = attribute.f
-    elif (attribute.type == 2):
+    elif attribute.type == 2:
         value = attribute.i
-    elif (attribute.type == 3):
+    elif attribute.type == 3:
         value = attribute.s
-    elif (attribute.type == 4):
+    elif attribute.type == 4:
         value = attribute.t
-    elif (attribute.type == 5):
+    elif attribute.type == 5:
         value = attribute.g
-    elif (attribute.type == 6):
+    elif attribute.type == 6:
         value = attribute.floats
-    elif (attribute.type == 7):
+    elif attribute.type == 7:
         value = attribute.ints
-    elif (attribute.type == 8):
+    elif attribute.type == 8:
         value = attribute.strings
-    elif (attribute.type == 9):
+    elif attribute.type == 9:
         value = attribute.tensors
-    elif (attribute.type == 10):
+    elif attribute.type == 10:
         value = attribute.graphs
     else:
-        raise ValueError('attribute {} has unsupported type {}.'.format(attribute.name, attribute.type))
+        raise ValueError(f"attribute {attribute.name} has unsupported type {attribute.type}.")
 
     return {attribute.name: value}
 
 
 def find_by_name(item_name, item_list):
-    '''
+    """
     Helper function to find item by name in a list.
         parameter item_name: name of the item.
         parameter item_list: list of items.
         return: item if found. None otherwise.
-    '''
+    """
     items = [item for item in item_list if item.name == item_name]
     return items[0] if len(items) > 0 else None
 
 
 def get_elem_index(elem_name, elem_list):
-    '''
+    """
     Helper function to return index of an item in a node list
-    '''
+    """
     elem_idx = -1
     for i in range(0, len(elem_list)):
         if elem_list[i] == elem_name:
             elem_idx = i
     return elem_idx
 
 
 def get_mul_node(inputs, output, name):
-    '''
+    """
     Helper function to create a Mul node.
         parameter inputs: list of input names.
         parameter output: output name.
         parameter name: name of the node.
         return: Mul node in NodeProto format.
-    '''
+    """
     return onnx.helper.make_node("Mul", inputs, [output], name)
 
 
 def generate_identified_filename(filename: Path, identifier: str) -> Path:
-    '''
-    Helper function to generate a identifiable filepath by concatenating the given identifier as a suffix.   
-    '''
-    return filename.parent.joinpath(filename.stem + identifier).with_suffix(filename.suffix)
+    """
+    Helper function to generate a identifiable filepath by concatenating the given identifier as a suffix.
+    """
+    return filename.parent.joinpath(filename.stem + identifier + filename.suffix)
+
 
 def apply_plot(hist, hist_edges):
     import sys
-    import numpy
+
     import matplotlib.pyplot as plt
+    import numpy
+
     numpy.set_printoptions(threshold=sys.maxsize)
     print("Histogram:")
     print(hist)
     print("Histogram Edges:")
     print(hist_edges)
     plt.stairs(hist, hist_edges, fill=True)
-    plt.xlabel('Tensor value')
-    plt.ylabel('Counts')
-    plt.title('Tensor value V.S. Counts')
+    plt.xlabel("Tensor value")
+    plt.ylabel("Counts")
+    plt.title("Tensor value V.S. Counts")
     plt.show()
 
+
 def write_calibration_table(calibration_cache):
-    '''
-    Helper function to write calibration table to files.   
-    '''
+    """
+    Helper function to write calibration table to files.
+    """
 
     import json
+
     import flatbuffers
-    import onnxruntime.quantization.CalTableFlatBuffers.TrtTable as TrtTable
+
     import onnxruntime.quantization.CalTableFlatBuffers.KeyValue as KeyValue
+    import onnxruntime.quantization.CalTableFlatBuffers.TrtTable as TrtTable
 
-    logging.info("calibration cache: {}".format(calibration_cache))
+    logging.info(f"calibration cache: {calibration_cache}")
 
-    with open("calibration.json", 'w') as file:
+    with open("calibration.json", "w") as file:
         file.write(json.dumps(calibration_cache))  # use `json.loads` to do the reverse
 
     # Serialize data using FlatBuffers
     builder = flatbuffers.Builder(1024)
     key_value_list = []
     for key in sorted(calibration_cache.keys()):
         values = calibration_cache[key]
@@ -398,33 +431,34 @@
     TrtTable.TrtTableStart(builder)
     TrtTable.TrtTableAddDict(builder, main_dict)
     cal_table = TrtTable.TrtTableEnd(builder)
 
     builder.Finish(cal_table)
     buf = builder.Output()
 
-    with open("calibration.flatbuffers", 'wb') as file:
+    with open("calibration.flatbuffers", "wb") as file:
         file.write(buf)
 
     # Deserialize data (for validation)
     if False:
         cal_table = TrtTable.TrtTable.GetRootAsTrtTable(buf, 0)
         dict_len = cal_table.DictLength()
         for i in range(dict_len):
             key_value = cal_table.Dict(i)
             logging.info(key_value.Key())
             logging.info(key_value.Value())
 
     # write plain text
-    with open("calibration.cache", 'w') as file:
+    with open("calibration.cache", "w") as file:
         for key in sorted(calibration_cache.keys()):
             value = calibration_cache[key]
-            s = key + ' ' + str(max(abs(value[0]), abs(value[1])))
+            s = key + " " + str(max(abs(value[0]), abs(value[1])))
             file.write(s)
-            file.write('\n')
+            file.write("\n")
+
 
 def smooth_distribution(p, eps=0.0001):
     """Given a discrete distribution (may have not been normalized to 1),
     smooth it by replacing zeros with eps multiplied by a scaling factor
     and taking the corresponding amount off the non-zero values.
     Ref: http://web.engr.illinois.edu/~hanj/cs412/bk3/KL-divergence.pdf
          https://github.com//apache/incubator-mxnet/blob/master/python/mxnet/contrib/quantization.py
@@ -436,14 +470,146 @@
     n_zeros = is_zeros.sum()
     n_nonzeros = p.size - n_zeros
 
     if not n_nonzeros:
         # raise ValueError('The discrete probability distribution is malformed. All entries are 0.')
         return -1
     eps1 = eps * float(n_zeros) / float(n_nonzeros)
-    assert eps1 < 1.0, 'n_zeros=%d, n_nonzeros=%d, eps1=%f' % (n_zeros, n_nonzeros, eps1)
+    assert eps1 < 1.0, "n_zeros=%d, n_nonzeros=%d, eps1=%f" % (
+        n_zeros,
+        n_nonzeros,
+        eps1,
+    )
 
     hist = p.astype(np.float32)
     hist += eps * is_zeros + (-eps1) * is_nonzeros
     assert (hist <= 0).sum() == 0
 
     return hist
+
+
+def model_has_external_data(model_path: Path):
+    model = onnx.load(model_path.as_posix(), load_external_data=False)
+    for intializer in model.graph.initializer:
+        if external_data_helper.uses_external_data(intializer):
+            return True
+    return False
+
+
+def optimize_model(model_path: Path, opt_model_path: Path):
+    """
+        Generate model that applies graph optimization (constant folding, etc.)
+        parameter model_path: path to the original onnx model
+        parameter opt_model_path: path to the optimized onnx model
+    :return: optimized onnx model
+    """
+    sess_option = SessionOptions()
+    sess_option.optimized_model_filepath = opt_model_path.as_posix()
+    sess_option.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_BASIC
+    kwargs = {}
+    # This will rename constant initializer names, disable it to make test pass.
+    kwargs["disabled_optimizers"] = ["ConstantSharing"]
+    _ = InferenceSession(model_path.as_posix(), sess_option, providers=["CPUExecutionProvider"], **kwargs)
+
+
+def add_pre_process_metadata(model):
+    """Tag the model that it went through quantization pre-processing"""
+    metadata_props = {"onnx.quant.pre_process": "onnxruntime.quant"}
+    if model.metadata_props:
+        for prop in model.metadata_props:
+            metadata_props.update({prop.key: prop.value})
+    onnx.helper.set_model_props(model, metadata_props)
+
+
+def model_has_pre_process_metadata(model):
+    """Check the model whether it went through quantization pre-processing"""
+    if model.metadata_props:
+        for prop in model.metadata_props:
+            if prop.key == "onnx.quant.pre_process" and prop.value == "onnxruntime.quant":
+                return True
+    return False
+
+
+def add_infer_metadata(model):
+    metadata_props = {"onnx.infer": "onnxruntime.quant"}
+    if model.metadata_props:
+        for p in model.metadata_props:
+            metadata_props.update({p.key: p.value})
+    onnx.helper.set_model_props(model, metadata_props)
+
+
+def model_has_infer_metadata(model):
+    if model.metadata_props:
+        for p in model.metadata_props:
+            if p.key == "onnx.infer" and p.value == "onnxruntime.quant":
+                return True
+    return False
+
+
+def load_model_with_shape_infer(model_path: Path):
+    inferred_model_path = generate_identified_filename(model_path, "-inferred")
+    onnx.shape_inference.infer_shapes_path(str(model_path), str(inferred_model_path))
+    model = onnx.load(inferred_model_path.as_posix())
+    inferred_model_path.unlink()
+    return model
+
+
+def load_model(model_path: Path, need_optimize: bool):
+    with tempfile.TemporaryDirectory(prefix="ort.quant.") as quant_tmp_dir:
+        if need_optimize and not model_has_external_data(model_path):
+            opt_model_path = Path(quant_tmp_dir).joinpath("model.onnx")
+            optimize_model(model_path, opt_model_path)
+            model_path = opt_model_path
+
+        model = load_model_with_shape_infer(model_path)
+        add_infer_metadata(model)
+        return model
+
+
+def save_and_reload_model(model):
+    with tempfile.TemporaryDirectory(prefix="ort.quant.") as quant_tmp_dir:
+        model_path = Path(quant_tmp_dir).joinpath("model.onnx")
+        onnx.external_data_helper.convert_model_to_external_data(model, all_tensors_to_one_file=True)
+        onnx.save_model(model, model_path.as_posix())
+        return load_model(model_path, False)
+
+
+def clone_model_with_shape_infer(model):
+    if model_has_infer_metadata(model):
+        cloned_model = onnx_proto.ModelProto()
+        cloned_model.CopyFrom(model)
+    else:
+        cloned_model = save_and_reload_model(model)
+    return cloned_model
+
+
+def tensor_proto_to_array(initializer):
+    if initializer.data_type == onnx_proto.TensorProto.FLOAT:
+        return onnx.numpy_helper.to_array(initializer)
+
+    raise ValueError(
+        f"Only float type is supported. Weights {initializer.name} is {type_to_name[initializer.data_type]}"
+    )
+
+
+def add_quant_suffix(tensor_name):
+    return tensor_name + "_QuantizeLinear"
+
+
+def add_quant_input_suffix(tensor_name):
+    return tensor_name + QUANT_INPUT_SUFFIX
+
+
+def add_quant_output_suffix(tensor_name):
+    return tensor_name + "_QuantizeLinear_Output"
+
+
+def add_dequant_suffix(tensor_name):
+    return tensor_name + "_DequantizeLinear"
+
+
+def add_dequant_input_suffix(tensor_name):
+    return tensor_name + "_DequantizeLinear_Input"
+
+
+def add_dequant_output_suffix(tensor_name):
+    return tensor_name + DEQUANT_OUTPUT_SUFFIX
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/registry.py` & `onnxruntime/quantization/registry.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,32 +1,35 @@
-from .quant_utils import QuantizationMode
+from .operators.activation import QDQRemovableActivation, QLinearActivation
 from .operators.argmax import QArgMax
-from .operators.base_operator import QuantOperatorBase
-from .operators.qdq_base_operator import QDQOperatorBase
-from .operators.matmul import MatMulInteger, QLinearMatMul, QDQMatMul
 from .operators.attention import AttentionQuant
-from .operators.embed_layernorm import EmbedLayerNormalizationQuant
-from .operators.gather import GatherQuant
-from .operators.conv import QLinearConv, ConvInteger, QDQConv
-from .operators.activation import QLinearActivation, QDQRemovableActivation
+from .operators.base_operator import QuantOperatorBase
 from .operators.binary_op import QLinearBinaryOp
-from .operators.maxpool import QDQMaxPool, QMaxPool
+from .operators.concat import QLinearConcat
+from .operators.conv import ConvInteger, QDQConv, QLinearConv
+from .operators.direct_q8 import Direct8BitOp, QDQDirect8BitOp
+from .operators.embed_layernorm import EmbedLayerNormalizationQuant
+from .operators.gather import GatherQuant, QDQGather
 from .operators.gavgpool import QGlobalAveragePool
+from .operators.gemm import QDQGemm, QLinearGemm
+from .operators.instnorm import QDQInstanceNormalization
 from .operators.lstm import LSTMQuant
-from .operators.split import QSplit
+from .operators.matmul import MatMulInteger, QDQMatMul, QLinearMatMul
+from .operators.maxpool import QDQMaxPool, QMaxPool
 from .operators.pad import QPad
-from .operators.direct_q8 import Direct8BitOp, QDQDirect8BitOp
-from .operators.resize import QResize, QDQResize
 from .operators.pooling import QLinearPool
-from .operators.concat import QLinearConcat, QDQConcat
-from .operators.gemm import QLinearGemm, QDQGemm
+from .operators.qdq_base_operator import QDQOperatorBase
+from .operators.resize import QDQResize, QResize
+from .operators.softmax import QDQSoftmax, QLinearSoftmax
+from .operators.split import QDQSplit, QSplit
+from .operators.where import QDQWhere, QLinearWhere
+from .quant_utils import QuantizationMode
 
 CommonOpsRegistry = {
     "Gather": GatherQuant,
-    "Transpose" : Direct8BitOp,
+    "Transpose": Direct8BitOp,
     "EmbedLayerNormalization": EmbedLayerNormalizationQuant,
 }
 
 IntegerOpsRegistry = {
     "Conv": ConvInteger,
     "MatMul": MatMulInteger,
     "Attention": AttentionQuant,
@@ -46,47 +49,56 @@
     "LeakyRelu": QLinearActivation,
     "Sigmoid": QLinearActivation,
     "MaxPool": QMaxPool,
     "GlobalAveragePool": QGlobalAveragePool,
     "Split": QSplit,
     "Pad": QPad,
     "Reshape": Direct8BitOp,
-    "Squeeze" : Direct8BitOp,
-    "Unsqueeze" : Direct8BitOp,
+    "Squeeze": Direct8BitOp,
+    "Unsqueeze": Direct8BitOp,
     "Resize": QResize,
-    "AveragePool" : QLinearPool,
+    "AveragePool": QLinearPool,
     "Concat": QLinearConcat,
+    "Softmax": QLinearSoftmax,
+    "Where": QLinearWhere,
 }
 QLinearOpsRegistry.update(CommonOpsRegistry)
 
 QDQRegistry = {
     "Conv": QDQConv,
+    "ConvTranspose": QDQConv,
     "Gemm": QDQGemm,
     "Clip": QDQRemovableActivation,
     "Relu": QDQRemovableActivation,
     "Reshape": QDQDirect8BitOp,
-    "Transpose" : QDQDirect8BitOp,
-    "Squeeze" : QDQDirect8BitOp,
-    "Unsqueeze" : QDQDirect8BitOp,
+    "Transpose": QDQDirect8BitOp,
+    "Squeeze": QDQDirect8BitOp,
+    "Unsqueeze": QDQDirect8BitOp,
     "Resize": QDQResize,
     "MaxPool": QDQMaxPool,
-    "AveragePool" : QDQDirect8BitOp,
-    "Concat": QDQConcat,
+    "AveragePool": QDQDirect8BitOp,
     "MatMul": QDQMatMul,
+    "Split": QDQSplit,
+    "Gather": QDQGather,
+    "Softmax": QDQSoftmax,
+    "Where": QDQWhere,
+    "InstanceNormalization": QDQInstanceNormalization,
 }
 
 
-def CreateDefaultOpQuantizer(onnx_quantizer, node):
+def CreateDefaultOpQuantizer(onnx_quantizer, node):  # noqa: N802
     return QuantOperatorBase(onnx_quantizer, node)
 
 
-def CreateOpQuantizer(onnx_quantizer, node):
+def CreateOpQuantizer(onnx_quantizer, node):  # noqa: N802
     registry = IntegerOpsRegistry if onnx_quantizer.mode == QuantizationMode.IntegerOps else QLinearOpsRegistry
-    if node.op_type in registry.keys():
-        return registry[node.op_type](onnx_quantizer, node)
+    if node.op_type in registry:
+        op_quantizer = registry[node.op_type](onnx_quantizer, node)
+        if op_quantizer.should_quantize():
+            return op_quantizer
     return QuantOperatorBase(onnx_quantizer, node)
 
 
-def CreateQDQQuantizer(onnx_quantizer, node):
-    if node.op_type in QDQRegistry.keys():
+def CreateQDQQuantizer(onnx_quantizer, node):  # noqa: N802
+    if node.op_type in QDQRegistry:
         return QDQRegistry[node.op_type](onnx_quantizer, node)
     return QDQOperatorBase(onnx_quantizer, node)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,56 +1,48 @@
 # automatically generated by the FlatBuffers compiler, do not modify
 
-# namespace: CalTableFlatBuffers
+# namespace: fbs
 
 import flatbuffers
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
-class KeyValue(object):
+class MapType(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAs(cls, buf, offset=0):
+    def GetRootAsMapType(cls, buf, offset):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
-        x = KeyValue()
+        x = MapType()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
-    def GetRootAsKeyValue(cls, buf, offset=0):
-        """This method is deprecated. Please switch to GetRootAs."""
-        return cls.GetRootAs(buf, offset)
-    # KeyValue
+    def MapTypeBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
+        return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
+
+    # MapType
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
-    # KeyValue
-    def Key(self):
+    # MapType
+    def KeyType(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
-            return self._tab.String(o + self._tab.Pos)
-        return None
+            return self._tab.Get(flatbuffers.number_types.Int32Flags, o + self._tab.Pos)
+        return 0
 
-    # KeyValue
-    def Value(self):
+    # MapType
+    def ValueType(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
-            return self._tab.String(o + self._tab.Pos)
+            x = self._tab.Indirect(o + self._tab.Pos)
+            from ort_flatbuffers_py.fbs.TypeInfo import TypeInfo
+            obj = TypeInfo()
+            obj.Init(self._tab.Bytes, x)
+            return obj
         return None
 
-def Start(builder): builder.StartObject(2)
-def KeyValueStart(builder):
-    """This method is deprecated. Please switch to Start."""
-    return Start(builder)
-def AddKey(builder, key): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(key), 0)
-def KeyValueAddKey(builder, key):
-    """This method is deprecated. Please switch to AddKey."""
-    return AddKey(builder, key)
-def AddValue(builder, value): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(value), 0)
-def KeyValueAddValue(builder, value):
-    """This method is deprecated. Please switch to AddValue."""
-    return AddValue(builder, value)
-def End(builder): return builder.EndObject()
-def KeyValueEnd(builder):
-    """This method is deprecated. Please switch to End."""
-    return End(builder)
+def MapTypeStart(builder): builder.StartObject(2)
+def MapTypeAddKeyType(builder, keyType): builder.PrependInt32Slot(0, keyType, 0)
+def MapTypeAddValueType(builder, valueType): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(valueType), 0)
+def MapTypeEnd(builder): return builder.EndObject()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,67 +1,55 @@
 # automatically generated by the FlatBuffers compiler, do not modify
 
-# namespace: CalTableFlatBuffers
+# namespace: fbs
 
 import flatbuffers
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
-class TrtTable(object):
+class TypeInfo(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAs(cls, buf, offset=0):
+    def GetRootAsTypeInfo(cls, buf, offset):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
-        x = TrtTable()
+        x = TypeInfo()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
-    def GetRootAsTrtTable(cls, buf, offset=0):
-        """This method is deprecated. Please switch to GetRootAs."""
-        return cls.GetRootAs(buf, offset)
-    # TrtTable
+    def TypeInfoBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
+        return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
+
+    # TypeInfo
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
-    # TrtTable
-    def Dict(self, j):
+    # TypeInfo
+    def Denotation(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
-            x = self._tab.Vector(o)
-            x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4
-            x = self._tab.Indirect(x)
-            from onnxruntime.quantization.CalTableFlatBuffers.KeyValue import KeyValue
-            obj = KeyValue()
-            obj.Init(self._tab.Bytes, x)
-            return obj
+            return self._tab.String(o + self._tab.Pos)
         return None
 
-    # TrtTable
-    def DictLength(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
+    # TypeInfo
+    def ValueType(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
-            return self._tab.VectorLen(o)
+            return self._tab.Get(flatbuffers.number_types.Uint8Flags, o + self._tab.Pos)
         return 0
 
-    # TrtTable
-    def DictIsNone(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
-        return o == 0
+    # TypeInfo
+    def Value(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
+        if o != 0:
+            from flatbuffers.table import Table
+            obj = Table(bytearray(), 0)
+            self._tab.Union(obj, o)
+            return obj
+        return None
 
-def Start(builder): builder.StartObject(1)
-def TrtTableStart(builder):
-    """This method is deprecated. Please switch to Start."""
-    return Start(builder)
-def AddDict(builder, dict): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(dict), 0)
-def TrtTableAddDict(builder, dict):
-    """This method is deprecated. Please switch to AddDict."""
-    return AddDict(builder, dict)
-def StartDictVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def TrtTableStartDictVector(builder, numElems):
-    """This method is deprecated. Please switch to Start."""
-    return StartDictVector(builder, numElems)
-def End(builder): return builder.EndObject()
-def TrtTableEnd(builder):
-    """This method is deprecated. Please switch to End."""
-    return End(builder)
+def TypeInfoStart(builder): builder.StartObject(3)
+def TypeInfoAddDenotation(builder, denotation): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(denotation), 0)
+def TypeInfoAddValueType(builder, valueType): builder.PrependUint8Slot(1, valueType, 0)
+def TypeInfoAddValue(builder, value): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(value), 0)
+def TypeInfoEnd(builder): return builder.EndObject()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/activation.py` & `onnxruntime/quantization/operators/activation.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,85 +1,117 @@
 import onnx
+
+from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType, attribute_to_kwarg, ms_domain
 from .base_operator import QuantOperatorBase
 from .qdq_base_operator import QDQOperatorBase
-from ..quant_utils import QuantizedValue, QuantizedValueType, attribute_to_kwarg, ms_domain
-from onnx import onnx_pb as onnx_proto
 
 
 class QLinearActivation(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
-    def QuantizeClipRelu(self):
+    def QuantizeClipRelu(self):  # noqa: N802
         node = self.node
-        assert (node.op_type == "Relu" or node.op_type == 'Clip')
+        assert node.op_type == "Relu" or node.op_type == "Clip"
 
         # When mode is QLinearOps, the output quantization params are calculated based on outputs from
         # activation nodes, therefore these nodes can be removed from the graph if they follow a quantized op.
         # If input to this node is not quantized then keep this node
-        if node.input[0] not in self.quantizer.quantized_value_map:
-            self.quantizer.new_nodes += [node]
-            return
+        # If activation is symmetric, not quantize the op and simply return
+        if node.input[0] not in self.quantizer.quantized_value_map or self.quantizer.is_activation_symmetric:
+            return super().quantize()
 
         quantized_value = self.quantizer.quantized_value_map[node.input[0]]
         self.quantizer.quantized_value_map[node.output[0]] = quantized_value
 
     def quantize(self):
         node = self.node
-        if node.op_type == "Relu" or node.op_type == 'Clip':
+        if node.op_type == "Relu" or node.op_type == "Clip":
             self.QuantizeClipRelu()
             return
 
-        nnapi_sigmoid_option = 'extra.Sigmoid.nnapi'
-        sigmoid_nnapi_mode = (node.op_type == 'Sigmoid' and
-                              nnapi_sigmoid_option in self.quantizer.extra_options and
-                              self.quantizer.extra_options[nnapi_sigmoid_option])
+        nnapi_sigmoid_option = "extra.Sigmoid.nnapi"
+        sigmoid_nnapi_mode = (
+            node.op_type == "Sigmoid"
+            and nnapi_sigmoid_option in self.quantizer.extra_options
+            and self.quantizer.extra_options[nnapi_sigmoid_option]
+        )
         use_scale = 1 / 256.0 if sigmoid_nnapi_mode else None
         use_zeropoint = 0 if sigmoid_nnapi_mode else None
 
         # No assert on op_type as it is controlled by registry
         # only try to quantize when given quantization parameters for it
-        data_found, output_scale_name, output_zp_name, _, _ = \
-            self.quantizer._get_quantization_params(node.output[0], use_scale, use_zeropoint)
-        quantized_input_names, zero_point_names, scale_names, nodes = self.quantizer.quantize_inputs(node, [0])
+        (
+            data_found,
+            output_scale_name,
+            output_zp_name,
+            _,
+            _,
+        ) = self.quantizer._get_quantization_params(node.output[0], use_scale, use_zeropoint)
+        (
+            quantized_input_names,
+            zero_point_names,
+            scale_names,
+            nodes,
+        ) = self.quantizer.quantize_activation(node, [0])
         if not data_found or quantized_input_names is None:
             return super().quantize()
 
-        qlinear_activation_output = node.output[0] + "_quantized"
+        qlinear_activation_output = node.output[0] + TENSOR_NAME_QUANT_SUFFIX
         qlinear_activation_name = ""
-        if node.name != "":
+        if node.name:
             qlinear_activation_name = node.name + "_quant"
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
 
         qlinear_activation_inputs = [
-            quantized_input_names[0], scale_names[0], zero_point_names[0], output_scale_name, output_zp_name
+            quantized_input_names[0],
+            scale_names[0],
+            zero_point_names[0],
+            output_scale_name,
+            output_zp_name,
         ]
 
-        qlinear_activation_node = onnx.helper.make_node("QLinear" + node.op_type, qlinear_activation_inputs,
-                                                        [qlinear_activation_output], qlinear_activation_name, **kwargs)
+        qlinear_activation_node = onnx.helper.make_node(
+            "QLinear" + node.op_type,
+            qlinear_activation_inputs,
+            [qlinear_activation_output],
+            qlinear_activation_name,
+            **kwargs,
+        )
 
         # Create an entry for this quantized value
-        q_output = QuantizedValue(node.output[0], qlinear_activation_output, output_scale_name, output_zp_name,
-                                  QuantizedValueType.Input)
+        q_output = QuantizedValue(
+            node.output[0],
+            qlinear_activation_output,
+            output_scale_name,
+            output_zp_name,
+            QuantizedValueType.Input,
+        )
         self.quantizer.quantized_value_map[node.output[0]] = q_output
 
         nodes.append(qlinear_activation_node)
         self.quantizer.new_nodes += nodes
 
 
 class QDQRemovableActivation(QDQOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
 
-        if self.quantizer.try_replacing_upstream_output(node.input[0], node.output[0]):
+        # If input to this node is not quantized then keep this node
+        if not self.quantizer.is_tensor_quantized(node.input[0]):
+            return
+
+        if not self.quantizer.is_activation_symmetric and self.quantizer.try_replacing_upstream_output(
+            node.input[0], node.output[0]
+        ):
             self.quantizer.remove_node(self.node)
         else:
-            self.quantizer.quantize_tensor(node.input[0])
+            self.quantizer.quantize_activation_tensor(node.input[0])
 
         if not self.disable_qdq_for_node_output:
-            self.quantizer.quantize_tensor(node.output[0])
+            self.quantizer.quantize_activation_tensor(node.output[0])
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/argmax.py` & `onnxruntime/quantization/operators/argmax.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 from .base_operator import QuantOperatorBase
 
+
 # Use the quantized tensor as input without DQ.
 class QArgMax(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
 
         quantized_input_value = self.quantizer.find_quantized_value(node.input[0])
         if quantized_input_value is None:
             self.quantizer.new_nodes += [node]
             return
 
         node.input[0] = quantized_input_value.q_name
-        self.quantizer.new_nodes += [node]
+        self.quantizer.new_nodes += [node]
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/attention.py` & `onnxruntime/quantization/operators/attention.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,42 +1,63 @@
 import onnx
-from .base_operator import QuantOperatorBase
+from onnx import onnx_pb as onnx_proto  # noqa: F401
+
 from ..quant_utils import attribute_to_kwarg, ms_domain
-from onnx import onnx_pb as onnx_proto
-'''
+from .base_operator import QuantOperatorBase
+
+"""
     Quantize Attention
-'''
+"""
 
 
 class AttentionQuant(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
+    def should_quantize(self):
+        return self.quantizer.should_quantize_node(self.node)
+
     def quantize(self):
-        '''
-            parameter node: Attention node.
-            parameter new_nodes_list: List of new nodes created before processing this node.
-            return: a list of nodes in topological order that represents quantized Attention node.
-        '''
+        """
+        parameter node: Attention node.
+        parameter new_nodes_list: List of new nodes created before processing this node.
+        return: a list of nodes in topological order that represents quantized Attention node.
+        """
         node = self.node
-        assert (node.op_type == "Attention")
+        assert node.op_type == "Attention"
 
         # TODO This is a temporary fix to stop exporting QAttention with qkv_hidden_sizes
         # attribute. This needs to be removed once the QAttention for varied q,k,v sizes
         # is implemented
         for attr in node.attribute:
-            if 'qkv_hidden_sizes' == attr.name:
+            if attr.name == "qkv_hidden_sizes":
                 return super().quantize()
 
-        (quantized_input_names, zero_point_names, scale_names, nodes) = \
-            self.quantizer.quantize_inputs(node, [0, 1], reduce_range=True, op_level_per_channel=True)
+        (
+            quantized_input_names,
+            zero_point_names,
+            scale_names,
+            nodes,
+        ) = self.quantizer.quantize_activation(node, [0])
+
+        (
+            quantized_input_names_weight,
+            zero_point_names_weight,
+            scale_names_weight,
+            nodes_weight,
+        ) = self.quantizer.quantize_weight(node, [1], reduce_range=True, op_level_per_channel=True)
+        quantized_input_names.extend(quantized_input_names_weight)
+        zero_point_names.extend(zero_point_names_weight)
+        scale_names.extend(scale_names_weight)
+        nodes.extend(nodes_weight)
+
         if quantized_input_names is None:
             return super().quantize()
 
-        qattention_name = "" if node.name == "" else node.name + "_quant"
+        qattention_name = "" if not node.name else node.name + "_quant"
 
         inputs = []
         inputs.extend(quantized_input_names)
         inputs.extend([node.input[2]])
         inputs.extend(scale_names)
         inputs.extend([node.input[3] if len(node.input) > 3 else ""])
         inputs.extend(zero_point_names)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/base_operator.py` & `onnxruntime/quantization/operators/base_operator.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,21 +1,26 @@
 class QuantOperatorBase:
     def __init__(self, onnx_quantizer, onnx_node):
         self.quantizer = onnx_quantizer
         self.node = onnx_node
 
+    def should_quantize(self):
+        if not self.quantizer.should_quantize_node(self.node):
+            return False
+
+        return self.quantizer.is_float_tensor(self.node.input[0])
+
     def quantize(self):
-        '''
+        """
         Given a node which does not support quantization, this method checks whether the input to
         this node is quantized and adds a DequantizeLinear node to dequantize this input back to FP32
             parameter node: Current node
             parameter new_nodes_list: List of new nodes created before processing current node
             return: List of new nodes created
-        '''
-        nodes = []
-        for index, node_input in enumerate(self.node.input):
+        """
+        for _, node_input in enumerate(self.node.input):
             dequantize_node = self.quantizer._dequantize_value(node_input)
             if dequantize_node is not None:
                 self.quantizer.new_nodes.append(dequantize_node)
 
         # Append the original node
-        self.quantizer.new_nodes.append(self.node)
+        self.quantizer.new_nodes.append(self.node)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/binary_op.py` & `onnxruntime/quantization/operators/binary_op.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,29 +1,39 @@
 import onnx
+from onnx import onnx_pb as onnx_proto  # noqa: F401
+
+from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType, attribute_to_kwarg, ms_domain
 from .base_operator import QuantOperatorBase
-from ..quant_utils import attribute_to_kwarg, ms_domain, QuantizedValue, QuantizedValueType
-from onnx import onnx_pb as onnx_proto
 
 
 class QLinearBinaryOp(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
 
-        data_found, output_scale_name, output_zp_name, _, _ = \
-            self.quantizer._get_quantization_params(node.output[0])
-        (quantized_input_names, zero_point_names, scale_names, nodes) = \
-            self.quantizer.quantize_inputs(node, [0, 1], initializer_use_weight_qType=False)
+        (
+            data_found,
+            output_scale_name,
+            output_zp_name,
+            _,
+            _,
+        ) = self.quantizer._get_quantization_params(node.output[0])
+        (
+            quantized_input_names,
+            zero_point_names,
+            scale_names,
+            nodes,
+        ) = self.quantizer.quantize_activation(node, [0, 1])
         if not data_found or quantized_input_names is None:
             return super().quantize()
 
-        qlinear_binary_math_output = node.output[0] + "_quantized"
-        qlinear_binary_math_name = node.name + "_quant" if node.name != "" else ""
+        qlinear_binary_math_output = node.output[0] + TENSOR_NAME_QUANT_SUFFIX
+        qlinear_binary_math_name = node.name + "_quant" if node.name else ""
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
 
         qlinear_binary_math_inputs = []
@@ -36,18 +46,27 @@
         qlinear_binary_math_inputs.append(scale_names[1])
         qlinear_binary_math_inputs.append(zero_point_names[1])
 
         # Output
         qlinear_binary_math_inputs.append(output_scale_name)
         qlinear_binary_math_inputs.append(output_zp_name)
 
-        qlinear_binary_math_node = onnx.helper.make_node("QLinear" + node.op_type, qlinear_binary_math_inputs,
-                                                         [qlinear_binary_math_output], qlinear_binary_math_name,
-                                                         **kwargs)
+        qlinear_binary_math_node = onnx.helper.make_node(
+            "QLinear" + node.op_type,
+            qlinear_binary_math_inputs,
+            [qlinear_binary_math_output],
+            qlinear_binary_math_name,
+            **kwargs,
+        )
         nodes.append(qlinear_binary_math_node)
 
         # Create an entry for this quantized value
-        q_output = QuantizedValue(node.output[0], qlinear_binary_math_output, output_scale_name, output_zp_name,
-                                  QuantizedValueType.Input)
+        q_output = QuantizedValue(
+            node.output[0],
+            qlinear_binary_math_output,
+            output_scale_name,
+            output_zp_name,
+            QuantizedValueType.Input,
+        )
         self.quantizer.quantized_value_map[node.output[0]] = q_output
 
         self.quantizer.new_nodes += nodes
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/concat.py` & `onnxruntime/quantization/operators/concat.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,47 +1,62 @@
 import onnx
+
+from ..quant_utils import (  # noqa: F401
+    TENSOR_NAME_QUANT_SUFFIX,
+    QuantizedValue,
+    QuantizedValueType,
+    attribute_to_kwarg,
+    ms_domain,
+)
 from .base_operator import QuantOperatorBase
-from .qdq_base_operator import QDQOperatorBase
-from ..quant_utils import QuantizedValue, attribute_to_kwarg, ms_domain, QuantizedValueType
+from .qdq_base_operator import QDQOperatorBase  # noqa: F401
+
 
 class QLinearConcat(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
 
-        data_found, output_scale_name, output_zp_name, _, _ = \
-            self.quantizer._get_quantization_params(node.output[0])
-        (q_input_names, zero_point_names, scale_names, nodes) = \
-            self.quantizer.quantize_inputs(node, [*range(0, len(node.input))], initializer_use_weight_qType=False)
+        (
+            data_found,
+            output_scale_name,
+            output_zp_name,
+            _,
+            _,
+        ) = self.quantizer._get_quantization_params(node.output[0])
+        (
+            q_input_names,
+            zero_point_names,
+            scale_names,
+            nodes,
+        ) = self.quantizer.quantize_activation(node, [*range(0, len(node.input))])
         if not data_found or q_input_names is None:
             return super().quantize()
 
         # Create an entry for output quantized value
         quantized_input_value = self.quantizer.quantized_value_map[node.input[0]]
-        quantized_output_value = QuantizedValue(node.output[0], node.output[0] + "_quantized",
-                                                output_scale_name, output_zp_name,
-                                                quantized_input_value.value_type)
+        quantized_output_value = QuantizedValue(
+            node.output[0],
+            node.output[0] + TENSOR_NAME_QUANT_SUFFIX,
+            output_scale_name,
+            output_zp_name,
+            quantized_input_value.value_type,
+        )
         self.quantizer.quantized_value_map[node.output[0]] = quantized_output_value
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
-        qnode_name = node.name + "_quant" if node.name != "" else ""
+        qnode_name = node.name + "_quant" if node.name else ""
 
         qlconcat_inputs = [output_scale_name, output_zp_name]
         for i in range(0, len(q_input_names)):
             qlconcat_inputs.extend([q_input_names[i], scale_names[i], zero_point_names[i]])
-        qlconcat_node = onnx.helper.make_node("QLinearConcat", qlconcat_inputs, [quantized_output_value.q_name], qnode_name, **kwargs)        
+        qlconcat_node = onnx.helper.make_node(
+            "QLinearConcat", qlconcat_inputs, [quantized_output_value.q_name], qnode_name, **kwargs
+        )
 
         self.quantizer.new_nodes += nodes
         self.quantizer.new_nodes += [qlconcat_node]
-
-class QDQConcat(QDQOperatorBase):
-    def __init__(self, onnx_quantizer, onnx_node):
-        super().__init__(onnx_quantizer, onnx_node)
-
-    def quantize(self):
-        super().quantize()
-        self.quantizer.quantize_tensor(self.node.output[0])
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/conv.py` & `onnxruntime/quantization/operators/matmul.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,186 +1,211 @@
+import itertools
+
 import onnx
-import numpy as np
+from onnx import onnx_pb as onnx_proto
+
+from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType, find_by_name, get_mul_node
 from .base_operator import QuantOperatorBase
 from .qdq_base_operator import QDQOperatorBase
-from ..quant_utils import find_by_name, get_mul_node, QuantizedValue, QuantizedValueType, attribute_to_kwarg, BiasToQuantize
-from onnx import onnx_pb as onnx_proto
 
 
-class ConvInteger(QuantOperatorBase):
+class QOpMatMul(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
-    def add_bias(self, nodes, scaled_output):
-        '''
-        Given a node, this function handles bias add by adding a "reshape" node on bias and an "add" node
-            parameter nodes: new nodes would be appended into nodes
-            parameter node: current node (Conv)
-            parameter scaled_output: output of quant conv without bias
-            parameter output: output of Conv
-            parameter bias_name: bias of Conv
-            return: the name of output
-        '''
-        node = self.node
-        model = self.quantizer.model
-        # Add tensors for the shape to be reshaped to
-        weight = find_by_name(node.input[1], model.initializer())
-        if weight is None:
-            raise ValueError("Expected {} to be an initializer".format(node.input[1]))
-
-        # Add reshape for correct broadcase
-        output = node.output[0]
-        reshape_input_data = node.input[2] # bias of Conv
-        reshape_input_shape = output + "_bias_reshape_shape"
-        reshape_output = output + "_bias_reshape_output"
-
-        shape = np.ones((len(weight.dims)), dtype=np.int64)
-        shape[1] = -1
-        init_shape = onnx.helper.make_tensor(reshape_input_shape, onnx_proto.TensorProto.INT64, [len(weight.dims)],
-                                             shape)
-        model.add_initializer(init_shape)
-
-        reshape_node = onnx.helper.make_node("Reshape", [reshape_input_data, reshape_input_shape], [reshape_output])
-        nodes.append(reshape_node)
-
-        # Add an Add operation for bias
-        add_node = onnx.helper.make_node("Add", [scaled_output, reshape_output], [output], output + "_bias_add")
-        nodes.append(add_node)
+    def should_quantize(self):
+        if not self.quantizer.should_quantize_node(self.node):
+            return False
+
+        if (not self.quantizer.is_float_tensor(self.node.input[1])) and (
+            not self.quantizer.is_float_tensor(self.node.input[0])
+        ):
+            return False
+
+        # do not quantize non-constant B matrices for matmul
+        if self.quantizer.q_matmul_const_b_only:
+            if not self.quantizer.find_initializer_in_path(self.node.input[1]):
+                print(f"Ignore MatMul due to non constant B: {self.quantizer.graph_scope}[{self.node.name}]")
+                return False
+        return True
 
-    def quantize(self):
-        node = self.node
-        assert (node.op_type == "Conv")
 
-        (quantized_input_names, zero_point_names, scale_names, nodes) = \
-            self.quantizer.quantize_inputs(node, [0, 1], reduce_range=self.quantizer.reduce_range)
+"""
+    Used when quantize mode is QuantizationMode.IntegerOps.
+"""
 
-        conv_integer_output = node.output[0] + "_output_quantized"
-        conv_integer_name = node.name + "_quant" if node.name != "" else ""
 
-        kwargs = {}
-        for attribute in node.attribute:
-            kwargs.update(attribute_to_kwarg(attribute))
-        conv_integer_node = onnx.helper.make_node("ConvInteger", quantized_input_names + zero_point_names,
-                                                  [conv_integer_output], conv_integer_name, **kwargs)
-        nodes.append(conv_integer_node)
-
-        # Add cast operation to cast convInteger output to float.
-        cast_op_output = conv_integer_output + "_cast_output"
-        cast_node = onnx.helper.make_node("Cast", [conv_integer_output], [cast_op_output],
-                                          conv_integer_output + "_cast",
-                                          to=onnx_proto.TensorProto.FLOAT)
+class MatMulInteger(QOpMatMul):
+    def __init__(self, onnx_quantizer, onnx_node):
+        super().__init__(onnx_quantizer, onnx_node)
+
+    def quantize(self):
+        node = self.node
+        assert node.op_type == "MatMul"
+        #  Get Quantized from both activation(input[0]) and weight(input[1])
+        (
+            quantized_input_names,
+            zero_point_names,
+            scale_names,
+            nodes,
+        ) = self.quantizer.quantize_activation(node, [0])
+
+        (
+            quantized_input_names_weight,
+            zero_point_names_weight,
+            scale_names_weight,
+            nodes_weight,
+        ) = self.quantizer.quantize_weight(node, [1], reduce_range=True, op_level_per_channel=True)
+        quantized_input_names.extend(quantized_input_names_weight)
+        zero_point_names.extend(zero_point_names_weight)
+        scale_names.extend(scale_names_weight)
+        nodes.extend(nodes_weight)
+
+        matmul_integer_output = node.output[0] + "_output_quantized"
+        matmul_integer_name = node.name + "_quant" if node.name else ""
+        matmul_integer_node = onnx.helper.make_node(
+            "MatMulInteger",
+            quantized_input_names + zero_point_names,
+            [matmul_integer_output],
+            matmul_integer_name,
+        )
+        nodes.append(matmul_integer_node)
+
+        # Add cast operation to cast matmulInteger output to float.
+        cast_op_output = matmul_integer_output + "_cast_output"
+        cast_node = onnx.helper.make_node(
+            "Cast",
+            [matmul_integer_output],
+            [cast_op_output],
+            matmul_integer_output + "_cast",
+            to=onnx_proto.TensorProto.FLOAT,
+        )
         nodes.append(cast_node)
 
         # Add mul operation to multiply scales of two inputs.
-        assert (len(scale_names) == 2)
-        if conv_integer_name != "":
-            scales_mul_op = conv_integer_name + "_scales_mul"
-        else:
-            scales_mul_op = scale_names[0] + "_" + scale_names[1] + "_mul"
+        assert len(scale_names) == 2
+        scales_mul_op = (
+            matmul_integer_name + "_scales_mul"
+            if matmul_integer_name
+            else scale_names[0] + "_" + scale_names[1] + "_mul"
+        )
 
         scales_mul_node = find_by_name(scales_mul_op, self.quantizer.new_nodes)
         if scales_mul_node is None:
             scales_mul_node = get_mul_node(scale_names, scales_mul_op + ":0", scales_mul_op)
             nodes.append(scales_mul_node)
 
         scales_mul_op_output = scales_mul_node.output[0]
 
-        has_bias = len(node.input) == 3
-        scaled_output_name = node.output[0] if not has_bias else node.output[0] + "quant_scaled_output"
-
-        # Add mul operation to multiply mul_scales_op result with output of ConvInteger
-        # and make the output of this node the same as output of original conv node.
-        output_scale_mul_op = conv_integer_name + "_output_scale_mul" if conv_integer_name != "" else ""
-        nodes.append(get_mul_node([cast_op_output, scales_mul_op_output], scaled_output_name, output_scale_mul_op))
+        # Add mul operation to multiply mul_scales_op result with output of MatMulInteger
+        # and make the output of this node the same as output of original matmul node.
+        output_scale_mul_op = ""
+        if matmul_integer_name:
+            output_scale_mul_op = matmul_integer_name + "_output_scale_mul"
+        nodes.append(
+            get_mul_node(
+                [cast_op_output, scales_mul_op_output],
+                node.output[0],
+                output_scale_mul_op,
+            )
+        )
+        self.quantizer.new_nodes += nodes
 
-        if has_bias:
-            self.add_bias(nodes, scaled_output_name)
 
-        self.quantizer.new_nodes += nodes
+"""
+    Used when quantize mode is QuantizationMode.QLinearOps
+"""
 
 
-class QLinearConv(QuantOperatorBase):
+class QLinearMatMul(QOpMatMul):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
-        assert (node.op_type == "Conv")
-
-        data_found, output_scale_name, output_zp_name, _, _ = \
-            self.quantizer._get_quantization_params(node.output[0])
-
-        if self.quantizer.is_input_a_weight(node.input[1]) and self.quantizer.is_per_channel():
-            (quantized_input_names, zero_point_names, scale_names, nodes) = \
-                self.quantizer.quantize_inputs(node, [0], reduce_range=self.quantizer.reduce_range)
-            quant_weight_tuple = self.quantizer.quantize_weight_per_channel(node.input[1], onnx_proto.TensorProto.INT8,
-                                                                            0)
-            quantized_input_names.append(quant_weight_tuple[0])
-            zero_point_names.append(quant_weight_tuple[1])
-            scale_names.append(quant_weight_tuple[2])
-        else:
-            (quantized_input_names, zero_point_names, scale_names, nodes) = \
-                self.quantizer.quantize_inputs(node, [0, 1], reduce_range=self.quantizer.reduce_range)
-
+        assert node.op_type == "MatMul"
+        #  Get Quantized from both activation(input[0]) and weight(input[1])
+        (
+            quantized_input_names,
+            zero_point_names,
+            scale_names,
+            nodes,
+        ) = self.quantizer.quantize_activation(node, [0])
+
+        (
+            quantized_input_names_weight,
+            zero_point_names_weight,
+            scale_names_weight,
+            nodes_weight,
+        ) = self.quantizer.quantize_weight(node, [1], reduce_range=True, op_level_per_channel=True)
+        quantized_input_names.extend(quantized_input_names_weight)
+        zero_point_names.extend(zero_point_names_weight)
+        scale_names.extend(scale_names_weight)
+
+        nodes.extend(nodes_weight)
+        (
+            data_found,
+            output_scale_name,
+            output_zp_name,
+            _,
+            _,
+        ) = self.quantizer._get_quantization_params(node.output[0])
         if not data_found or quantized_input_names is None:
             return super().quantize()
 
-        quantized_bias_name = ""
-        bias_present = False
-        if len(node.input) == 3:
-            quantized_bias_name = self.quantizer.quantize_bias_static(node.input[2], node.input[0], node.input[1])
-            bias_present = True
-
-        qlinear_conv_output = node.output[0] + "_quantized"
-        qlinear_conv_name = qlinear_conv_name = node.name + "_quant" if node.name != "" else ""
-
-        kwargs = {}
-        for attribute in node.attribute:
-            kwargs.update(attribute_to_kwarg(attribute))
-        qlinear_conv_inputs = []
+        qlinear_matmul_output = node.output[0] + TENSOR_NAME_QUANT_SUFFIX
+        qlinear_matmul_name = node.name + "_quant" if node.name else ""
+
+        qlinear_matmul_inputs = []
         # Input 0
-        qlinear_conv_inputs.append(quantized_input_names[0])
-        qlinear_conv_inputs.append(scale_names[0])
-        qlinear_conv_inputs.append(zero_point_names[0])
+        qlinear_matmul_inputs.append(quantized_input_names[0])
+        qlinear_matmul_inputs.append(scale_names[0])
+        qlinear_matmul_inputs.append(zero_point_names[0])
         # Input 1
-        qlinear_conv_inputs.append(quantized_input_names[1])
-        qlinear_conv_inputs.append(scale_names[1])
-        qlinear_conv_inputs.append(zero_point_names[1])
-
-        # Output
-        qlinear_conv_inputs.append(output_scale_name)
-        qlinear_conv_inputs.append(output_zp_name)
-
-        if bias_present:
-            qlinear_conv_inputs.append(quantized_bias_name)
-
-        qlinear_conv_node = onnx.helper.make_node("QLinearConv", qlinear_conv_inputs, [qlinear_conv_output],
-                                                  qlinear_conv_name, **kwargs)
-        nodes.append(qlinear_conv_node)
+        qlinear_matmul_inputs.append(quantized_input_names[1])
+        qlinear_matmul_inputs.append(scale_names[1])
+        qlinear_matmul_inputs.append(zero_point_names[1])
+        # Output quantization parameter
+        qlinear_matmul_inputs.append(output_scale_name)
+        qlinear_matmul_inputs.append(output_zp_name)
+
+        qlinear_matmul_node = onnx.helper.make_node(
+            "QLinearMatMul",
+            qlinear_matmul_inputs,
+            [qlinear_matmul_output],
+            qlinear_matmul_name,
+        )
+        nodes.append(qlinear_matmul_node)
 
         # Create an entry for this quantized value
-        q_output = QuantizedValue(node.output[0], qlinear_conv_output, output_scale_name, output_zp_name,
-                                  QuantizedValueType.Input)
+        q_output = QuantizedValue(
+            node.output[0],
+            qlinear_matmul_output,
+            output_scale_name,
+            output_zp_name,
+            QuantizedValueType.Input,
+        )
         self.quantizer.quantized_value_map[node.output[0]] = q_output
 
         self.quantizer.new_nodes += nodes
 
 
-class QDQConv(QDQOperatorBase):
+class QDQMatMul(QDQOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
-        assert (node.op_type == "Conv")
-
-        self.quantizer.quantize_tensor(node.input[0])
-        if not self.disable_qdq_for_node_output:
-            self.quantizer.quantize_tensor(node.output[0])
+        assert node.op_type == "MatMul"
 
-        if self.quantizer.is_per_channel():
-            self.quantizer.quantize_tensor_per_channel(node.input[1], 0)
+        if self.disable_qdq_for_node_output:
+            nodes_to_iterate = node.input
         else:
-            self.quantizer.quantize_tensor(node.input[1])
+            nodes_to_iterate = itertools.chain(node.input, node.output)
 
-        if len(node.input) == 3:
-            self.quantizer.quantize_bias_tensor(node.input[2], node.input[0], node.input[1])
+        for tensor_name in nodes_to_iterate:
+            # only support per-channel quantization on weight
+            if self.quantizer.is_per_channel() and find_by_name(tensor_name, self.quantizer.model.initializer()):
+                channel_axis = self.quantizer.qdq_op_type_per_channel_support_to_axis.get(node.op_type, 1)
+                self.quantizer.quantize_weight_tensor_per_channel(tensor_name, channel_axis)
+            else:
+                self.quantizer.quantize_activation_tensor(tensor_name)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/direct_q8.py` & `onnxruntime/quantization/operators/direct_q8.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,11 @@
+from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType
 from .base_operator import QuantOperatorBase
 from .qdq_base_operator import QDQOperatorBase
-from ..quant_utils import QuantizedValue, QuantizedValueType
+
 
 # For operators that support 8bits operations directly, and output could
 # reuse input[0]'s type, zeropoint, scale; For example,Transpose, Reshape, etc.
 class Direct8BitOp(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
@@ -15,49 +16,63 @@
             # Keep backward compatiblity
             # Quantize when input[0] is quantized already. Otherwise keep it.
             quantized_input_value = self.quantizer.find_quantized_value(node.input[0])
             if quantized_input_value is None:
                 self.quantizer.new_nodes += [node]
                 return
 
-            quantized_output_value = QuantizedValue(node.output[0], node.output[0] + "_quantized",
-                                                    quantized_input_value.scale_name, quantized_input_value.zp_name,
-                                                    quantized_input_value.value_type)
+            quantized_output_value = QuantizedValue(
+                node.output[0],
+                node.output[0] + TENSOR_NAME_QUANT_SUFFIX,
+                quantized_input_value.scale_name,
+                quantized_input_value.zp_name,
+                quantized_input_value.value_type,
+            )
             self.quantizer.quantized_value_map[node.output[0]] = quantized_output_value
 
             node.input[0] = quantized_input_value.q_name
             node.output[0] = quantized_output_value.q_name
             self.quantizer.new_nodes += [node]
 
         else:
             # Force quantize those ops if possible, use exclude node list if this is not you want
-            if (not self.quantizer.is_valid_quantize_weight(node.input[0])):
+            if not self.quantizer.is_valid_quantize_weight(node.input[0]):
                 super().quantize()
                 return
 
-            (quantized_input_names, zero_point_names, scale_names, nodes) = \
-                self.quantizer.quantize_inputs(node, [0])
+            (
+                quantized_input_names,
+                zero_point_names,
+                scale_names,
+                nodes,
+            ) = self.quantizer.quantize_activation(node, [0])
             if quantized_input_names is None:
                 return super().quantize()
 
             # Create an entry for output quantized value
-            quantized_output_value = QuantizedValue(node.output[0], node.output[0] + "_quantized",
-                                                    scale_names[0], zero_point_names[0],
-                                                    QuantizedValueType.Input)
+            quantized_output_value = QuantizedValue(
+                node.output[0],
+                node.output[0] + TENSOR_NAME_QUANT_SUFFIX,
+                scale_names[0],
+                zero_point_names[0],
+                QuantizedValueType.Input,
+            )
             self.quantizer.quantized_value_map[node.output[0]] = quantized_output_value
 
             node.input[0] = quantized_input_names[0]
             node.output[0] = quantized_output_value.q_name
             nodes.append(node)
 
             self.quantizer.new_nodes += nodes
 
 
-
 class QDQDirect8BitOp(QDQOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
-        self.quantizer.quantize_tensor(self.node.input[0])
-        if not self.disable_qdq_for_node_output:
-            self.quantizer.quantize_tensor(self.node.output[0])
+        if self.quantizer.force_quantize_no_input_check:
+            self.quantizer.quantize_activation_tensor(self.node.input[0])
+            if not self.disable_qdq_for_node_output:
+                self.quantizer.quantize_activation_tensor(self.node.output[0], self.node.input[0])
+        elif self.quantizer.is_tensor_quantized(self.node.input[0]) and not self.disable_qdq_for_node_output:
+            self.quantizer.quantize_activation_tensor(self.node.output[0], self.node.input[0])
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/embed_layernorm.py` & `onnxruntime/quantization/operators/embed_layernorm.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,50 +1,61 @@
-import onnx
 import logging
-from .base_operator import QuantOperatorBase
+
+import onnx
+from onnx import onnx_pb as onnx_proto  # noqa: F401
+
 from ..quant_utils import attribute_to_kwarg, ms_domain
-from onnx import onnx_pb as onnx_proto
+from .base_operator import QuantOperatorBase
 
-'''
+"""
 Quantizes the EmbedLayerNorm fused ONNXRuntime Op.
 
 This Quant operator keeps the input and segment IDs at int32 but will quantize all initializer and
 weight inputs associated with the node to uint8.
-'''
+"""
+
+
 class EmbedLayerNormalizationQuant(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
+    def should_quantize(self):
+        return self.quantizer.should_quantize_node(self.node)
+
     def quantize(self):
         node = self.node
-        assert (node.op_type == "EmbedLayerNormalization")
+        assert node.op_type == "EmbedLayerNormalization"
 
         if len(node.output) > 2:
             logging.info(f"Quantization is not applied to {node.name} since it has 3 outputs")
             return super().quantize()
 
-        '''
+        """
         Pre-quantization EmbedLayerNorm inputs:
         [0] input_ids (int32)
         [1] segment_ids (int32)
         [2] word_embedding (float32)
         [3] position_embedding (float32)
         [4] segment_embedding (float32)
         [5] gamma (float32)
         [6] beta (float32)
         [7] mask (int32) (optional)
-        '''
-        (quantized_input_names, zero_point_names, scale_names, nodes) = \
-            self.quantizer.quantize_inputs(node, [2, 3, 4, 5, 6])
+        """
+        (
+            quantized_input_names,
+            zero_point_names,
+            scale_names,
+            nodes,
+        ) = self.quantizer.quantize_activation(node, [2, 3, 4, 5, 6])
         if quantized_input_names is None:
             return super().quantize()
 
-        qembed_layer_norm_name = "" if node.name == "" else node.name + "_quant"
+        qembed_layer_norm_name = "" if not node.name else node.name + "_quant"
 
-        '''
+        """
         Quantized Input Tensor List
         [0] input_ids (int32)
         [1] segment_ids (int32)
         [2] word_embedding (uint8)
         [3] position_embedding (uint8)
         [4] segment_embedding (uint8)
         [5] gamma (uint8)
@@ -56,15 +67,15 @@
         [11] gamma_scale (float)
         [12] beta_scale (float)
         [13] word_embedding_zero_point (uint8)
         [14] position_embedding_zero_point (uint8)
         [15] segment_embedding_zero_point (uint8)
         [16] gamma_zero_point (uint8)
         [17] beta_zero_point (uint8)
-        '''
+        """
         inputs = []
         # 'input_ids'
         inputs.extend([node.input[0]])
         # 'segment_ids'
         inputs.extend([node.input[1]])
         # 'word_embedding_quant'
         inputs.extend([quantized_input_names[0]])
@@ -94,12 +105,17 @@
         inputs.extend([zero_point_names[4]])
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
 
-        qembed_layer_norm_node = onnx.helper.make_node("QEmbedLayerNormalization", inputs, node.output,
-                                                       qembed_layer_norm_name, **kwargs)
+        qembed_layer_norm_node = onnx.helper.make_node(
+            "QEmbedLayerNormalization",
+            inputs,
+            node.output,
+            qembed_layer_norm_name,
+            **kwargs,
+        )
         nodes.append(qembed_layer_norm_node)
 
         self.quantizer.new_nodes += nodes
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/gavgpool.py` & `onnxruntime/quantization/operators/gavgpool.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,42 +1,62 @@
 import onnx
+
+from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType, attribute_to_kwarg, ms_domain
 from .base_operator import QuantOperatorBase
-from ..quant_utils import attribute_to_kwarg, ms_domain, QuantizedValue, QuantizedValueType
 
 
 class QGlobalAveragePool(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
-        assert (node.op_type == "GlobalAveragePool")
+        assert node.op_type == "GlobalAveragePool"
 
         # If input to this node is not quantized then keep this node.
         if node.input[0] not in self.quantizer.quantized_value_map:
             return super().quantize()
 
         quantized_input_value = self.quantizer.quantized_value_map[node.input[0]]
 
         # Create an entry for output quantized value.
         quantized_input_value = self.quantizer.quantized_value_map[node.input[0]]
-        data_found, output_scale_name_from_parameter, output_zp_name_from_parameter, _, _ = \
-            self.quantizer._get_quantization_params(node.output[0])
+        (
+            data_found,
+            output_scale_name_from_parameter,
+            output_zp_name_from_parameter,
+            _,
+            _,
+        ) = self.quantizer._get_quantization_params(node.output[0])
         # Just use input scale and zp if parameters for output is not specified.
         output_scale_name = output_scale_name_from_parameter if data_found else quantized_input_value.scale_name
         output_zp_name = output_zp_name_from_parameter if data_found else quantized_input_value.zp_name
-        quantized_output_value = QuantizedValue(node.output[0], node.output[0] + "_quantized", output_scale_name,
-                                                output_zp_name, QuantizedValueType.Input)
+        quantized_output_value = QuantizedValue(
+            node.output[0],
+            node.output[0] + TENSOR_NAME_QUANT_SUFFIX,
+            output_scale_name,
+            output_zp_name,
+            QuantizedValueType.Input,
+        )
         self.quantizer.quantized_value_map[node.output[0]] = quantized_output_value
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
         kwargs["channels_last"] = 0
-        qnode_name = node.name + "_quant" if node.name != "" else ""
+        qnode_name = node.name + "_quant" if node.name else ""
 
-        qnode = onnx.helper.make_node("QLinear" + node.op_type, [
-            quantized_input_value.q_name, quantized_input_value.scale_name, quantized_input_value.zp_name,
-            output_scale_name, output_zp_name
-        ], [quantized_output_value.q_name], qnode_name, **kwargs)
+        qnode = onnx.helper.make_node(
+            "QLinear" + node.op_type,
+            [
+                quantized_input_value.q_name,
+                quantized_input_value.scale_name,
+                quantized_input_value.zp_name,
+                output_scale_name,
+                output_zp_name,
+            ],
+            [quantized_output_value.q_name],
+            qnode_name,
+            **kwargs,
+        )
         self.quantizer.new_nodes += [qnode]
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/gemm.py` & `onnxruntime/quantization/operators/gemm.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,117 +1,160 @@
-import onnx
-import numpy as np
 import logging
-from .base_operator import QuantOperatorBase
-from .qdq_base_operator import QDQOperatorBase
-from ..quant_utils import find_by_name, get_mul_node, QuantizedValue, QuantizedValueType, attribute_to_kwarg, ms_domain
+
+import numpy as np  # noqa: F401
+import onnx
 from onnx import onnx_pb as onnx_proto
 
+from ..quant_utils import find_by_name  # noqa: F401
+from ..quant_utils import get_mul_node  # noqa: F401
+from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType, attribute_to_kwarg, ms_domain
+from .base_operator import QuantOperatorBase  # noqa: F401
+from .matmul import QOpMatMul
+from .qdq_base_operator import QDQOperatorBase
+
 
-def is_B_transposed(gemm_node):
-    transB_attribute = [attr for attr in gemm_node.attribute if attr.name == 'transB']
+def is_B_transposed(gemm_node):  # noqa: N802
+    transB_attribute = [attr for attr in gemm_node.attribute if attr.name == "transB"]  # noqa: N806
     if len(transB_attribute):
-        return 0 < onnx.helper.get_attribute_value(transB_attribute[0])
+        return onnx.helper.get_attribute_value(transB_attribute[0]) > 0
 
     return False
 
+
 def get_beta(gemm_node):
-    beta_attribute = [attr for attr in gemm_node.attribute if attr.name == 'beta']
+    beta_attribute = [attr for attr in gemm_node.attribute if attr.name == "beta"]
     if len(beta_attribute):
         return onnx.helper.get_attribute_value(beta_attribute[0])
 
     return 1.0
 
+
 def set_default_beta(gemm_node):
-    beta_attribute = [attr for attr in gemm_node.attribute if attr.name == 'beta']
+    beta_attribute = [attr for attr in gemm_node.attribute if attr.name == "beta"]
     if len(beta_attribute):
         beta_attribute[0].f = 1.0
 
     return 1.0
 
-class QLinearGemm(QuantOperatorBase):
+
+class QLinearGemm(QOpMatMul):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
-        assert (node.op_type == "Gemm")
+        assert node.op_type == "Gemm"
 
-        data_found, output_scale_name, output_zp_name, _, _ = \
-            self.quantizer._get_quantization_params(node.output[0])
-
-        if self.quantizer.is_input_a_weight(node.input[1]) and self.quantizer.is_per_channel():
-            (quantized_input_names, zero_point_names, scale_names, nodes) = \
-                self.quantizer.quantize_inputs(node, [0], reduce_range=self.quantizer.reduce_range)
-            quant_weight_tuple = self.quantizer.quantize_weight_per_channel(node.input[1], onnx_proto.TensorProto.INT8,
-                                                                            0 if is_B_transposed(node) else 1)
+        (
+            data_found,
+            output_scale_name,
+            output_zp_name,
+            _,
+            _,
+        ) = self.quantizer._get_quantization_params(node.output[0])
+
+        if self.quantizer.is_input_a_initializer(node.input[1]) and self.quantizer.is_per_channel():
+            (
+                quantized_input_names,
+                zero_point_names,
+                scale_names,
+                nodes,
+            ) = self.quantizer.quantize_activation(node, [0])
+            quant_weight_tuple = self.quantizer.quantize_weight_per_channel(
+                node.input[1],
+                onnx_proto.TensorProto.INT8,
+                0 if is_B_transposed(node) else 1,
+            )
             quantized_input_names.append(quant_weight_tuple[0])
             zero_point_names.append(quant_weight_tuple[1])
             scale_names.append(quant_weight_tuple[2])
         else:
-            (quantized_input_names, zero_point_names, scale_names, nodes) = \
-                self.quantizer.quantize_inputs(node, [0, 1], reduce_range=self.quantizer.reduce_range)
+            #  Get Quantized from both activation(input[0]) and weight(input[1])
+            (
+                quantized_input_names,
+                zero_point_names,
+                scale_names,
+                nodes,
+            ) = self.quantizer.quantize_activation(node, [0])
+
+            (
+                quantized_input_names_weight,
+                zero_point_names_weight,
+                scale_names_weight,
+                nodes_weight,
+            ) = self.quantizer.quantize_weight(node, [1], reduce_range=self.quantizer.reduce_range)
+            quantized_input_names.extend(quantized_input_names_weight)
+            zero_point_names.extend(zero_point_names_weight)
+            scale_names.extend(scale_names_weight)
+            nodes.extend(nodes_weight)
 
         if not data_found or quantized_input_names is None:
             return super().quantize()
 
         quantized_bias_name = ""
         if len(node.input) == 3:
-            if not self.quantizer.is_input_a_weight(node.input[2]):
+            if not self.quantizer.is_input_a_initializer(node.input[2]):
                 return super().quantize()
 
-            quantized_bias_name = self.quantizer.quantize_bias_static(node.input[2], node.input[0], node.input[1], get_beta(self.node))
+            quantized_bias_name = self.quantizer.quantize_bias_static(
+                node.input[2], node.input[0], node.input[1], get_beta(self.node)
+            )
 
-        qgemm_output = node.output[0] + "_quantized"
-        qgemm_name = qgemm_name = node.name + "_quant" if node.name != "" else ""
+        qgemm_output = node.output[0] + TENSOR_NAME_QUANT_SUFFIX
+        qgemm_name = node.name + "_quant" if node.name else ""
 
         kwargs = {}
         for attribute in node.attribute:
             if attribute.name != "beta":
                 kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
 
         # generate input
         qgemm_inputs = []
         for i in range(2):
             qgemm_inputs.extend([quantized_input_names[i], scale_names[i], zero_point_names[i]])
 
         qgemm_inputs.extend([quantized_bias_name, output_scale_name, output_zp_name])
 
-        qgemm_node = onnx.helper.make_node("QGemm", qgemm_inputs, [qgemm_output],
-                                                  qgemm_name, **kwargs)
+        qgemm_node = onnx.helper.make_node("QGemm", qgemm_inputs, [qgemm_output], qgemm_name, **kwargs)
         nodes.append(qgemm_node)
 
         # Create an entry for this quantized value
-        q_output = QuantizedValue(node.output[0], qgemm_output, output_scale_name, output_zp_name,
-                                  QuantizedValueType.Input)
+        q_output = QuantizedValue(
+            node.output[0],
+            qgemm_output,
+            output_scale_name,
+            output_zp_name,
+            QuantizedValueType.Input,
+        )
         self.quantizer.quantized_value_map[node.output[0]] = q_output
 
         self.quantizer.new_nodes += nodes
 
 
 class QDQGemm(QDQOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
-        assert (node.op_type == "Gemm")
+        assert node.op_type == "Gemm"
 
-        self.quantizer.quantize_tensor(node.input[0])
+        self.quantizer.quantize_activation_tensor(node.input[0])
         if not self.disable_qdq_for_node_output:
-            self.quantizer.quantize_tensor(node.output[0])
+            self.quantizer.quantize_activation_tensor(node.output[0])
 
         if self.quantizer.is_per_channel():
-            self.quantizer.quantize_tensor_per_channel(node.input[1], 0 if is_B_transposed(node) else 1)
+            self.quantizer.quantize_weight_tensor_per_channel(node.input[1], 0 if is_B_transposed(node) else 1)
         else:
-            self.quantizer.quantize_tensor(node.input[1])
+            self.quantizer.quantize_weight_tensor(node.input[1])
 
         if len(node.input) == 3:
-            if self.quantizer.is_input_a_weight(node.input[2]):
+            if self.quantizer.is_input_a_initializer(node.input[2]):
                 self.quantizer.quantize_bias_tensor(node.input[2], node.input[0], node.input[1], get_beta(self.node))
                 set_default_beta(self.node)
             else:
                 logging.warning(
-                    "Bias of Gemm node '{}' is not constant. Please exclude this node for better performance."
-                    .format(self.node.name))
-
+                    "Bias of Gemm node '{}' is not constant. Please exclude this node for better performance.".format(
+                        self.node.name
+                    )
+                )
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/maxpool.py` & `onnxruntime/quantization/operators/maxpool.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 
 class QMaxPool(Direct8BitOp):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
-        assert (node.op_type == "MaxPool")
+        assert node.op_type == "MaxPool"
 
         # if version is less than 12, go to normal quantize.
         if self.quantizer.opset_version < 12:
             super(Direct8BitOp, self).quantize()
             return
 
         # Direct 8bits op
@@ -20,15 +20,15 @@
 
 class QDQMaxPool(QDQDirect8BitOp):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
-        assert (node.op_type == "MaxPool")
+        assert node.op_type == "MaxPool"
 
         # if version is less than 12, just no change
         if self.quantizer.opset_version < 12:
             return
 
         # Direct 8bits op
         return super().quantize()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/pad.py` & `onnxruntime/quantization/operators/pad.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,69 +1,89 @@
 import onnx
-import numpy as np
+
+from ..quant_utils import (
+    TENSOR_NAME_QUANT_SUFFIX,
+    QuantizedValue,
+    QuantizedValueType,
+    attribute_to_kwarg,
+    quantize_nparray,
+)
 from .base_operator import QuantOperatorBase
-from ..quant_utils import QuantizedValue, QuantizedValueType, attribute_to_kwarg, quantize_nparray
 
 
 class QPad(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
-        assert (node.op_type == "Pad")
+        assert node.op_type == "Pad"
 
         # Only after version 11, it has the optional constant_value
         # If input[0] is not quantized, do not quanitize this node
         if (self.quantizer.opset_version < 11) or (node.input[0] not in self.quantizer.quantized_value_map):
             super().quantize()
             return
         quantized_input_value = self.quantizer.quantized_value_map[node.input[0]]
 
         kwargs = {}
         for attribute in node.attribute:
             kv = attribute_to_kwarg(attribute)
             kwargs.update(kv)
 
-        if 'mode' not in kwargs or kwargs['mode'] == b'constant':
+        if "mode" not in kwargs or kwargs["mode"] == b"constant":
             if len(node.input) > 2:  # There is 3rd input 'constant_value'
                 zp_tensor = self.quantizer.model.get_initializer(quantized_input_value.zp_name)
                 scale_tensor = self.quantizer.model.get_initializer(quantized_input_value.scale_name)
                 if zp_tensor is None or scale_tensor is None:
                     super().quantize()
                     return
 
                 padding_constant_initializer = self.quantizer.model.get_initializer(node.input[2])
                 if padding_constant_initializer is not None:
                     zp_array = onnx.numpy_helper.to_array(zp_tensor)
                     zp_value = zp_array.item() if zp_array.ndim == 0 else zp_array[0]
                     scale_array = onnx.numpy_helper.to_array(scale_tensor)
                     scale_value = scale_array.item() if scale_array.ndim == 0 else scale_array[0]
                     padding_constant_array = onnx.numpy_helper.to_array(padding_constant_initializer)
-                    quantized_padding_constant_array = quantize_nparray(self.quantizer.input_qType,
-                                                                        padding_constant_array, scale_value, zp_value)
-                    quantized_padding_constant_name = node.input[2] + "_quantized"
+                    quantized_padding_constant_array = quantize_nparray(
+                        self.quantizer.activation_qType,
+                        padding_constant_array,
+                        scale_value,
+                        zp_value,
+                    )
+                    quantized_padding_constant_name = node.input[2] + TENSOR_NAME_QUANT_SUFFIX
                     quantized_padding_constant_initializer = onnx.numpy_helper.from_array(
-                        quantized_padding_constant_array, quantized_padding_constant_name)
+                        quantized_padding_constant_array,
+                        quantized_padding_constant_name,
+                    )
                     # Suppose this padding constant initializer only used by the node
                     self.quantizer.model.remove_initializer(padding_constant_initializer)
                     self.quantizer.model.add_initializer(quantized_padding_constant_initializer)
                     node.input[2] = quantized_padding_constant_name
                 else:
                     # TODO: check quantize_inputs after sub graph is supported
-                    pad_value_qnodes = self.quantizer._get_quantize_input_nodes(node, 2, self.quantizer.input_qType,
-                                                                                quantized_input_value.scale_name,
-                                                                                quantized_input_value.zp_name)
-                    self.quantizer.new_nodes += [pad_value_qnodes]
-                    node.input[2] = pad_value_qnodes.output[0]
+                    pad_value_qnodes = self.quantizer._get_quantize_input_nodes(
+                        node,
+                        2,
+                        self.quantizer.activation_qType,
+                        quantized_input_value.scale_name,
+                        quantized_input_value.zp_name,
+                    )
+                    self.quantizer.new_nodes.extend(pad_value_qnodes)
+                    node.input[2] = pad_value_qnodes[0].output[0]
             else:
                 node.input.extend([quantized_input_value.zp_name])  # pad zero_point for original zero
 
         # Create an entry for output quantized value
-        quantized_output_value = QuantizedValue(node.output[0], node.output[0] + "_quantized",
-                                                quantized_input_value.scale_name, quantized_input_value.zp_name,
-                                                QuantizedValueType.Input)
+        quantized_output_value = QuantizedValue(
+            node.output[0],
+            node.output[0] + TENSOR_NAME_QUANT_SUFFIX,
+            quantized_input_value.scale_name,
+            quantized_input_value.zp_name,
+            QuantizedValueType.Input,
+        )
         self.quantizer.quantized_value_map[node.output[0]] = quantized_output_value
 
         node.input[0] = quantized_input_value.q_name
         node.output[0] = quantized_output_value.q_name
         self.quantizer.new_nodes += [node]
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/pooling.py` & `onnxruntime/quantization/operators/pooling.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,43 +1,67 @@
 import onnx
+
+from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType, attribute_to_kwarg, ms_domain
 from .base_operator import QuantOperatorBase
-from ..quant_utils import attribute_to_kwarg, ms_domain, QuantizedValue, QuantizedValueType
+
 
 class QLinearPool(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
 
         # only try to quantize when given quantization parameters for it
-        data_found, output_scale_name, output_zp_name, _, _ = \
-            self.quantizer._get_quantization_params(node.output[0])
+        (
+            data_found,
+            output_scale_name,
+            output_zp_name,
+            _,
+            _,
+        ) = self.quantizer._get_quantization_params(node.output[0])
 
         # get quantized input tensor names, quantize input if needed
-        quantized_input_names, input_zero_point_names, input_scale_names, nodes = self.quantizer.quantize_inputs(node, [0])
+        (
+            quantized_input_names,
+            input_zero_point_names,
+            input_scale_names,
+            nodes,
+        ) = self.quantizer.quantize_activation(node, [0])
 
         if not data_found or quantized_input_names is None:
             return super().quantize()
 
         # Create an entry for output quantized value.
-        qlinear_output_name = node.output[0] + "_quantized"
+        qlinear_output_name = node.output[0] + TENSOR_NAME_QUANT_SUFFIX
         quantized_output_value = QuantizedValue(
-            node.output[0], qlinear_output_name, output_scale_name, output_zp_name, QuantizedValueType.Input)
+            node.output[0],
+            qlinear_output_name,
+            output_scale_name,
+            output_zp_name,
+            QuantizedValueType.Input,
+        )
         self.quantizer.quantized_value_map[node.output[0]] = quantized_output_value
 
         # Create qlinear pool node for given type (AveragePool, etc)
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
-        qlinear_node_name = node.name + "_quant" if node.name != "" else ""
+        qlinear_node_name = node.name + "_quant" if node.name else ""
         qnode = onnx.helper.make_node(
             "QLinear" + node.op_type,
-            [quantized_input_names[0], input_scale_names[0], input_zero_point_names[0], output_scale_name, output_zp_name],
+            [
+                quantized_input_names[0],
+                input_scale_names[0],
+                input_zero_point_names[0],
+                output_scale_name,
+                output_zp_name,
+            ],
             [qlinear_output_name],
             qlinear_node_name,
-            **kwargs)
+            **kwargs,
+        )
 
         # add all newly created nodes
         nodes.append(qnode)
         self.quantizer.new_nodes += nodes
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/resize.py` & `onnxruntime/quantization/operators/resize.py`

 * *Files 3% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 
 class QResize(Direct8BitOp):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
-        assert (node.op_type == "Resize")
+        assert node.op_type == "Resize"
 
         # if version is less than 11, go to normal quantize.
         if self.quantizer.opset_version < 11:
             super(Direct8BitOp, self).quantize()
             return
 
         # Direct 8bits op
@@ -20,15 +20,15 @@
 
 class QDQResize(QDQDirect8BitOp):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
-        assert (node.op_type == "Resize")
+        assert node.op_type == "Resize"
 
         # if version is less than 11, just keep this node
         if self.quantizer.opset_version < 11:
             return
 
         # Direct 8bits op
         return super().quantize()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/quantization/operators/split.py` & `onnxruntime/quantization/operators/split.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,39 +1,63 @@
 import onnx
-from .base_operator import QuantOperatorBase
+
 from ..quant_utils import QuantizedValue, QuantizedValueType, attribute_to_kwarg
-from onnx import onnx_pb as onnx_proto
+from .base_operator import QuantOperatorBase
+from .qdq_base_operator import QDQOperatorBase
 
 
 class QSplit(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
-        quantized_input_names, zero_point_names, scale_names, nodes = self.quantizer.quantize_inputs(node, [0])
+        (
+            quantized_input_names,
+            zero_point_names,
+            scale_names,
+            nodes,
+        ) = self.quantizer.quantize_activation(node, [0])
         if quantized_input_names is None:
             return super().quantize()
 
         quantized_node_name = ""
-        if node.name != "":
+        if node.name:
             quantized_node_name = node.name + "_quant"
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
 
         # Output just derive the scale/zero from input
         quantized_output_names = []
         for output_name in node.output:
             quantized_output_name = output_name + "quantized"
             quantized_output_names.append(quantized_output_name)
-            q_output = QuantizedValue(output_name, quantized_output_name, scale_names[0], zero_point_names[0],
-                                      QuantizedValueType.Input)
+            q_output = QuantizedValue(
+                output_name,
+                quantized_output_name,
+                scale_names[0],
+                zero_point_names[0],
+                QuantizedValueType.Input,
+            )
             self.quantizer.quantized_value_map[output_name] = q_output
 
         if len(node.input) > 1:
-            quantized_input_names = quantized_input_names.extend(node.input[1:])
-        quantized_node = onnx.helper.make_node(node.op_type, quantized_input_names, quantized_output_names,
-                                               quantized_node_name, **kwargs)
+            quantized_input_names.extend(node.input[1:])
+        quantized_node = onnx.helper.make_node(
+            node.op_type, quantized_input_names, quantized_output_names, quantized_node_name, **kwargs
+        )
 
         nodes.append(quantized_node)
         self.quantizer.new_nodes += nodes
+
+
+class QDQSplit(QDQOperatorBase):
+    def quantize(self):
+        node = self.node
+        assert node.op_type == "Split"
+
+        if not self.quantizer.is_tensor_quantized(node.input[0]):
+            self.quantizer.quantize_activation_tensor(node.input[0])
+        if not self.disable_qdq_for_node_output:
+            for output in node.output:
+                self.quantizer.quantize_activation_tensor(output, node.input[0])
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/__init__.py` & `onnxruntime/tools/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 # appended to the __init__.py in the onnxruntime module's 'tools' folder from /tools/python/util/__init__append.py
 import importlib.util
+
 have_torch = importlib.util.find_spec("torch")
 if have_torch:
-    from .pytorch_export_helpers import infer_input_info  # noqa
+    from .pytorch_export_helpers import infer_input_info  # noqa: F401
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/check_onnx_model_mobile_usability.py` & `onnxruntime/tools/check_onnx_model_mobile_usability.py`

 * *Files 18% similar despite different names*

```diff
@@ -2,59 +2,66 @@
 # Licensed under the MIT License.
 
 import argparse
 import logging
 import pathlib
 
 # need this before the mobile helper imports for some reason
-logging.basicConfig(format='%(levelname)s:  %(message)s')
+logging.basicConfig(format="%(levelname)s:  %(message)s")
 
-from .mobile_helpers import check_model_can_use_ort_mobile_pkg, usability_checker  # noqa
+from .mobile_helpers import check_model_can_use_ort_mobile_pkg, usability_checker  # noqa: E402
 
 
 def check_usability():
     parser = argparse.ArgumentParser(
-        description='''Analyze an ONNX model to determine how well it will work in mobile scenarios, and whether
-        it is likely to be able to use the pre-built ONNX Runtime Mobile Android or iOS package.''',
-        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
-
-    parser.add_argument('--config_path',
-                        help='Path to required operators and types configuration used to build '
-                             'the pre-built ORT mobile package.',
-                        required=False,
-                        type=pathlib.Path,
-                        default=check_model_can_use_ort_mobile_pkg.get_default_config_path())
-    parser.add_argument('--log_level', choices=['debug', 'info', 'warning', 'error'],
-                        default='info', help='Logging level')
-    parser.add_argument('model_path', help='Path to ONNX model to check', type=pathlib.Path)
+        description="""Analyze an ONNX model to determine how well it will work in mobile scenarios, and whether
+        it is likely to be able to use the pre-built ONNX Runtime Mobile Android or iOS package.""",
+        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
+    )
+
+    parser.add_argument(
+        "--config_path",
+        help="Path to required operators and types configuration used to build the pre-built ORT mobile package.",
+        required=False,
+        type=pathlib.Path,
+        default=check_model_can_use_ort_mobile_pkg.get_default_config_path(),
+    )
+    parser.add_argument(
+        "--log_level", choices=["debug", "info", "warning", "error"], default="info", help="Logging level"
+    )
+    parser.add_argument("model_path", help="Path to ONNX model to check", type=pathlib.Path)
 
     args = parser.parse_args()
-    logger = logging.getLogger('check_usability')
+    logger = logging.getLogger("check_usability")
 
-    if args.log_level == 'debug':
+    if args.log_level == "debug":
         logger.setLevel(logging.DEBUG)
-    elif args.log_level == 'info':
+    elif args.log_level == "info":
         logger.setLevel(logging.INFO)
-    elif args.log_level == 'warning':
+    elif args.log_level == "warning":
         logger.setLevel(logging.WARNING)
     else:
         logger.setLevel(logging.ERROR)
 
     try_eps = usability_checker.analyze_model(args.model_path, skip_optimize=False, logger=logger)
     check_model_can_use_ort_mobile_pkg.run_check(args.model_path, args.config_path, logger)
 
-    logger.info("Run `python -m onnxruntime.tools.convert_onnx_models_to_ort ...` to convert the ONNX model to ORT "
-                "format. "
-                "By default, the conversion tool will create an ORT format model with saved optimizations which can "
-                "potentially be applied at runtime (with a .with_runtime_opt.ort file extension) for use with NNAPI "
-                "or CoreML, and a fully optimized ORT format model (with a .ort file extension) for use with the CPU "
-                "EP.")
+    logger.info(
+        "Run `python -m onnxruntime.tools.convert_onnx_models_to_ort ...` to convert the ONNX model to ORT "
+        "format. "
+        "By default, the conversion tool will create an ORT format model with saved optimizations which can "
+        "potentially be applied at runtime (with a .with_runtime_opt.ort file extension) for use with NNAPI "
+        "or CoreML, and a fully optimized ORT format model (with a .ort file extension) for use with the CPU "
+        "EP."
+    )
     if try_eps:
-        logger.info("As NNAPI or CoreML may provide benefits with this model it is recommended to compare the "
-                    "performance of the <model>.with_runtime_opt.ort model using the NNAPI EP on Android, and the "
-                    "CoreML EP on iOS, against the performance of the <model>.ort model using the CPU EP.")
+        logger.info(
+            "As NNAPI or CoreML may provide benefits with this model it is recommended to compare the "
+            "performance of the <model>.with_runtime_opt.ort model using the NNAPI EP on Android, and the "
+            "CoreML EP on iOS, against the performance of the <model>.ort model using the CPU EP."
+        )
     else:
         logger.info("For optimal performance the <model>.ort model should be used with the CPU EP. ")
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     check_usability()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/convert_onnx_models_to_ort.py` & `onnxruntime/tools/convert_onnx_models_to_ort.py`

 * *Files 12% similar despite different names*

```diff
@@ -7,62 +7,86 @@
 import enum
 import os
 import pathlib
 import tempfile
 import typing
 
 import onnxruntime as ort
+
 from .file_utils import files_from_file_or_dir, path_match_suffix_ignore_case
 from .onnx_model_utils import get_optimization_level
 from .ort_format_model import create_config_from_models
 
 
 class OptimizationStyle(enum.Enum):
     Fixed = 0
     Runtime = 1
 
 
-def _optimization_suffix(optimization_style: OptimizationStyle, suffix: str):
-    return "{}{}".format(".with_runtime_opt" if optimization_style == OptimizationStyle.Runtime else "",
-                         suffix)
+def _optimization_suffix(optimization_level_str: str, optimization_style: OptimizationStyle, suffix: str):
+    return "{}{}{}".format(
+        f".{optimization_level_str}" if optimization_level_str != "all" else "",
+        ".with_runtime_opt" if optimization_style == OptimizationStyle.Runtime else "",
+        suffix,
+    )
+
 
+def _create_config_file_path(
+    model_path_or_dir: pathlib.Path,
+    output_dir: typing.Optional[pathlib.Path],
+    optimization_level_str: str,
+    optimization_style: OptimizationStyle,
+    enable_type_reduction: bool,
+):
+    config_name = "{}{}".format(
+        "required_operators_and_types" if enable_type_reduction else "required_operators",
+        _optimization_suffix(optimization_level_str, optimization_style, ".config"),
+    )
 
-def _create_config_file_path(model_path_or_dir: pathlib.Path,
-                             optimization_style: OptimizationStyle,
-                             enable_type_reduction: bool):
-    config_name = "{}{}".format('required_operators_and_types' if enable_type_reduction else 'required_operators',
-                                _optimization_suffix(optimization_style, ".config"))
     if model_path_or_dir.is_dir():
-        return model_path_or_dir / config_name
-    return model_path_or_dir.with_suffix(f".{config_name}")
+        return (output_dir or model_path_or_dir) / config_name
+
+    model_config_path = model_path_or_dir.with_suffix(f".{config_name}")
+
+    if output_dir is not None:
+        return output_dir / model_config_path.name
+
+    return model_config_path
 
 
-def _create_session_options(optimization_level: ort.GraphOptimizationLevel,
-                            output_model_path: pathlib.Path,
-                            custom_op_library: pathlib.Path,
-                            session_options_config_entries: typing.Dict[str, str]):
+def _create_session_options(
+    optimization_level: ort.GraphOptimizationLevel,
+    output_model_path: pathlib.Path,
+    custom_op_library: pathlib.Path,
+    session_options_config_entries: typing.Dict[str, str],
+):
     so = ort.SessionOptions()
     so.optimized_model_filepath = str(output_model_path)
     so.graph_optimization_level = optimization_level
 
     if custom_op_library:
         so.register_custom_ops_library(str(custom_op_library))
 
     for key, value in session_options_config_entries.items():
         so.add_session_config_entry(key, value)
 
     return so
 
 
-def _convert(model_path_or_dir: pathlib.Path, output_dir: typing.Optional[pathlib.Path],
-             optimization_level_str: str, optimization_style: OptimizationStyle,
-             custom_op_library: pathlib.Path, create_optimized_onnx_model: bool, allow_conversion_failures: bool,
-             target_platform: str, session_options_config_entries: typing.Dict[str, str]) \
-        -> typing.List[pathlib.Path]:
-
+def _convert(
+    model_path_or_dir: pathlib.Path,
+    output_dir: typing.Optional[pathlib.Path],
+    optimization_level_str: str,
+    optimization_style: OptimizationStyle,
+    custom_op_library: pathlib.Path,
+    create_optimized_onnx_model: bool,
+    allow_conversion_failures: bool,
+    target_platform: str,
+    session_options_config_entries: typing.Dict[str, str],
+) -> typing.List[pathlib.Path]:
     model_dir = model_path_or_dir if model_path_or_dir.is_dir() else model_path_or_dir.parent
     output_dir = output_dir or model_dir
 
     optimization_level = get_optimization_level(optimization_level_str)
 
     def is_model_file_to_convert(file_path: pathlib.Path):
         if not path_match_suffix_ignore_case(file_path, ".onnx"):
@@ -73,206 +97,265 @@
             print(f"Ignoring '{file_path}'")
             return False
         return True
 
     models = files_from_file_or_dir(model_path_or_dir, is_model_file_to_convert)
 
     if len(models) == 0:
-        raise ValueError("No model files were found in '{}'".format(model_path_or_dir))
+        raise ValueError(f"No model files were found in '{model_path_or_dir}'")
 
-    providers = ['CPUExecutionProvider']
+    providers = ["CPUExecutionProvider"]
 
     # if the optimization level is 'all' we manually exclude the NCHWc transformer. It's not applicable to ARM
     # devices, and creates a device specific model which won't run on all hardware.
     # If someone really really really wants to run it they could manually create an optimized onnx model first,
     # or they could comment out this code.
     optimizer_filter = None
-    if optimization_level == ort.GraphOptimizationLevel.ORT_ENABLE_ALL and target_platform != 'amd64':
-        optimizer_filter = ['NchwcTransformer']
+    if optimization_level == ort.GraphOptimizationLevel.ORT_ENABLE_ALL and target_platform != "amd64":
+        optimizer_filter = ["NchwcTransformer"]
 
     converted_models = []
 
     for model in models:
         try:
-
             relative_model_path = model.relative_to(model_dir)
 
             (output_dir / relative_model_path).parent.mkdir(parents=True, exist_ok=True)
 
             ort_target_path = (output_dir / relative_model_path).with_suffix(
-                _optimization_suffix(optimization_style, ".ort"))
+                _optimization_suffix(optimization_level_str, optimization_style, ".ort")
+            )
 
             if create_optimized_onnx_model:
                 # Create an ONNX file with the same optimization level that will be used for the ORT format file.
                 # This allows the ONNX equivalent of the ORT format model to be easily viewed in Netron.
                 # If runtime optimizations are saved in the ORT format model, there may be some difference in the
                 # graphs at runtime between the ORT format model and this saved ONNX model.
-                optimized_target_path = (output_dir / relative_model_path).with_suffix(".optimized.onnx")
-                so = _create_session_options(optimization_level, optimized_target_path, custom_op_library,
-                                             session_options_config_entries)
+                optimized_target_path = (output_dir / relative_model_path).with_suffix(
+                    _optimization_suffix(optimization_level_str, optimization_style, ".optimized.onnx")
+                )
+                so = _create_session_options(
+                    optimization_level, optimized_target_path, custom_op_library, session_options_config_entries
+                )
                 if optimization_style == OptimizationStyle.Runtime:
                     # Limit the optimizations to those that can run in a model with runtime optimizations.
-                    so.add_session_config_entry('optimization.minimal_build_optimizations', 'apply')
+                    so.add_session_config_entry("optimization.minimal_build_optimizations", "apply")
 
-                print("Saving optimized ONNX model {} to {}".format(model, optimized_target_path))
-                _ = ort.InferenceSession(str(model), sess_options=so, providers=providers,
-                                         disabled_optimizers=optimizer_filter)
+                print(f"Saving optimized ONNX model {model} to {optimized_target_path}")
+                _ = ort.InferenceSession(
+                    str(model), sess_options=so, providers=providers, disabled_optimizers=optimizer_filter
+                )
 
             # Load ONNX model, optimize, and save to ORT format
-            so = _create_session_options(optimization_level, ort_target_path, custom_op_library,
-                                         session_options_config_entries)
-            so.add_session_config_entry('session.save_model_format', 'ORT')
+            so = _create_session_options(
+                optimization_level, ort_target_path, custom_op_library, session_options_config_entries
+            )
+            so.add_session_config_entry("session.save_model_format", "ORT")
             if optimization_style == OptimizationStyle.Runtime:
-                so.add_session_config_entry('optimization.minimal_build_optimizations', 'save')
+                so.add_session_config_entry("optimization.minimal_build_optimizations", "save")
 
-            print("Converting optimized ONNX model {} to ORT format model {}".format(model, ort_target_path))
-            _ = ort.InferenceSession(str(model), sess_options=so, providers=providers,
-                                     disabled_optimizers=optimizer_filter)
+            print(f"Converting optimized ONNX model {model} to ORT format model {ort_target_path}")
+            _ = ort.InferenceSession(
+                str(model), sess_options=so, providers=providers, disabled_optimizers=optimizer_filter
+            )
 
             converted_models.append(ort_target_path)
 
             # orig_size = os.path.getsize(onnx_target_path)
             # new_size = os.path.getsize(ort_target_path)
             # print("Serialized {} to {}. Sizes: orig={} new={} diff={} new:old={:.4f}:1.0".format(
             #     onnx_target_path, ort_target_path, orig_size, new_size, new_size - orig_size, new_size / orig_size))
         except Exception as e:
-            print("Error converting {}: {}".format(model, e))
+            print(f"Error converting {model}: {e}")
             if not allow_conversion_failures:
                 raise
 
-    print("Converted {}/{} models successfully.".format(len(converted_models), len(models)))
+    print(f"Converted {len(converted_models)}/{len(models)} models successfully.")
 
     return converted_models
 
 
 def parse_args():
     parser = argparse.ArgumentParser(
         os.path.basename(__file__),
-        description='''Convert the ONNX format model/s in the provided directory to ORT format models.
+        description="""Convert the ONNX format model/s in the provided directory to ORT format models.
         All files with a `.onnx` extension will be processed. For each one, an ORT format model will be created in the
-        same directory. A configuration file will also be created containing the list of required operators for all
+        given output directory, if specified, or the same directory.
+        A configuration file will also be created containing the list of required operators for all
         converted models. This configuration file should be used as input to the minimal build via the
         `--include_ops_by_config` parameter.
-        '''
+        """,
+    )
+
+    parser.add_argument(
+        "--output_dir",
+        type=pathlib.Path,
+        help="Provide an output directory for the converted model/s and configuration file. "
+        "If unspecified, the converted ORT format model/s will be in the same directory as the ONNX model/s.",
     )
 
-    parser.add_argument('--optimization_style',
-                        nargs='+',
-                        default=[OptimizationStyle.Fixed.name, OptimizationStyle.Runtime.name],
-                        choices=[e.name for e in OptimizationStyle],
-                        help="Style of optimization to perform on the ORT format model. "
-                             "Multiple values may be provided. The conversion will run once for each value. "
-                             "The general guidance is to use models optimized with "
-                             f"'{OptimizationStyle.Runtime.name}' style when using NNAPI or CoreML and "
-                             f"'{OptimizationStyle.Fixed.name}' style otherwise. "
-                             f"'{OptimizationStyle.Fixed.name}': Run optimizations directly before saving the ORT "
-                             "format model. This bakes in any platform-specific optimizations. "
-                             f"'{OptimizationStyle.Runtime.name}': Run basic optimizations directly and save certain "
-                             "other optimizations to be applied at runtime if possible. This is useful when using a "
-                             "compiling EP like NNAPI or CoreML that may run an unknown (at model conversion time) "
-                             "number of nodes. The saved optimizations can further optimize nodes not assigned to the "
-                             "compiling EP at runtime.")
-
-    parser.add_argument('--enable_type_reduction', action='store_true',
-                        help='Add operator specific type information to the configuration file to potentially reduce '
-                             'the types supported by individual operator implementations.')
-
-    parser.add_argument('--custom_op_library', type=pathlib.Path, default=None,
-                        help='Provide path to shared library containing custom operator kernels to register.')
-
-    parser.add_argument('--save_optimized_onnx_model', action='store_true',
-                        help='Save the optimized version of each ONNX model. '
-                             'This will have the same level of optimizations applied as the ORT format model.')
-
-    parser.add_argument('--allow_conversion_failures', action='store_true',
-                        help='Whether to proceed after encountering model conversion failures.')
-
-    parser.add_argument('--nnapi_partitioning_stop_ops',
-                        help='Specify the list of NNAPI EP partitioning stop ops. '
-                             'In particular, specify the value of the "ep.nnapi.partitioning_stop_ops" session '
-                             'options config entry.')
-
-    parser.add_argument('--target_platform', type=str, default=None, choices=['arm', 'amd64'],
-                        help='Specify the target platform where the exported model will be used. '
-                             'This parameter can be used to choose between platform-specific options, '
-                             'such as QDQIsInt8Allowed(arm), NCHWc (amd64) and NHWC (arm/amd64) format, different '
-                             'optimizer level options, etc.')
-
-    parser.add_argument('model_path_or_dir', type=pathlib.Path,
-                        help='Provide path to ONNX model or directory containing ONNX model/s to convert. '
-                             'All files with a .onnx extension, including those in subdirectories, will be '
-                             'processed.')
+    parser.add_argument(
+        "--optimization_style",
+        nargs="+",
+        default=[OptimizationStyle.Fixed.name, OptimizationStyle.Runtime.name],
+        choices=[e.name for e in OptimizationStyle],
+        help="Style of optimization to perform on the ORT format model. "
+        "Multiple values may be provided. The conversion will run once for each value. "
+        "The general guidance is to use models optimized with "
+        f"'{OptimizationStyle.Runtime.name}' style when using NNAPI or CoreML and "
+        f"'{OptimizationStyle.Fixed.name}' style otherwise. "
+        f"'{OptimizationStyle.Fixed.name}': Run optimizations directly before saving the ORT "
+        "format model. This bakes in any platform-specific optimizations. "
+        f"'{OptimizationStyle.Runtime.name}': Run basic optimizations directly and save certain "
+        "other optimizations to be applied at runtime if possible. This is useful when using a "
+        "compiling EP like NNAPI or CoreML that may run an unknown (at model conversion time) "
+        "number of nodes. The saved optimizations can further optimize nodes not assigned to the "
+        "compiling EP at runtime.",
+    )
+
+    parser.add_argument(
+        "--enable_type_reduction",
+        action="store_true",
+        help="Add operator specific type information to the configuration file to potentially reduce "
+        "the types supported by individual operator implementations.",
+    )
+
+    parser.add_argument(
+        "--custom_op_library",
+        type=pathlib.Path,
+        default=None,
+        help="Provide path to shared library containing custom operator kernels to register.",
+    )
+
+    parser.add_argument(
+        "--save_optimized_onnx_model",
+        action="store_true",
+        help="Save the optimized version of each ONNX model. "
+        "This will have the same level of optimizations applied as the ORT format model.",
+    )
+
+    parser.add_argument(
+        "--allow_conversion_failures",
+        action="store_true",
+        help="Whether to proceed after encountering model conversion failures.",
+    )
+
+    parser.add_argument(
+        "--target_platform",
+        type=str,
+        default=None,
+        choices=["arm", "amd64"],
+        help="Specify the target platform where the exported model will be used. "
+        "This parameter can be used to choose between platform-specific options, "
+        "such as QDQIsInt8Allowed(arm), NCHWc (amd64) and NHWC (arm/amd64) format, different "
+        "optimizer level options, etc.",
+    )
+
+    parser.add_argument(
+        "model_path_or_dir",
+        type=pathlib.Path,
+        help="Provide path to ONNX model or directory containing ONNX model/s to convert. "
+        "All files with a .onnx extension, including those in subdirectories, will be "
+        "processed.",
+    )
 
     return parser.parse_args()
 
 
 def convert_onnx_models_to_ort():
     args = parse_args()
 
+    output_dir = None
+    if args.output_dir is not None:
+        if not args.output_dir.is_dir():
+            args.output_dir.mkdir(parents=True)
+        output_dir = args.output_dir.resolve(strict=True)
+
     optimization_styles = [OptimizationStyle[style_str] for style_str in args.optimization_style]
-    optimization_level_str = 'all'
+    # setting optimization level is not expected to be needed by typical users, but it can be set with this
+    # environment variable
+    optimization_level_str = os.getenv("ORT_CONVERT_ONNX_MODELS_TO_ORT_OPTIMIZATION_LEVEL", "all")
     model_path_or_dir = args.model_path_or_dir.resolve()
     custom_op_library = args.custom_op_library.resolve() if args.custom_op_library else None
 
     if not model_path_or_dir.is_dir() and not model_path_or_dir.is_file():
-        raise FileNotFoundError("Model path '{}' is not a file or directory.".format(model_path_or_dir))
+        raise FileNotFoundError(f"Model path '{model_path_or_dir}' is not a file or directory.")
 
     if custom_op_library and not custom_op_library.is_file():
-        raise FileNotFoundError("Unable to find custom operator library '{}'".format(custom_op_library))
+        raise FileNotFoundError(f"Unable to find custom operator library '{custom_op_library}'")
 
     session_options_config_entries = {}
 
-    if args.nnapi_partitioning_stop_ops is not None:
-        session_options_config_entries["ep.nnapi.partitioning_stop_ops"] = args.nnapi_partitioning_stop_ops
-
-    if args.target_platform == 'arm':
+    if args.target_platform == "arm":
         session_options_config_entries["session.qdqisint8allowed"] = "1"
     else:
         session_options_config_entries["session.qdqisint8allowed"] = "0"
 
     for optimization_style in optimization_styles:
-        print("Converting models with optimization style '{}' and level '{}'".format(
-            optimization_style.name, optimization_level_str))
+        print(
+            "Converting models with optimization style '{}' and level '{}'".format(
+                optimization_style.name, optimization_level_str
+            )
+        )
 
         converted_models = _convert(
-            model_path_or_dir=model_path_or_dir, output_dir=None,
-            optimization_level_str=optimization_level_str, optimization_style=optimization_style,
+            model_path_or_dir=model_path_or_dir,
+            output_dir=output_dir,
+            optimization_level_str=optimization_level_str,
+            optimization_style=optimization_style,
             custom_op_library=custom_op_library,
             create_optimized_onnx_model=args.save_optimized_onnx_model,
             allow_conversion_failures=args.allow_conversion_failures,
             target_platform=args.target_platform,
-            session_options_config_entries=session_options_config_entries)
+            session_options_config_entries=session_options_config_entries,
+        )
 
         with contextlib.ExitStack() as context_stack:
             if optimization_style == OptimizationStyle.Runtime:
                 # Convert models again without runtime optimizations.
                 # Runtime optimizations may not end up being applied, so we need to use both converted models with and
                 # without runtime optimizations to get a complete set of ops that may be needed for the config file.
                 model_dir = model_path_or_dir if model_path_or_dir.is_dir() else model_path_or_dir.parent
                 temp_output_dir = context_stack.enter_context(
-                    tempfile.TemporaryDirectory(dir=model_dir, suffix=".without_runtime_opt"))
+                    tempfile.TemporaryDirectory(dir=model_dir, suffix=".without_runtime_opt")
+                )
                 session_options_config_entries_for_second_conversion = session_options_config_entries.copy()
                 # Limit the optimizations to those that can run in a model with runtime optimizations.
                 session_options_config_entries_for_second_conversion[
-                    "optimization.minimal_build_optimizations"] = "apply"
+                    "optimization.minimal_build_optimizations"
+                ] = "apply"
 
-                print("Converting models again without runtime optimizations to generate a complete config file. "
-                      "These converted models are temporary and will be deleted.")
+                print(
+                    "Converting models again without runtime optimizations to generate a complete config file. "
+                    "These converted models are temporary and will be deleted."
+                )
                 converted_models += _convert(
-                    model_path_or_dir=model_path_or_dir, output_dir=temp_output_dir,
-                    optimization_level_str=optimization_level_str, optimization_style=OptimizationStyle.Fixed,
+                    model_path_or_dir=model_path_or_dir,
+                    output_dir=temp_output_dir,
+                    optimization_level_str=optimization_level_str,
+                    optimization_style=OptimizationStyle.Fixed,
                     custom_op_library=custom_op_library,
                     create_optimized_onnx_model=False,  # not useful as they would be created in a temp directory
                     allow_conversion_failures=args.allow_conversion_failures,
                     target_platform=args.target_platform,
-                    session_options_config_entries=session_options_config_entries_for_second_conversion)
-
-            print("Generating config file from ORT format models with optimization style '{}' and level '{}'".format(
-                optimization_style.name, optimization_level_str))
+                    session_options_config_entries=session_options_config_entries_for_second_conversion,
+                )
 
-            config_file = _create_config_file_path(model_path_or_dir, optimization_style, args.enable_type_reduction)
+            print(
+                "Generating config file from ORT format models with optimization style '{}' and level '{}'".format(
+                    optimization_style.name, optimization_level_str
+                )
+            )
+
+            config_file = _create_config_file_path(
+                model_path_or_dir,
+                output_dir,
+                optimization_level_str,
+                optimization_style,
+                args.enable_type_reduction,
+            )
 
             create_config_from_models(converted_models, config_file, args.enable_type_reduction)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     convert_onnx_models_to_ort()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/file_utils.py` & `onnxruntime/tools/file_utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,35 +1,35 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
+import os
 import pathlib
 import typing
-import os
 
 
 def path_match_suffix_ignore_case(path: typing.Union[pathlib.Path, str], suffix: str) -> bool:
-    '''
+    """
     Returns whether `path` ends in `suffix`, ignoring case.
-    '''
+    """
     if not isinstance(path, str):
         path = str(path)
     return path.casefold().endswith(suffix.casefold())
 
 
-def files_from_file_or_dir(file_or_dir_path: typing.Union[pathlib.Path, str],
-                           predicate: typing.Callable[[pathlib.Path], bool] = lambda _: True) \
-        -> typing.List[pathlib.Path]:
-    '''
+def files_from_file_or_dir(
+    file_or_dir_path: typing.Union[pathlib.Path, str], predicate: typing.Callable[[pathlib.Path], bool] = lambda _: True
+) -> typing.List[pathlib.Path]:
+    """
     Gets the files in `file_or_dir_path` satisfying `predicate`.
     If `file_or_dir_path` is a file, the single file is considered. Otherwise, all files in the directory are
     considered.
     :param file_or_dir_path: Path to a file or directory.
     :param predicate: Predicate to determine if a file is included.
     :return: A list of files.
-    '''
+    """
     if not isinstance(file_or_dir_path, pathlib.Path):
         file_or_dir_path = pathlib.Path(file_or_dir_path)
 
     selected_files = []
 
     def process_file(file_path: pathlib.Path):
         if predicate(file_path):
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/make_dynamic_shape_fixed.py` & `onnxruntime/tools/make_dynamic_shape_fixed.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,46 +1,61 @@
 #!/usr/bin/env python3
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 import argparse
-import onnx
 import os
 import pathlib
 import sys
 
-from .onnx_model_utils import make_dim_param_fixed, make_input_shape_fixed, fix_output_shapes
+import onnx
+
+from .onnx_model_utils import fix_output_shapes, make_dim_param_fixed, make_input_shape_fixed
 
 
 def make_dynamic_shape_fixed_helper():
-    parser = argparse.ArgumentParser(f'{os.path.basename(__file__)}:{make_dynamic_shape_fixed_helper.__name__}',
-                                     description='''
+    parser = argparse.ArgumentParser(
+        f"{os.path.basename(__file__)}:{make_dynamic_shape_fixed_helper.__name__}",
+        description="""
                                      Assign a fixed value to a dim_param or input shape
-                                     Provide either dim_param and dim_value or input_name and input_shape.''')
+                                     Provide either dim_param and dim_value or input_name and input_shape.""",
+    )
 
-    parser.add_argument('--dim_param', type=str, required=False,
-                        help="Symbolic parameter name. Provide dim_value if specified.")
-    parser.add_argument('--dim_value', type=int, required=False,
-                        help="Value to replace dim_param with in the model. Must be > 0.")
-    parser.add_argument('--input_name', type=str, required=False,
-                        help="Model input name to replace shape of. Provide input_shape if specified.")
-    parser.add_argument('--input_shape', type=lambda x: [int(i) for i in x.split(',')], required=False,
-                        help="Shape to use for input_shape. Provide comma separated list for the shape. "
-                             "All values must be > 0. e.g. --input_shape 1,3,256,256")
+    parser.add_argument(
+        "--dim_param", type=str, required=False, help="Symbolic parameter name. Provide dim_value if specified."
+    )
+    parser.add_argument(
+        "--dim_value", type=int, required=False, help="Value to replace dim_param with in the model. Must be > 0."
+    )
+    parser.add_argument(
+        "--input_name",
+        type=str,
+        required=False,
+        help="Model input name to replace shape of. Provide input_shape if specified.",
+    )
+    parser.add_argument(
+        "--input_shape",
+        type=lambda x: [int(i) for i in x.split(",")],
+        required=False,
+        help="Shape to use for input_shape. Provide comma separated list for the shape. "
+        "All values must be > 0. e.g. --input_shape 1,3,256,256",
+    )
 
-    parser.add_argument('input_model', type=pathlib.Path, help='Provide path to ONNX model to update.')
-    parser.add_argument('output_model', type=pathlib.Path, help='Provide path to write updated ONNX model to.')
+    parser.add_argument("input_model", type=pathlib.Path, help="Provide path to ONNX model to update.")
+    parser.add_argument("output_model", type=pathlib.Path, help="Provide path to write updated ONNX model to.")
 
     args = parser.parse_args()
 
-    if (args.dim_param and args.input_name) or \
-            (not args.dim_param and not args.input_name) or \
-            (args.dim_param and (not args.dim_value or args.dim_value < 1)) or \
-            (args.input_name and (not args.input_shape or any([value < 1 for value in args.input_shape]))):
-        print('Invalid usage.')
+    if (
+        (args.dim_param and args.input_name)
+        or (not args.dim_param and not args.input_name)
+        or (args.dim_param and (not args.dim_value or args.dim_value < 1))
+        or (args.input_name and (not args.input_shape or any([value < 1 for value in args.input_shape])))
+    ):
+        print("Invalid usage.")
         parser.print_help()
         sys.exit(-1)
 
     model = onnx.load(str(args.input_model.resolve(strict=True)))
 
     if args.dim_param:
         make_dim_param_fixed(model.graph, args.dim_param, args.dim_value)
@@ -49,9 +64,9 @@
 
     # update the output shapes to make them fixed if possible.
     fix_output_shapes(model)
 
     onnx.save(model, str(args.output_model.resolve()))
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     make_dynamic_shape_fixed_helper()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/onnx_model_utils.py` & `onnxruntime/tools/onnx_model_utils.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,68 +1,70 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 import logging
-import onnx
-import onnxruntime as ort
 import pathlib
 
+import onnx
 from onnx import version_converter
 
+import onnxruntime as ort
+
 
 def iterate_graph_per_node_func(graph, per_node_func, **func_args):
-    '''
+    """
     Iterate the graph including subgraphs calling the per_node_func for each node.
     :param graph: Graph to iterate
     :param per_node_func: Function to call for each node. Signature is fn(node: onnx:NodeProto, **kwargs)
     :param func_args: The keyword args to pass through.
-    '''
+    """
 
     for node in graph.node:
         per_node_func(node, **func_args)
         # recurse into subgraph for control flow nodes (Scan/Loop/If)
         for attr in node.attribute:
-            if attr.HasField('g'):
+            if attr.HasField("g"):
                 iterate_graph_per_node_func(attr.g, per_node_func, **func_args)
 
 
 def iterate_graph_per_graph_func(graph, per_graph_func, **func_args):
-    '''
+    """
     Iterate the graph including subgraphs calling the per_graph_func for each Graph.
     :param graph: Graph to iterate
     :param per_graph_func: Function to call for each graph. Signature is fn(graph: onnx:GraphProto, **kwargs)
     :param func_args: The keyword args to pass through.
-    '''
+    """
 
     per_graph_func(graph, **func_args)
 
     for node in graph.node:
         # recurse into subgraph for control flow nodes (Scan/Loop/If)
         for attr in node.attribute:
-            if attr.HasField('g'):
+            if attr.HasField("g"):
                 iterate_graph_per_graph_func(attr.g, per_graph_func, **func_args)
 
 
 def get_opsets_imported(model: onnx.ModelProto):
-    '''
+    """
     Get the opsets imported by the model
     :param model: Model to check.
     :return: Map of domain to opset.
-    '''
+    """
     opsets = {}
     for entry in model.opset_import:
         # if empty it's ai.onnx
-        domain = entry.domain or 'ai.onnx'
+        domain = entry.domain or "ai.onnx"
         opsets[domain] = entry.version
 
     return opsets
 
 
-def update_onnx_opset(model_path: pathlib.Path, opset: int, out_path: pathlib.Path = None,
-                      logger: logging.Logger = None):
+def update_onnx_opset(
+    model_path: pathlib.Path, opset: int, out_path: pathlib.Path = None, logger: logging.Logger = None
+):
     """
     Helper to update the opset of a model using onnx version_converter. Target opset must be greater than current opset.
     :param model_path: Path to model to update
     :param opset: Opset to update model to
     :param out_path: Optional output path for updated model to be saved to.
     :param logger: Optional logger for diagnostic output
     :returns: Updated onnx.ModelProto
@@ -80,193 +82,198 @@
         onnx.save(new_model, str(out_path))
         if logger:
             logger.info("Saved updated model to %s", out_path)
 
     return new_model
 
 
-def optimize_model(model_path: pathlib.Path,
-                   output_path: pathlib.Path,
-                   level: ort.GraphOptimizationLevel = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC,
-                   log_level: int = 3):
-    '''
+def optimize_model(
+    model_path: pathlib.Path,
+    output_path: pathlib.Path,
+    level: ort.GraphOptimizationLevel = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC,
+    log_level: int = 3,
+):
+    """
     Optimize an ONNX model using ONNX Runtime to the specified level
     :param model_path: Path to ONNX model
     :param output_path: Path to save optimized model to.
     :param level: onnxruntime.GraphOptimizationLevel to use. Default is ORT_ENABLE_BASIC.
     :param log_level: Log level. Defaults to Error (3) so we don't get output about unused initializers being removed.
                       Warning (2) or Info (1) may be desirable in some scenarios.
-    '''
+    """
     so = ort.SessionOptions()
     so.optimized_model_filepath = str(output_path.resolve())
     so.graph_optimization_level = level
     so.log_severity_level = log_level
 
     # create session to optimize. this will write the updated model to output_path
-    _ = ort.InferenceSession(str(model_path.resolve(strict=True)), so, providers=['CPUExecutionProvider'])
+    _ = ort.InferenceSession(str(model_path.resolve(strict=True)), so, providers=["CPUExecutionProvider"])
 
 
 def _replace_symbolic_dim_value(graph: onnx.GraphProto, **kwargs):
-    param_to_replace = kwargs['dim_param']
-    value = kwargs['value']
+    param_to_replace = kwargs["dim_param"]
+    value = kwargs["value"]
 
     def update_dim_values(value_infos):
         for vi in value_infos:
             if vi.type.HasField("tensor_type"):
                 shape = vi.type.tensor_type.shape
                 if shape:
                     for dim in shape.dim:
-                        if dim.HasField('dim_param') and dim.dim_param == param_to_replace:
+                        if dim.HasField("dim_param") and dim.dim_param == param_to_replace:
                             dim.Clear()
                             dim.dim_value = value
 
     update_dim_values(graph.input)
     update_dim_values(graph.output)
     update_dim_values(graph.value_info)
 
 
 def _remove_invalid_dim_values_impl(graph: onnx.GraphProto):
     def clear_invalid_values(value):
         if value.type.HasField("tensor_type"):
             shape = value.type.tensor_type.shape
             if shape:
                 for dim in shape.dim:
-                    if dim.HasField('dim_value') and dim.dim_value < 1:
+                    if dim.HasField("dim_value") and dim.dim_value < 1:
                         dim.Clear()
 
     for i in graph.input:
         clear_invalid_values(i)
 
     for o in graph.output:
         clear_invalid_values(o)
 
     for vi in graph.value_info:
         clear_invalid_values(vi)
 
 
 def remove_invalid_dim_values(graph: onnx.GraphProto):
-    '''
+    """
     Iterate the graph and subgraphs, unsetting any dim_value entries that have a value of less than 1.
     These are typically erroneously inserted by a converter to represent a dynamic dimension.
     :param graph: GraphProto to update
-    '''
+    """
     iterate_graph_per_graph_func(graph, _remove_invalid_dim_values_impl)
 
 
 def make_dim_param_fixed(graph: onnx.GraphProto, param_name: str, value: int):
-    '''
+    """
     Iterate all values in the graph, replacing dim_param in a tensor shape with the provided value.
     :param graph: GraphProto to update
     :param param_name: dim_param to set
     :param value: value to use
-    '''
+    """
     iterate_graph_per_graph_func(graph, _replace_symbolic_dim_value, dim_param=param_name, value=value)
 
 
 def make_input_shape_fixed(graph: onnx.GraphProto, input_name: str, fixed_shape: [int]):
-    '''
+    """
     Update the named graph input to set shape to the provided value. This can be used to set unknown dims as well
     as to replace dim values.
     If setting the input shape replaces a dim_param, update any other values in the graph that use the dim_param.
     :param graph: Graph to update
     :param input_name: Name of graph input to update.
     :param fixed_shape: Shape to use.
-    '''
+    """
 
     # remove any invalid dim values first. typically this is a dim_value of -1.
     remove_invalid_dim_values(graph)
 
     for i in graph.input:
         if i.name == input_name:
             if not i.type.HasField("tensor_type"):
-                raise ValueError(f'Input {input_name} is not a tensor')
+                raise ValueError(f"Input {input_name} is not a tensor")
 
             # graph inputs are required to have a shape to provide the rank
             shape = i.type.tensor_type.shape
             if len(shape.dim) != len(fixed_shape):
-                raise ValueError(
-                    f'Rank mismatch. Existing:{len(shape.dim)} Replacement:{len(fixed_shape)}')
+                raise ValueError(f"Rank mismatch. Existing:{len(shape.dim)} Replacement:{len(fixed_shape)}")
 
             for idx, dim in enumerate(shape.dim):
                 # check any existing fixed dims match
-                if dim.HasField('dim_value'):
+                if dim.HasField("dim_value"):
                     if dim.dim_value != fixed_shape[idx]:
                         raise ValueError(
                             f"Can't replace existing fixed size of {dim.dim_value} with {fixed_shape[idx]} "
-                            f"for dimension {idx + 1}")
-                elif dim.HasField('dim_param'):
+                            f"for dimension {idx + 1}"
+                        )
+                elif dim.HasField("dim_param"):
                     # replacing a dim_param so have to do that through the entire graph
                     make_dim_param_fixed(graph, dim.dim_param, fixed_shape[idx])
                 else:
                     # replacing an unknown dim
                     dim.Clear()
                     dim.dim_value = fixed_shape[idx]
 
             return
 
-    raise ValueError(f'Input {input_name} was not found in graph inputs. '
-                     f'Valid input names are: {",".join([i.name for i in graph.input])}')
+    raise ValueError(
+        f"Input {input_name} was not found in graph inputs. "
+        f'Valid input names are: {",".join([i.name for i in graph.input])}'
+    )
 
 
 def fix_output_shapes(model: onnx.ModelProto):
-    '''
+    """
     Update the output shapesof a model where the input shape/s were made fixed, if possible.
     This is mainly to make the model usage clearer if the output shapes can be inferred from the new input shapes.
     :param model: Model that had input shapes fixed.
-    '''
+    """
 
     # get a version of the model with shape inferencing info in it. this will provide fixed output shapes if possible.
     m2 = onnx.shape_inference.infer_shapes(model)
     onnx.checker.check_model(m2)
 
     for idx, o in enumerate(model.graph.output):
         if not is_fixed_size_tensor(o):
             new_o = m2.graph.output[idx]
             if is_fixed_size_tensor(new_o):
                 o.type.tensor_type.shape.CopyFrom(new_o.type.tensor_type.shape)
 
 
-def _create_producer_consumer_link(node_to_producers: dict, node_to_consumers: dict,
-                                   producer: onnx.NodeProto, consumer: onnx.NodeProto):
-    '''
+def _create_producer_consumer_link(
+    node_to_producers: dict, node_to_consumers: dict, producer: onnx.NodeProto, consumer: onnx.NodeProto
+):
+    """
     Create links between two nodes for a value produced by one and consumed by the other.
     :param node_to_producers: Map of NodeProto to set of nodes that produce values the node consumes as inputs.
     :param node_to_consumers: Map of NodeProto to set of nodes that consume values the node produces as outputs.
     :param producer: Producer node
     :param consumer: Consumer node
-    '''
+    """
 
     if consumer not in node_to_producers:
         node_to_producers[consumer] = set()
 
     if producer not in node_to_consumers:
         node_to_consumers[producer] = set()
 
     # add entry mapping this node to the producer of this input
     node_to_producers[consumer].add(producer)
     node_to_consumers[producer].add(consumer)
 
 
 def _map_node_dependencies(graph: onnx.GraphProto, node_to_producers: dict, node_to_consumers: dict):
-    graph_inputs = set([i.name for i in graph.input])
-    initializers = set([i.name for i in graph.initializer])
+    graph_inputs = {i.name for i in graph.input}
+    initializers = {i.name for i in graph.initializer}
 
     # map of value name to node that creates it. copy parent values but override if values get shadowed
     producers = {}
 
     implicit_inputs = set()
 
     def is_local_value(value):
         return value in producers or value in initializers or value in graph_inputs
 
     for node in graph.node:
         inputs = [i for i in node.input]
 
         for attr in node.attribute:
-            if attr.HasField('g'):
+            if attr.HasField("g"):
                 subgraph_implicit_inputs = _map_node_dependencies(attr.g, node_to_producers, node_to_consumers)
                 inputs += subgraph_implicit_inputs
 
         for i in inputs:
             if not i:
                 # missing optional input
                 continue
@@ -281,76 +288,77 @@
         for o in node.output:
             producers[o] = node
 
     return implicit_inputs
 
 
 def get_producer_consumer_maps(graph: onnx.GraphProto):
-    '''
+    """
     Get maps for connections between the node that produces each value and the nodes that consume the value.
     Processing includes subgraphs. As the map key is a Node instance from the Graph there should be no ambiguity.
     :param graph: Graph to process.
     :return: Tuple with two maps.
              First is node_to_producers map of a node to set of all nodes producing input it consumes.
              Second is node_to_consumers map of a node to set of all nodes consuming output it creates.
              e.g. NodeA and NodeB provide inputs to NodeC. NodeC provides input to NodeD
              node_to_consumers[NodeA] = set([NodeC])
              node_to_consumers[NodeB] = set([NodeC])
              node_to_producers[NodeC] = set([NodeA, NodeB])
              node_to_consumers[NodeC] = set([NodeD])
              node_to_producers[NodeD] = set([NodeC])
-    '''
+    """
 
     # use a hash of the object id for NodeProto.
     # we need this for the partitioning checker where we keep maps with nodes as the key.
     onnx.NodeProto.__hash__ = lambda self: id(self)
 
     node_to_producers = {}  # map of node instance to nodes producing input values it consumes
     node_to_consumers = {}  # map of node instance to nodes consuming output values it produces
 
     implicit_inputs = _map_node_dependencies(graph, node_to_producers, node_to_consumers)
 
     # top level graph should have no implicit inputs
     if implicit_inputs:
-        raise ValueError('This appears to be an invalid model with missing inputs of '
-                         f'{",".join(sorted(implicit_inputs))}')
+        raise ValueError(
+            f'This appears to be an invalid model with missing inputs of {",".join(sorted(implicit_inputs))}'
+        )
 
     return node_to_producers, node_to_consumers
 
 
 def is_fixed_size_tensor(value: onnx.ValueInfoProto):
-    '''
+    """
     Check if value is a tensor with a fixed shape.
     :param value: onnx.ValueInfoProto to check
     :return: True if value is a tensor, with a shape, where all dimensions have fixed values.
-    '''
+    """
 
     is_fixed = False
     if value.type.HasField("tensor_type"):
         shape = value.type.tensor_type.shape
         if shape:
             is_fixed = True  # scalar has no dims so set to True and unset if we hit a dim without a valid value
             for dim in shape.dim:
-                if dim.HasField('dim_value') and dim.dim_value > 0:
+                if dim.HasField("dim_value") and dim.dim_value > 0:
                     continue
 
                 # anything else means it's a dynamic value
                 is_fixed = False
                 break
 
     return is_fixed
 
 
 def get_optimization_level(level):
-    '''Convert string to GraphOptimizationLevel.'''
-    if level == 'disable':
+    """Convert string to GraphOptimizationLevel."""
+    if level == "disable":
         return ort.GraphOptimizationLevel.ORT_DISABLE_ALL
-    if level == 'basic':
+    if level == "basic":
         # Constant folding and other optimizations that only use ONNX operators
         return ort.GraphOptimizationLevel.ORT_ENABLE_BASIC
-    if level == 'extended':
+    if level == "extended":
         # Optimizations using custom operators, excluding NCHWc and NHWC layout optimizers
         return ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED
-    if level == 'all':
+    if level == "all":
         return ort.GraphOptimizationLevel.ORT_ENABLE_ALL
 
-    raise ValueError('Invalid optimization level of ' + level)
+    raise ValueError("Invalid optimization level of " + level)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/onnx_randomizer.py` & `onnxruntime/tools/onnx_randomizer.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,90 +1,85 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 # An offline standalone script to declassify an ONNX model by randomizing the tensor data in initializers.
 # The ORT Performance may change especially on generative models.
 
 import argparse
-import numpy as np
-from onnx import onnx_pb, numpy_helper, save_model, load_model
 from pathlib import Path
 
+import numpy as np
+from onnx import load_model, numpy_helper, onnx_pb, save_model
+
 # An experimental small value for differentiating shape data and weights.
 # The tensor data with larger size can't be shape data.
 # User may adjust this value as needed.
 SIZE_THRESHOLD = 10
 
+
 def graph_iterator(model, func):
     graph_queue = [model.graph]
     while graph_queue:
         graph = graph_queue.pop(0)
         func(graph)
         for node in graph.node:
             for attr in node.attribute:
                 if attr.type == onnx_pb.AttributeProto.AttributeType.GRAPH:
-                    assert (isinstance(attr.g, onnx_pb.GraphProto))
+                    assert isinstance(attr.g, onnx_pb.GraphProto)
                     graph_queue.append(attr.g)
                 if attr.type == onnx_pb.AttributeProto.AttributeType.GRAPHS:
                     for g in attr.graphs:
-                        assert (isinstance(g, onnx_pb.GraphProto))
+                        assert isinstance(g, onnx_pb.GraphProto)
                         graph_queue.append(g)
 
 
 def randomize_graph_initializer(graph):
     for i_tensor in graph.initializer:
         array = numpy_helper.to_array(i_tensor)
         # TODO: need to find a better way to differentiate shape data and weights.
         if array.size > SIZE_THRESHOLD:
-            random_array = np.random.uniform(array.min(),
-                                             array.max(),
-                                             size=array.shape).astype(
-                                                 array.dtype)
+            random_array = np.random.uniform(array.min(), array.max(), size=array.shape).astype(array.dtype)
             o_tensor = numpy_helper.from_array(random_array, i_tensor.name)
             i_tensor.CopyFrom(o_tensor)
 
 
 def main():
-    parser = argparse.ArgumentParser(
-        description='Randomize the weights of an ONNX model')
-    parser.add_argument('-m',
-                        type=str,
-                        required=True,
-                        help='input onnx model path')
-    parser.add_argument('-o',
-                        type=str,
-                        required=True,
-                        help='output onnx model path')
-    parser.add_argument("--use_external_data_format",
-                        required=False,
-                        action="store_true",
-                        help="Store or Save in external data format")
-    parser.add_argument("--all_tensors_to_one_file",
-                        required=False,
-                        action="store_true",
-                        help="Save all tensors to one file")
+    parser = argparse.ArgumentParser(description="Randomize the weights of an ONNX model")
+    parser.add_argument("-m", type=str, required=True, help="input onnx model path")
+    parser.add_argument("-o", type=str, required=True, help="output onnx model path")
+    parser.add_argument(
+        "--use_external_data_format",
+        required=False,
+        action="store_true",
+        help="Store or Save in external data format",
+    )
+    parser.add_argument(
+        "--all_tensors_to_one_file",
+        required=False,
+        action="store_true",
+        help="Save all tensors to one file",
+    )
     args = parser.parse_args()
 
     data_path = None
     if args.use_external_data_format:
         if Path(args.m).parent == Path(args.o).parent:
-            raise RuntimeError(
-                "Please specify output directory with different parent path to input directory."
-            )
+            raise RuntimeError("Please specify output directory with different parent path to input directory.")
         if args.all_tensors_to_one_file:
             data_path = Path(args.o).name + ".data"
 
     Path(args.o).parent.mkdir(parents=True, exist_ok=True)
-    onnx_model = load_model(args.m,
-                            load_external_data=args.use_external_data_format)
+    onnx_model = load_model(args.m, load_external_data=args.use_external_data_format)
     graph_iterator(onnx_model, randomize_graph_initializer)
-    save_model(onnx_model,
-               args.o,
-               save_as_external_data=args.use_external_data_format,
-               all_tensors_to_one_file=args.all_tensors_to_one_file,
-               location=data_path)
+    save_model(
+        onnx_model,
+        args.o,
+        save_as_external_data=args.use_external_data_format,
+        all_tensors_to_one_file=args.all_tensors_to_one_file,
+        location=data_path,
+    )
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/onnxruntime_test.py` & `onnxruntime/tools/onnxruntime_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,32 +1,39 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 import argparse
-import onnxruntime as onnxrt
-import numpy as np
 import os
 import sys
 from timeit import default_timer as timer
 
-float_dict = {'tensor(float16)': 'float16', 'tensor(float)': 'float32', 'tensor(double)': 'float64'}
+import numpy as np
+
+import onnxruntime as onnxrt
+
+float_dict = {
+    "tensor(float16)": "float16",
+    "tensor(float)": "float32",
+    "tensor(double)": "float64",
+}
 
 integer_dict = {
-    'tensor(int32)': 'int32',
-    'tensor(int8)': 'int8',
-    'tensor(uint8)': 'uint8',
-    'tensor(int16)': 'int16',
-    'tensor(uint16)': 'uint16',
-    'tensor(int64)': 'int64',
-    'tensor(uint64)': 'uint64'
+    "tensor(int32)": "int32",
+    "tensor(int8)": "int8",
+    "tensor(uint8)": "uint8",
+    "tensor(int16)": "int16",
+    "tensor(uint16)": "uint16",
+    "tensor(int64)": "int64",
+    "tensor(uint64)": "uint64",
 }
 
-def generate_feeds(sess, symbolic_dims={}):
+
+def generate_feeds(sess, symbolic_dims={}):  # noqa: B006
     feeds = {}
     for input_meta in sess.get_inputs():
         # replace any symbolic dimensions
         shape = []
         for dim in input_meta.shape:
             if not dim:
                 # unknown dim
@@ -39,89 +46,116 @@
                     shape.append(1)
             else:
                 shape.append(dim)
 
         if input_meta.type in float_dict:
             feeds[input_meta.name] = np.random.rand(*shape).astype(float_dict[input_meta.type])
         elif input_meta.type in integer_dict:
-            feeds[input_meta.name] = np.random.uniform(high=1000,
-                                                       size=tuple(shape)).astype(integer_dict[input_meta.type])
-        elif input_meta.type == 'tensor(bool)':
-            feeds[input_meta.name] = np.random.randint(2, size=tuple(shape)).astype('bool')
+            feeds[input_meta.name] = np.random.uniform(high=1000, size=tuple(shape)).astype(
+                integer_dict[input_meta.type]
+            )
+        elif input_meta.type == "tensor(bool)":
+            feeds[input_meta.name] = np.random.randint(2, size=tuple(shape)).astype("bool")
         else:
-            print("unsupported input type {} for input {}".format(input_meta.type, input_meta.name))
+            print(f"unsupported input type {input_meta.type} for input {input_meta.name}")
             sys.exit(-1)
     return feeds
 
+
 # simple test program for loading onnx model, feeding all inputs and running the model num_iters times.
-def run_model(model_path,
-              num_iters=1,
-              debug=None,
-              profile=None,
-              symbolic_dims={},
-              feeds=None,
-              override_initializers=True):
+def run_model(
+    model_path,
+    num_iters=1,
+    debug=None,
+    profile=None,
+    symbolic_dims={},  # noqa: B006
+    feeds=None,
+    override_initializers=True,
+):
     if debug:
-        print("Pausing execution ready for debugger to attach to pid: {}".format(os.getpid()))
+        print(f"Pausing execution ready for debugger to attach to pid: {os.getpid()}")
         print("Press key to continue.")
         sys.stdin.read(1)
 
     sess_options = None
     if profile:
         sess_options = onnxrt.SessionOptions()
         sess_options.enable_profiling = True
         sess_options.profile_file_prefix = os.path.basename(model_path)
 
-    sess = onnxrt.InferenceSession(model_path, sess_options=sess_options, providers=onnxrt.get_available_providers())
+    sess = onnxrt.InferenceSession(
+        model_path,
+        sess_options=sess_options,
+        providers=onnxrt.get_available_providers(),
+    )
     meta = sess.get_modelmeta()
 
     if not feeds:
         feeds = generate_feeds(sess, symbolic_dims)
 
     if override_initializers:
         # Starting with IR4 some initializers provide default values
         # and can be overridden (available in IR4). For IR < 4 models
         # the list would be empty
         for initializer in sess.get_overridable_initializers():
             shape = [dim if dim else 1 for dim in initializer.shape]
             if initializer.type in float_dict:
                 feeds[initializer.name] = np.random.rand(*shape).astype(float_dict[initializer.type])
             elif initializer.type in integer_dict:
-                feeds[initializer.name] = np.random.uniform(high=1000,
-                                                            size=tuple(shape)).astype(integer_dict[initializer.type])
-            elif initializer.type == 'tensor(bool)':
-                feeds[initializer.name] = np.random.randint(2, size=tuple(shape)).astype('bool')
+                feeds[initializer.name] = np.random.uniform(high=1000, size=tuple(shape)).astype(
+                    integer_dict[initializer.type]
+                )
+            elif initializer.type == "tensor(bool)":
+                feeds[initializer.name] = np.random.randint(2, size=tuple(shape)).astype("bool")
             else:
-                print("unsupported initializer type {} for initializer {}".format(initializer.type, initializer.name))
+                print(f"unsupported initializer type {initializer.type} for initializer {initializer.name}")
                 sys.exit(-1)
 
     start = timer()
-    for i in range(num_iters):
+    for _i in range(num_iters):
         outputs = sess.run([], feeds)  # fetch all outputs
     end = timer()
 
-    print("model: {}".format(meta.graph_name))
-    print("version: {}".format(meta.version))
-    print("iterations: {}".format(num_iters))
-    print("avg latency: {} ms".format(((end - start) * 1000) / num_iters))
+    print(f"model: {meta.graph_name}")
+    print(f"version: {meta.version}")
+    print(f"iterations: {num_iters}")
+    print(f"avg latency: {((end - start) * 1000) / num_iters} ms")
 
     if profile:
         trace_file = sess.end_profiling()
-        print("trace file written to: {}".format(trace_file))
+        print(f"trace file written to: {trace_file}")
 
     return 0, feeds, num_iters > 0 and outputs
 
 
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser(description='Simple ONNX Runtime Test Tool.')
-    parser.add_argument('model_path', help='model path')
-    parser.add_argument('num_iters', nargs='?', type=int, default=1000, help='model run iterations. default=1000')
-    parser.add_argument('--debug', action='store_true', help='pause execution to allow attaching a debugger.')
-    parser.add_argument('--profile', action='store_true', help='enable chrome timeline trace profiling.')
-    parser.add_argument('--symbolic_dims', default={}, type=lambda s: dict(x.split("=") for x in s.split(",")),
-                        help='Comma separated name=value pairs for any symbolic dimensions in the model input. '
-                             'e.g. --symbolic_dims batch=1,seqlen=5. '
-                             'If not provided, the value of 1 will be used for all symbolic dimensions.')
+def main():
+    parser = argparse.ArgumentParser(description="Simple ONNX Runtime Test Tool.")
+    parser.add_argument("model_path", help="model path")
+    parser.add_argument(
+        "num_iters",
+        nargs="?",
+        type=int,
+        default=1000,
+        help="model run iterations. default=1000",
+    )
+    parser.add_argument(
+        "--debug",
+        action="store_true",
+        help="pause execution to allow attaching a debugger.",
+    )
+    parser.add_argument("--profile", action="store_true", help="enable chrome timeline trace profiling.")
+    parser.add_argument(
+        "--symbolic_dims",
+        default={},
+        type=lambda s: dict(x.split("=") for x in s.split(",")),
+        help="Comma separated name=value pairs for any symbolic dimensions in the model input. "
+        "e.g. --symbolic_dims batch=1,seqlen=5. "
+        "If not provided, the value of 1 will be used for all symbolic dimensions.",
+    )
 
     args = parser.parse_args()
     exit_code, _, _ = run_model(args.model_path, args.num_iters, args.debug, args.profile, args.symbolic_dims)
     sys.exit(exit_code)
+
+
+if __name__ == "__main__":
+    main()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/optimize_onnx_model.py` & `onnxruntime/tools/optimize_onnx_model.py`

 * *Files 12% similar despite different names*

```diff
@@ -6,41 +6,50 @@
 import os
 import pathlib
 
 from .onnx_model_utils import get_optimization_level, optimize_model
 
 
 def optimize_model_helper():
-    parser = argparse.ArgumentParser(f'{os.path.basename(__file__)}:{optimize_model_helper.__name__}',
-                                     description='''
+    parser = argparse.ArgumentParser(
+        f"{os.path.basename(__file__)}:{optimize_model_helper.__name__}",
+        description="""
                                      Optimize an ONNX model using ONNX Runtime to the specified level.
                                      See https://onnxruntime.ai/docs/performance/graph-optimizations.html for more
-                                     details of the optimization levels.'''
-                                     )
+                                     details of the optimization levels.""",
+    )
 
-    parser.add_argument('--opt_level', default='basic',
-                        choices=['disable', 'basic', 'extended', 'all'],
-                        help="Optimization level to use.")
-    parser.add_argument('--log_level', choices=['debug', 'info', 'warning', 'error'], type=str, required=False,
-                        default='error',
-                        help="Log level. Defaults to Error so we don't get output about unused initializers "
-                             "being removed. Warning or Info may be desirable in some scenarios.")
+    parser.add_argument(
+        "--opt_level",
+        default="basic",
+        choices=["disable", "basic", "extended", "all"],
+        help="Optimization level to use.",
+    )
+    parser.add_argument(
+        "--log_level",
+        choices=["debug", "info", "warning", "error"],
+        type=str,
+        required=False,
+        default="error",
+        help="Log level. Defaults to Error so we don't get output about unused initializers "
+        "being removed. Warning or Info may be desirable in some scenarios.",
+    )
 
-    parser.add_argument('input_model', type=pathlib.Path, help='Provide path to ONNX model to update.')
-    parser.add_argument('output_model', type=pathlib.Path, help='Provide path to write optimized ONNX model to.')
+    parser.add_argument("input_model", type=pathlib.Path, help="Provide path to ONNX model to update.")
+    parser.add_argument("output_model", type=pathlib.Path, help="Provide path to write optimized ONNX model to.")
 
     args = parser.parse_args()
 
-    if args.log_level == 'error':
+    if args.log_level == "error":
         log_level = 3
-    elif args.log_level == 'debug':
+    elif args.log_level == "debug":
         log_level = 0  # ORT verbose level
-    elif args.log_level == 'info':
+    elif args.log_level == "info":
         log_level = 1
-    elif args.log_level == 'warning':
+    elif args.log_level == "warning":
         log_level = 2
 
     optimize_model(args.input_model, args.output_model, get_optimization_level(args.opt_level), log_level)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     optimize_model_helper()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/pytorch_export_contrib_ops.py` & `onnxruntime/tools/pytorch_export_contrib_ops.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,33 +1,33 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 """
 Support for registering ONNX Runtime's built-in contrib ops with
 PyTorch-ONNX exporter (torch.onnx.export).
 """
-
 import typing
 
 try:
-    from torch.onnx import register_custom_op_symbolic
+    # TODO(justinchuby): Create a function to alert users when torch is not installed
+    import torch
 except ModuleNotFoundError:
-    raise ModuleNotFoundError(
-        "This module is only useful in combination with PyTorch. "
-        "To install PyTorch see https://pytorch.org/.")
-import torch.onnx.symbolic_helper as sym_help
-import torch.onnx.symbolic_registry as sym_registry
+    raise ModuleNotFoundError(  # noqa: B904
+        "This module is only useful in combination with PyTorch. To install PyTorch see https://pytorch.org/."
+    )
+
+from torch.onnx import symbolic_helper
 
 _OPSET_VERSION = 1
 _registered_ops: typing.AbstractSet[str] = set()
 
 
 def _reg(symbolic_fn: typing.Callable):
     name = "::%s" % symbolic_fn.__name__
-    register_custom_op_symbolic(name, symbolic_fn, _OPSET_VERSION)
+    torch.onnx.register_custom_op_symbolic(name, symbolic_fn, _OPSET_VERSION)
     _registered_ops.add(name)
 
 
 def register():
     """Register ONNX Runtime's built-in contrib ops.
 
     Should be run before torch.onnx.export().
@@ -38,54 +38,71 @@
         #   'bilinear'      : onnx::Constant[value={0}]
         #   'nearest'       : onnx::Constant[value={1}]
         #   'bicubic'       : onnx::Constant[value={2}]
         # padding_mode
         #   'zeros'         : onnx::Constant[value={0}]
         #   'border'        : onnx::Constant[value={1}]
         #   'reflection'    : onnx::Constant[value={2}]
-        mode = sym_help._maybe_get_const(mode, "i")
-        padding_mode = sym_help._maybe_get_const(padding_mode, "i")
-        mode_str = ['bilinear', 'nearest', 'bicubic'][mode]
-        padding_mode_str = ['zeros', 'border', 'reflection'][padding_mode]
-        align_corners = int(sym_help._maybe_get_const(align_corners, "b"))
+        mode = symbolic_helper._maybe_get_const(mode, "i")
+        padding_mode = symbolic_helper._maybe_get_const(padding_mode, "i")
+        mode_str = ["bilinear", "nearest", "bicubic"][mode]
+        padding_mode_str = ["zeros", "border", "reflection"][padding_mode]
+        align_corners = int(symbolic_helper._maybe_get_const(align_corners, "b"))
 
         # From opset v13 onward, the output shape can be specified with
         # (N, C, H, W) (N, H_out, W_out, 2) => (N, C, H_out, W_out)
         # input_shape = input.type().sizes()
         # gird_shape = grid.type().sizes()
         # output_shape = input_shape[:2] + gird_shape[1:3]
         # g.op(...).setType(input.type().with_sizes(output_shape))
 
-        return g.op("com.microsoft::GridSample", input, grid,
-                    mode_s=mode_str,
-                    padding_mode_s=padding_mode_str,
-                    align_corners_i=align_corners)
+        return g.op(
+            "com.microsoft::GridSample",
+            input,
+            grid,
+            mode_s=mode_str,
+            padding_mode_s=padding_mode_str,
+            align_corners_i=align_corners,
+        )
+
     _reg(grid_sampler)
 
     def inverse(g, self):
         return g.op("com.microsoft::Inverse", self).setType(self.type())
+
     _reg(inverse)
 
-    def gelu(g, self):
-        return g.op("com.microsoft::Gelu", self).setType(self.type())
+    @torch.onnx.symbolic_helper.parse_args("v", "s")
+    def gelu(g, self: torch._C.Value, approximate: str = "none"):
+        # Use microsoft::Gelu for performance if possible. It only supports approximate == "none"
+        if approximate == "none":
+            return g.op("com.microsoft::Gelu", self).setType(self.type())
+        return torch.onnx.symbolic_opset9.gelu(g, self, approximate)
+
     _reg(gelu)
 
     def triu(g, self, diagonal):
         return g.op("com.microsoft::Trilu", self, diagonal, upper_i=1).setType(self.type())
+
     _reg(triu)
 
     def tril(g, self, diagonal):
         return g.op("com.microsoft::Trilu", self, diagonal, upper_i=0).setType(self.type())
-    _reg(tril)
 
+    _reg(tril)
 
 
 def unregister():
     """Unregister ONNX Runtime's built-in contrib ops."""
-    # TODO: replace this once PyTorch supports unregister natively.
-    # https://msdata.visualstudio.com/Vienna/_workitems/edit/1342343
     for name in _registered_ops:
-        ns, kind = name.split("::")
-        for version in sym_help._onnx_stable_opsets:
-            if (version >= _OPSET_VERSION and
-                sym_registry.is_registered_op(kind, ns, version)):
-                del sym_registry._registry[(ns, version)][kind]
+        try:
+            torch.onnx.unregister_custom_op_symbolic(name, _OPSET_VERSION)
+        except AttributeError:
+            # The symbolic_registry module was removed in PyTorch 1.13.
+            # We are importing it here for backwards compatibility
+            # because unregister_custom_op_symbolic is not available before PyTorch 1.12
+            from torch.onnx import symbolic_registry
+
+            namespace, kind = name.split("::")
+            for version in symbolic_helper._onnx_stable_opsets:
+                if version >= _OPSET_VERSION and symbolic_registry.is_registered_op(kind, namespace, version):
+                    del symbolic_registry._registry[(namespace, version)][kind]
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/pytorch_export_helpers.py` & `onnxruntime/tools/pytorch_export_helpers.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 import inspect
-import torch
-
 from collections import abc
 
+import torch
+
 
 def _parse_inputs_for_onnx_export(all_input_parameters, inputs, kwargs):
-    # extracted from https://github.com/microsoft/onnxruntime/blob/239c6ad3f021ff7cc2e6247eb074bd4208dc11e2/orttraining/orttraining/python/training/ortmodule/_io.py#L433  # noqa
+    # extracted from https://github.com/microsoft/onnxruntime/blob/239c6ad3f021ff7cc2e6247eb074bd4208dc11e2/orttraining/orttraining/python/training/ortmodule/_io.py#L433
 
     def _add_input(name, input):
         """Returns number of expanded inputs that _add_input processed"""
 
         if input is None:
             # Drop all None inputs and return 0.
             return 0
@@ -50,25 +50,27 @@
     var_positional_idx = 0
     num_expanded_non_none_positional_inputs = 0
 
     for input_idx, input_parameter in enumerate(all_input_parameters):
         if input_parameter.kind == inspect.Parameter.VAR_POSITIONAL:
             # VAR_POSITIONAL parameter carries all *args parameters from original forward method
             for args_i in range(input_idx, len(inputs)):
-                name = f'{input_parameter.name}_{var_positional_idx}'
+                name = f"{input_parameter.name}_{var_positional_idx}"
                 var_positional_idx += 1
                 inp = inputs[args_i]
                 num_expanded_non_none_positional_inputs += _add_input(name, inp)
-        elif input_parameter.kind == inspect.Parameter.POSITIONAL_ONLY or \
-                input_parameter.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD or \
-                input_parameter.kind == inspect.Parameter.KEYWORD_ONLY:
+        elif (
+            input_parameter.kind == inspect.Parameter.POSITIONAL_ONLY
+            or input_parameter.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD
+            or input_parameter.kind == inspect.Parameter.KEYWORD_ONLY
+        ):
             # All positional non-*args and non-**kwargs are processed here
             name = input_parameter.name
             inp = None
-            input_idx += var_positional_idx
+            input_idx += var_positional_idx  # noqa: PLW2901
             is_positional = True
             if input_idx < len(inputs) and inputs[input_idx] is not None:
                 inp = inputs[input_idx]
             elif name in kwargs and kwargs[name] is not None:
                 inp = kwargs[name]
                 is_positional = False
             num_expanded_non_none_inputs_local = _add_input(name, inp)
@@ -80,46 +82,50 @@
                 if name not in input_names:
                     _add_input(name, inp)
 
     return input_names
 
 
 def _flatten_module_input(names, args, kwargs):
-    '''Flatten args and kwargs in a single tuple of tensors.'''
-    # extracted from https://github.com/microsoft/onnxruntime/blob/239c6ad3f021ff7cc2e6247eb074bd4208dc11e2/orttraining/orttraining/python/training/ortmodule/_io.py#L110  # noqa
+    """Flatten args and kwargs in a single tuple of tensors."""
+    # extracted from https://github.com/microsoft/onnxruntime/blob/239c6ad3f021ff7cc2e6247eb074bd4208dc11e2/orttraining/orttraining/python/training/ortmodule/_io.py#L110
+
+    def is_primitive_type(value):
+        return type(value) in {int, bool, float}
 
-    def is_primitive_type(value): return type(value) in {int, bool, float}
-    def to_tensor(value): return torch.tensor(value)
+    def to_tensor(value):
+        return torch.tensor(value)
 
     ret = [to_tensor(arg) if is_primitive_type(arg) else arg for arg in args]
-    ret += [to_tensor(kwargs[name]) if is_primitive_type(kwargs[name])
-            else kwargs[name] for name in names if name in kwargs]
+    ret += [
+        to_tensor(kwargs[name]) if is_primitive_type(kwargs[name]) else kwargs[name] for name in names if name in kwargs
+    ]
 
     # if kwargs is empty, append an empty dictionary at the end of the sample inputs to make exporter
     # happy. This is because the exporter is confused with kwargs and dictionary inputs otherwise.
     if not kwargs:
         ret.append({})
 
     return tuple(ret)
 
 
 def infer_input_info(module: torch.nn.Module, *inputs, **kwargs):
-    '''
+    """
     Infer the input names and order from the arguments used to execute a PyTorch module for usage exporting
     the model via torch.onnx.export.
     Assumes model is on CPU. Use `module.to(torch.device('cpu'))` if it isn't.
 
     Example usage:
     input_names, inputs_as_tuple = infer_input_info(module, ...)
     torch.onnx.export(module, inputs_as_type, 'model.onnx', input_names=input_names, output_names=[...], ...)
 
     :param module: Module
     :param inputs: Positional inputs
     :param kwargs: Keyword argument inputs
     :return: Tuple of ordered input names and input values. These can be used directly with torch.onnx.export as the
             `input_names` and `inputs` arguments.
-    '''
+    """
     module_parameters = inspect.signature(module.forward).parameters.values()
     input_names = _parse_inputs_for_onnx_export(module_parameters, inputs, kwargs)
     inputs_as_tuple = _flatten_module_input(input_names, inputs, kwargs)
 
     return input_names, inputs_as_tuple
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/reduced_build_config_parser.py` & `onnxruntime/tools/reduced_build_config_parser.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 import os
 
 # Check if the flatbuffers module is available. If not we cannot handle type reduction information in the config.
 try:
-    import flatbuffers  # noqa
+    import flatbuffers  # noqa: F401
+
     have_flatbuffers = True
-    from .ort_format_model import GloballyAllowedTypesOpTypeImplFilter, OperatorTypeUsageManager  # noqa
+    from .ort_format_model import GloballyAllowedTypesOpTypeImplFilter, OperatorTypeUsageManager
 except ImportError:
     have_flatbuffers = False
 
 
 def parse_config(config_file: str, enable_type_reduction: bool = False):
-    '''
+    """
     Parse the configuration file and return the required operators dictionary and an
     OpTypeImplFilterInterface instance.
 
     Configuration file lines can do the following:
     1. specify required operators
     2. specify globally allowed types for all operators
     3. specify what it means for no required operators to be specified
@@ -74,18 +75,18 @@
                                   If False the type information will be ignored.
                                   If the flatbuffers module is unavailable type information will be ignored as the
                                   type-based filtering has a dependency on the ORT flatbuffers schema.
     :return: required_ops: Dictionary of domain:opset:[ops] for required operators. If None, all operators are
                            required.
              op_type_impl_filter: OpTypeImplFilterInterface instance if type reduction is enabled, the flatbuffers
                                   module is available, and type reduction information is present. None otherwise.
-    '''
+    """
 
     if not os.path.isfile(config_file):
-        raise ValueError('Configuration file {} does not exist'.format(config_file))
+        raise ValueError(f"Configuration file {config_file} does not exist")
 
     # only enable type reduction when flatbuffers is available
     enable_type_reduction = enable_type_reduction and have_flatbuffers
 
     required_ops = {}
     no_ops_specified_means_all_ops_are_required = False
     op_type_usage_manager = OperatorTypeUsageManager() if enable_type_reduction else None
@@ -97,44 +98,44 @@
             return True
 
         if line.startswith("!globally_allowed_types;"):  # handle globally allowed types
             if enable_type_reduction:
                 nonlocal globally_allowed_types
                 if globally_allowed_types is not None:
                     raise RuntimeError("Globally allowed types were already specified.")
-                globally_allowed_types = set(segment.strip() for segment in line.split(';')[1].split(','))
+                globally_allowed_types = {segment.strip() for segment in line.split(";")[1].split(",")}
             return True
 
         if line == "!no_ops_specified_means_all_ops_are_required":  # handle all ops required line
             nonlocal no_ops_specified_means_all_ops_are_required
             no_ops_specified_means_all_ops_are_required = True
             return True
 
         return False
 
-    with open(config_file, 'r') as config:
+    with open(config_file) as config:
         for line in [orig_line.strip() for orig_line in config.readlines()]:
             if process_non_op_line(line):
                 continue
 
-            domain, opset_str, operators_str = [segment.strip() for segment in line.split(';')]
-            opsets = [int(s) for s in opset_str.split(',')]
+            domain, opset_str, operators_str = (segment.strip() for segment in line.split(";"))
+            opsets = [int(s) for s in opset_str.split(",")]
 
             # any type reduction information is serialized json that starts/ends with { and }.
             # type info is optional for each operator.
-            if '{' in operators_str:
+            if "{" in operators_str:
                 has_op_type_reduction_info = True
 
                 # parse the entries in the json dictionary with type info
                 operators = set()
                 cur = 0
                 end = len(operators_str)
                 while cur < end:
-                    next_comma = operators_str.find(',', cur)
-                    next_open_brace = operators_str.find('{', cur)
+                    next_comma = operators_str.find(",", cur)
+                    next_open_brace = operators_str.find("{", cur)
 
                     if next_comma == -1:
                         next_comma = end
 
                     # the json string starts with '{', so if that is found (next_open_brace != -1)
                     # before the next comma (which would be the start of the next operator if there is no type info
                     # for the current operator), we have type info to parse.
@@ -146,36 +147,36 @@
                         operators.add(operator)
 
                         # parse out the json dictionary with the type info by finding the closing brace that matches
                         # the opening brace
                         i = next_open_brace + 1
                         num_open_braces = 1
                         while num_open_braces > 0 and i < end:
-                            if operators_str[i] == '{':
+                            if operators_str[i] == "{":
                                 num_open_braces += 1
-                            elif operators_str[i] == '}':
+                            elif operators_str[i] == "}":
                                 num_open_braces -= 1
                             i += 1
 
                         if num_open_braces != 0:
-                            raise RuntimeError('Mismatched { and } in type string: ' + operators_str[next_open_brace:])
+                            raise RuntimeError("Mismatched { and } in type string: " + operators_str[next_open_brace:])
 
                         if op_type_usage_manager:
                             type_str = operators_str[next_open_brace:i]
                             op_type_usage_manager.restore_from_config_entry(domain, operator, type_str)
 
                         cur = i + 1
                     else:
                         # comma or end of line is next
                         end_str = next_comma if next_comma != -1 else end
                         operators.add(operators_str[cur:end_str].strip())
                         cur = end_str + 1
 
             else:
-                operators = set([op.strip() for op in operators_str.split(',')])
+                operators = {op.strip() for op in operators_str.split(",")}
 
             for opset in opsets:
                 if domain not in required_ops:
                     required_ops[domain] = {opset: operators}
                 elif opset not in required_ops[domain]:
                     required_ops[domain][opset] = operators
                 else:
@@ -186,15 +187,16 @@
 
     op_type_impl_filter = None
     if enable_type_reduction:
         if not has_op_type_reduction_info:
             op_type_usage_manager = None
         if globally_allowed_types is not None and op_type_usage_manager is not None:
             raise RuntimeError(
-                "Specifying globally allowed types and per-op type reduction info together is unsupported.")
+                "Specifying globally allowed types and per-op type reduction info together is unsupported."
+            )
 
         if globally_allowed_types is not None:
             op_type_impl_filter = GloballyAllowedTypesOpTypeImplFilter(globally_allowed_types)
         elif op_type_usage_manager is not None:
             op_type_impl_filter = op_type_usage_manager.make_op_type_impl_filter()
 
     return required_ops, op_type_impl_filter
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/symbolic_shape_infer.py` & `onnxruntime/tools/symbolic_shape_infer.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,55 +1,63 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 # -*- coding: UTF-8 -*-
 import argparse
 import logging
+
 import numpy as np
 import onnx
-from onnx import helper, numpy_helper, shape_inference
 import sympy
-
+from onnx import helper, numpy_helper, shape_inference
 from packaging import version
+
 assert version.parse(onnx.__version__) >= version.parse("1.8.0")
 
-logger = logging.getLogger(__name__) 
+logger = logging.getLogger(__name__)
 
 
 def get_attribute(node, attr_name, default_value=None):
     found = [attr for attr in node.attribute if attr.name == attr_name]
     if found:
         return helper.get_attribute_value(found[0])
     return default_value
 
 
 def get_dim_from_proto(dim):
-    return getattr(dim, dim.WhichOneof('value')) if type(dim.WhichOneof('value')) == str else None
+    return getattr(dim, dim.WhichOneof("value")) if type(dim.WhichOneof("value")) == str else None
 
 
 def is_sequence(type_proto):
-    cls_type = type_proto.WhichOneof('value')
-    assert cls_type in ['tensor_type', 'sequence_type']
-    return cls_type == 'sequence_type'
+    cls_type = type_proto.WhichOneof("value")
+    assert cls_type in ["tensor_type", "sequence_type"]
+    return cls_type == "sequence_type"
 
 
 def get_shape_from_type_proto(type_proto):
     assert not is_sequence(type_proto)
-    if type_proto.tensor_type.HasField('shape'):
+    if type_proto.tensor_type.HasField("shape"):
         return [get_dim_from_proto(d) for d in type_proto.tensor_type.shape.dim]
     else:
         return None  # note no shape is different from shape without dim (scalar)
 
 
+def get_elem_type_from_type_proto(type_proto):
+    if is_sequence(type_proto):
+        return type_proto.sequence_type.elem_type.tensor_type.elem_type
+    else:
+        return type_proto.tensor_type.elem_type
+
+
 def get_shape_from_value_info(vi):
-    cls_type = vi.type.WhichOneof('value')
+    cls_type = vi.type.WhichOneof("value")
     if cls_type is None:
         return None
     if is_sequence(vi.type):
-        if 'tensor_type' == vi.type.sequence_type.elem_type.WhichOneof('value'):
+        if vi.type.sequence_type.elem_type.WhichOneof("value") == "tensor_type":
             return get_shape_from_type_proto(vi.type.sequence_type.elem_type)
         else:
             return None
     else:
         return get_shape_from_type_proto(vi.type)
 
 
@@ -60,24 +68,24 @@
 
 
 def get_shape_from_sympy_shape(sympy_shape):
     return [None if i is None else (int(i) if is_literal(i) else str(i)) for i in sympy_shape]
 
 
 def is_literal(dim):
-    return type(dim) in [int, np.int64, np.int32, sympy.Integer] or (hasattr(dim, 'is_number') and dim.is_number)
+    return type(dim) in [int, np.int64, np.int32, sympy.Integer] or (hasattr(dim, "is_number") and dim.is_number)
 
 
 def handle_negative_axis(axis, rank):
     assert axis < rank and axis >= -rank
     return axis if axis >= 0 else rank + axis
 
 
 def get_opset(mp, domain=None):
-    domain = domain or ['', 'onnx', 'ai.onnx']
+    domain = domain or ["", "onnx", "ai.onnx"]
     if type(domain) != list:
         domain = [domain]
     for opset in mp.opset_import:
         if opset.domain in domain:
             return opset.version
 
     return None
@@ -111,101 +119,119 @@
             value = value * v
     else:
         value = x
     return value
 
 
 class SymbolicShapeInference:
-    def __init__(self, int_max, auto_merge, guess_output_rank, verbose, prefix=''):
+    def __init__(self, int_max, auto_merge, guess_output_rank, verbose, prefix=""):
         self.dispatcher_ = {
-            'Add': self._infer_symbolic_compute_ops,
-            'ArrayFeatureExtractor': self._infer_ArrayFeatureExtractor,
-            'AveragePool': self._infer_Pool,
-            'BatchNormalization': self._infer_BatchNormalization,
-            'Cast': self._infer_Cast,
-            'CategoryMapper': self._infer_CategoryMapper,
-            'Compress': self._infer_Compress,
-            'Concat': self._infer_Concat,
-            'ConcatFromSequence': self._infer_ConcatFromSequence,
-            'Constant': self._infer_Constant,
-            'ConstantOfShape': self._infer_ConstantOfShape,
-            'Conv': self._infer_Conv,
-            'CumSum': self._pass_on_shape_and_type,
-            'Div': self._infer_symbolic_compute_ops,
-            'Einsum': self._infer_Einsum,
-            'Expand': self._infer_Expand,
-            'Equal': self._infer_symbolic_compute_ops,
-            'Floor': self._infer_symbolic_compute_ops,
-            'Gather': self._infer_Gather,
-            'GatherElements': self._infer_GatherElements,
-            'GatherND': self._infer_GatherND,
-            'Gelu': self._pass_on_shape_and_type,
-            'If': self._infer_If,
-            'Loop': self._infer_Loop,
-            'MatMul': self._infer_MatMul,
-            'MatMulInteger16': self._infer_MatMulInteger,
-            'MaxPool': self._infer_Pool,
-            'Max': self._infer_symbolic_compute_ops,
-            'Min': self._infer_symbolic_compute_ops,
-            'Mul': self._infer_symbolic_compute_ops,
-            'NonMaxSuppression': self._infer_NonMaxSuppression,
-            'NonZero': self._infer_NonZero,
-            'OneHot': self._infer_OneHot,
-            'Pad': self._infer_Pad,
-            'Range': self._infer_Range,
-            'Reciprocal': self._pass_on_shape_and_type,
-            'ReduceSum': self._infer_ReduceSum,
-            'ReduceProd': self._infer_ReduceProd,
-            'Reshape': self._infer_Reshape,
-            'Resize': self._infer_Resize,
-            'Round': self._pass_on_shape_and_type,
-            'Scan': self._infer_Scan,
-            'ScatterElements': self._infer_ScatterElements,
-            'SequenceAt': self._infer_SequenceAt,
-            'SequenceInsert': self._infer_SequenceInsert,
-            'Shape': self._infer_Shape,
-            'Size': self._infer_Size,
-            'Slice': self._infer_Slice,
-            'SoftmaxCrossEntropyLoss': self._infer_SoftmaxCrossEntropyLoss,
-            'SoftmaxCrossEntropyLossInternal': self._infer_SoftmaxCrossEntropyLoss,
-            'NegativeLogLikelihoodLossInternal': self._infer_SoftmaxCrossEntropyLoss,
-            'Split': self._infer_Split,
-            'SplitToSequence': self._infer_SplitToSequence,
-            'Squeeze': self._infer_Squeeze,
-            'Sub': self._infer_symbolic_compute_ops,
-            'Tile': self._infer_Tile,
-            'TopK': self._infer_TopK,
-            'Transpose': self._infer_Transpose,
-            'Unsqueeze': self._infer_Unsqueeze,
-            'Where': self._infer_symbolic_compute_ops,
-            'ZipMap': self._infer_ZipMap,
-            'Neg': self._infer_symbolic_compute_ops,
+            "Add": self._infer_symbolic_compute_ops,
+            "ArrayFeatureExtractor": self._infer_ArrayFeatureExtractor,
+            "AveragePool": self._infer_Pool,
+            "BatchNormalization": self._infer_BatchNormalization,
+            "Cast": self._infer_Cast,
+            "CategoryMapper": self._infer_CategoryMapper,
+            "Compress": self._infer_Compress,
+            "Concat": self._infer_Concat,
+            "ConcatFromSequence": self._infer_ConcatFromSequence,
+            "Constant": self._infer_Constant,
+            "ConstantOfShape": self._infer_ConstantOfShape,
+            "Conv": self._infer_Conv,
+            "CumSum": self._pass_on_shape_and_type,
+            "Div": self._infer_symbolic_compute_ops,
+            "Einsum": self._infer_Einsum,
+            "Expand": self._infer_Expand,
+            "Equal": self._infer_symbolic_compute_ops,
+            "Floor": self._infer_symbolic_compute_ops,
+            "Gather": self._infer_Gather,
+            "GatherElements": self._infer_GatherElements,
+            "GatherND": self._infer_GatherND,
+            "Identity": self._pass_on_shape_and_type,
+            "If": self._infer_If,
+            "Loop": self._infer_Loop,
+            "MatMul": self._infer_MatMul,
+            "MatMulInteger16": self._infer_MatMulInteger,
+            "MaxPool": self._infer_Pool,
+            "Max": self._infer_symbolic_compute_ops,
+            "Min": self._infer_symbolic_compute_ops,
+            "Mul": self._infer_symbolic_compute_ops,
+            "NonMaxSuppression": self._infer_NonMaxSuppression,
+            "NonZero": self._infer_NonZero,
+            "OneHot": self._infer_OneHot,
+            "Pad": self._infer_Pad,
+            "Range": self._infer_Range,
+            "Reciprocal": self._pass_on_shape_and_type,
+            "ReduceSum": self._infer_ReduceSum,
+            "ReduceProd": self._infer_ReduceProd,
+            "RelativePositionBias": self._infer_RelativePositionBias,
+            "Reshape": self._infer_Reshape,
+            "Resize": self._infer_Resize,
+            "Round": self._pass_on_shape_and_type,
+            "Scan": self._infer_Scan,
+            "ScatterElements": self._infer_ScatterElements,
+            "SequenceAt": self._infer_SequenceAt,
+            "SequenceInsert": self._infer_SequenceInsert,
+            "Shape": self._infer_Shape,
+            "Size": self._infer_Size,
+            "Slice": self._infer_Slice,
+            "SoftmaxCrossEntropyLoss": self._infer_SoftmaxCrossEntropyLoss,
+            "SoftmaxCrossEntropyLossInternal": self._infer_SoftmaxCrossEntropyLoss,
+            "NegativeLogLikelihoodLossInternal": self._infer_SoftmaxCrossEntropyLoss,
+            "Split": self._infer_Split,
+            "SplitToSequence": self._infer_SplitToSequence,
+            "Squeeze": self._infer_Squeeze,
+            "Sub": self._infer_symbolic_compute_ops,
+            "Tile": self._infer_Tile,
+            "TopK": self._infer_TopK,
+            "Transpose": self._infer_Transpose,
+            "Unsqueeze": self._infer_Unsqueeze,
+            "Where": self._infer_symbolic_compute_ops,
+            "ZipMap": self._infer_ZipMap,
+            "Neg": self._infer_symbolic_compute_ops,
             # contrib ops:
-            'Attention': self._infer_Attention,
-            'BiasGelu': self._infer_BiasGelu,
-            'EmbedLayerNormalization': self._infer_EmbedLayerNormalization,
-            'FastGelu': self._infer_FastGelu,
-            'Gelu': self._infer_Gelu,
-            'LayerNormalization': self._infer_LayerNormalization,
-            'LongformerAttention': self._infer_LongformerAttention,
-            'PythonOp': self._infer_PythonOp,
-            'SkipLayerNormalization': self._infer_SkipLayerNormalization
+            "Attention": self._infer_Attention,
+            "PackedAttention": self._infer_PackedAttention,
+            "RemovePadding": self._infer_RemovePadding,
+            "RestorePadding": self._infer_RestorePadding,
+            "BiasGelu": self._infer_BiasGelu,
+            "MultiHeadAttention": self._infer_MultiHeadAttention,
+            "EmbedLayerNormalization": self._infer_EmbedLayerNormalization,
+            "FastGelu": self._infer_FastGelu,
+            "Gelu": self._infer_Gelu,
+            "GemmFastGelu": self._infer_GemmFastGelu,
+            "LayerNormalization": self._infer_LayerNormalization,
+            "LongformerAttention": self._infer_LongformerAttention,
+            "PythonOp": self._infer_PythonOp,
+            "SimplifiedLayerNormalization": self._infer_LayerNormalization,
+            "SkipLayerNormalization": self._infer_SkipLayerNormalization,
+            "SkipSimplifiedLayerNormalization": self._infer_SkipLayerNormalization,
+            "GroupNorm": self._infer_GroupNorm,
+            "BiasSplitGelu": self._infer_BiasSplitGelu,
+            "BiasAdd": self._infer_BiasAdd,
+            "NhwcConv": self._infer_NhwcConv,
         }
         self.aten_op_dispatcher_ = {
-            'aten::embedding': self._infer_Gather,
-            'aten::bitwise_or': self._infer_aten_bitwise_or,
-            'aten::diagonal': self._infer_aten_diagonal,
-            'aten::max_pool2d_with_indices': self._infer_aten_pool2d,
-            'aten::multinomial': self._infer_aten_multinomial,
-            'aten::unfold': self._infer_aten_unfold,
-            'aten::argmax': self._infer_aten_argmax,
-            'aten::avg_pool2d': self._infer_aten_pool2d,
-            'aten::_adaptive_avg_pool2d': self._infer_aten_pool2d,
-            'aten::binary_cross_entropy_with_logits': self._infer_aten_bce,
-            'aten::numpy_T': self._infer_Transpose,
+            "embedding": self._infer_Gather,
+            "bitwise_or": self._infer_aten_bitwise_or,
+            "diagonal": self._infer_aten_diagonal,
+            "max_pool2d_with_indices": self._infer_aten_pool2d,
+            "max": self._infer_aten_minmax,
+            "min": self._infer_aten_minmax,
+            "multinomial": self._infer_aten_multinomial,
+            "unfold": self._infer_aten_unfold,
+            "argmax": self._infer_aten_argmax,
+            "avg_pool2d": self._infer_aten_pool2d,
+            "_adaptive_avg_pool2d": self._infer_aten_pool2d,
+            "numpy_T": self._infer_Transpose,
+            "native_group_norm": self._infer_aten_group_norm,
+            "upsample_nearest1d": self._infer_aten_upsample,
+            "upsample_nearest2d": self._infer_aten_upsample,
+            "upsample_nearest3d": self._infer_aten_upsample,
+            "upsample_bilinear2d": self._infer_aten_upsample,
         }
         self.run_ = True
         self.suggested_merge_ = {}
         self.symbolic_dims_ = {}
         self.input_symbols_ = {}
         self.auto_merge_ = auto_merge
         self.guess_output_rank_ = guess_output_rank
@@ -237,15 +263,15 @@
             for s in symbols:
                 if type(self.symbolic_dims_[s]) == sympy.Symbol:
                     map_to = s
                     break
         # when nothing to map to, use the shorter one
         if map_to is None:
             if self.verbose_ > 0:
-                logger.warning('Potential unsafe merge between symbolic expressions: ({})'.format(','.join(symbols)))
+                logger.warning("Potential unsafe merge between symbolic expressions: ({})".format(",".join(symbols)))
             symbols_list = list(symbols)
             lens = [len(s) for s in symbols_list]
             map_to = symbols_list[lens.index(min(lens))]
             symbols.remove(map_to)
 
         for s in symbols:
             if s == map_to:
@@ -270,37 +296,44 @@
                         d.dim_value = int(v)
                     else:
                         d.dim_param = v
 
     def _preprocess(self, in_mp):
         self.out_mp_ = onnx.ModelProto()
         self.out_mp_.CopyFrom(in_mp)
-        self.graph_inputs_ = dict([(i.name, i) for i in list(self.out_mp_.graph.input)])
-        self.initializers_ = dict([(i.name, i) for i in self.out_mp_.graph.initializer])
-        self.known_vi_ = dict([(i.name, i) for i in list(self.out_mp_.graph.input)])
+        self.graph_inputs_ = {i.name: i for i in list(self.out_mp_.graph.input)}
+        self.initializers_ = {i.name: i for i in self.out_mp_.graph.initializer}
+        self.known_vi_ = {i.name: i for i in list(self.out_mp_.graph.input)}
         self.known_vi_.update(
-            dict([(i.name, helper.make_tensor_value_info(i.name, i.data_type, list(i.dims)))
-                  for i in self.out_mp_.graph.initializer]))
+            {
+                i.name: helper.make_tensor_value_info(i.name, i.data_type, list(i.dims))
+                for i in self.out_mp_.graph.initializer
+            }
+        )
 
     def _merge_symbols(self, dims):
         if not all([type(d) == str for d in dims]):
             if self.auto_merge_:
                 unique_dims = list(set(dims))
                 is_int = [is_literal(d) for d in unique_dims]
                 assert sum(is_int) <= 1  # if there are more than 1 unique ints, something is wrong
                 if sum(is_int) == 1:
                     int_dim = is_int.index(1)
                     if self.verbose_ > 0:
-                        logger.debug('dim {} has been merged with value {}'.format(
-                            unique_dims[:int_dim] + unique_dims[int_dim + 1:], unique_dims[int_dim]))
+                        logger.debug(
+                            "dim {} has been merged with value {}".format(
+                                unique_dims[:int_dim] + unique_dims[int_dim + 1 :],
+                                unique_dims[int_dim],
+                            )
+                        )
                     self._check_merged_dims(unique_dims, allow_broadcast=False)
                     return unique_dims[int_dim]
                 else:
                     if self.verbose_ > 0:
-                        logger.debug('dim {} has been mergd with dim {}'.format(unique_dims[1:], unique_dims[0]))
+                        logger.debug(f"dim {unique_dims[1:]} has been merged with dim {unique_dims[0]}")
                     return dims[0]
             else:
                 return None
         if all([d == dims[0] for d in dims]):
             return dims[0]
         merged = [self.suggested_merge_[d] if d in self.suggested_merge_ else d for d in dims]
         if all([d == merged[0] for d in merged]):
@@ -327,38 +360,52 @@
                 if not new_dim:
                     # warning about unsupported broadcast when not auto merge
                     # note that auto merge has the risk of incorrectly merge symbols while one of them being 1
                     # for example, 'a' = 1, 'b' = 5 at runtime is valid broadcasting, but with auto merge 'a' == 'b'
                     if self.auto_merge_:
                         self._add_suggested_merge([dim1, dim2], apply=True)
                     else:
-                        logger.warning('unsupported broadcast between ' + str(dim1) + ' ' + str(dim2))
-            new_shape = [new_dim] + new_shape
+                        logger.warning("unsupported broadcast between " + str(dim1) + " " + str(dim2))
+            new_shape = [new_dim, *new_shape]
         return new_shape
 
     def _get_shape(self, node, idx):
         name = node.input[idx]
         if name in self.known_vi_:
             vi = self.known_vi_[name]
             return get_shape_from_value_info(vi)
         else:
             assert name in self.initializers_
             return list(self.initializers_[name].dims)
 
+    def _try_get_shape(self, node, idx):
+        if idx > len(node.input) - 1:
+            return None
+        name = node.input[idx]
+        if name in self.known_vi_:
+            vi = self.known_vi_[name]
+            return get_shape_from_value_info(vi)
+        if name in self.initializers_:
+            return list(self.initializers_[name].dims)
+        return None
+
     def _get_shape_rank(self, node, idx):
         return len(self._get_shape(node, idx))
 
     def _get_sympy_shape(self, node, idx):
         sympy_shape = []
         for d in self._get_shape(node, idx):
             if type(d) == str:
-                sympy_shape.append(self.symbolic_dims_[d] if d in
-                                   self.symbolic_dims_ else sympy.Symbol(d, integer=True, nonnegative=True))
+                sympy_shape.append(
+                    self.symbolic_dims_[d]
+                    if d in self.symbolic_dims_
+                    else sympy.Symbol(d, integer=True, nonnegative=True)
+                )
             else:
-                assert None != d
+                assert None is not d
                 sympy_shape.append(d)
         return sympy_shape
 
     def _get_value(self, node, idx):
         name = node.input[idx]
         assert name in self.sympy_data_ or name in self.initializers_
         return self.sympy_data_[name] if name in self.sympy_data_ else numpy_helper.to_array(self.initializers_[name])
@@ -369,136 +416,169 @@
         name = node.input[idx]
         if name in self.sympy_data_ or name in self.initializers_:
             return self._get_value(node, idx)
         return None
 
     def _update_computed_dims(self, new_sympy_shape):
         for i, new_dim in enumerate(new_sympy_shape):
-            if not is_literal(new_dim) and not type(new_dim) == str:
+            if not is_literal(new_dim) and type(new_dim) != str:
                 str_dim = str(new_dim)
                 if str_dim in self.suggested_merge_:
                     if is_literal(self.suggested_merge_[str_dim]):
                         continue  # no need to create dim for literals
                     new_sympy_shape[i] = self.symbolic_dims_[self.suggested_merge_[str_dim]]
                 else:
                     # add new_dim if it's a computational expression
-                    if not str(new_dim) in self.symbolic_dims_:
+                    if str(new_dim) not in self.symbolic_dims_:
                         self.symbolic_dims_[str(new_dim)] = new_dim
 
     def _onnx_infer_single_node(self, node):
         # skip onnx shape inference for some ops, as they are handled in _infer_*
         skip_infer = node.op_type in [
-            'If', 'Loop', 'Scan', 'SplitToSequence', 'ZipMap', \
-            # contrib ops
-
-
-            'Attention', 'BiasGelu', \
-            'EmbedLayerNormalization', \
-            'FastGelu', 'Gelu', 'LayerNormalization', \
-            'LongformerAttention', \
-            'SkipLayerNormalization', \
-            'PythonOp'
+            "If",
+            "Loop",
+            "Scan",
+            "SplitToSequence",
+            "ZipMap",  # contrib ops
+            "Attention",
+            "BiasGelu",
+            "EmbedLayerNormalization",
+            "FastGelu",
+            "Gelu",
+            "GemmFastGelu",
+            "LayerNormalization",
+            "LongformerAttention",
+            "RelativePositionBias",
+            "RemovePadding",
+            "RestorePadding",
+            "SimplifiedLayerNormalization",
+            "SkipLayerNormalization",
+            "SkipSimplifiedLayerNormalization",
+            "PackedAttention",
+            "PythonOp",
+            "MultiHeadAttention",
+            "GroupNorm",
+            "BiasSplitGelu",
+            "BiasAdd",
+            "NhwcConv",
         ]
 
         if not skip_infer:
             # Only pass initializers that satisfy the following condition:
             # (1) Operator need value of some input for shape inference.
             #     For example, Unsqueeze in opset 13 uses the axes input to calculate shape of output.
             # (2) opset version >= 9. In older version, initializer is required in graph input by onnx spec.
             # (3) The initializer is not in graph input. The means the node input is "constant" in inference.
             initializers = []
-            if (get_opset(self.out_mp_) >= 9) and node.op_type in ['Unsqueeze']:
+            if (get_opset(self.out_mp_) >= 9) and node.op_type in ["Unsqueeze"]:
                 initializers = [
-                    self.initializers_[name] for name in node.input
+                    self.initializers_[name]
+                    for name in node.input
                     if (name in self.initializers_ and name not in self.graph_inputs_)
                 ]
 
             # run single node inference with self.known_vi_ shapes
-            tmp_graph = helper.make_graph([node], 'tmp', [self.known_vi_[i] for i in node.input if i],
-                                          [make_named_value_info(i) for i in node.output], initializers)
+            tmp_graph = helper.make_graph(
+                [node],
+                "tmp",
+                [self.known_vi_[i] for i in node.input if i],
+                [make_named_value_info(i) for i in node.output],
+                initializers,
+            )
 
             self.tmp_mp_.graph.CopyFrom(tmp_graph)
 
             self.tmp_mp_ = shape_inference.infer_shapes(self.tmp_mp_)
 
         for i_o in range(len(node.output)):
             o = node.output[i_o]
-            vi = self.out_mp_.graph.value_info.add()
-            if not skip_infer:
-                vi.CopyFrom(self.tmp_mp_.graph.output[i_o])
-            else:
-                vi.name = o
-            self.known_vi_[o] = vi
+            if o:  # skip optional output
+                vi = self.out_mp_.graph.value_info.add()
+                if not skip_infer:
+                    vi.CopyFrom(self.tmp_mp_.graph.output[i_o])
+                else:
+                    vi.name = o
+                self.known_vi_[o] = vi
 
     def _onnx_infer_subgraph(self, node, subgraph, use_node_input=True, inc_subgraph_id=True):
         if self.verbose_ > 2:
-            logger.debug('Inferencing subgraph of node {} with output({}...): {}'.format(node.name, node.output[0],
-                                                                                  node.op_type))
+            logger.debug(f"Inferencing subgraph of node {node.name} with output({node.output[0]}...): {node.op_type}")
         # node inputs are not passed directly to the subgraph
         # it's up to the node dispatcher to prepare subgraph input
         # for example, with Scan/Loop, subgraph input shape would be trimmed from node input shape
         # besides, inputs in subgraph could shadow implicit inputs
-        subgraph_inputs = set([i.name for i in list(subgraph.initializer) + list(subgraph.input)])
-        subgraph_implicit_input = set([name for name in self.known_vi_.keys() if not name in subgraph_inputs])
-        tmp_graph = helper.make_graph(list(subgraph.node), 'tmp',
-                                      list(subgraph.input) + [self.known_vi_[i] for i in subgraph_implicit_input],
-                                      [make_named_value_info(i.name) for i in subgraph.output])
+        subgraph_inputs = {i.name for i in list(subgraph.initializer) + list(subgraph.input)}
+        subgraph_implicit_input = {name for name in self.known_vi_ if name not in subgraph_inputs}
+        tmp_graph = helper.make_graph(
+            list(subgraph.node),
+            "tmp",
+            list(subgraph.input) + [self.known_vi_[i] for i in subgraph_implicit_input],
+            [make_named_value_info(i.name) for i in subgraph.output],
+        )
         tmp_graph.initializer.extend([i for i in self.out_mp_.graph.initializer if i.name in subgraph_implicit_input])
         tmp_graph.initializer.extend(subgraph.initializer)
         self.tmp_mp_.graph.CopyFrom(tmp_graph)
 
-        symbolic_shape_inference = SymbolicShapeInference(self.int_max_,
-                                                          self.auto_merge_,
-                                                          self.guess_output_rank_,
-                                                          self.verbose_,
-                                                          prefix=self.prefix_ + '_' + str(self.subgraph_id_))
+        symbolic_shape_inference = SymbolicShapeInference(
+            self.int_max_,
+            self.auto_merge_,
+            self.guess_output_rank_,
+            self.verbose_,
+            prefix=self.prefix_ + "_" + str(self.subgraph_id_),
+        )
         if inc_subgraph_id:
             self.subgraph_id_ += 1
 
-        all_shapes_inferred = False
         symbolic_shape_inference._preprocess(self.tmp_mp_)
         symbolic_shape_inference.suggested_merge_ = self.suggested_merge_.copy()
         while symbolic_shape_inference.run_:
-            all_shapes_inferred = symbolic_shape_inference._infer_impl(self.sympy_data_.copy())
+            symbolic_shape_inference._infer_impl(self.sympy_data_.copy())
         symbolic_shape_inference._update_output_from_vi()
         if use_node_input:
             # if subgraph uses node input, it needs to update to merged dims
-            subgraph.ClearField('input')
-            subgraph.input.extend(symbolic_shape_inference.out_mp_.graph.input[:len(node.input)])
-        subgraph.ClearField('output')
+            subgraph.ClearField("input")
+            subgraph.input.extend(symbolic_shape_inference.out_mp_.graph.input[: len(node.input)])
+        subgraph.ClearField("output")
         subgraph.output.extend(symbolic_shape_inference.out_mp_.graph.output)
-        subgraph.ClearField('value_info')
+        subgraph.ClearField("value_info")
         subgraph.value_info.extend(symbolic_shape_inference.out_mp_.graph.value_info)
-        subgraph.ClearField('node')
+        subgraph.ClearField("node")
         subgraph.node.extend(symbolic_shape_inference.out_mp_.graph.node)
         # for new symbolic dims from subgraph output, add to main graph symbolic dims
         subgraph_shapes = [get_shape_from_value_info(o) for o in symbolic_shape_inference.out_mp_.graph.output]
-        subgraph_new_symbolic_dims = set(
-            [d for s in subgraph_shapes if s for d in s if type(d) == str and not d in self.symbolic_dims_])
+        subgraph_new_symbolic_dims = {
+            d for s in subgraph_shapes if s for d in s if type(d) == str and d not in self.symbolic_dims_
+        }
         new_dims = {}
         for d in subgraph_new_symbolic_dims:
             assert d in symbolic_shape_inference.symbolic_dims_
             new_dims[d] = symbolic_shape_inference.symbolic_dims_[d]
         self.symbolic_dims_.update(new_dims)
         return symbolic_shape_inference
 
-    def _get_int_values(self, node, broadcast=False):
+    def _get_int_or_float_values(self, node, broadcast=False, allow_float_values=False):
+        def int_or_float(value, allow_float_values):
+            # If casting into int has precision loss: keep float output
+            if allow_float_values and value % 1 != 0:
+                return value
+            return int(value)
+
         values = [self._try_get_value(node, i) for i in range(len(node.input))]
         if all([v is not None for v in values]):
             # some shape compute is in floating point, cast to int for sympy
             for i, v in enumerate(values):
                 if type(v) != np.ndarray:
                     continue
                 if len(v.shape) > 1:
                     new_v = None  # ignore value for rank > 1
                 elif len(v.shape) == 0:
-                    new_v = int(v.item())
+                    new_v = int_or_float(v.item(), allow_float_values)
                 else:
                     assert len(v.shape) == 1
-                    new_v = [int(vv) for vv in v]
+                    new_v = [int_or_float(vv, allow_float_values) for vv in v]
                 values[i] = new_v
         values_len = [len(v) if type(v) == list else 0 for v in values]
         max_len = max(values_len)
         if max_len >= 1 and broadcast:
             # broadcast
             for i, v in enumerate(values):
                 if v is None:
@@ -510,111 +590,139 @@
                         assert len(v) == max_len
                 else:
                     values[i] = [v] * max_len
         return values
 
     def _compute_on_sympy_data(self, node, op_func):
         assert len(node.output) == 1
-        values = self._get_int_values(node, broadcast=True)
+
+        # Before mul & div operations
+        # cast inputs into interger might lose decimal part and reduce precision
+        # keep them as float, finish the operation, then cast the result into integer
+        if node.op_type in ["Mul", "Div"]:
+            values = self._get_int_or_float_values(node, broadcast=True, allow_float_values=True)
+        else:
+            values = self._get_int_or_float_values(node, broadcast=True)
+
         if all([v is not None for v in values]):
             is_list = [type(v) == list for v in values]
             as_list = any(is_list)
             if as_list:
                 self.sympy_data_[node.output[0]] = [op_func(vs) for vs in zip(*values)]
             else:
                 self.sympy_data_[node.output[0]] = op_func(values)
 
     def _pass_on_sympy_data(self, node):
-        assert len(node.input) == 1 or node.op_type in ['Reshape', 'Unsqueeze', 'Squeeze']
+        assert len(node.input) == 1 or node.op_type in [
+            "Reshape",
+            "Unsqueeze",
+            "Squeeze",
+        ]
         self._compute_on_sympy_data(node, lambda x: x[0])
 
     def _pass_on_shape_and_type(self, node):
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                          self._get_shape(node, 0)))
+            helper.make_tensor_value_info(
+                node.output[0],
+                get_elem_type_from_type_proto(self.known_vi_[node.input[0]].type),
+                self._get_shape(node, 0),
+            )
+        )
 
     def _new_symbolic_dim(self, prefix, dim):
-        new_dim = '{}_d{}'.format(prefix, dim)
+        new_dim = f"{prefix}_d{dim}"
         if new_dim in self.suggested_merge_:
             v = self.suggested_merge_[new_dim]
             new_symbolic_dim = sympy.Integer(int(v)) if is_literal(v) else v
         else:
             new_symbolic_dim = sympy.Symbol(new_dim, integer=True, nonnegative=True)
             self.symbolic_dims_[new_dim] = new_symbolic_dim
         return new_symbolic_dim
 
     def _new_symbolic_dim_from_output(self, node, out_idx=0, dim=0):
         return self._new_symbolic_dim(
-            '{}{}_{}_o{}_'.format(node.op_type, self.prefix_,
-                                  list(self.out_mp_.graph.node).index(node), out_idx), dim)
+            "{}{}_{}_o{}_".format(
+                node.op_type,
+                self.prefix_,
+                list(self.out_mp_.graph.node).index(node),
+                out_idx,
+            ),
+            dim,
+        )
 
     def _new_symbolic_shape(self, rank, node, out_idx=0):
         return [self._new_symbolic_dim_from_output(node, out_idx, i) for i in range(rank)]
 
-    def _compute_conv_pool_shape(self, node):
+    def _compute_conv_pool_shape(self, node, channels_last=False):
         sympy_shape = self._get_sympy_shape(node, 0)
         if len(node.input) > 1:
-            W_shape = self._get_sympy_shape(node, 1)
+            W_shape = self._get_sympy_shape(node, 1)  # noqa: N806
             rank = len(W_shape) - 2  # number of spatial axes
-            kernel_shape = W_shape[-rank:]
-            sympy_shape[1] = W_shape[0]
+            kernel_shape = W_shape[-rank - 1 : -1] if channels_last else W_shape[-rank:]
+            sympy_shape[3 if channels_last else 1] = W_shape[0]
         else:
-            W_shape = None
-            kernel_shape = get_attribute(node, 'kernel_shape')
+            W_shape = None  # noqa: N806
+            kernel_shape = get_attribute(node, "kernel_shape")
             rank = len(kernel_shape)
 
         assert len(sympy_shape) == rank + 2
 
         # only need to symbolic shape inference if input has symbolic dims in spatial axes
-        is_symbolic_dims = [not is_literal(i) for i in sympy_shape[-rank:]]
+        spatial_shape = sympy_shape[-rank - 1 : -1] if channels_last else sympy_shape[-rank:]
+        is_symbolic_dims = [not is_literal(i) for i in spatial_shape]
 
         if not any(is_symbolic_dims):
             shape = get_shape_from_value_info(self.known_vi_[node.output[0]])
             if len(shape) > 0:
                 assert len(sympy_shape) == len(shape)
-                sympy_shape[-rank:] = [sympy.Integer(d) for d in shape[-rank:]]
+                if channels_last:
+                    sympy_shape[-rank - 1 : -1] = [sympy.Integer(d) for d in shape[-rank - 1 : -1]]
+                else:
+                    sympy_shape[-rank:] = [sympy.Integer(d) for d in shape[-rank:]]
                 return sympy_shape
 
-        dilations = get_attribute(node, 'dilations', [1] * rank)
-        strides = get_attribute(node, 'strides', [1] * rank)
+        dilations = get_attribute(node, "dilations", [1] * rank)
+        strides = get_attribute(node, "strides", [1] * rank)
         effective_kernel_shape = [(k - 1) * d + 1 for k, d in zip(kernel_shape, dilations)]
-        pads = get_attribute(node, 'pads')
+        pads = get_attribute(node, "pads")
         if pads is None:
             pads = [0] * (2 * rank)
-            auto_pad = get_attribute(node, 'auto_pad', b'NOTSET').decode('utf-8')
-            if auto_pad != 'VALID' and auto_pad != 'NOTSET':
+            auto_pad = get_attribute(node, "auto_pad", b"NOTSET").decode("utf-8")
+            if auto_pad != "VALID" and auto_pad != "NOTSET":
                 try:
                     residual = [sympy.Mod(d, s) for d, s in zip(sympy_shape[-rank:], strides)]
                     total_pads = [
                         max(0, (k - s) if r == 0 else (k - r))
                         for k, s, r in zip(effective_kernel_shape, strides, residual)
                     ]
                 except TypeError:  # sympy may throw TypeError: cannot determine truth value of Relational
-                    total_pads = [max(0, (k - s)) for k, s in zip(effective_kernel_shape, strides)
-                                  ]  # assuming no residual if sympy throws error
-            elif auto_pad == 'VALID':
+                    total_pads = [
+                        max(0, (k - s)) for k, s in zip(effective_kernel_shape, strides)
+                    ]  # assuming no residual if sympy throws error
+            elif auto_pad == "VALID":
                 total_pads = []
             else:
                 total_pads = [0] * rank
         else:
             assert len(pads) == 2 * rank
             total_pads = [p1 + p2 for p1, p2 in zip(pads[:rank], pads[rank:])]
 
-        ceil_mode = get_attribute(node, 'ceil_mode', 0)
+        ceil_mode = get_attribute(node, "ceil_mode", 0)
         for i in range(rank):
-            effective_input_size = sympy_shape[-rank + i]
+            effective_input_size = sympy_shape[-rank + i + (-1 if channels_last else 0)]
             if len(total_pads) > 0:
                 effective_input_size = effective_input_size + total_pads[i]
             if ceil_mode:
                 strided_kernel_positions = sympy.ceiling(
-                    (effective_input_size - effective_kernel_shape[i]) / strides[i])
+                    (effective_input_size - effective_kernel_shape[i]) / strides[i]
+                )
             else:
                 strided_kernel_positions = (effective_input_size - effective_kernel_shape[i]) // strides[i]
-            sympy_shape[-rank + i] = strided_kernel_positions + 1
+            sympy_shape[-rank + i + (-1 if channels_last else 0)] = strided_kernel_positions + 1
         return sympy_shape
 
     def _check_merged_dims(self, dims, allow_broadcast=True):
         if allow_broadcast:
             dims = [d for d in dims if not (is_literal(d) and int(d) <= 1)]
         if not all([d == dims[0] for d in dims]):
             self._add_suggested_merge(dims, apply=True)
@@ -634,127 +742,136 @@
             new_shape = rhs_shape[:rhs_reduce_dim] + [rhs_shape[-1]]
         elif rhs_rank == 1:
             lhs_reduce_dim = -1
             new_shape = lhs_shape[:lhs_reduce_dim]
         else:
             lhs_reduce_dim = -1
             rhs_reduce_dim = -2
-            new_shape = self._broadcast_shapes(lhs_shape[:-2], rhs_shape[:-2]) + [lhs_shape[-2]] + [rhs_shape[-1]]
+            new_shape = [*self._broadcast_shapes(lhs_shape[:-2], rhs_shape[:-2]), lhs_shape[-2]] + [rhs_shape[-1]]
         # merge reduce dim
-        self._check_merged_dims([lhs_shape[lhs_reduce_dim], rhs_shape[rhs_reduce_dim]], allow_broadcast=False)
+        self._check_merged_dims(
+            [lhs_shape[lhs_reduce_dim], rhs_shape[rhs_reduce_dim]],
+            allow_broadcast=False,
+        )
         if output_dtype is None:
             # infer output_dtype from input type when not specified
             output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, new_shape))
 
     def _fuse_tensor_type(self, node, out_idx, dst_type, src_type):
-        '''
+        """
         update dst_tensor_type to be compatible with src_tensor_type when dimension mismatches
-        '''
-        dst_tensor_type = dst_type.sequence_type.elem_type.tensor_type if is_sequence(
-            dst_type) else dst_type.tensor_type
-        src_tensor_type = src_type.sequence_type.elem_type.tensor_type if is_sequence(
-            src_type) else src_type.tensor_type
+        """
+        dst_tensor_type = (
+            dst_type.sequence_type.elem_type.tensor_type if is_sequence(dst_type) else dst_type.tensor_type
+        )
+        src_tensor_type = (
+            src_type.sequence_type.elem_type.tensor_type if is_sequence(src_type) else src_type.tensor_type
+        )
         if dst_tensor_type.elem_type != src_tensor_type.elem_type:
             node_id = node.name if node.name else node.op_type
-            raise ValueError(f"For node {node_id}, dst_tensor_type.elem_type != src_tensor_type.elem_type: "
-                             f"{onnx.onnx_pb.TensorProto.DataType.Name(dst_tensor_type.elem_type)} vs "
-                             f"{onnx.onnx_pb.TensorProto.DataType.Name(src_tensor_type.elem_type)}")
-        if dst_tensor_type.HasField('shape'):
+            raise ValueError(
+                f"For node {node_id}, dst_tensor_type.elem_type != src_tensor_type.elem_type: "
+                f"{onnx.onnx_pb.TensorProto.DataType.Name(dst_tensor_type.elem_type)} vs "
+                f"{onnx.onnx_pb.TensorProto.DataType.Name(src_tensor_type.elem_type)}"
+            )
+        if dst_tensor_type.HasField("shape"):
             for di, ds in enumerate(zip(dst_tensor_type.shape.dim, src_tensor_type.shape.dim)):
                 if ds[0] != ds[1]:
                     # create a new symbolic dimension for node/out_idx/mismatch dim id in dst_tensor_type for tensor_type
                     # for sequence_type, clear the dimension
                     new_dim = onnx.TensorShapeProto.Dimension()
                     if not is_sequence(dst_type):
                         new_dim.dim_param = str(self._new_symbolic_dim_from_output(node, out_idx, di))
                     dst_tensor_type.shape.dim[di].CopyFrom(new_dim)
         else:
             dst_tensor_type.CopyFrom(src_tensor_type)
 
-    def _infer_ArrayFeatureExtractor(self, node):
+    def _infer_ArrayFeatureExtractor(self, node):  # noqa: N802
         data_shape = self._get_shape(node, 0)
         indices_shape = self._get_shape(node, 1)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                          data_shape[:-1] + indices_shape))
+            helper.make_tensor_value_info(
+                node.output[0],
+                self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                data_shape[:-1] + indices_shape,
+            )
+        )
 
     def _infer_symbolic_compute_ops(self, node):
         funcs = {
-            'Add':
-            lambda l: l[0] + l[1],
-            'Div':
-            lambda l: l[0] // l[1],  # integer div in sympy
-            'Equal':
-            lambda l: l[0] == l[1],
-            'Floor':
-            lambda l: sympy.floor(l[0]),
-            'Max':
-            lambda l: l[1] if is_literal(l[0]) and int(l[0]) < -self.int_max_ else
-            (l[0] if is_literal(l[1]) and int(l[1]) < -self.int_max_ else sympy.Max(l[0], l[1])),
-            'Min':
-            lambda l: l[1] if is_literal(l[0]) and int(l[0]) > self.int_max_ else
-            (l[0] if is_literal(l[1]) and int(l[1]) > self.int_max_ else sympy.Min(l[0], l[1])),
-            'Mul':
-            lambda l: l[0] * l[1],
-            'Sub':
-            lambda l: l[0] - l[1],
-            'Where':
-            lambda l: l[1] if l[0] else l[2],
-            'Neg':
-            lambda l: -l[0]
+            "Add": lambda l: l[0] + l[1],  # noqa: E741
+            "Div": lambda l: int(l[0] // l[1])  # noqa: E741
+            if isinstance(l[0] // l[1], float)
+            else l[0] // l[1],  # integer div in sympy
+            "Equal": lambda l: l[0] == l[1],  # noqa: E741
+            "Floor": lambda l: sympy.floor(l[0]),  # noqa: E741
+            "Max": lambda l: l[1]  # noqa: E741
+            if is_literal(l[0]) and int(l[0]) < -self.int_max_
+            else (l[0] if is_literal(l[1]) and int(l[1]) < -self.int_max_ else sympy.Max(l[0], l[1])),
+            "Min": lambda l: l[1]  # noqa: E741
+            if is_literal(l[0]) and int(l[0]) > self.int_max_
+            else (l[0] if is_literal(l[1]) and int(l[1]) > self.int_max_ else sympy.Min(l[0], l[1])),
+            "Mul": lambda l: int(l[0] * l[1]) if isinstance(l[0] * l[1], float) else l[0] * l[1],  # noqa: E741
+            "Sub": lambda l: l[0] - l[1],  # noqa: E741
+            "Where": lambda l: l[1] if l[0] else l[2],  # noqa: E741
+            "Neg": lambda l: -l[0],  # noqa: E741
         }
         assert node.op_type in funcs
         self._compute_on_sympy_data(node, funcs[node.op_type])
 
-    def _infer_Cast(self, node):
+    def _infer_Cast(self, node):  # noqa: N802
         self._pass_on_sympy_data(node)
 
-    def _infer_CategoryMapper(self, node):
+    def _infer_CategoryMapper(self, node):  # noqa: N802
         input_type = self.known_vi_[node.input[0]].type.tensor_type.elem_type
         if input_type == onnx.TensorProto.STRING:
             output_type = onnx.TensorProto.INT64
         else:
             output_type = onnx.TensorProto.STRING
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_type, self._get_shape(node, 0)))
 
-    def _infer_Compress(self, node):
+    def _infer_Compress(self, node):  # noqa: N802
         input_shape = self._get_shape(node, 0)
         # create a new symbolic dimension for Compress output
         compress_len = str(self._new_symbolic_dim_from_output(node))
-        axis = get_attribute(node, 'axis')
-        if axis == None:
+        axis = get_attribute(node, "axis")
+        if axis is None:
             # when axis is not specified, input is flattened before compress so output is 1D
             output_shape = [compress_len]
         else:
             output_shape = input_shape
             output_shape[handle_negative_axis(axis, len(input_shape))] = compress_len
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                          output_shape))
+            helper.make_tensor_value_info(
+                node.output[0],
+                self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                output_shape,
+            )
+        )
 
-    def _infer_Concat(self, node):
+    def _infer_Concat(self, node):  # noqa: N802
         if any([i in self.sympy_data_ or i in self.initializers_ for i in node.input]):
-            values = self._get_int_values(node)
+            values = self._get_int_or_float_values(node)
             if all([v is not None for v in values]):
-                assert 0 == get_attribute(node, 'axis')
+                assert get_attribute(node, "axis") == 0
                 self.sympy_data_[node.output[0]] = []
                 for i in range(len(node.input)):
                     value = values[i]
                     if type(value) == list:
                         self.sympy_data_[node.output[0]].extend(value)
                     else:
                         self.sympy_data_[node.output[0]].append(value)
 
         sympy_shape = self._get_sympy_shape(node, 0)
-        axis = handle_negative_axis(get_attribute(node, 'axis'), len(sympy_shape))
+        axis = handle_negative_axis(get_attribute(node, "axis"), len(sympy_shape))
         for i_idx in range(1, len(node.input)):
             input_shape = self._get_sympy_shape(node, i_idx)
             if input_shape:
                 sympy_shape[axis] = sympy_shape[axis] + input_shape[axis]
         self._update_computed_dims(sympy_shape)
         # merge symbolic dims for non-concat axes
         for d in range(len(sympy_shape)):
@@ -766,184 +883,231 @@
             merged = self._merge_symbols(dims)
             if type(merged) == str:
                 sympy_shape[d] = self.symbolic_dims_[merged] if merged else None
             else:
                 sympy_shape[d] = merged
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                          get_shape_from_sympy_shape(sympy_shape)))
+            helper.make_tensor_value_info(
+                node.output[0],
+                self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                get_shape_from_sympy_shape(sympy_shape),
+            )
+        )
 
-    def _infer_ConcatFromSequence(self, node):
+    def _infer_ConcatFromSequence(self, node):  # noqa: N802
         seq_shape = self._get_shape(node, 0)
-        new_axis = 1 if get_attribute(node, 'new_axis') else 0
-        axis = handle_negative_axis(get_attribute(node, 'axis'), len(seq_shape) + new_axis)
+        new_axis = 1 if get_attribute(node, "new_axis") else 0
+        axis = handle_negative_axis(get_attribute(node, "axis"), len(seq_shape) + new_axis)
         concat_dim = str(self._new_symbolic_dim_from_output(node, 0, axis))
         new_shape = seq_shape
         if new_axis:
             new_shape = seq_shape[:axis] + [concat_dim] + seq_shape[axis:]
         else:
             new_shape[axis] = concat_dim
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(
-                node.output[0], self.known_vi_[node.input[0]].type.sequence_type.elem_type.tensor_type.elem_type,
-                new_shape))
+                node.output[0],
+                self.known_vi_[node.input[0]].type.sequence_type.elem_type.tensor_type.elem_type,
+                new_shape,
+            )
+        )
 
-    def _infer_Constant(self, node):
-        t = get_attribute(node, 'value')
+    def _infer_Constant(self, node):  # noqa: N802
+        t = get_attribute(node, "value")
         self.sympy_data_[node.output[0]] = numpy_helper.to_array(t)
 
-    def _infer_ConstantOfShape(self, node):
-        sympy_shape = self._get_int_values(node)[0]
+    def _infer_ConstantOfShape(self, node):  # noqa: N802
+        sympy_shape = self._get_int_or_float_values(node)[0]
         vi = self.known_vi_[node.output[0]]
         if sympy_shape is not None:
             if type(sympy_shape) != list:
                 sympy_shape = [sympy_shape]
             self._update_computed_dims(sympy_shape)
             # update sympy data if output type is int, and shape is known
             if vi.type.tensor_type.elem_type == onnx.TensorProto.INT64 and all([is_literal(x) for x in sympy_shape]):
                 self.sympy_data_[node.output[0]] = np.ones(
-                    [int(x)
-                     for x in sympy_shape], dtype=np.int64) * numpy_helper.to_array(get_attribute(node, 'value', 0))
+                    [int(x) for x in sympy_shape], dtype=np.int64
+                ) * numpy_helper.to_array(get_attribute(node, "value", 0))
         else:
             # create new dynamic shape
             # note input0 is a 1D vector of shape, the new symbolic shape has the rank of the shape vector length
             sympy_shape = self._new_symbolic_shape(self._get_shape(node, 0)[0], node)
 
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], vi.type.tensor_type.elem_type,
-                                          get_shape_from_sympy_shape(sympy_shape)))
+            helper.make_tensor_value_info(
+                node.output[0],
+                vi.type.tensor_type.elem_type,
+                get_shape_from_sympy_shape(sympy_shape),
+            )
+        )
 
-    def _infer_Conv(self, node):
+    def _infer_Conv(self, node):  # noqa: N802
         sympy_shape = self._compute_conv_pool_shape(node)
         self._update_computed_dims(sympy_shape)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], vi.type.tensor_type.elem_type,
-                                          get_shape_from_sympy_shape(sympy_shape)))
+            helper.make_tensor_value_info(
+                node.output[0],
+                vi.type.tensor_type.elem_type,
+                get_shape_from_sympy_shape(sympy_shape),
+            )
+        )
 
-    def _infer_Einsum(self, node):
+    def _infer_NhwcConv(self, node):  # noqa: N802
+        sympy_shape = self._compute_conv_pool_shape(node, channels_last=True)
+        self._update_computed_dims(sympy_shape)
+        vi = self.known_vi_[node.output[0]]
+        vi.CopyFrom(
+            helper.make_tensor_value_info(
+                node.output[0],
+                self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                get_shape_from_sympy_shape(sympy_shape),
+            )
+        )
+
+    def _infer_Einsum(self, node):  # noqa: N802
         # ref:https://github.com/onnx/onnx/blob/623dfaa0151b2e4ce49779c3ec31cbd78c592b80/onnx/defs/math/defs.cc#L3275
-        equation = get_attribute(node, 'equation')
-        equation = equation.replace(b' ', b'')
-        mid_index = equation.find(b'->')
+        equation = get_attribute(node, "equation")
+        equation = equation.replace(b" ", b"")
+        mid_index = equation.find(b"->")
         left_equation = equation[:mid_index] if mid_index != -1 else equation
 
         num_operands = 0
         num_ellipsis = 0
         num_ellipsis_indices = 0
 
         letter_to_dim = {}
 
-        terms = left_equation.split(b',')
+        terms = left_equation.split(b",")
         for term in terms:
-            ellipsis_index = term.find(b'...')
+            ellipsis_index = term.find(b"...")
             shape = self._get_shape(node, num_operands)
             rank = len(shape)
             if ellipsis_index != -1:
                 if num_ellipsis == 0:
                     num_ellipsis_indices = rank - len(term) + 3
                 num_ellipsis = num_ellipsis + 1
             for i in range(1, rank + 1):
                 letter = term[-i]
-                if letter != 46: # letter != b'.'
+                if letter != 46:  # letter != b'.'
                     dim = shape[-i]
                     if letter not in letter_to_dim.keys():
                         letter_to_dim[letter] = dim
                     elif type(dim) != sympy.Symbol:
                         letter_to_dim[letter] = dim
             num_operands = num_operands + 1
 
         new_sympy_shape = []
         from collections import OrderedDict
+
         num_letter_occurrences = OrderedDict()
         if mid_index != -1:
-            right_equation = equation[mid_index + 2:]
-            right_ellipsis_index = right_equation.find(b'...')
+            right_equation = equation[mid_index + 2 :]
+            right_ellipsis_index = right_equation.find(b"...")
             if right_ellipsis_index != -1:
                 for i in range(num_ellipsis_indices):
                     new_sympy_shape.append(shape[i])
             for c in right_equation:
-                if c != 46: # c != b'.'
+                if c != 46:  # c != b'.'
                     new_sympy_shape.append(letter_to_dim[c])
         else:
             for i in range(num_ellipsis_indices):
                 new_sympy_shape.append(shape[i])
             for c in left_equation:
-                if c != 44 and c != 46: # c != b',' and c != b'.':
+                if c != 44 and c != 46:  # c != b',' and c != b'.':
                     if c in num_letter_occurrences:
                         num_letter_occurrences[c] = num_letter_occurrences[c] + 1
                     else:
                         num_letter_occurrences[c] = 1
             for key, value in num_letter_occurrences.items():
                 if value == 1:
                     new_sympy_shape.append(letter_to_dim[key])
 
         output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, new_sympy_shape))
 
-    def _infer_Expand(self, node):
+    def _infer_Expand(self, node):  # noqa: N802
         expand_to_shape = as_list(self._try_get_value(node, 1), keep_none=True)
         if expand_to_shape is not None:
             # new_shape's dim can come from shape value
             self._update_computed_dims(expand_to_shape)
             shape = self._get_shape(node, 0)
             new_shape = self._broadcast_shapes(shape, get_shape_from_sympy_shape(expand_to_shape))
             vi = self.known_vi_[node.output[0]]
             vi.CopyFrom(
-                helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                              new_shape))
+                helper.make_tensor_value_info(
+                    node.output[0],
+                    self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                    new_shape,
+                )
+            )
 
-    def _infer_Gather(self, node):
+    def _infer_Gather(self, node):  # noqa: N802
         data_shape = self._get_shape(node, 0)
-        axis = handle_negative_axis(get_attribute(node, 'axis', 0), len(data_shape))
+        axis = handle_negative_axis(get_attribute(node, "axis", 0), len(data_shape))
         indices_shape = self._get_shape(node, 1)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                          data_shape[:axis] + indices_shape + data_shape[axis + 1:]))
+            helper.make_tensor_value_info(
+                node.output[0],
+                self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                data_shape[:axis] + indices_shape + data_shape[axis + 1 :],
+            )
+        )
         # for 1D input, do some sympy compute
-        if node.input[0] in self.sympy_data_ and len(data_shape) == 1 and 0 == get_attribute(node, 'axis', 0):
+        if node.input[0] in self.sympy_data_ and len(data_shape) == 1 and get_attribute(node, "axis", 0) == 0:
             idx = self._try_get_value(node, 1)
             if idx is not None:
                 data = self.sympy_data_[node.input[0]]
                 if type(data) == list:
                     if type(idx) == np.ndarray and len(idx.shape) == 1:
                         self.sympy_data_[node.output[0]] = [data[int(i)] for i in idx]
                     else:
                         self.sympy_data_[node.output[0]] = data[int(idx)]
                 else:
                     assert idx == 0 or idx == -1
                     self.sympy_data_[node.output[0]] = data
 
-    def _infer_GatherElements(self, node):
+    def _infer_GatherElements(self, node):  # noqa: N802
         indices_shape = self._get_shape(node, 1)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                          indices_shape))
+            helper.make_tensor_value_info(
+                node.output[0],
+                self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                indices_shape,
+            )
+        )
 
-    def _infer_GatherND(self, node):
+    def _infer_GatherND(self, node):  # noqa: N802
         data_shape = self._get_shape(node, 0)
         data_rank = len(data_shape)
         indices_shape = self._get_shape(node, 1)
-        indices_rank = len(indices_shape)
+        len(indices_shape)
         last_index_dimension = indices_shape[-1]
         assert is_literal(last_index_dimension) and last_index_dimension <= data_rank
         new_shape = indices_shape[:-1] + data_shape[last_index_dimension:]
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                          new_shape))
+            helper.make_tensor_value_info(
+                node.output[0],
+                self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                new_shape,
+            )
+        )
 
-    def _infer_If(self, node):
+    def _infer_If(self, node):  # noqa: N802
         # special case for constant condition, in case there are mismatching shape from the non-executed branch
-        subgraphs = [get_attribute(node, 'then_branch'), get_attribute(node, 'else_branch')]
+        subgraphs = [
+            get_attribute(node, "then_branch"),
+            get_attribute(node, "else_branch"),
+        ]
         cond = self._try_get_value(node, 0)
         if cond is not None:
             if as_scalar(cond) > 0:
                 subgraphs[1].CopyFrom(subgraphs[0])
             else:
                 subgraphs[0].CopyFrom(subgraphs[1])
 
@@ -958,16 +1122,16 @@
                     self._fuse_tensor_type(node, i_out, vi.type, subgraph.output[i_out].type)
 
                 # pass on sympy data from subgraph, if cond is constant
                 if cond is not None and i_sub == (0 if as_scalar(cond) > 0 else 1):
                     if subgraph.output[i_out].name in subgraph_infer.sympy_data_:
                         self.sympy_data_[vi.name] = subgraph_infer.sympy_data_[subgraph.output[i_out].name]
 
-    def _infer_Loop(self, node):
-        subgraph = get_attribute(node, 'body')
+    def _infer_Loop(self, node):  # noqa: N802
+        subgraph = get_attribute(node, "body")
         assert len(subgraph.input) == len(node.input)
         num_loop_carried = len(node.input) - 2  # minus the length and initial loop condition
         # when sequence_type is used as loop carried input
         # needs to run subgraph infer twice if the tensor shape in sequence contains None
         for i, si in enumerate(subgraph.input):
             si_name = si.name
             si.CopyFrom(self.known_vi_[node.input[i]])
@@ -998,66 +1162,75 @@
                         new_dim.dim_param = str(self._new_symbolic_dim_from_output(node, i_out, di))
                         si.type.tensor_type.shape.dim[di].CopyFrom(new_dim)
                         so.type.tensor_type.shape.dim[di].CopyFrom(new_dim)
                         need_second_infer = True
 
         if need_second_infer:
             if self.verbose_ > 2:
-                logger.debug("Rerun Loop: {}({}...), because of sequence in loop carried variables".format(
-                    node.name, node.output[0]))
+                logger.debug(
+                    "Rerun Loop: {}({}...), because of sequence in loop carried variables".format(
+                        node.name, node.output[0]
+                    )
+                )
             self._onnx_infer_subgraph(node, subgraph, inc_subgraph_id=False)
 
         # create a new symbolic dimension for iteration dependent dimension
         loop_iter_dim = str(self._new_symbolic_dim_from_output(node))
         for i in range(len(node.output)):
             vi = self.known_vi_[node.output[i]]
             vi.CopyFrom(subgraph.output[i + 1])  # first subgraph output is condition, not in node output
             if i >= num_loop_carried:
                 assert not is_sequence(vi.type)  # TODO: handle loop accumulation in sequence_type
                 subgraph_vi_dim = subgraph.output[i + 1].type.tensor_type.shape.dim
-                vi.type.tensor_type.shape.ClearField('dim')
+                vi.type.tensor_type.shape.ClearField("dim")
                 vi_dim = vi.type.tensor_type.shape.dim
                 vi_dim.add().dim_param = loop_iter_dim
                 vi_dim.extend(list(subgraph_vi_dim))
             vi.name = node.output[i]
 
-    def _infer_MatMul(self, node):
+    def _infer_MatMul(self, node):  # noqa: N802
         self._compute_matmul_shape(node)
 
-    def _infer_MatMulInteger(self, node):
+    def _infer_MatMulInteger(self, node):  # noqa: N802
         self._compute_matmul_shape(node, onnx.TensorProto.INT32)
 
-    def _infer_NonMaxSuppression(self, node):
+    def _infer_NonMaxSuppression(self, node):  # noqa: N802
         selected = str(self._new_symbolic_dim_from_output(node))
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64, [selected, 3]))
 
-    def _infer_NonZero(self, node):
+    def _infer_NonZero(self, node):  # noqa: N802
         input_rank = self._get_shape_rank(node, 0)
         # create a new symbolic dimension for NonZero output
         nz_len = str(self._new_symbolic_dim_from_output(node, 0, 1))
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], vi.type.tensor_type.elem_type, [input_rank, nz_len]))
 
-    def _infer_OneHot(self, node):
+    def _infer_OneHot(self, node):  # noqa: N802
         sympy_shape = self._get_sympy_shape(node, 0)
         depth = self._try_get_value(node, 1)
-        axis = get_attribute(node, 'axis', -1)
+        axis = get_attribute(node, "axis", -1)
         axis = handle_negative_axis(axis, len(sympy_shape) + 1)
         new_shape = get_shape_from_sympy_shape(
-            sympy_shape[:axis] + [self._new_symbolic_dim_from_output(node) if not is_literal(depth) else depth] +
-            sympy_shape[axis:])
+            sympy_shape[:axis]
+            + [self._new_symbolic_dim_from_output(node) if not is_literal(depth) else depth]
+            + sympy_shape[axis:]
+        )
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[2]].type.tensor_type.elem_type,
-                                          new_shape))
+            helper.make_tensor_value_info(
+                node.output[0],
+                self.known_vi_[node.input[2]].type.tensor_type.elem_type,
+                new_shape,
+            )
+        )
 
-    def _infer_Pad(self, node):
+    def _infer_Pad(self, node):  # noqa: N802
         if get_opset(self.out_mp_) <= 10:
-            pads = get_attribute(node, 'pads')
+            pads = get_attribute(node, "pads")
         else:
             pads = self._try_get_value(node, 1)
 
         sympy_shape = self._get_sympy_shape(node, 0)
         rank = len(sympy_shape)
 
         if pads is not None:
@@ -1069,36 +1242,39 @@
         else:
             # dynamic pads, create new symbolic dimensions
             new_sympy_shape = self._new_symbolic_shape(rank, node)
         output_tp = self.known_vi_[node.input[0]].type.tensor_type.elem_type
 
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], output_tp, get_shape_from_sympy_shape(new_sympy_shape)))
+            helper.make_tensor_value_info(node.output[0], output_tp, get_shape_from_sympy_shape(new_sympy_shape))
+        )
 
-    def _infer_Pool(self, node):
+    def _infer_Pool(self, node):  # noqa: N802
         sympy_shape = self._compute_conv_pool_shape(node)
         self._update_computed_dims(sympy_shape)
         for o in node.output:
             if not o:
                 continue
             vi = self.known_vi_[o]
             vi.CopyFrom(
-                helper.make_tensor_value_info(o, vi.type.tensor_type.elem_type,
-                                              get_shape_from_sympy_shape(sympy_shape)))
+                helper.make_tensor_value_info(
+                    o,
+                    vi.type.tensor_type.elem_type,
+                    get_shape_from_sympy_shape(sympy_shape),
+                )
+            )
 
     def _infer_aten_bitwise_or(self, node):
         shape0 = self._get_shape(node, 0)
         shape1 = self._get_shape(node, 1)
         new_shape = self._broadcast_shapes(shape0, shape1)
         t0 = self.known_vi_[node.input[0]]
         vi = self.known_vi_[node.output[0]]
-        vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], t0.type.tensor_type.elem_type,
-                                            new_shape))
+        vi.CopyFrom(helper.make_tensor_value_info(node.output[0], t0.type.tensor_type.elem_type, new_shape))
 
     def _infer_aten_diagonal(self, node):
         sympy_shape = self._get_sympy_shape(node, 0)
         rank = len(sympy_shape)
         offset = self._try_get_value(node, 1)
         dim1 = self._try_get_value(node, 2)
         dim2 = self._try_get_value(node, 3)
@@ -1119,42 +1295,83 @@
         else:
             diag_shape = sympy.Max(0, sympy.Min(shape1 + offset, shape2))
         new_shape.append(diag_shape)
 
         if node.output[0]:
             vi = self.known_vi_[node.output[0]]
             vi.CopyFrom(
-                helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                              get_shape_from_sympy_shape(new_shape)))
+                helper.make_tensor_value_info(
+                    node.output[0],
+                    self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                    get_shape_from_sympy_shape(new_shape),
+                )
+            )
 
     def _infer_aten_multinomial(self, node):
         sympy_shape = self._get_sympy_shape(node, 0)
         rank = len(sympy_shape)
-        assert rank in [1,2]
+        assert rank in [1, 2]
         num_samples = self._try_get_value(node, 1)
         di = rank - 1
         last_dim = num_samples if num_samples else str(self._new_symbolic_dim_from_output(node, 0, di))
         output_shape = sympy_shape[:-1] + [last_dim]
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64,
-                                          get_shape_from_sympy_shape(output_shape)))
+            helper.make_tensor_value_info(
+                node.output[0],
+                onnx.TensorProto.INT64,
+                get_shape_from_sympy_shape(output_shape),
+            )
+        )
 
     def _infer_aten_pool2d(self, node):
         sympy_shape = self._get_sympy_shape(node, 0)
         assert len(sympy_shape) == 4
         sympy_shape[-2:] = [self._new_symbolic_dim_from_output(node, 0, i) for i in [2, 3]]
         self._update_computed_dims(sympy_shape)
         for i, o in enumerate(node.output):
             if not o:
                 continue
             vi = self.known_vi_[o]
             elem_type = onnx.TensorProto.INT64 if i == 1 else self.known_vi_[node.input[0]].type.tensor_type.elem_type
             vi.CopyFrom(helper.make_tensor_value_info(o, elem_type, get_shape_from_sympy_shape(sympy_shape)))
 
+    def _infer_aten_minmax(self, node):
+        vi = self.known_vi_[node.output[0]]
+        if len(node.input) == 1:
+            vi.CopyFrom(
+                helper.make_tensor_value_info(
+                    node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type, []
+                )
+            )
+        else:
+            assert len(node.input) == 3
+            keepdim = self._try_get_value(node, 2)
+            assert keepdim is not None  # can only handle known keepdim case.
+            dim = self._try_get_value(node, 1)
+            if dim is None:
+                rank = self._get_shape_rank(node, 0)
+                output_shape = self._new_symbolic_shape(rank if keepdim else rank - 1, node)
+            else:
+                shape = self._get_sympy_shape(node, 0)
+                dim = handle_negative_axis(dim, len(shape))
+                output_shape = shape[:dim]
+                if keepdim:
+                    output_shape += [1]
+                output_shape += shape[dim + 1 :]
+
+            output_shape = get_shape_from_sympy_shape(output_shape)
+            vi.CopyFrom(
+                helper.make_tensor_value_info(
+                    node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type, output_shape
+                )
+            )
+            vi1 = self.known_vi_[node.output[1]]
+            vi1.CopyFrom(helper.make_tensor_value_info(node.output[1], onnx.TensorProto.INT64, output_shape))
+
     def _infer_aten_unfold(self, node):
         sympy_shape = self._get_sympy_shape(node, 0)
         dimension = self._try_get_value(node, 1)
         size = self._try_get_value(node, 2)
         step = self._try_get_value(node, 3)
         if dimension is not None and size is not None and step is not None:
             assert dimension < len(sympy_shape)
@@ -1163,20 +1380,24 @@
         else:
             rank = len(sympy_shape)
             sympy_shape = self._new_symbolic_shape(rank + 1, node)
         self._update_computed_dims(sympy_shape)
         if node.output[0]:
             vi = self.known_vi_[node.output[0]]
             vi.CopyFrom(
-                helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                              get_shape_from_sympy_shape(sympy_shape)))
+                helper.make_tensor_value_info(
+                    node.output[0],
+                    self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                    get_shape_from_sympy_shape(sympy_shape),
+                )
+            )
 
     def _infer_aten_argmax(self, node):
         new_shape = None
-        if node.input[1] == '':
+        if not node.input[1]:
             # The argmax of the flattened input is returned.
             new_shape = []
         else:
             dim = self._try_get_value(node, 1)
             keepdim = self._try_get_value(node, 2)
             if keepdim is not None:
                 sympy_shape = self._get_sympy_shape(node, 0)
@@ -1191,96 +1412,149 @@
                     sympy_shape = self._new_symbolic_shape(rank if keepdim else rank - 1, node)
                 self._update_computed_dims(sympy_shape)
                 new_shape = get_shape_from_sympy_shape(sympy_shape)
         if node.output[0] and new_shape is not None:
             vi = self.known_vi_[node.output[0]]
             vi.CopyFrom(helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64, new_shape))
 
-    def _infer_aten_bce(self, node):
-        reduction = self._try_get_value(node, 4)
-        if reduction is None:
-            reduction = 1
-        elem_type = self.known_vi_[node.input[0]].type.tensor_type.elem_type
-        vi = self.known_vi_[node.output[0]]
-        if reduction == 0:
-            vi.type.tensor_type.elem_type = elem_type
-            vi.type.tensor_type.shape.CopyFrom(onnx.TensorShapeProto())
-        else:
-            vi.CopyFrom(helper.make_tensor_value_info(vi.name, elem_type, self._get_shape(node, 0)))
+    def _infer_aten_group_norm(self, node):
+        self._propagate_shape_and_type(node)
+        input_shape = self._get_shape(node, 0)
+        N = input_shape[0] if input_shape is not None and len(input_shape) != 0 else None  # noqa: N806
+        group = self._try_get_value(node, 6)
+        output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+        for i in [1, 2]:
+            if node.output[i]:
+                vi = self.known_vi_[node.output[i]]
+                vi.CopyFrom(
+                    helper.make_tensor_value_info(
+                        node.output[i],
+                        output_dtype,
+                        [
+                            N if N is not None else str(self._new_symbolic_dim_from_output(node, i, 0)),
+                            as_scalar(group)
+                            if group is not None
+                            else str(self._new_symbolic_dim_from_output(node, i, 1)),
+                        ],
+                    )
+                )
 
-    def _infer_BatchNormalization(self, node):
+    def _infer_aten_upsample(self, node):
+        new_shape = None
+        input_shape = self._get_shape(node, 0)
+        if input_shape is not None:
+            new_shape = input_shape[:2]
+            output_size = self._try_get_value(node, 1)
+            if output_size is not None:
+                new_shape += [dim_size.item() if type(dim_size) == np.int64 else dim_size for dim_size in output_size]
+            else:
+                rank = len(input_shape)
+                new_shape += [str(self._new_symbolic_dim_from_output(node, 0, i)) for i in range(2, rank)]
+        if node.output[0] and new_shape is not None:
+            output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+            vi = self.known_vi_[node.output[0]]
+            vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, new_shape))
+
+    def _infer_BatchNormalization(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
         # this works for opsets < 14 and 14 since we check i < len(node.output) in the loop
         for i in [1, 2, 3, 4]:
-            if i < len(node.output) and node.output[i] != "":
+            if i < len(node.output) and node.output[i]:
                 # all of these parameters have the same shape as the 1st input
                 self._propagate_shape_and_type(node, input_index=1, output_index=i)
 
-    def _infer_Range(self, node):
+    def _infer_Range(self, node):  # noqa: N802
         vi = self.known_vi_[node.output[0]]
-        input_data = self._get_int_values(node)
+        input_data = self._get_int_or_float_values(node)
         if all([i is not None for i in input_data]):
             start = as_scalar(input_data[0])
             limit = as_scalar(input_data[1])
             delta = as_scalar(input_data[2])
             new_sympy_shape = [sympy.Max(sympy.ceiling((limit - start) / delta), 0)]
         else:
             new_sympy_shape = [self._new_symbolic_dim_from_output(node)]
         self._update_computed_dims(new_sympy_shape)
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                          get_shape_from_sympy_shape(new_sympy_shape)))
+            helper.make_tensor_value_info(
+                node.output[0],
+                self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                get_shape_from_sympy_shape(new_sympy_shape),
+            )
+        )
 
-    def _infer_ReduceSum(self, node):
-        keep_dims = get_attribute(node, 'keepdims', 1)
+    def _infer_ReduceSum(self, node):  # noqa: N802
+        keep_dims = get_attribute(node, "keepdims", 1)
         if get_opset(self.out_mp_) >= 13 and len(node.input) > 1:
             # ReduceSum changes axes to input[1] in opset 13
             axes = self._try_get_value(node, 1)
             vi = self.known_vi_[node.output[0]]
             if axes is None:
                 assert keep_dims  # can only handle keep_dims==True when axes is unknown, by generating new ranks
                 vi.CopyFrom(
                     helper.make_tensor_value_info(
-                        node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                        get_shape_from_sympy_shape(self._new_symbolic_shape(self._get_shape_rank(node, 0), node))))
+                        node.output[0],
+                        self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                        get_shape_from_sympy_shape(self._new_symbolic_shape(self._get_shape_rank(node, 0), node)),
+                    )
+                )
             else:
                 shape = self._get_shape(node, 0)
                 output_shape = []
                 axes = [handle_negative_axis(a, len(shape)) for a in axes]
                 for i, d in enumerate(shape):
                     if i in axes:
                         if keep_dims:
                             output_shape.append(1)
                     else:
                         output_shape.append(d)
                 vi.CopyFrom(
-                    helper.make_tensor_value_info(node.output[0],
-                                                  self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                                  output_shape))
-
-    def _infer_ReduceProd(self, node):
-        axes = get_attribute(node, 'axes')
-        keep_dims = get_attribute(node, 'keepdims', 1)
+                    helper.make_tensor_value_info(
+                        node.output[0],
+                        self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                        output_shape,
+                    )
+                )
+
+    def _infer_ReduceProd(self, node):  # noqa: N802
+        axes = get_attribute(node, "axes")
+        keep_dims = get_attribute(node, "keepdims", 1)
         if keep_dims == 0 and axes == [0]:
-            data = self._get_int_values(node)[0]
+            data = self._get_int_or_float_values(node)[0]
             if data is not None:
                 self.sympy_data_[node.output[0]] = sympy_reduce_product(data)
 
-    def _infer_Reshape(self, node):
+    def _infer_RelativePositionBias(self, node):  # noqa: N802
+        seq_len = self._try_get_value(node, 1)
+        real_seq_len = self._try_get_value(node, 2)
+        if seq_len is None or real_seq_len is None:
+            return
+        num_heads = self._get_sympy_shape(node, 0)[1]
+
+        new_shape = [1, num_heads, str(seq_len), str(real_seq_len)]
+
+        output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+        vi = self.known_vi_[node.output[0]]
+        vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, new_shape))
+
+    def _infer_Reshape(self, node):  # noqa: N802
         shape_value = self._try_get_value(node, 1)
         vi = self.known_vi_[node.output[0]]
         if shape_value is None:
             shape_shape = self._get_shape(node, 1)
             assert len(shape_shape) == 1
             shape_rank = shape_shape[0]
             assert is_literal(shape_rank)
             vi.CopyFrom(
-                helper.make_tensor_value_info(node.output[0], vi.type.tensor_type.elem_type,
-                                              get_shape_from_sympy_shape(self._new_symbolic_shape(shape_rank, node))))
+                helper.make_tensor_value_info(
+                    node.output[0],
+                    vi.type.tensor_type.elem_type,
+                    get_shape_from_sympy_shape(self._new_symbolic_shape(shape_rank, node)),
+                )
+            )
         else:
             input_sympy_shape = self._get_sympy_shape(node, 0)
             total = int(1)
             for d in input_sympy_shape:
                 total = total * d
             new_sympy_shape = []
             deferred_dim_idx = -1
@@ -1301,41 +1575,48 @@
             assert new_sympy_shape.count(-1) < 2
             if -1 in new_sympy_shape:
                 new_dim = total // non_deferred_size
                 new_sympy_shape[deferred_dim_idx] = new_dim
 
             self._update_computed_dims(new_sympy_shape)
             vi.CopyFrom(
-                helper.make_tensor_value_info(node.output[0], vi.type.tensor_type.elem_type,
-                                              get_shape_from_sympy_shape(new_sympy_shape)))
+                helper.make_tensor_value_info(
+                    node.output[0],
+                    vi.type.tensor_type.elem_type,
+                    get_shape_from_sympy_shape(new_sympy_shape),
+                )
+            )
 
         self._pass_on_sympy_data(node)
 
-    def _infer_Resize(self, node):
+    def _infer_Resize(self, node):  # noqa: N802
         vi = self.known_vi_[node.output[0]]
         input_sympy_shape = self._get_sympy_shape(node, 0)
         if get_opset(self.out_mp_) <= 10:
             scales = self._try_get_value(node, 1)
             if scales is not None:
                 new_sympy_shape = [sympy.simplify(sympy.floor(d * s)) for d, s in zip(input_sympy_shape, scales)]
                 self._update_computed_dims(new_sympy_shape)
                 vi.CopyFrom(
-                    helper.make_tensor_value_info(node.output[0],
-                                                  self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                                  get_shape_from_sympy_shape(new_sympy_shape)))
+                    helper.make_tensor_value_info(
+                        node.output[0],
+                        self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                        get_shape_from_sympy_shape(new_sympy_shape),
+                    )
+                )
         else:
             roi = self._try_get_value(node, 1)
             scales = self._try_get_value(node, 2)
             sizes = self._try_get_value(node, 3)
             if sizes is not None:
                 new_sympy_shape = [sympy.simplify(sympy.floor(s)) for s in sizes]
                 self._update_computed_dims(new_sympy_shape)
             elif scales is not None:
                 rank = len(scales)
-                if get_attribute(node, 'coordinate_transformation_mode') == 'tf_crop_and_resize':
+                if get_attribute(node, "coordinate_transformation_mode") == "tf_crop_and_resize":
                     assert len(roi) == 2 * rank
                     roi_start = list(roi)[:rank]
                     roi_end = list(roi)[rank:]
                 else:
                     roi_start = [0] * rank
                     roi_end = [1] * rank
                 scales = list(scales)
@@ -1344,90 +1625,129 @@
                     for d, start, end, scale in zip(input_sympy_shape, roi_start, roi_end, scales)
                 ]
                 self._update_computed_dims(new_sympy_shape)
             else:
                 new_sympy_shape = self._new_symbolic_shape(self._get_shape_rank(node, 0), node)
 
             vi.CopyFrom(
-                helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                              get_shape_from_sympy_shape(new_sympy_shape)))
-
-    def _infer_Scan(self, node):
-        subgraph = get_attribute(node, 'body')
-        num_scan_inputs = get_attribute(node, 'num_scan_inputs')
-        scan_input_axes = get_attribute(node, 'scan_input_axes', [0] * num_scan_inputs)
+                helper.make_tensor_value_info(
+                    node.output[0],
+                    self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                    get_shape_from_sympy_shape(new_sympy_shape),
+                )
+            )
+
+    def _infer_Scan(self, node):  # noqa: N802
+        subgraph = get_attribute(node, "body")
+        num_scan_inputs = get_attribute(node, "num_scan_inputs")
+        scan_input_axes = get_attribute(node, "scan_input_axes", [0] * num_scan_inputs)
         num_scan_states = len(node.input) - num_scan_inputs
         scan_input_axes = [
             handle_negative_axis(ax, self._get_shape_rank(node, i + num_scan_states))
             for i, ax in enumerate(scan_input_axes)
         ]
-        # We may have cases where the subgraph has optionial inputs that appear in both subgraph's input and initializer,
+        # We may have cases where the subgraph has optional inputs that appear in both subgraph's input and initializer,
         # but not in the node's input. In such cases, the input model might be invalid, but let's skip those optional inputs.
         assert len(subgraph.input) >= len(node.input)
-        subgraph_inputs = subgraph.input[:len(node.input)]
+        subgraph_inputs = subgraph.input[: len(node.input)]
         for i, si in enumerate(subgraph_inputs):
             subgraph_name = si.name
             si.CopyFrom(self.known_vi_[node.input[i]])
             if i >= num_scan_states:
                 scan_input_dim = si.type.tensor_type.shape.dim
                 scan_input_dim.remove(scan_input_dim[scan_input_axes[i - num_scan_states]])
             si.name = subgraph_name
         self._onnx_infer_subgraph(node, subgraph)
         num_scan_outputs = len(node.output) - num_scan_states
-        scan_output_axes = get_attribute(node, 'scan_output_axes', [0] * num_scan_outputs)
+        scan_output_axes = get_attribute(node, "scan_output_axes", [0] * num_scan_outputs)
         scan_input_dim = get_shape_from_type_proto(self.known_vi_[node.input[-1]].type)[scan_input_axes[-1]]
         for i, o in enumerate(node.output):
             vi = self.known_vi_[o]
             if i >= num_scan_states:
                 shape = get_shape_from_type_proto(subgraph.output[i].type)
                 new_dim = handle_negative_axis(scan_output_axes[i - num_scan_states], len(shape) + 1)
                 shape = shape[:new_dim] + [scan_input_dim] + shape[new_dim:]
                 vi.CopyFrom(helper.make_tensor_value_info(o, subgraph.output[i].type.tensor_type.elem_type, shape))
             else:
                 vi.CopyFrom(subgraph.output[i])
             vi.name = o
 
-    def _infer_ScatterElements(self, node):
+    def _infer_ScatterElements(self, node):  # noqa: N802
         data_shape = self._get_shape(node, 0)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                          data_shape))
+            helper.make_tensor_value_info(
+                node.output[0],
+                self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                data_shape,
+            )
+        )
 
-    def _infer_SequenceAt(self, node):
+    def _infer_SequenceAt(self, node):  # noqa: N802
         # need to create new symbolic dimension if sequence shape has None:
         seq_shape = self._get_shape(node, 0)
         vi = self.known_vi_[node.output[0]]
         if seq_shape is not None:
             for di, d in enumerate(seq_shape):
                 if d is not None:
                     continue
                 new_dim = onnx.TensorShapeProto.Dimension()
                 new_dim.dim_param = str(self._new_symbolic_dim_from_output(node, 0, di))
                 vi.type.tensor_type.shape.dim[di].CopyFrom(new_dim)
 
-    def _infer_SequenceInsert(self, node):
+    def _infer_SequenceInsert(self, node):  # noqa: N802
         # workaround bug in onnx's shape inference
         vi_seq = self.known_vi_[node.input[0]]
         vi_tensor = self.known_vi_[node.input[1]]
         vi_out_seq = self.known_vi_[node.output[0]]
         vi_out_seq.CopyFrom(vi_seq)
         vi_out_seq.name = node.output[0]
         self._fuse_tensor_type(node, 0, vi_out_seq.type, vi_tensor.type)
 
-    def _infer_Shape(self, node):
+    def _infer_Shape(self, node):  # noqa: N802
         self.sympy_data_[node.output[0]] = self._get_sympy_shape(node, 0)
 
-    def _infer_Size(self, node):
+    def _infer_Size(self, node):  # noqa: N802
         sympy_shape = self._get_sympy_shape(node, 0)
         self.sympy_data_[node.output[0]] = sympy_reduce_product(sympy_shape)
         self.known_vi_[node.output[0]].CopyFrom(
-            helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64, []))
+            helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64, [])
+        )
+
+    def _infer_Slice(self, node):  # noqa: N802
+        # SymPy fails to prove that `x_0 + ... + x_n >= 0` if one of `x_i` is a `sympy.Min(a, b)`,
+        # even when the relation holds for both `a` and `b`.
+        #
+        # When given `expr` of form `min(a, b) + ...`, this function returns `[a + ..., b + ...]`,
+        # so that we can prove inequalities for both expressions separately.
+        #
+        # If the number of `min(...)` subexpressions is not exactly one, this function just returns `[expr]`.
+        def flatten_min(expr):
+            assert isinstance(expr, sympy.Add), f"Expected a sum of two arguments, got {expr}"
+            min_positions = [idx for idx in range(len(expr.args)) if isinstance(expr.args[idx], sympy.Min)]
+            if len(min_positions) == 1:
+                min_pos = min_positions[0]
+
+                def replace_min_with_arg(arg_idx):
+                    replaced = list(expr.args)
+                    assert isinstance(
+                        replaced[min_pos], sympy.Min
+                    ), f"Expected a sympy.Min() at position {min_pos}, got {replaced[min_pos]}"
+                    assert (
+                        len(replaced[min_pos].args) == 2
+                    ), f"Expected a sympy.Min() with exactly 2 arguments, got {replaced[min_pos]}"
+                    replaced[min_pos] = replaced[min_pos].args[arg_idx]
+                    return sympy.Add(*replaced)
+
+                return [
+                    replace_min_with_arg(0),
+                    replace_min_with_arg(1),
+                ]
+            return [expr]
 
-    def _infer_Slice(self, node):
         def less_equal(x, y):
             try:
                 return bool(x <= y)
             except TypeError:
                 pass
             try:
                 return bool(y >= x)
@@ -1436,33 +1756,37 @@
             try:
                 return bool(-x >= -y)
             except TypeError:
                 pass
             try:
                 return bool(-y <= -x)
             except TypeError:
-                # the last attempt; this may raise TypeError
+                pass
+            try:
                 return bool(y - x >= 0)
+            except TypeError:
+                # the last attempt; this may raise TypeError
+                return all(bool(d >= 0) for d in flatten_min(y - x))
 
         def handle_negative_index(index, bound):
-            """ normalizes a negative index to be in [0, bound) """
+            """normalizes a negative index to be in [0, bound)"""
             try:
                 if not less_equal(0, index):
                     if is_literal(index) and index <= -self.int_max_:
                         # this case is handled separately
                         return index
                     return bound + index
             except TypeError:
-                logger.warning("Cannot determine if {} < 0".format(index))
+                logger.warning(f"Cannot determine if {index} < 0")
             return index
 
         if get_opset(self.out_mp_) <= 9:
-            axes = get_attribute(node, 'axes')
-            starts = get_attribute(node, 'starts')
-            ends = get_attribute(node, 'ends')
+            axes = get_attribute(node, "axes")
+            starts = get_attribute(node, "starts")
+            ends = get_attribute(node, "ends")
             if not axes:
                 axes = list(range(len(starts)))
             steps = [1] * len(axes)
         else:
             starts = as_list(self._try_get_value(node, 1), keep_none=True)
             ends = as_list(self._try_get_value(node, 2), keep_none=True)
             axes = self._try_get_value(node, 3)
@@ -1481,163 +1805,195 @@
                     new_sympy_shape[i] = self._new_symbolic_dim_from_output(node, 0, i)
             else:
                 new_sympy_shape = get_shape_from_sympy_shape(new_sympy_shape)
                 for i in axes:
                     new_sympy_shape[i] = self._new_symbolic_dim_from_output(node, 0, i)
         else:
             for i, s, e, t in zip(axes, starts, ends, steps):
-                e = handle_negative_index(e, new_sympy_shape[i])
+                e = handle_negative_index(e, new_sympy_shape[i])  # noqa: PLW2901
                 if is_literal(e):
                     if e >= self.int_max_:
-                        e = new_sympy_shape[i]
+                        e = new_sympy_shape[i]  # noqa: PLW2901
                     elif e <= -self.int_max_:
-                        e = 0 if s > 0 else -1
+                        e = 0 if s > 0 else -1  # noqa: PLW2901
                     elif is_literal(new_sympy_shape[i]):
                         if e < 0:
-                            e = max(0, e + new_sympy_shape[i])
-                        e = min(e, new_sympy_shape[i])
+                            e = max(0, e + new_sympy_shape[i])  # noqa: PLW2901
+                        e = min(e, new_sympy_shape[i])  # noqa: PLW2901
                     else:
                         if e > 0:
-                            e = sympy.Min(e, new_sympy_shape[i]
-                                          ) if e > 1 else e  #special case for slicing first to make computation easier
+                            e = (  # noqa: PLW2901
+                                sympy.Min(e, new_sympy_shape[i]) if e > 1 else e
+                            )  # special case for slicing first to make computation easier
                 else:
                     if is_literal(new_sympy_shape[i]):
-                        e = sympy.Min(e, new_sympy_shape[i])
+                        e = sympy.Min(e, new_sympy_shape[i])  # noqa: PLW2901
                     else:
                         try:
                             if not less_equal(e, new_sympy_shape[i]):
-                                e = new_sympy_shape[i]
+                                e = new_sympy_shape[i]  # noqa: PLW2901
                         except Exception:
-                            logger.warning('Unable to determine if {} <= {}, treat as equal'.format(e, new_sympy_shape[i]))
-                            e = new_sympy_shape[i]
+                            logger.warning(f"Unable to determine if {e} <= {new_sympy_shape[i]}, treat as equal")
+                            e = new_sympy_shape[i]  # noqa: PLW2901
 
-                s = handle_negative_index(s, new_sympy_shape[i])
+                s = handle_negative_index(s, new_sympy_shape[i])  # noqa: PLW2901
                 if is_literal(new_sympy_shape[i]) and is_literal(s):
-                    s = max(0, min(s, new_sympy_shape[i]))
+                    s = max(0, min(s, new_sympy_shape[i]))  # noqa: PLW2901
 
                 new_sympy_shape[i] = sympy.simplify((e - s + t + (-1 if t > 0 else 1)) // t)
 
             self._update_computed_dims(new_sympy_shape)
 
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], vi.type.tensor_type.elem_type,
-                                          get_shape_from_sympy_shape(new_sympy_shape)))
+            helper.make_tensor_value_info(
+                node.output[0],
+                vi.type.tensor_type.elem_type,
+                get_shape_from_sympy_shape(new_sympy_shape),
+            )
+        )
 
         # handle sympy_data if needed, for slice in shape computation
-        if (node.input[0] in self.sympy_data_ and [0] == axes and len(starts) == 1 and len(ends) == 1
-                and len(steps) == 1):
+        if (
+            node.input[0] in self.sympy_data_
+            and [0] == axes
+            and len(starts) == 1
+            and len(ends) == 1
+            and len(steps) == 1
+        ):
             input_sympy_data = self.sympy_data_[node.input[0]]
-            if type(input_sympy_data) == list or (type(input_sympy_data) == np.array
-                                                  and len(input_sympy_data.shape) == 1):
-                self.sympy_data_[node.output[0]] = input_sympy_data[starts[0]:ends[0]:steps[0]]
+            if type(input_sympy_data) == list or (
+                type(input_sympy_data) == np.array and len(input_sympy_data.shape) == 1
+            ):
+                self.sympy_data_[node.output[0]] = input_sympy_data[starts[0] : ends[0] : steps[0]]
 
-    def _infer_SoftmaxCrossEntropyLoss(self, node):
+    def _infer_SoftmaxCrossEntropyLoss(self, node):  # noqa: N802
         vi = self.known_vi_[node.output[0]]
         elem_type = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+
+        # If output type is explicit specified in attribute, we use it as output tensor type.
+        specified_output_type = get_attribute(node, "output_type", None)
+        if specified_output_type is not None:
+            elem_type = specified_output_type
+
         vi.type.tensor_type.elem_type = elem_type
         vi.type.tensor_type.shape.CopyFrom(onnx.TensorShapeProto())
 
         if len(node.output) > 1:
             data_shape = self._get_shape(node, 0)
             vi = self.known_vi_[node.output[1]]
             vi.CopyFrom(helper.make_tensor_value_info(vi.name, elem_type, data_shape))
 
-    def _infer_Split_Common(self, node, make_value_info_func):
+    def _infer_Split_Common(self, node, make_value_info_func):  # noqa: N802
         input_sympy_shape = self._get_sympy_shape(node, 0)
-        axis = handle_negative_axis(get_attribute(node, 'axis', 0), len(input_sympy_shape))
-        split = get_attribute(node, 'split')
+        axis = handle_negative_axis(get_attribute(node, "axis", 0), len(input_sympy_shape))
+        split = get_attribute(node, "split")
         if not split:
             num_outputs = len(node.output)
             split = [input_sympy_shape[axis] / sympy.Integer(num_outputs)] * num_outputs
             self._update_computed_dims(split)
         else:
             split = [sympy.Integer(s) for s in split]
 
         for i_o in range(len(split)):
             vi = self.known_vi_[node.output[i_o]]
             vi.CopyFrom(
                 make_value_info_func(
-                    node.output[i_o], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                    get_shape_from_sympy_shape(input_sympy_shape[:axis] + [split[i_o]] + input_sympy_shape[axis + 1:])))
+                    node.output[i_o],
+                    self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                    get_shape_from_sympy_shape(input_sympy_shape[:axis] + [split[i_o]] + input_sympy_shape[axis + 1 :]),
+                )
+            )
             self.known_vi_[vi.name] = vi
 
-    def _infer_Split(self, node):
+    def _infer_Split(self, node):  # noqa: N802
         self._infer_Split_Common(node, helper.make_tensor_value_info)
 
-    def _infer_SplitToSequence(self, node):
+    def _infer_SplitToSequence(self, node):  # noqa: N802
         self._infer_Split_Common(node, helper.make_sequence_value_info)
 
-    def _infer_Squeeze(self, node):
+    def _infer_Squeeze(self, node):  # noqa: N802
         input_shape = self._get_shape(node, 0)
         op_set = get_opset(self.out_mp_)
 
         # Depending on op-version 'axes' are provided as attribute or via 2nd input
         if op_set < 13:
-            axes = get_attribute(node, 'axes')
+            axes = get_attribute(node, "axes")
             assert self._try_get_value(node, 1) is None
         else:
             axes = self._try_get_value(node, 1)
-            assert get_attribute(node, 'axes') is None
+            assert get_attribute(node, "axes") is None
 
         if axes is None:
             # No axes have been provided (neither via attribute nor via input).
             # In this case the 'Shape' op should remove all axis with dimension 1.
             # For symbolic dimensions we guess they are !=1.
             output_shape = [s for s in input_shape if s != 1]
             if self.verbose_ > 0:
                 symbolic_dimensions = [s for s in input_shape if type(s) != int]
                 if len(symbolic_dimensions) > 0:
-                    logger.debug(f"Symbolic dimensions in input shape of op: '{node.op_type}' node: '{node.name}'. " +
-                                 f"Assuming the following dimensions are never equal to 1: {symbolic_dimensions}")
+                    logger.debug(
+                        f"Symbolic dimensions in input shape of op: '{node.op_type}' node: '{node.name}'. "
+                        f"Assuming the following dimensions are never equal to 1: {symbolic_dimensions}"
+                    )
         else:
             axes = [handle_negative_axis(a, len(input_shape)) for a in axes]
             output_shape = []
             for i in range(len(input_shape)):
                 if i not in axes:
                     output_shape.append(input_shape[i])
                 else:
                     assert input_shape[i] == 1 or type(input_shape[i]) != int
                     if self.verbose_ > 0 and type(input_shape[i]) != int:
-                        logger.debug(f"Symbolic dimensions in input shape of op: '{node.op_type}' node: '{node.name}'. " +
-                                     f"Assuming the dimension '{input_shape[i]}' at index {i} of the input to be equal to 1.")
+                        logger.debug(
+                            f"Symbolic dimensions in input shape of op: '{node.op_type}' node: '{node.name}'. "
+                            f"Assuming the dimension '{input_shape[i]}' at index {i} of the input to be equal to 1."
+                        )
 
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                          output_shape))
+            helper.make_tensor_value_info(
+                node.output[0],
+                self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                output_shape,
+            )
+        )
         self._pass_on_sympy_data(node)
 
-    def _infer_Tile(self, node):
+    def _infer_Tile(self, node):  # noqa: N802
         repeats_value = self._try_get_value(node, 1)
         new_sympy_shape = []
         if repeats_value is not None:
             input_sympy_shape = self._get_sympy_shape(node, 0)
             for i, d in enumerate(input_sympy_shape):
                 new_dim = d * repeats_value[i]
                 new_sympy_shape.append(new_dim)
             self._update_computed_dims(new_sympy_shape)
         else:
             new_sympy_shape = self._new_symbolic_shape(self._get_shape_rank(node, 0), node)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], vi.type.tensor_type.elem_type,
-                                          get_shape_from_sympy_shape(new_sympy_shape)))
+            helper.make_tensor_value_info(
+                node.output[0],
+                vi.type.tensor_type.elem_type,
+                get_shape_from_sympy_shape(new_sympy_shape),
+            )
+        )
 
-    def _infer_TopK(self, node):
+    def _infer_TopK(self, node):  # noqa: N802
         rank = self._get_shape_rank(node, 0)
-        axis = handle_negative_axis(get_attribute(node, 'axis', -1), rank)
+        axis = handle_negative_axis(get_attribute(node, "axis", -1), rank)
         new_shape = self._get_shape(node, 0)
 
         if get_opset(self.out_mp_) <= 9:
-            k = get_attribute(node, 'k')
+            k = get_attribute(node, "k")
         else:
-            k = self._get_int_values(node)[1]
+            k = self._get_int_or_float_values(node)[1]
 
-        if k == None:
+        if k is None:
             k = self._new_symbolic_dim_from_output(node)
         else:
             k = as_scalar(k)
 
         if type(k) in [int, str]:
             new_shape[axis] = k
         else:
@@ -1648,33 +2004,34 @@
             )  # note that TopK dim could be computed in sympy_data, so need to update computed_dims when it enters shape
             new_shape = get_shape_from_sympy_shape(new_sympy_shape)
 
         for i_o in range(len(node.output)):
             vi = self.known_vi_[node.output[i_o]]
             vi.CopyFrom(helper.make_tensor_value_info(node.output[i_o], vi.type.tensor_type.elem_type, new_shape))
 
-    def _infer_Transpose(self, node):
+    def _infer_Transpose(self, node):  # noqa: N802
         if node.input[0] in self.sympy_data_:
             data_shape = self._get_shape(node, 0)
-            perm = get_attribute(node, 'perm', reversed(list(range(len(data_shape)))))
+            perm = get_attribute(node, "perm", reversed(list(range(len(data_shape)))))
             input_data = self.sympy_data_[node.input[0]]
-            self.sympy_data_[node.output[0]] = np.transpose(np.array(input_data).reshape(*data_shape),
-                                                            axes=tuple(perm)).flatten().tolist()
+            self.sympy_data_[node.output[0]] = (
+                np.transpose(np.array(input_data).reshape(*data_shape), axes=tuple(perm)).flatten().tolist()
+            )
 
-    def _infer_Unsqueeze(self, node):
+    def _infer_Unsqueeze(self, node):  # noqa: N802
         input_shape = self._get_shape(node, 0)
         op_set = get_opset(self.out_mp_)
 
         # Depending on op-version 'axes' are provided as attribute or via 2nd input
         if op_set < 13:
-            axes = get_attribute(node, 'axes')
+            axes = get_attribute(node, "axes")
             assert self._try_get_value(node, 1) is None
         else:
             axes = self._try_get_value(node, 1)
-            assert get_attribute(node, 'axes') is None
+            assert get_attribute(node, "axes") is None
 
         output_rank = len(input_shape) + len(axes)
         axes = [handle_negative_axis(a, output_rank) for a in axes]
 
         input_axis = 0
         output_shape = []
         for i in range(output_rank):
@@ -1682,149 +2039,329 @@
                 output_shape.append(1)
             else:
                 output_shape.append(input_shape[input_axis])
                 input_axis += 1
 
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
-            helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
-                                          output_shape))
+            helper.make_tensor_value_info(
+                node.output[0],
+                self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                output_shape,
+            )
+        )
 
         self._pass_on_sympy_data(node)
 
-    def _infer_ZipMap(self, node):
+    def _infer_ZipMap(self, node):  # noqa: N802
         map_key_type = None
-        if get_attribute(node, 'classlabels_int64s') is not None:
+        if get_attribute(node, "classlabels_int64s") is not None:
             map_key_type = onnx.TensorProto.INT64
-        elif get_attribute(node, 'classlabels_strings') is not None:
+        elif get_attribute(node, "classlabels_strings") is not None:
             map_key_type = onnx.TensorProto.STRING
 
         assert map_key_type is not None
         new_vi = onnx.ValueInfoProto()
         new_vi.name = node.output[0]
         new_vi.type.sequence_type.elem_type.map_type.value_type.tensor_type.elem_type = onnx.TensorProto.FLOAT
         new_vi.type.sequence_type.elem_type.map_type.key_type = map_key_type
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(new_vi)
 
-    def _infer_Attention(self, node):
+    def _infer_Attention(self, node):  # noqa: N802
         shape = self._get_shape(node, 0)
-        shape_bias = self._get_shape(node, 2)
-        assert len(shape) == 3 and len(shape_bias) == 1
-        qkv_hidden_sizes_attr = get_attribute(node, 'qkv_hidden_sizes')
-        if qkv_hidden_sizes_attr is not None:
-            assert len(qkv_hidden_sizes_attr) == 3
-            shape[2] = int(qkv_hidden_sizes_attr[2])
-        else:
-            shape[2] = int(shape_bias[0] / 3)
-        output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
-        vi = self.known_vi_[node.output[0]]
-        vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, shape))
+        shape_weights = self._get_shape(node, 1)
+        shape_bias = self._try_get_shape(node, 2)
+        if shape_bias is not None:
+            assert len(shape_bias) == 1
+        tripled_hidden_size = shape_bias[0] if shape_bias is not None else shape_weights[1]
+        if shape and len(shape) == 3:
+            qkv_hidden_sizes_attr = get_attribute(node, "qkv_hidden_sizes")
+            if qkv_hidden_sizes_attr is not None:
+                assert len(qkv_hidden_sizes_attr) == 3
+                shape[2] = int(qkv_hidden_sizes_attr[2])
+            elif isinstance(tripled_hidden_size, int):
+                shape[2] = int(tripled_hidden_size / 3)
+            output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+            vi = self.known_vi_[node.output[0]]
+            vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, shape))
 
-        if len(node.output) > 1:
-            # input shape: (batch_size, sequence_length, hidden_size)
-            # past shape: (2, batch_size, num_heads, past_sequence_length, head_size)
-            # mask shape: (batch_size, total_sequence_length) or (batch_size, sequence_length, total_sequence_length) or (batch_size, 1, max_seq_len, max_seq_len)
-            # present shape: (2, batch_size, num_heads, total_sequence_length, head_size), where total_sequence_length=sequence_length+past_sequence_length
-            input_shape = self._get_shape(node, 0)
-            past_shape = self._get_shape(node, 4)
-            mask_shape = self._get_shape(node, 3)
-            if len(past_shape) == 5:
-                if len(mask_shape) in [2, 3]:
-                    past_shape[3] = mask_shape[-1]
-                elif isinstance(input_shape[1], int) and isinstance(past_shape[3], int):
-                    past_shape[3] = input_shape[1] + past_shape[3]
+            if len(node.output) > 1:
+                # input shape: (batch_size, sequence_length, hidden_size)
+                # past shape: (2, batch_size, num_heads, past_sequence_length, head_size)
+                # mask shape: (batch_size, total_sequence_length) or (batch_size, sequence_length, total_sequence_length) or (batch_size, 1, max_seq_len, max_seq_len)
+                # present shape: (2, batch_size, num_heads, total_sequence_length, head_size), where total_sequence_length=sequence_length+past_sequence_length
+                input_shape = self._get_shape(node, 0)
+                past_shape = self._get_shape(node, 4) if len(node.input) > 4 and node.input[4] else []
+                mask_shape = self._get_shape(node, 3) if len(node.input) > 3 and node.input[3] else []
+
+                if past_shape and len(past_shape) == 5:
+                    if mask_shape and len(mask_shape) in [2, 3]:
+                        past_shape[3] = mask_shape[-1]
+                    elif input_shape and len(input_shape) == 3:
+                        if isinstance(input_shape[1], int) and isinstance(past_shape[3], int):
+                            past_shape[3] = input_shape[1] + past_shape[3]
+                        else:
+                            past_shape[3] = f"{past_shape[3]}+{input_shape[1]}"
+                    vi = self.known_vi_[node.output[1]]
+                    vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, past_shape))
+                # No past input but present output still exists
                 else:
-                    past_shape[3] = f"{past_shape[3]}+{input_shape[1]}"
-                vi = self.known_vi_[node.output[1]]
-                vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, past_shape))
+                    num_heads = get_attribute(node, "num_heads")
+                    head_size = input_shape[2] // num_heads
+                    present_shape = [2, input_shape[0], num_heads, input_shape[1], head_size]
+                    vi = self.known_vi_[node.output[1]]
+                    vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, present_shape))
+
+    def _infer_PackedAttention(self, node):  # noqa: N802
+        shape = self._get_shape(node, 0)
+        shape_weights = self._get_shape(node, 1)
+        shape_bias = self._try_get_shape(node, 2)
+        if shape_bias is not None:
+            assert len(shape_bias) == 1
+        tripled_hidden_size = shape_bias[0] if shape_bias is not None else shape_weights[1]
+        if shape and len(shape) == 2:
+            qkv_hidden_sizes_attr = get_attribute(node, "qkv_hidden_sizes")
+            if qkv_hidden_sizes_attr is not None:
+                assert len(qkv_hidden_sizes_attr) == 3
+                shape[1] = int(qkv_hidden_sizes_attr[2])
+            elif isinstance(tripled_hidden_size, int):
+                shape[1] = int(tripled_hidden_size / 3)
+            output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+            vi = self.known_vi_[node.output[0]]
+            vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, shape))
+
+    def _infer_RemovePadding(self, node):  # noqa: N802
+        shape = self._get_shape(node, 0)
+        if shape and len(shape) == 3:
+            output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+            vi = self.known_vi_[node.output[0]]
+            vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, ["token_count", shape[2]]))
+
+            vi_token_offset = self.known_vi_[node.output[1]]
+            vi_token_offset.CopyFrom(
+                helper.make_tensor_value_info(node.output[1], onnx.TensorProto.INT32, [shape[0], shape[1]])
+            )
+
+            vi_cumulated_seq_len = self.known_vi_[node.output[2]]
+            vi_cumulated_seq_len.CopyFrom(
+                helper.make_tensor_value_info(node.output[2], onnx.TensorProto.INT32, ["batch_size + 1"])
+            )
+
+            vi_max_seq_len = self.known_vi_[node.output[3]]
+            vi_max_seq_len.CopyFrom(helper.make_tensor_value_info(node.output[3], onnx.TensorProto.INT32, [1]))
+
+    def _infer_RestorePadding(self, node):  # noqa: N802
+        shape_input = self._get_shape(node, 0)
+        shape_token_offset = self._get_shape(node, 1)
+        if shape_input and len(shape_input) == 2 and shape_token_offset and len(shape_token_offset) == 2:
+            output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+            vi = self.known_vi_[node.output[0]]
+
+            output_shape = [shape_token_offset[0], shape_token_offset[1], shape_input[1]]
+            vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, output_shape))
 
-    def _infer_BiasGelu(self, node):
+    def _infer_BiasGelu(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
-    def _infer_FastGelu(self, node):
+    def _infer_MultiHeadAttention(self, node):  # noqa: N802
+        # Output 0 has shape (batch_size, sequence_length, v_hidden_size)
+        # Q, K and V without packing:
+        #   Input 0 (query) has shape (batch_size, sequence_length, hidden_size)
+        #   Input 1 (key) has shape (batch_size, kv_sequence_length, hidden_size) or (batch_size, num_heads, kv_sequence_length, head_size)
+        #   Input 2 (value) has shape (batch_size, kv_sequence_length, v_hidden_size) or (batch_size, num_heads, kv_sequence_length, head_size)
+        # Packed KV:
+        #   Input 0 (query) has shape (batch_size, sequence_length, hidden_size)
+        #   Input 1 (batch_size, kv_sequence_length, num_heads, 2, head_size)
+        #   Input 2  nullptr
+        # Packed QKV:
+        #   Input 0 (batch_size, sequence_length, num_heads, 3, head_size)
+        #   Input 1  nullptr
+        #   Input 2  nullptr
+
+        query_shape = self._get_shape(node, 0)
+        total_sequence_length = None
+        output_dtype = None
+        if query_shape is not None:
+            if len(query_shape) == 3:
+                key_shape = self._try_get_shape(node, 1)
+                # By default, hidden size is same for Q/K/V. Only need check v_hidden_size when value is provided.
+                output_shape = query_shape
+                if key_shape is not None and len(key_shape) == 3:
+                    value_shape = self._try_get_shape(node, 2)
+                    if value_shape is not None and len(value_shape) == 3:
+                        output_shape[2] = value_shape[2]
+                    total_sequence_length = key_shape[1]
+
+                output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+                vi = self.known_vi_[node.output[0]]
+                vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, output_shape))
+
+            elif len(query_shape) == 5:
+                if isinstance(query_shape[2], int) and isinstance(query_shape[4], int):
+                    output_shape = [query_shape[0], query_shape[1], query_shape[2] * query_shape[4]]
+                else:
+                    output_shape = [query_shape[0], query_shape[1], f"{query_shape[2]}*{query_shape[4]}"]
+
+                total_sequence_length = query_shape[1]
+
+                output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+                vi = self.known_vi_[node.output[0]]
+                vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, output_shape))
+
+            if len(node.output) > 1:
+                batch_size = query_shape[0]
+                num_heads = get_attribute(node, "num_heads")
+
+                head_size = None
+                if len(query_shape) == 3:
+                    head_size = (
+                        int(query_shape[2] / num_heads)
+                        if isinstance(query_shape[2], int)
+                        else f"{query_shape[2]}/{num_heads}"
+                    )
+                else:
+                    head_size = query_shape[4]
+
+                past_shape = self._try_get_shape(node, 6)
+
+                if past_shape is not None:
+                    if isinstance(past_shape[2], int) and isinstance(total_sequence_length, int):
+                        total_sequence_length = past_shape[2] + total_sequence_length
+                    else:
+                        total_sequence_length = f"{past_shape[2]}+{total_sequence_length}"
+
+                present_shape = [batch_size, num_heads, total_sequence_length, head_size]
+
+                assert output_dtype is not None
+                vi = self.known_vi_[node.output[1]]
+                vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, present_shape))
+                vi = self.known_vi_[node.output[2]]
+                vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, present_shape))
+
+    def _infer_FastGelu(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
-    def _infer_Gelu(self, node):
+    def _infer_Gelu(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
-    def _infer_LayerNormalization(self, node):
+    def _infer_GemmFastGelu(self, node):  # noqa: N802
+        self._compute_matmul_shape(node)
+
+    def _infer_LayerNormalization(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
-    def _infer_LongformerAttention(self, node):
+    def _infer_LongformerAttention(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
-    def _infer_EmbedLayerNormalization(self, node):
+    def _infer_EmbedLayerNormalization(self, node):  # noqa: N802
         input_ids_shape = self._get_shape(node, 0)
         word_embedding_shape = self._get_shape(node, 2)
         assert len(input_ids_shape) == 2 and len(word_embedding_shape) == 2
-        output_shape = input_ids_shape + [word_embedding_shape[1]]
+        output_shape = [*input_ids_shape, word_embedding_shape[1]]
 
         word_embedding_dtype = self.known_vi_[node.input[2]].type.tensor_type.elem_type
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], word_embedding_dtype, output_shape))
 
-        mask_index_shape = [input_ids_shape[0]]
-        vi = self.known_vi_[node.output[1]]
-        vi.CopyFrom(helper.make_tensor_value_info(node.output[1], onnx.TensorProto.INT32, mask_index_shape))
+        if len(node.output) > 1 and node.output[1]:
+            mask_index_shape = [input_ids_shape[0]]
+            vi = self.known_vi_[node.output[1]]
+            vi.CopyFrom(helper.make_tensor_value_info(node.output[1], onnx.TensorProto.INT32, mask_index_shape))
 
         if len(node.output) > 2:
-            # Optional output of add before layer nomalization is done
+            # Optional output of add before layer normalization is done
             # shape is same as the output
             vi = self.known_vi_[node.output[2]]
             vi.CopyFrom(helper.make_tensor_value_info(node.output[2], word_embedding_dtype, output_shape))
 
-    def _infer_SkipLayerNormalization(self, node):
+    def _infer_SkipLayerNormalization(self, node):  # noqa: N802
+        self._propagate_shape_and_type(node)
+
+        # If the SkipLayerNormalization node contains the optional
+        # output for inference, infer the shape and type for it too
+        if len(node.output) > 3:
+            self._propagate_shape_and_type(node, 0, 3)
+
+    def _infer_GroupNorm(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
-    def _infer_PythonOp(self, node):
-        output_tensor_types = get_attribute(node, 'output_tensor_types')
+    def _infer_BiasSplitGelu(self, node):  # noqa: N802
+        input_shape = self._get_shape(node, 0)
+        bias_shape = self._get_shape(node, 1)
+        if input_shape and bias_shape and isinstance(bias_shape[0], int):
+            output_shape = input_shape
+            output_shape[2] = int(bias_shape[0] / 2)
+            vi = self.known_vi_[node.output[0]]
+            output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+            vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, output_shape))
+
+    def _infer_BiasAdd(self, node):  # noqa: N802
+        self._propagate_shape_and_type(node)
+
+    def _infer_PythonOp(self, node):  # noqa: N802
+        output_tensor_types = get_attribute(node, "output_tensor_types")
         assert output_tensor_types
-        output_tensor_ranks = get_attribute(node, 'output_tensor_ranks')
+        output_tensor_ranks = get_attribute(node, "output_tensor_ranks")
         assert output_tensor_ranks
 
-        # set the context output seperately.
+        # set the context output separately.
         # The first output is autograd's context.
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64, []))
-
-        # Outputs after autograd's context are tensors.
-        # We assume their ranks are fixed for different model inputs.
-        for i in range(len(node.output) - 1):
-            # Process the i-th tensor outputs.
-            vi = self.known_vi_[node.output[i + 1]]
-            sympy_shape = self._new_symbolic_shape(output_tensor_ranks[i], node)
-            shape = get_shape_from_sympy_shape(sympy_shape)
-            value_info = helper.make_tensor_value_info(node.output[i + 1], output_tensor_types[i], shape)
-            vi.CopyFrom(value_info)
+        if get_attribute(node, "name").decode() in ["_InspectActivation", "_IncrementStep"]:
+            # PythonOp with name being "_InspectActivation" or "_IncrementStep" will behave exactly same as a normal
+            # PythonOp when execution. The only difference is that
+            # 1). those ops having same number of tensor inputs and tensor outputs;
+            # 2). and the i-th output tensor's shape is same as i-th input tensor's shape.
+            # Be noted, the count of custom autograd function might be bigger than output count, because there might
+            # be other non-tensor constant inputs (string, object, int, tuple, etc). But we did not make those constant
+            # inputs as ONNX op's input, instead they are stored in the attributes.
+            assert len(node.output) == len(node.input) + 1  # The output contains one extra context info.
+            for input_index in range(len(node.output) - 1):
+                # Process the i-th tensor outputs.
+                vi = self.known_vi_[node.output[input_index + 1]]
+                shape = self._get_shape(node, input_index)
+                output_dtype = self.known_vi_[node.input[input_index]].type.tensor_type.elem_type
+                vi.CopyFrom(helper.make_tensor_value_info(node.output[input_index + 1], output_dtype, shape))
+        else:
+            # Outputs after autograd's context are tensors.
+            # We assume their ranks are fixed for different model inputs.
+            for i in range(len(node.output) - 1):
+                # Process the i-th tensor outputs.
+                vi = self.known_vi_[node.output[i + 1]]
+                sympy_shape = self._new_symbolic_shape(output_tensor_ranks[i], node)
+                shape = get_shape_from_sympy_shape(sympy_shape)
+                value_info = helper.make_tensor_value_info(node.output[i + 1], output_tensor_types[i], shape)
+                vi.CopyFrom(value_info)
 
     def _propagate_shape_and_type(self, node, input_index=0, output_index=0):
         shape = self._get_shape(node, input_index)
         output_dtype = self.known_vi_[node.input[input_index]].type.tensor_type.elem_type
         vi = self.known_vi_[node.output[output_index]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[output_index], output_dtype, shape))
 
     def _is_none_dim(self, dim_value):
         if type(dim_value) != str:
             return False
         if "unk__" not in dim_value:
             return False
-        if dim_value in self.symbolic_dims_.keys():
+        if dim_value in self.symbolic_dims_:
             return False
         return True
-    
+
     def _is_shape_contains_none_dim(self, out_shape):
         for out in out_shape:
             if self._is_none_dim(out):
                 return out
         return None
-    
+
     def _infer_impl(self, start_sympy_data=None):
         self.sympy_data_ = start_sympy_data or {}
-        self.out_mp_.graph.ClearField('value_info')
+        self.out_mp_.graph.ClearField("value_info")
         self._apply_suggested_merge(graph_input_only=True)
         self.input_symbols_ = set()
         for i in self.out_mp_.graph.input:
             input_shape = get_shape_from_value_info(i)
             if input_shape is None:
                 continue
 
@@ -1849,27 +2386,30 @@
                 # Since inputs are not produced by other ops, we can assume positivity
                 self.symbolic_dims_[s] = sympy.Symbol(s, integer=True, positive=True)
         # create a temporary ModelProto for single node inference
         # note that we remove initializer to have faster inference
         # for tensor ops like Reshape/Tile/Expand that read initializer, we need to do sympy computation based inference anyways
         self.tmp_mp_ = onnx.ModelProto()
         self.tmp_mp_.CopyFrom(self.out_mp_)
-        self.tmp_mp_.graph.ClearField('initializer')
+        self.tmp_mp_.graph.ClearField("initializer")
 
         # compute prerequesite for node for topological sort
         # node with subgraphs may have dependency on implicit inputs, which will affect topological sort
         prereq_for_node = {}  # map from node to all its inputs, including implicit ones in subgraph
 
         def get_prereq(node):
-            names = set(i for i in node.input if i)
+            names = {i for i in node.input if i}
             subgraphs = []
-            if 'If' == node.op_type:
-                subgraphs = [get_attribute(node, 'then_branch'), get_attribute(node, 'else_branch')]
-            elif node.op_type in ['Loop', 'Scan']:
-                subgraphs = [get_attribute(node, 'body')]
+            if node.op_type == "If":
+                subgraphs = [
+                    get_attribute(node, "then_branch"),
+                    get_attribute(node, "else_branch"),
+                ]
+            elif node.op_type in ["Loop", "Scan"]:
+                subgraphs = [get_attribute(node, "body")]
             for g in subgraphs:
                 g_outputs_and_initializers = {i.name for i in g.initializer}
                 g_prereq = set()
                 for n in g.node:
                     g_outputs_and_initializers.update(n.output)
                 for n in g.node:
                     g_prereq.update([i for i in get_prereq(n) if i not in g_outputs_and_initializers])
@@ -1881,144 +2421,200 @@
             return names
 
         for n in self.tmp_mp_.graph.node:
             prereq_for_node[n.output[0]] = get_prereq(n)
 
         # topological sort nodes, note there might be dead nodes so we check if all graph outputs are reached to terminate
         sorted_nodes = []
-        sorted_known_vi = set([i.name for i in list(self.out_mp_.graph.input) + list(self.out_mp_.graph.initializer)])
+        sorted_known_vi = {i.name for i in list(self.out_mp_.graph.input) + list(self.out_mp_.graph.initializer)}
         if any([o.name in sorted_known_vi for o in self.out_mp_.graph.output]):
             # Loop/Scan will have some graph output in graph inputs, so don't do topological sort
             sorted_nodes = self.out_mp_.graph.node
         else:
             while not all([o.name in sorted_known_vi for o in self.out_mp_.graph.output]):
                 old_sorted_nodes_len = len(sorted_nodes)
                 for node in self.out_mp_.graph.node:
                     if (node.output[0] not in sorted_known_vi) and all(
-                        [i in sorted_known_vi for i in prereq_for_node[node.output[0]] if i]):
+                        [i in sorted_known_vi for i in prereq_for_node[node.output[0]] if i]
+                    ):
                         sorted_known_vi.update(node.output)
                         sorted_nodes.append(node)
                 if old_sorted_nodes_len == len(sorted_nodes) and not all(
-                    [o.name in sorted_known_vi for o in self.out_mp_.graph.output]):
-                    raise Exception('Invalid model with cyclic graph')
+                    [o.name in sorted_known_vi for o in self.out_mp_.graph.output]
+                ):
+                    raise Exception("Invalid model with cyclic graph")
 
         for node in sorted_nodes:
             assert all([i in self.known_vi_ for i in node.input if i])
             self._onnx_infer_single_node(node)
             known_aten_op = False
             if node.op_type in self.dispatcher_:
                 self.dispatcher_[node.op_type](node)
-            elif node.op_type in ['ConvTranspose']:
+            elif node.op_type in ["ConvTranspose"]:
                 # onnx shape inference ops like ConvTranspose may have empty shape for symbolic input
                 # before adding symbolic compute for them
                 # mark the output type as UNDEFINED to allow guessing of rank
                 vi = self.known_vi_[node.output[0]]
                 if len(vi.type.tensor_type.shape.dim) == 0:
                     vi.type.tensor_type.elem_type = onnx.TensorProto.UNDEFINED
-            elif node.op_type == 'ATen' and node.domain == 'org.pytorch.aten':
+            elif node.op_type == "ATen" and node.domain == "org.pytorch.aten":
                 for attr in node.attribute:
                     # TODO: Is overload_name needed?
-                    if attr.name == 'operator':
-                        aten_op_name = attr.s.decode('utf-8') if isinstance(attr.s, bytes) else attr.s
+                    if attr.name == "operator":
+                        aten_op_name = attr.s.decode("utf-8") if isinstance(attr.s, bytes) else attr.s
                         if aten_op_name in self.aten_op_dispatcher_:
                             known_aten_op = True
                             self.aten_op_dispatcher_[aten_op_name](node)
                         break
 
             if self.verbose_ > 2:
-                logger.debug(node.op_type + ': ' + node.name)
+                logger.debug(node.op_type + ": " + node.name)
                 for i, name in enumerate(node.input):
-                    logger.debug('  Input {}: {} {}'.format(i, name, 'initializer' if name in self.initializers_ else ''))
+                    logger.debug(
+                        "  Input {}: {} {}".format(i, name, "initializer" if name in self.initializers_ else "")
+                    )
 
             # onnx automatically merge dims with value, i.e. Mul(['aaa', 'bbb'], [1000, 1]) -> [1000, 'bbb']
             # symbolic shape inference needs to apply merge of 'aaa' -> 1000 in this case
             if node.op_type in [
-                    'Add', 'Sub', 'Mul', 'Div', 'MatMul', 'MatMulInteger', 'MatMulInteger16', 'Where', 'Sum'
+                "Add",
+                "Sub",
+                "Mul",
+                "Div",
+                "MatMul",
+                "MatMulInteger",
+                "MatMulInteger16",
+                "Where",
+                "Sum",
             ]:
                 vi = self.known_vi_[node.output[0]]
                 out_rank = len(get_shape_from_type_proto(vi.type))
                 in_shapes = [self._get_shape(node, i) for i in range(len(node.input))]
-                for d in range(out_rank - (2 if node.op_type in ['MatMul', 'MatMulInteger', 'MatMulInteger16'] else 0)):
+                for d in range(out_rank - (2 if node.op_type in ["MatMul", "MatMulInteger", "MatMulInteger16"] else 0)):
                     in_dims = [s[len(s) - out_rank + d] for s in in_shapes if len(s) + d >= out_rank]
                     if len(in_dims) > 1:
                         self._check_merged_dims(in_dims, allow_broadcast=True)
 
             for i_o in range(len(node.output)):
+                # Special case: We do not care about the training related
+                # outputs of SkipLayerNormalization
+                if (
+                    node.op_type == "SkipLayerNormalization" or node.op_type == "SkipSimplifiedLayerNormalization"
+                ) and i_o in [1, 2]:
+                    continue
+
                 vi = self.known_vi_[node.output[i_o]]
                 out_type = vi.type
-                out_type_kind = out_type.WhichOneof('value')
+                out_type_kind = out_type.WhichOneof("value")
 
                 # do not process shape for non-tensors
-                if out_type_kind not in ['tensor_type', 'sparse_tensor_type', None]:
+                if out_type_kind not in ["tensor_type", "sparse_tensor_type", None]:
                     if self.verbose_ > 2:
-                        if out_type_kind == 'sequence_type':
-                            seq_cls_type = out_type.sequence_type.elem_type.WhichOneof('value')
-                            if 'tensor_type' == seq_cls_type:
-                                logger.debug('  {}: sequence of {} {}'.format(
-                                    node.output[i_o], str(get_shape_from_value_info(vi)),
-                                    onnx.TensorProto.DataType.Name(
-                                        vi.type.sequence_type.elem_type.tensor_type.elem_type)))
+                        if out_type_kind == "sequence_type":
+                            seq_cls_type = out_type.sequence_type.elem_type.WhichOneof("value")
+                            if seq_cls_type == "tensor_type":
+                                logger.debug(
+                                    "  {}: sequence of {} {}".format(
+                                        node.output[i_o],
+                                        str(get_shape_from_value_info(vi)),
+                                        onnx.TensorProto.DataType.Name(
+                                            vi.type.sequence_type.elem_type.tensor_type.elem_type
+                                        ),
+                                    )
+                                )
                             else:
-                                logger.debug('  {}: sequence of {}'.format(node.output[i_o], seq_cls_type))
+                                logger.debug(f"  {node.output[i_o]}: sequence of {seq_cls_type}")
                         else:
-                            logger.debug('  {}: {}'.format(node.output[i_o], out_type_kind))
+                            logger.debug(f"  {node.output[i_o]}: {out_type_kind}")
                     continue
 
                 out_shape = get_shape_from_value_info(vi)
                 out_type_undefined = out_type.tensor_type.elem_type == onnx.TensorProto.UNDEFINED
                 if self.verbose_ > 2:
-                    logger.debug('  {}: {} {}'.format(node.output[i_o], str(out_shape),
-                                               onnx.TensorProto.DataType.Name(vi.type.tensor_type.elem_type)))
+                    logger.debug(
+                        "  {}: {} {}".format(
+                            node.output[i_o],
+                            str(out_shape),
+                            onnx.TensorProto.DataType.Name(vi.type.tensor_type.elem_type),
+                        )
+                    )
                     if node.output[i_o] in self.sympy_data_:
-                        logger.debug('  Sympy Data: ' + str(self.sympy_data_[node.output[i_o]]))
+                        logger.debug("  Sympy Data: " + str(self.sympy_data_[node.output[i_o]]))
 
                 # onnx >= 1.11.0, use unk__#index instead of None when the shape dim is uncertain
-                if (out_shape is not None and (None in out_shape or self._is_shape_contains_none_dim(out_shape))) or out_type_undefined:
+                if (
+                    out_shape is not None and (None in out_shape or self._is_shape_contains_none_dim(out_shape))
+                ) or out_type_undefined:
                     if self.auto_merge_:
                         if node.op_type in [
-                                'Add', 'Sub', 'Mul', 'Div', 'MatMul', 'MatMulInteger', 'MatMulInteger16', 'Concat',
-                                'Where', 'Sum', 'Equal', 'Less', 'Greater', 'LessOrEqual', 'GreaterOrEqual'
+                            "Add",
+                            "Sub",
+                            "Mul",
+                            "Div",
+                            "MatMul",
+                            "MatMulInteger",
+                            "MatMulInteger16",
+                            "Concat",
+                            "Where",
+                            "Sum",
+                            "Equal",
+                            "Less",
+                            "Greater",
+                            "LessOrEqual",
+                            "GreaterOrEqual",
+                            "Min",
+                            "Max",
                         ]:
                             shapes = [self._get_shape(node, i) for i in range(len(node.input))]
-                            if node.op_type in ['MatMul', 'MatMulInteger', 'MatMulInteger16']:
+                            if node.op_type in [
+                                "MatMul",
+                                "MatMulInteger",
+                                "MatMulInteger16",
+                            ]:
                                 if None in out_shape or self._is_shape_contains_none_dim(out_shape):
                                     if None in out_shape:
                                         idx = out_shape.index(None)
                                     else:
                                         idx = out_shape.index(self._is_shape_contains_none_dim(out_shape))
                                     dim_idx = [len(s) - len(out_shape) + idx for s in shapes]
                                     # only support auto merge for MatMul for dim < rank-2 when rank > 2
                                     assert len(shapes[0]) > 2 and dim_idx[0] < len(shapes[0]) - 2
                                     assert len(shapes[1]) > 2 and dim_idx[1] < len(shapes[1]) - 2
-                        elif node.op_type == 'Expand':
+                        elif node.op_type == "Expand":
                             # auto merge for cases like Expand([min(batch, 1), min(seq, 512)], [batch, seq])
-                            shapes = [self._get_shape(node, 0), self._get_value(node, 1)]
+                            shapes = [
+                                self._get_shape(node, 0),
+                                self._get_value(node, 1),
+                            ]
                         else:
                             shapes = []
 
                         if shapes:
                             for idx in range(len(out_shape)):
                                 if out_shape[idx] is not None and not self._is_none_dim(out_shape[idx]):
                                     continue
                                 # note that the broadcasting rule aligns from right to left
                                 # if a tensor has a lower rank (dim_idx[idx] < 0), it would automatically broadcast and need no merge
                                 dim_idx = [len(s) - len(out_shape) + idx for s in shapes]
                                 if len(dim_idx) > 0:
-                                    self._add_suggested_merge([
-                                        s[i] if is_literal(s[i]) else str(s[i]) for s, i in zip(shapes, dim_idx)
-                                        if i >= 0
-                                    ])
+                                    self._add_suggested_merge(
+                                        [
+                                            s[i] if is_literal(s[i]) else str(s[i])
+                                            for s, i in zip(shapes, dim_idx)
+                                            if i >= 0
+                                        ]
+                                    )
                             self.run_ = True
                         else:
                             self.run_ = False
                     else:
                         self.run_ = False
 
                     # create new dynamic dims for ops not handled by symbolic shape inference
-                    if self.run_ == False and not node.op_type in self.dispatcher_ and not known_aten_op:
+                    if self.run_ is False and node.op_type not in self.dispatcher_ and not known_aten_op:
                         is_unknown_op = out_type_undefined and (out_shape is None or len(out_shape) == 0)
                         if is_unknown_op:
                             # unknown op to ONNX, maybe from higher opset or other domain
                             # only guess the output rank from input 0 when using guess_output_rank option
                             out_rank = self._get_shape_rank(node, 0) if self.guess_output_rank_ else -1
                         else:
                             # valid ONNX op, but not handled by symbolic shape inference, just assign dynamic shape
@@ -2029,92 +2625,152 @@
                             if out_type_undefined:
                                 # guess output data type from input vi if not defined
                                 out_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
                             else:
                                 # otherwise, use original data type
                                 out_dtype = vi.type.tensor_type.elem_type
                             vi.CopyFrom(
-                                helper.make_tensor_value_info(vi.name, out_dtype,
-                                                              get_shape_from_sympy_shape(new_shape)))
+                                helper.make_tensor_value_info(
+                                    vi.name,
+                                    out_dtype,
+                                    get_shape_from_sympy_shape(new_shape),
+                                )
+                            )
 
                             if self.verbose_ > 0:
                                 if is_unknown_op:
-                                    logger.debug("Possible unknown op: {} node: {}, guessing {} shape".format(
-                                                 node.op_type, node.name, vi.name))
+                                    logger.debug(
+                                        "Possible unknown op: {} node: {}, guessing {} shape".format(
+                                            node.op_type, node.name, vi.name
+                                        )
+                                    )
                                 if self.verbose_ > 2:
-                                    logger.debug('  {}: {} {}'.format(node.output[i_o], str(new_shape),
-                                                                      vi.type.tensor_type.elem_type))
+                                    logger.debug(
+                                        "  {}: {} {}".format(
+                                            node.output[i_o],
+                                            str(new_shape),
+                                            vi.type.tensor_type.elem_type,
+                                        )
+                                    )
 
                             self.run_ = True
                             continue  # continue the inference after guess, no need to stop as no merge is needed
 
                     if self.verbose_ > 0 or not self.auto_merge_ or out_type_undefined:
-                        logger.debug('Stopping at incomplete shape inference at ' + node.op_type + ': ' + node.name)
-                        logger.debug('node inputs:')
+                        logger.debug("Stopping at incomplete shape inference at " + node.op_type + ": " + node.name)
+                        logger.debug("node inputs:")
                         for i in node.input:
                             logger.debug(self.known_vi_[i])
-                        logger.debug('node outputs:')
+                        logger.debug("node outputs:")
                         for o in node.output:
                             logger.debug(self.known_vi_[o])
                         if self.auto_merge_ and not out_type_undefined:
-                            logger.debug('Merging: ' + str(self.suggested_merge_))
+                            logger.debug("Merging: " + str(self.suggested_merge_))
                     return False
 
         self.run_ = False
         return True
 
     def _update_output_from_vi(self):
         for output in self.out_mp_.graph.output:
             if output.name in self.known_vi_:
                 output.CopyFrom(self.known_vi_[output.name])
 
     @staticmethod
     def infer_shapes(in_mp, int_max=2**31 - 1, auto_merge=False, guess_output_rank=False, verbose=0):
         onnx_opset = get_opset(in_mp)
         if (not onnx_opset) or onnx_opset < 7:
-            logger.warning('Only support models of onnx opset 7 and above.')
+            logger.warning("Only support models of onnx opset 7 and above.")
             return None
         symbolic_shape_inference = SymbolicShapeInference(int_max, auto_merge, guess_output_rank, verbose)
         all_shapes_inferred = False
         symbolic_shape_inference._preprocess(in_mp)
         while symbolic_shape_inference.run_:
             all_shapes_inferred = symbolic_shape_inference._infer_impl()
         symbolic_shape_inference._update_output_from_vi()
         if not all_shapes_inferred:
+            onnx.save_model(symbolic_shape_inference.out_mp_, "sym_shape_infer_temp.onnx", save_as_external_data=True)
             raise Exception("Incomplete symbolic shape inference")
         return symbolic_shape_inference.out_mp_
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
-    parser.add_argument('--input', required=True, help='The input model file')
-    parser.add_argument('--output', help='The output model file')
-    parser.add_argument('--auto_merge',
-                        help='Automatically merge symbolic dims when confliction happens',
-                        action='store_true',
-                        default=False)
-    parser.add_argument('--int_max',
-                        help='maximum value for integer to be treated as boundless for ops like slice',
-                        type=int,
-                        default=2**31 - 1)
-    parser.add_argument('--guess_output_rank',
-                        help='guess output rank to be the same as input 0 for unknown ops',
-                        action='store_true',
-                        default=False)
-    parser.add_argument('--verbose',
-                        help='Prints detailed logs of inference, 0: turn off, 1: warnings, 3: detailed',
-                        type=int,
-                        default=0)
+    parser.add_argument("--input", required=True, help="The input model file")
+    parser.add_argument("--output", help="The output model file")
+    parser.add_argument(
+        "--auto_merge",
+        help="Automatically merge symbolic dims when confliction happens",
+        action="store_true",
+        default=False,
+    )
+    parser.add_argument(
+        "--int_max",
+        help="maximum value for integer to be treated as boundless for ops like slice",
+        type=int,
+        default=2**31 - 1,
+    )
+    parser.add_argument(
+        "--guess_output_rank",
+        help="guess output rank to be the same as input 0 for unknown ops",
+        action="store_true",
+        default=False,
+    )
+    parser.add_argument(
+        "--verbose",
+        help="Prints detailed logs of inference, 0: turn off, 1: warnings, 3: detailed",
+        type=int,
+        default=0,
+    )
+    parser.add_argument(
+        "--save_as_external_data",
+        help="Saving an ONNX model to external data",
+        action="store_true",
+        default=False,
+    )
+    parser.add_argument(
+        "--all_tensors_to_one_file",
+        help="Saving all the external data to one file",
+        action="store_true",
+        default=False,
+    )
+    parser.add_argument(
+        "--external_data_location",
+        help="The file location to save the external file",
+        default="./",
+    )
+    parser.add_argument(
+        "--external_data_size_threshold",
+        help="The size threshold for external data",
+        type=int,
+        default=1024,
+    )
     return parser.parse_args()
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     args = parse_arguments()
-    logger.info('input model: ' + args.input)
+    logger.info("input model: " + args.input)
     if args.output:
-        logger.info('output model ' + args.output)
-    logger.info('Doing symbolic shape inference...')
-    out_mp = SymbolicShapeInference.infer_shapes(onnx.load(args.input), args.int_max, args.auto_merge,
-                                                 args.guess_output_rank, args.verbose)
+        logger.info("output model " + args.output)
+    logger.info("Doing symbolic shape inference...")
+    out_mp = SymbolicShapeInference.infer_shapes(
+        onnx.load(args.input),
+        args.int_max,
+        args.auto_merge,
+        args.guess_output_rank,
+        args.verbose,
+    )
     if args.output and out_mp:
-        onnx.save(out_mp, args.output)
-        logger.info('Done!')
+        if args.save_as_external_data:
+            onnx.save_model(
+                out_mp,
+                args.output,
+                save_as_external_data=True,
+                all_tensors_to_one_file=args.all_tensors_to_one_file,
+                location=args.external_data_location,
+                size_threshold=args.external_data_size_threshold,
+                convert_attribute=False,
+            )
+        else:
+            onnx.save(out_mp, args.output)
+        logger.info("Done!")
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/update_onnx_opset.py` & `onnxruntime/tools/update_onnx_opset.py`

 * *Files 18% similar despite different names*

```diff
@@ -6,24 +6,26 @@
 import os
 import pathlib
 
 from .onnx_model_utils import update_onnx_opset
 
 
 def update_onnx_opset_helper():
-    parser = argparse.ArgumentParser(f'{os.path.basename(__file__)}:{update_onnx_opset_helper.__name__}',
-                                     description='''
+    parser = argparse.ArgumentParser(
+        f"{os.path.basename(__file__)}:{update_onnx_opset_helper.__name__}",
+        description="""
                                      Update the ONNX opset of the model.
                                      New opset must be later than the existing one.
                                      If not specified will update to opset 15.
-                                     ''')
+                                     """,
+    )
 
-    parser.add_argument('--opset', type=int, required=False, default=15, help="ONNX opset to update to.")
-    parser.add_argument('input_model', type=pathlib.Path, help='Provide path to ONNX model to update.')
-    parser.add_argument('output_model', type=pathlib.Path, help='Provide path to write updated ONNX model to.')
+    parser.add_argument("--opset", type=int, required=False, default=15, help="ONNX opset to update to.")
+    parser.add_argument("input_model", type=pathlib.Path, help="Provide path to ONNX model to update.")
+    parser.add_argument("output_model", type=pathlib.Path, help="Provide path to write updated ONNX model to.")
 
     args = parser.parse_args()
     update_onnx_opset(args.input_model, args.opset, args.output_model)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     update_onnx_opset_helper()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/__init__.py` & `onnxruntime/tools/ort_format_model/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -3,24 +3,23 @@
 
 import os
 import sys
 
 # need to add the path to the ORT flatbuffers python module before we import anything else here.
 # we also auto-magically adjust to whether we're running from the ORT repo, or from within the ORT python package
 script_dir = os.path.dirname(os.path.realpath(__file__))
-fbs_py_schema_dirname = 'ort_flatbuffers_py'
+fbs_py_schema_dirname = "ort_flatbuffers_py"
 if os.path.isdir(os.path.join(script_dir, fbs_py_schema_dirname)):
     # fbs bindings are in this directory, so we're running in the ORT python package
     ort_fbs_py_parent_dir = script_dir
 else:
     # running directly from ORT repo, so fbs bindings are under onnxruntime/core/flatbuffers
-    ort_root = os.path.abspath(os.path.join(script_dir, '..', '..', '..', '..'))
-    ort_fbs_py_parent_dir = os.path.join(ort_root, 'onnxruntime', 'core', 'flatbuffers')
+    ort_root = os.path.abspath(os.path.join(script_dir, "..", "..", "..", ".."))
+    ort_fbs_py_parent_dir = os.path.join(ort_root, "onnxruntime", "core", "flatbuffers")
 
 sys.path.append(ort_fbs_py_parent_dir)
 
-from .utils import create_config_from_models  # noqa
-from .ort_model_processor import OrtFormatModelProcessor  # noqa
-from .operator_type_usage_processors import (  # noqa
-    GloballyAllowedTypesOpTypeImplFilter,
-    OpTypeImplFilterInterface,
-    OperatorTypeUsageManager)
+from .operator_type_usage_processors import GloballyAllowedTypesOpTypeImplFilter  # noqa: E402, F401
+from .operator_type_usage_processors import OperatorTypeUsageManager  # noqa: E402, F401
+from .operator_type_usage_processors import OpTypeImplFilterInterface  # noqa: E402, F401
+from .ort_model_processor import OrtFormatModelProcessor  # noqa: E402, F401
+from .utils import create_config_from_models  # noqa: E402, F401
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py` & `onnxruntime/tools/ort_format_model/operator_type_usage_processors.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,326 +1,349 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 import json
 import typing
+from abc import ABC, abstractmethod
+
 import ort_flatbuffers_py.fbs as fbs
 
-from abc import ABC, abstractmethod
 from .types import FbsTypeInfo, value_name_to_typestr
 
 
 def _create_op_key(domain: str, optype: str):
-    return '{}:{}'.format(domain, optype)
+    return f"{domain}:{optype}"
 
 
 def _ort_constant_for_domain(domain: str):
-    '''
+    """
     Map a string domain value to the internal ONNX Runtime constant for that domain.
     :param domain: Domain string to map.
     :return: Internal ONNX Runtime constant
-    '''
+    """
 
     # constants are defined in <ORT root>/include/onnxruntime/core/graph/constants.h
     # This list is limited to just the domains we have processors for
-    domain_to_constant_map = {'ai.onnx': 'kOnnxDomain',
-                              'ai.onnx.ml': 'kMLDomain',
-                              'com.microsoft': 'kMSDomain'}
+    domain_to_constant_map = {"ai.onnx": "kOnnxDomain", "ai.onnx.ml": "kMLDomain", "com.microsoft": "kMSDomain"}
 
     if domain not in domain_to_constant_map:
-        raise ValueError('Domain {} not found in map to ONNX Runtime constant. Please update map.'.format(domain))
+        raise ValueError(f"Domain {domain} not found in map to ONNX Runtime constant. Please update map.")
 
     return domain_to_constant_map[domain]
 
 
 def _reg_type_to_cpp_type(reg_type: str):
     if reg_type == "string":
         return "std::string"
     return reg_type
 
 
 def _split_reg_types(reg_types_str: str):
-    '''
+    """
     Split on underscores but append "_t" to the previous element.
-    '''
+    """
     tokens = reg_types_str.split("_")
     reg_types = []
     for token in tokens:
         if token == "t" and len(reg_types) > 0:
             reg_types[-1] += "_t"
         else:
             reg_types += [token]
     return reg_types
 
 
 class TypeUsageProcessor(ABC):
-    '''
+    """
     Abstract base class for processors which implement operator specific logic to determine the type or types required.
-    '''
+    """
+
     def __init__(self, domain: str, optype: str):
         self.domain = domain
         self.optype = optype
         self.name = _create_op_key(domain, optype)
 
     @abstractmethod
     def process_node(self, node: fbs.Node, value_name_to_typeinfo: dict):
         pass
 
-    def is_typed_registration_needed(self, type_in_registration: str,
-                                     globally_allowed_types: typing.Optional[typing.Set[str]]):
-        '''
+    def is_typed_registration_needed(
+        self, type_in_registration: str, globally_allowed_types: typing.Optional[typing.Set[str]]
+    ):
+        """
         Given the string from a kernel registration, determine if the registration is required or not.
         :param type_in_registration: Type string from kernel registration
         :param globally_allowed_types: Optional set of globally allowed types. If provided, these types take precedence
                                        in determining the required types.
         :return: True is required. False if not.
-        '''
+        """
         # Not all operators have typed registrations, so this is optionally implemented by derived classes
-        raise RuntimeError('Did not expect processor for {} to have typed registrations.'.format(self.name))
+        raise RuntimeError(f"Did not expect processor for {self.name} to have typed registrations.")
 
     def get_cpp_entry(self):
-        '''
+        """
         Get the C++ code that specifies this operator's required types.
         :return: List with any applicable C++ code for this operator's required types. One line per entry.
-        '''
+        """
         # Not applicable for some ops, so return no lines by default.
         return []
 
     @abstractmethod
     def to_config_entry(self):
-        '''
+        """
         Generate a configuration file entry in JSON format with the required types for the operator.
         :return: JSON string with required type information.
-        '''
+        """
         pass
 
     @abstractmethod
     def from_config_entry(self, entry: str):
-        '''
+        """
         Re-create the types required from a configuration file entry created with to_config_entry.
         NOTE: Any existing type information should be cleared prior to re-creating from a config file entry.
         :param entry: Configuration file entry
-        '''
+        """
         pass
 
 
 class DefaultTypeUsageProcessor(TypeUsageProcessor):
-    '''
+    """
     Operator processor which tracks the types used for selected input/s and/or output/s.
-    '''
+    """
 
-    def __init__(self, domain: str, optype: str, inputs: [int] = [0], outputs: [int] = [],
-                 required_input_types: typing.Dict[int, typing.Set[str]] = {},
-                 required_output_types: typing.Dict[int, typing.Set[str]] = {}):
-        '''
+    def __init__(
+        self,
+        domain: str,
+        optype: str,
+        inputs: [int] = [0],  # noqa: B006
+        outputs: [int] = [],  # noqa: B006
+        required_input_types: typing.Dict[int, typing.Set[str]] = {},  # noqa: B006
+        required_output_types: typing.Dict[int, typing.Set[str]] = {},  # noqa: B006
+    ):
+        """
         Create DefaultTypeUsageProcessor. Types for one or more inputs and/or outputs can be tracked by the processor.
         The default is to track the types required for input 0, as this is the most common use case in ONNX.
 
         Required input and output types may be specified. These are only applicable to is_typed_registration_needed().
         If a registration type matches a required type, the typed registration is needed.
         There is a separate mechanism for specifying required types from C++ for kernels with untyped registration.
 
         :param domain: Operator domain.
         :param optype: Operator name.
         :param inputs: Inputs to track. Zero based index. May be empty.
         :param outputs: Outputs to track. Zero based index. May be empty.
         :param required_input_types: Required input types. May be empty.
         :param required_output_types: Required output types. May be empty.
-        '''
+        """
         super().__init__(domain, optype)
         self._input_types = {}
         self._output_types = {}
 
         for i in inputs:
             self._input_types[i] = set()
 
         for o in outputs:
             self._output_types[o] = set()
 
         if not inputs and not outputs:
-            raise ValueError('At least one input or output must be tracked')
+            raise ValueError("At least one input or output must be tracked")
 
         self._required_input_types = required_input_types
         self._required_output_types = required_output_types
 
     def _is_type_enabled(self, reg_type, index, required_types, allowed_type_set):
         cpp_type = _reg_type_to_cpp_type(reg_type)
         return cpp_type in required_types.get(index, set()) or cpp_type in allowed_type_set
 
     def is_input_type_enabled(self, reg_type, index, allowed_type_set=None):
-        '''Whether input type is enabled based on required and allowed types.'''
+        """Whether input type is enabled based on required and allowed types."""
         if allowed_type_set is None:
             allowed_type_set = self._input_types[index]
         return self._is_type_enabled(reg_type, index, self._required_input_types, allowed_type_set)
 
     def is_output_type_enabled(self, reg_type, index, allowed_type_set=None):
-        '''Whether output type is enabled based on required and allowed types.'''
+        """Whether output type is enabled based on required and allowed types."""
         if allowed_type_set is None:
             allowed_type_set = self._output_types[index]
         return self._is_type_enabled(reg_type, index, self._required_output_types, allowed_type_set)
 
     def process_node(self, node: fbs.Node, value_name_to_typeinfo: dict):
-        for i in self._input_types.keys():
+        for i in self._input_types:
             if i >= node.InputsLength():
                 # Some operators have fewer inputs in earlier versions where data that was as an attribute
                 # become an input in later versions to allow it to be dynamically provided. Allow for that.
                 # e.g. Slice-1 had attributes for the indices, and Slice-10 moved those to be inputs
                 # raise RuntimeError('Node has {} outputs. Tracker for {} incorrectly configured as it requires {}.'
                 #                    .format(node.OutputsLength(), self.name, o))
                 pass
             else:
                 type_str = value_name_to_typestr(node.Inputs(i), value_name_to_typeinfo)
                 self._input_types[i].add(type_str)
 
-        for o in self._output_types.keys():
+        for o in self._output_types:
             # Don't know of any ops where the number of outputs changed across versions, so require a valid length
             if o >= node.OutputsLength():
-                raise RuntimeError('Node has {} outputs. Tracker for {} incorrectly configured as it requires {}.'
-                                   .format(node.OutputsLength(), self.name, o))
+                raise RuntimeError(
+                    "Node has {} outputs. Tracker for {} incorrectly configured as it requires {}.".format(
+                        node.OutputsLength(), self.name, o
+                    )
+                )
 
             type_str = value_name_to_typestr(node.Outputs(o), value_name_to_typeinfo)
             self._output_types[o].add(type_str)
 
-    def is_typed_registration_needed(self, type_in_registration: str,
-                                     globally_allowed_types: typing.Optional[typing.Set[str]]):
+    def is_typed_registration_needed(
+        self, type_in_registration: str, globally_allowed_types: typing.Optional[typing.Set[str]]
+    ):
         if 0 not in self._input_types.keys():
             # currently all standard typed registrations are for input 0.
             # custom registrations can be handled by operator specific processors (e.g. OneHotProcessor below).
-            raise RuntimeError('Expected typed registration to use type from input 0. Node:{}'.format(self.name))
+            raise RuntimeError(f"Expected typed registration to use type from input 0. Node:{self.name}")
 
         return self.is_input_type_enabled(type_in_registration, 0, globally_allowed_types)
 
     def get_cpp_entry(self):
         entries = []
         domain = _ort_constant_for_domain(self.domain)
         for i in sorted(self._input_types.keys()):
             if self._input_types[i]:
-                entries.append('ORT_SPECIFY_OP_KERNEL_ARG_ALLOWED_TYPES({}, {}, Input, {}, {});'
-                               .format(domain, self.optype, i, ', '.join(sorted(self._input_types[i]))))
+                entries.append(
+                    "ORT_SPECIFY_OP_KERNEL_ARG_ALLOWED_TYPES({}, {}, Input, {}, {});".format(
+                        domain, self.optype, i, ", ".join(sorted(self._input_types[i]))
+                    )
+                )
 
         for o in sorted(self._output_types.keys()):
             if self._output_types[o]:
-                entries.append('ORT_SPECIFY_OP_KERNEL_ARG_ALLOWED_TYPES({}, {}, Output, {}, {});'
-                               .format(domain, self.optype, o, ', '.join(sorted(self._output_types[o]))))
+                entries.append(
+                    "ORT_SPECIFY_OP_KERNEL_ARG_ALLOWED_TYPES({}, {}, Output, {}, {});".format(
+                        domain, self.optype, o, ", ".join(sorted(self._output_types[o]))
+                    )
+                )
 
         return entries
 
     def to_config_entry(self):
         # convert the sets of types to lists so they can easily written out using the json model
-        aggregate_info = {'inputs': {}, 'outputs': {}}
+        aggregate_info = {"inputs": {}, "outputs": {}}
 
         # filter out empty entries and sort the types
         for i in sorted(self._input_types.keys()):
             if self._input_types[i]:
-                aggregate_info['inputs'][i] = sorted(self._input_types[i])
+                aggregate_info["inputs"][i] = sorted(self._input_types[i])
 
         for o in sorted(self._output_types.keys()):
             if self._output_types[o]:
-                aggregate_info['outputs'][o] = sorted(self._output_types[o])
+                aggregate_info["outputs"][o] = sorted(self._output_types[o])
 
         # remove any empty keys
-        if not aggregate_info['inputs']:
-            aggregate_info.pop('inputs')
-        if not aggregate_info['outputs']:
-            aggregate_info.pop('outputs')
+        if not aggregate_info["inputs"]:
+            aggregate_info.pop("inputs")
+        if not aggregate_info["outputs"]:
+            aggregate_info.pop("outputs")
 
         entry = json.dumps(aggregate_info) if aggregate_info else None
         return entry
 
     def from_config_entry(self, entry: str):
         self._input_types.clear()
         self._output_types.clear()
 
         aggregate_info = json.loads(entry)
-        if 'inputs' in aggregate_info:
-            for i_str, values in aggregate_info['inputs'].items():
+        if "inputs" in aggregate_info:
+            for i_str, values in aggregate_info["inputs"].items():
                 self._input_types[int(i_str)] = set(values)
 
-        if 'outputs' in aggregate_info:
-            for o_str, values in aggregate_info['outputs'].items():
+        if "outputs" in aggregate_info:
+            for o_str, values in aggregate_info["outputs"].items():
                 self._output_types[int(o_str)] = set(values)
 
 
 class Input1TypedRegistrationProcessor(DefaultTypeUsageProcessor):
-    '''
+    """
     Processor for operators where the second input type is used in a typed kernel registration.
-    '''
+    """
+
     def __init__(self, domain: str, optype: str):
         # init with tracking of input 1 only.
         super().__init__(domain, optype, inputs=[1], outputs=[])
 
-    def is_typed_registration_needed(self, type_in_registration: str,
-                                     globally_allowed_types: typing.Optional[typing.Set[str]]):
+    def is_typed_registration_needed(
+        self, type_in_registration: str, globally_allowed_types: typing.Optional[typing.Set[str]]
+    ):
         return self.is_input_type_enabled(type_in_registration, 1, globally_allowed_types)
 
 
 class Output0TypedRegistrationProcessor(DefaultTypeUsageProcessor):
-    '''
+    """
     Processor for operators where the first output type is used in a typed kernel registration.
-    '''
+    """
+
     def __init__(self, domain: str, optype: str):
         # init with tracking of output 0 only.
         super().__init__(domain, optype, inputs=[], outputs=[0])
 
-    def is_typed_registration_needed(self, type_in_registration: str,
-                                     globally_allowed_types: typing.Optional[typing.Set[str]]):
+    def is_typed_registration_needed(
+        self, type_in_registration: str, globally_allowed_types: typing.Optional[typing.Set[str]]
+    ):
         return self.is_output_type_enabled(type_in_registration, 0, globally_allowed_types)
 
 
 class OneHotProcessor(TypeUsageProcessor):
-    '''
+    """
     Processor for the OneHot operator, which requires custom logic as the type registration key is a concatenation of
     the three types involved instead of a single type name.
-    '''
+    """
+
     def __init__(self):
-        super().__init__('ai.onnx', 'OneHot')
+        super().__init__("ai.onnx", "OneHot")
         self._triples = set()
 
     def process_node(self, node: fbs.Node, value_name_to_typeinfo: dict):
         type0 = value_name_to_typestr(node.Inputs(0), value_name_to_typeinfo)
         type1 = value_name_to_typestr(node.Inputs(1), value_name_to_typeinfo)
         type2 = value_name_to_typestr(node.Inputs(2), value_name_to_typeinfo)
         # types in kernel registration are ordered this way: input (T1), output (T3), depth (T2)
         key = (type0, type2, type1)
         self._triples.add(key)
 
-    def is_typed_registration_needed(self, type_in_registration: str,
-                                     globally_allowed_types: typing.Optional[typing.Set[str]]):
+    def is_typed_registration_needed(
+        self, type_in_registration: str, globally_allowed_types: typing.Optional[typing.Set[str]]
+    ):
         # the OneHot registration involves a concatenation of the 3 types involved
         reg_types = tuple([_reg_type_to_cpp_type(reg_type) for reg_type in _split_reg_types(type_in_registration)])
         if globally_allowed_types is not None:
             return all(reg_type in globally_allowed_types for reg_type in reg_types)
         else:
             return reg_types in self._triples
 
     def to_config_entry(self):
         if not self._triples:
             return None
 
-        aggregate_info = {'custom': sorted(self._triples)}
+        aggregate_info = {"custom": sorted(self._triples)}
         entry = json.dumps(aggregate_info)
         return entry
 
     def from_config_entry(self, entry: str):
         self._triples.clear()
         aggregate_info = json.loads(entry)
-        if 'custom' in aggregate_info:
-            self._triples = set([tuple(triple) for triple in aggregate_info['custom']])
+        if "custom" in aggregate_info:
+            self._triples = {tuple(triple) for triple in aggregate_info["custom"]}
 
 
 def _create_operator_type_usage_processors():
-    '''
+    """
     Create a set of processors that determine the required types for all enabled operators.
     :return: Dictionary of operator key to processor. Key is 'domain:operator (e.g. ai.onnx:Cast)'.
-    '''
+    """
     operator_processors = {}
 
     def add(processor):
         if processor.name in operator_processors:
-            raise RuntimeError('Duplicate processor for ' + processor.name)
+            raise RuntimeError("Duplicate processor for " + processor.name)
 
         operator_processors[processor.name] = processor
 
     # Starting with ops from:
     #   - Priority 1P models
     #   - Mobilenet + SSD Mobilenet + MobileBert
     #   - some known large kernels
@@ -331,261 +354,318 @@
     #    com.microsoft: DynamicQuantizeMatMul, MatMulIntegerToFloat
     # - Only one type supported in the ORT implementation:
     #    ai.onnx: NonMaxSuppression
     #    com.microsoft: FusedConv, FusedGemm, FusedMatMul
     # - Implementation does not have any significant type specific code:
     #    ai.onnx: Concat, Flatten, Not, Reshape, Shape, Squeeze, Unsqueeze
     #
-    default_processor_onnx_ops = ['Abs', 'ArgMax', 'ArgMin', 'AveragePool',
-                                  'BatchNormalization', 'BitShift',
-                                  'Ceil', 'Clip', 'Conv', 'CumSum',
-                                  'Exp', 'Expand',
-                                  'Floor',
-                                  'Gemm',
-                                  'IsNaN',
-                                  'Log', 'LogSoftmax', 'LpNormalization',
-                                  'MatMul', 'Max', 'MaxPool', 'Mean', 'Min',
-                                  'NonZero',
-                                  'Pad',
-                                  'QLinearConv', 'QLinearMatMul',
-                                  'Range', 'Reciprocal', 'ReduceL1', 'ReduceL2', 'ReduceLogSum', 'ReduceLogSumExp',
-                                  'ReduceMax', 'ReduceMean', 'ReduceMin', 'ReduceProd', 'ReduceSum', 'ReduceSumSquare',
-                                  'Relu', 'Resize', 'ReverseSequence', 'RoiAlign', 'Round',
-                                  'Scatter', 'ScatterElements', 'ScatterND', 'Shrink', 'Sigmoid', 'Sign', 'Sin',
-                                  'Softmax', 'Split', 'SplitToSequence', 'Sqrt', 'Sum',
-                                  'Tanh', 'TopK', 'Transpose',
-                                  'Unique']
+    default_processor_onnx_ops = [
+        "Abs",
+        "ArgMax",
+        "ArgMin",
+        "AveragePool",
+        "BatchNormalization",
+        "BitShift",
+        "Ceil",
+        "Clip",
+        "Conv",
+        "CumSum",
+        "Exp",
+        "Expand",
+        "Floor",
+        "Gemm",
+        "IsNaN",
+        "Log",
+        "LogSoftmax",
+        "LpNormalization",
+        "MatMul",
+        "Max",
+        "MaxPool",
+        "Mean",
+        "Min",
+        "NonZero",
+        "Pad",
+        "QLinearConv",
+        "QLinearMatMul",
+        "Range",
+        "Reciprocal",
+        "ReduceL1",
+        "ReduceL2",
+        "ReduceLogSum",
+        "ReduceLogSumExp",
+        "ReduceMax",
+        "ReduceMean",
+        "ReduceMin",
+        "ReduceProd",
+        "ReduceSum",
+        "ReduceSumSquare",
+        "Relu",
+        "Resize",
+        "ReverseSequence",
+        "RoiAlign",
+        "Round",
+        "Scatter",
+        "ScatterElements",
+        "ScatterND",
+        "Shrink",
+        "Sigmoid",
+        "Sign",
+        "Sin",
+        "Softmax",
+        "Split",
+        "SplitToSequence",
+        "Sqrt",
+        "Sum",
+        "Tanh",
+        "TopK",
+        "Transpose",
+        "Unique",
+    ]
 
     # ops that are used to manipulate shapes or indices so require int32_t and int64_t to be available
-    default_processor_onnx_ops_requiring_ints_for_input_0 = ['Add',
-                                                             'Concat',
-                                                             'Div',
-                                                             'Equal',
-                                                             'Greater',
-                                                             'Less',
-                                                             'Mul',
-                                                             'Neg',  # used in tflite TransposeConv conversion
-                                                             'Sub']
+    default_processor_onnx_ops_requiring_ints_for_input_0 = [
+        "Add",
+        "Concat",
+        "Div",
+        "Equal",
+        "Greater",
+        "Less",
+        "Mul",
+        "Neg",  # used in tflite TransposeConv conversion
+        "Sub",
+    ]
 
     # NOTE: QLinearConv has ONNX and internal implementations
-    internal_ops = ['QLinearAdd', 'QLinearMul', 'QLinearConv']
+    internal_ops = ["QLinearAdd", "QLinearMul", "QLinearConv"]
 
     # TODO - review and add ML ops as needed
     # ML Op notes.
     #  CastMap: Switch on value type of input map type, and output type
     #  DictVectorizer: Templatized on key+value of input so need to handle like OneHot with custom processor
     #  LabelEncoder: Implementation switches on input and output types (only supports string and int64 in T1 and T2)
     #  LinearClassifier: Internal switch on input type and also switch on output type
     #  SVMClassifier: ditto
     #  TreeEnsembleClassifier: Templatized on input type and also switch on output type
     #  ZipMap: Switch on output type (derived from attributes)
     default_processor_onnxml_ops = []
 
-    [add(DefaultTypeUsageProcessor('ai.onnx', op)) for op in default_processor_onnx_ops]
-    [add(DefaultTypeUsageProcessor('ai.onnx', op, required_input_types={0: {"int32_t", "int64_t"}}))
-        for op in default_processor_onnx_ops_requiring_ints_for_input_0]
-    [add(DefaultTypeUsageProcessor('ai.onnx.ml', op)) for op in default_processor_onnxml_ops]
-    [add(DefaultTypeUsageProcessor('com.microsoft', op)) for op in internal_ops]
+    [add(DefaultTypeUsageProcessor("ai.onnx", op)) for op in default_processor_onnx_ops]
+    [
+        add(DefaultTypeUsageProcessor("ai.onnx", op, required_input_types={0: {"int32_t", "int64_t"}}))
+        for op in default_processor_onnx_ops_requiring_ints_for_input_0
+    ]
+    [add(DefaultTypeUsageProcessor("ai.onnx.ml", op)) for op in default_processor_onnxml_ops]
+    [add(DefaultTypeUsageProcessor("com.microsoft", op)) for op in internal_ops]
 
     #
     # Operators that require custom handling
     #
 
     # Cast switches on types of input 0 and output 0
-    add(DefaultTypeUsageProcessor('ai.onnx', 'Cast', inputs=[0], outputs=[0]))
+    add(DefaultTypeUsageProcessor("ai.onnx", "Cast", inputs=[0], outputs=[0]))
 
     # Operators that switch on the type of input 0 and 1
-    add(DefaultTypeUsageProcessor('ai.onnx', 'Gather', inputs=[0, 1]))
-    add(DefaultTypeUsageProcessor('ai.onnx', 'GatherElements', inputs=[0, 1]))
-    add(DefaultTypeUsageProcessor('ai.onnx', 'Pow', inputs=[0, 1]))
-    add(DefaultTypeUsageProcessor('ai.onnx', 'Slice', inputs=[0, 1]))
+    add(DefaultTypeUsageProcessor("ai.onnx", "Gather", inputs=[0, 1]))
+    add(DefaultTypeUsageProcessor("ai.onnx", "GatherElements", inputs=[0, 1]))
+    add(DefaultTypeUsageProcessor("ai.onnx", "Pow", inputs=[0, 1]))
+    add(DefaultTypeUsageProcessor("ai.onnx", "Slice", inputs=[0, 1]))
 
     # Operators that switch on output type
-    add(DefaultTypeUsageProcessor('ai.onnx', 'ConstantOfShape', inputs=[], outputs=[0]))
+    add(DefaultTypeUsageProcessor("ai.onnx", "ConstantOfShape", inputs=[], outputs=[0]))
 
     # Random generator ops produce new data so we track the output type
-    onnx_random_ops = ['RandomNormal', 'RandomNormalLike', 'RandomUniform', 'RandomUniformLike', 'Multinomial']
-    [add(DefaultTypeUsageProcessor('ai.onnx', op, inputs=[], outputs=[0])) for op in onnx_random_ops]
+    onnx_random_ops = ["RandomNormal", "RandomNormalLike", "RandomUniform", "RandomUniformLike", "Multinomial"]
+    [add(DefaultTypeUsageProcessor("ai.onnx", op, inputs=[], outputs=[0])) for op in onnx_random_ops]
 
     # Where always has a boolean first input so track the second input type for typed registration
-    add(Input1TypedRegistrationProcessor('ai.onnx', 'Where'))
+    add(Input1TypedRegistrationProcessor("ai.onnx", "Where"))
 
     # we only support 'float' as input for [Dynamic]QuantizeLinear so just track the output type
     # as that's what is used in the typed registration
-    add(Output0TypedRegistrationProcessor('ai.onnx', 'QuantizeLinear'))
-    add(Output0TypedRegistrationProcessor('ai.onnx', 'DynamicQuantizeLinear'))
+    add(Output0TypedRegistrationProcessor("ai.onnx", "QuantizeLinear"))
+    add(Output0TypedRegistrationProcessor("ai.onnx", "DynamicQuantizeLinear"))
 
     # make sure all the dequantize types are enabled. we use int32_t for parts of GEMM and Conv so just
     # enabling int8 and uint8 is not enough.
     # TODO: Only apply required types to the global type list and ignore if it's model based per-op type reduction
-    add(DefaultTypeUsageProcessor('ai.onnx', 'DequantizeLinear', inputs=[0],
-                                  required_input_types={0: {'int8_t', 'uint8_t', 'int32_t'}}))
+    add(
+        DefaultTypeUsageProcessor(
+            "ai.onnx", "DequantizeLinear", inputs=[0], required_input_types={0: {"int8_t", "uint8_t", "int32_t"}}
+        )
+    )
 
     # OneHot concatenates type strings into a triple in the typed registration
     #   e.g. float_int64_t_int64_t
     add(OneHotProcessor())
 
     return operator_processors
 
 
 class OpTypeImplFilterInterface(ABC):
-    '''
+    """
     Class that filters operator implementations based on type.
-    '''
+    """
+
     @abstractmethod
     def is_typed_registration_needed(self, domain: str, optype: str, type_registration_str: str):
-        '''
+        """
         Given the string from a kernel registration, determine if the registration is required or not.
         :param domain: Operator domain.
         :param optype: Operator type.
         :param type_registration_str: Type string from kernel registration
         :return: True is required. False if not.
-        '''
+        """
         pass
 
     @abstractmethod
     def get_cpp_entries(self):
-        '''
+        """
         Get the C++ code that specifies the operator types to enable.
         :return: List of strings. One line of C++ code per entry.
-        '''
+        """
         pass
 
 
 class OperatorTypeUsageManager:
-    '''
+    """
     Class to manage the operator type usage processors.
     TODO: Currently the type tracking is not specific to a version of the operator.
     It's unclear how/where version specific logic could/should be added, and it would add significant complexity
     to track types on a per-version basis. Not clear there's enough benefit from doing so either.
-    '''
+    """
+
     def __init__(self):
         self._all_operator_processors = _create_operator_type_usage_processors()  # all possible processors
         self._operator_processors = {}  # processors we have actually used so we can limit output to be meaningful
 
     def _get_op_processor(self, key):
-        'Add the processor to _operator_processors as it is about to be used.'
+        "Add the processor to _operator_processors as it is about to be used."
         processor = None
         if key in self._all_operator_processors:
             if key not in self._operator_processors:
                 self._operator_processors[key] = self._all_operator_processors[key]
 
             processor = self._operator_processors[key]
 
         return processor
 
     def process_node(self, node: fbs.Node, value_name_to_typeinfo: dict):
-        '''
+        """
         Process a Node and record info on the types used.
         :param node: Node from ORT format model
         :param value_name_to_typeinfo: Map of value names to TypeInfo instances
-        '''
+        """
         optype = node.OpType().decode()
-        domain = node.Domain().decode() or 'ai.onnx'  # empty domain defaults to ai.onnx
+        domain = node.Domain().decode() or "ai.onnx"  # empty domain defaults to ai.onnx
 
         key = _create_op_key(domain, optype)
         op_processor = self._get_op_processor(key)
         if op_processor:
             op_processor.process_node(node, value_name_to_typeinfo)
 
     def get_config_entry(self, domain: str, optype: str):
-        '''
+        """
         Get the config entry specifying the types for this operator.
         :param domain: Operator domain.
         :param optype: Operator type.
         :return: JSON string with type info if available, else None
-        '''
+        """
         key = _create_op_key(domain, optype)
         config_str = None
         if key in self._operator_processors:
             config_str = self._operator_processors[key].to_config_entry()
 
         return config_str
 
     def restore_from_config_entry(self, domain: str, optype: str, config_entry: str):
-        '''
+        """
         Restore the per-operator type information from a configuration file entry.
         :param domain: Operator domain.
         :param optype: Operator type.
         :param config_entry: JSON string with type info as created by get_config_entry
-        '''
+        """
         key = _create_op_key(domain, optype)
         op_processor = self._get_op_processor(key)
         if op_processor:
             op_processor.from_config_entry(config_entry)
 
     def debug_dump(self):
-
-        print('C++ code that will be emitted:')
+        print("C++ code that will be emitted:")
         [print(cpp_line) for cpp_line in self.get_cpp_entries()]
 
-        print('Config file type information that will be returned by get_config_entry:')
+        print("Config file type information that will be returned by get_config_entry:")
         for key in sorted(self._operator_processors.keys()):
             entry = self._operator_processors[key].to_config_entry()
             if entry:
-                print('{} -> {}'.format(key, entry))
+                print(f"{key} -> {entry}")
 
                 # roundtrip test to validate that we can initialize the processor from the entry and get the
                 # same values back
                 self._operator_processors[key].from_config_entry(entry)
-                assert(entry == self._operator_processors[key].to_config_entry())
+                assert entry == self._operator_processors[key].to_config_entry()
 
     class _OpTypeImplFilter(OpTypeImplFilterInterface):
         def __init__(self, manager):
             self._manager = manager
 
         def is_typed_registration_needed(self, domain: str, optype: str, type_registration_str: str):
             needed = True  # we keep the registration unless the per-operator processor says not to
             key = _create_op_key(domain, optype)
             if key in self._manager._operator_processors:
                 needed = self._manager._operator_processors[key].is_typed_registration_needed(
-                    type_in_registration=type_registration_str, globally_allowed_types=None)
+                    type_in_registration=type_registration_str, globally_allowed_types=None
+                )
 
             return needed
 
         def get_cpp_entries(self):
             entries = []
             for key in sorted(self._manager._operator_processors.keys()):
                 entries.extend(self._manager._operator_processors[key].get_cpp_entry())
 
             return entries
 
     def make_op_type_impl_filter(self):
-        '''
+        """
         Creates an OpTypeImplFilterInterface instance from this manager.
         Filtering uses the manager's operator type usage processor state.
-        '''
+        """
         return OperatorTypeUsageManager._OpTypeImplFilter(self)
 
 
 class GloballyAllowedTypesOpTypeImplFilter(OpTypeImplFilterInterface):
-    '''
+    """
     Operator implementation filter which uses globally allowed types.
-    '''
+    """
+
     _valid_allowed_types = set(FbsTypeInfo.tensordatatype_to_string.values())
 
     def __init__(self, globally_allowed_types: typing.Set[str]):
         self._operator_processors = _create_operator_type_usage_processors()
 
         if not globally_allowed_types.issubset(self._valid_allowed_types):
-            raise ValueError("Globally allowed types must all be valid. Invalid types: {}"
-                             .format(sorted(globally_allowed_types - self._valid_allowed_types)))
+            raise ValueError(
+                "Globally allowed types must all be valid. Invalid types: {}".format(
+                    sorted(globally_allowed_types - self._valid_allowed_types)
+                )
+            )
 
         self._globally_allowed_types = globally_allowed_types
 
     def is_typed_registration_needed(self, domain: str, optype: str, type_registration_str: str):
         key = _create_op_key(domain, optype)
         if key in self._operator_processors:
             needed = self._operator_processors[key].is_typed_registration_needed(
-                type_in_registration=type_registration_str,
-                globally_allowed_types=self._globally_allowed_types)
+                type_in_registration=type_registration_str, globally_allowed_types=self._globally_allowed_types
+            )
         else:
             needed = _reg_type_to_cpp_type(type_registration_str) in self._globally_allowed_types
 
         return needed
 
     def get_cpp_entries(self):
-        return ["ORT_SPECIFY_OP_KERNEL_GLOBAL_ALLOWED_TYPES({});".format(
-            ", ".join(sorted(self._globally_allowed_types)))]
+        return [
+            "ORT_SPECIFY_OP_KERNEL_GLOBAL_ALLOWED_TYPES({});".format(", ".join(sorted(self._globally_allowed_types)))
+        ]
 
     def global_type_list(self):
         return self._globally_allowed_types
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_model_processor.py` & `onnxruntime/tools/ort_format_model/ort_model_processor.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,70 +1,71 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 import ort_flatbuffers_py.fbs as fbs
+
 from .operator_type_usage_processors import OperatorTypeUsageManager
 
 
 class OrtFormatModelProcessor:
-    'Class to process an ORT format model and determine required operators and types.'
+    "Class to process an ORT format model and determine required operators and types."
 
     def __init__(self, model_path: str, required_ops: dict, processors: OperatorTypeUsageManager):
-        '''
+        """
         Initialize ORT format model processor
         :param model_path: Path to model to load
         :param required_ops: Dictionary required operator information will be added to.
         :param processors: Operator type usage processors which will be called for each matching Node.
-        '''
+        """
         self._required_ops = required_ops  # dictionary of {domain: {opset:[operators]}}
-        self._file = open(model_path, 'rb').read()
+        self._file = open(model_path, "rb").read()  # noqa: SIM115
         self._buffer = bytearray(self._file)
         if not fbs.InferenceSession.InferenceSession.InferenceSessionBufferHasIdentifier(self._buffer, 0):
-            raise RuntimeError("File does not appear to be a valid ORT format model: '{}'".format(model_path))
+            raise RuntimeError(f"File does not appear to be a valid ORT format model: '{model_path}'")
         self._model = fbs.InferenceSession.InferenceSession.GetRootAsInferenceSession(self._buffer, 0).Model()
         self._op_type_processors = processors
 
     @staticmethod
-    def _setup_type_info(graph: fbs.Graph, outer_scope_value_typeinfo={}):
-        '''
+    def _setup_type_info(graph: fbs.Graph, outer_scope_value_typeinfo={}):  # noqa: B006
+        """
         Setup the node args for this level of Graph.
         We copy the current list which represents the outer scope values, and add the local node args to that
         to create the valid list of values for the current Graph.
         :param graph: Graph to create NodeArg list for
         :param outer_scope_value_typeinfo: TypeInfo for outer scope values. Empty for the top-level graph in a model.
         :return: Dictionary of NodeArg name to TypeInfo
-        '''
+        """
         value_name_to_typeinfo = outer_scope_value_typeinfo.copy()
         for j in range(0, graph.NodeArgsLength()):
             n = graph.NodeArgs(j)
             value_name_to_typeinfo[n.Name()] = n.Type()  # TypeInfo for this NodeArg's name
 
         return value_name_to_typeinfo
 
     def _add_required_op(self, domain: str, opset: int, op_type: str):
         if domain not in self._required_ops:
-            self._required_ops[domain] = {opset: set([op_type])}
+            self._required_ops[domain] = {opset: {op_type}}
         elif opset not in self._required_ops[domain]:
-            self._required_ops[domain][opset] = set([op_type])
+            self._required_ops[domain][opset] = {op_type}
         else:
             self._required_ops[domain][opset].add(op_type)
 
     def _process_graph(self, graph: fbs.Graph, outer_scope_value_typeinfo: dict):
-        '''
+        """
         Process one level of the Graph, descending into any subgraphs when they are found
         :param outer_scope_value_typeinfo: Outer scope NodeArg dictionary from ancestor graphs
-        '''
+        """
         # Merge the TypeInfo for all values in this level of the graph with the outer scope value TypeInfo.
         value_name_to_typeinfo = OrtFormatModelProcessor._setup_type_info(graph, outer_scope_value_typeinfo)
 
         for i in range(0, graph.NodesLength()):
             node = graph.Nodes(i)
 
             optype = node.OpType().decode()
-            domain = node.Domain().decode() or 'ai.onnx'  # empty domain defaults to ai.onnx
+            domain = node.Domain().decode() or "ai.onnx"  # empty domain defaults to ai.onnx
 
             self._add_required_op(domain, node.SinceVersion(), optype)
 
             if self._op_type_processors:
                 self._op_type_processors.process_node(node, value_name_to_typeinfo)
 
             # Read all the attributes
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/types.py` & `onnxruntime/tools/ort_format_model/types.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,52 +3,52 @@
 
 import ort_flatbuffers_py.fbs as fbs
 
 
 class FbsTypeInfo:
     "Class to provide conversion between ORT flatbuffers schema values and C++ types"
     tensordatatype_to_string = {
-        fbs.TensorDataType.TensorDataType.FLOAT: 'float',
-        fbs.TensorDataType.TensorDataType.UINT8: 'uint8_t',
-        fbs.TensorDataType.TensorDataType.INT8: 'int8_t',
-        fbs.TensorDataType.TensorDataType.UINT16: 'uint16_t',
-        fbs.TensorDataType.TensorDataType.INT16: 'int16_t',
-        fbs.TensorDataType.TensorDataType.INT32: 'int32_t',
-        fbs.TensorDataType.TensorDataType.INT64: 'int64_t',
-        fbs.TensorDataType.TensorDataType.STRING: 'std::string',
-        fbs.TensorDataType.TensorDataType.BOOL: 'bool',
-        fbs.TensorDataType.TensorDataType.FLOAT16: 'MLFloat16',
-        fbs.TensorDataType.TensorDataType.DOUBLE: 'double',
-        fbs.TensorDataType.TensorDataType.UINT32: 'uint32_t',
-        fbs.TensorDataType.TensorDataType.UINT64: 'uint64_t',
+        fbs.TensorDataType.TensorDataType.FLOAT: "float",
+        fbs.TensorDataType.TensorDataType.UINT8: "uint8_t",
+        fbs.TensorDataType.TensorDataType.INT8: "int8_t",
+        fbs.TensorDataType.TensorDataType.UINT16: "uint16_t",
+        fbs.TensorDataType.TensorDataType.INT16: "int16_t",
+        fbs.TensorDataType.TensorDataType.INT32: "int32_t",
+        fbs.TensorDataType.TensorDataType.INT64: "int64_t",
+        fbs.TensorDataType.TensorDataType.STRING: "std::string",
+        fbs.TensorDataType.TensorDataType.BOOL: "bool",
+        fbs.TensorDataType.TensorDataType.FLOAT16: "MLFloat16",
+        fbs.TensorDataType.TensorDataType.DOUBLE: "double",
+        fbs.TensorDataType.TensorDataType.UINT32: "uint32_t",
+        fbs.TensorDataType.TensorDataType.UINT64: "uint64_t",
         # fbs.TensorDataType.TensorDataType.COMPLEX64: 'complex64 is not supported',
         # fbs.TensorDataType.TensorDataType.COMPLEX128: 'complex128 is not supported',
-        fbs.TensorDataType.TensorDataType.BFLOAT16: 'BFloat16'
+        fbs.TensorDataType.TensorDataType.BFLOAT16: "BFloat16",
     }
 
     @staticmethod
     def typeinfo_to_str(type: fbs.TypeInfo):
         value_type = type.ValueType()
         value = type.Value()
-        type_str = 'unknown'
+        type_str = "unknown"
 
         if value_type == fbs.TypeInfoValue.TypeInfoValue.tensor_type:
             tensor_type_and_shape = fbs.TensorTypeAndShape.TensorTypeAndShape()
             tensor_type_and_shape.Init(value.Bytes, value.Pos)
             elem_type = tensor_type_and_shape.ElemType()
             type_str = FbsTypeInfo.tensordatatype_to_string[elem_type]
 
         elif value_type == fbs.TypeInfoValue.TypeInfoValue.map_type:
             map_type = fbs.MapType.MapType()
             map_type.init(value.Bytes, value.Pos)
             key_type = map_type.KeyType()  # TensorDataType
             key_type_str = FbsTypeInfo.tensordatatype_to_string[key_type]
             value_type = map_type.ValueType()  # TypeInfo
             value_type_str = FbsTypeInfo.typeinfo_to_str(value_type)
-            type_str = 'std::map<{},{}>'.format(key_type_str, value_type_str)
+            type_str = f"std::map<{key_type_str},{value_type_str}>"
 
         elif value_type == fbs.TypeInfoValue.TypeInfoValue.sequence_type:
             sequence_type = fbs.SequenceType.SequenceType()
             sequence_type.Init(value.Bytes, value.Pos)
             elem_type = sequence_type.ElemType()  # TypeInfo
             elem_type_str = FbsTypeInfo.typeinfo_to_str(elem_type)
             # TODO: Decide if we need to wrap the type in a std::vector. Issue is that the element type is internal
@@ -56,25 +56,25 @@
             # For now, return the element type (which will be the Tensor element type, or a map<A,B>) as
             # an operator input or output will either be a sequence or a not, so we don't need to disambiguate
             # between the two (i.e. we know if the returned value refers to the contents of a sequence, and can
             # handle whether it's the element type of a Tensor in the sequence, or the map type in a sequence of maps
             # due to this).
             type_str = elem_type_str
         else:
-            raise ValueError('Unknown or missing value type of {}'.format(value_type))
+            raise ValueError(f"Unknown or missing value type of {value_type}")
 
         return type_str
 
 
 def get_typeinfo(name: str, value_name_to_typeinfo: dict) -> fbs.TypeInfo:
-    'Lookup a name in a dictionary mapping value name to TypeInfo.'
+    "Lookup a name in a dictionary mapping value name to TypeInfo."
     if name not in value_name_to_typeinfo:
-        raise RuntimeError('Missing TypeInfo entry for ' + name)
+        raise RuntimeError("Missing TypeInfo entry for " + name)
 
     return value_name_to_typeinfo[name]  # TypeInfo object
 
 
 def value_name_to_typestr(name: str, value_name_to_typeinfo: dict):
-    'Lookup TypeInfo for value name and convert to a string representing the C++ type.'
+    "Lookup TypeInfo for value name and convert to a string representing the C++ type."
     type = get_typeinfo(name, value_name_to_typeinfo)
     type_str = FbsTypeInfo.typeinfo_to_str(type)
     return type_str
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/utils.py` & `onnxruntime/tools/ort_format_model/utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 import pathlib
 import typing
 
+from ..logger import get_logger
 from .operator_type_usage_processors import OperatorTypeUsageManager
 from .ort_model_processor import OrtFormatModelProcessor
 
-from ..logger import get_logger
 log = get_logger("ort_format_model.utils")
 
 
 def _extract_ops_and_types_from_ort_models(model_files: typing.Iterable[pathlib.Path], enable_type_reduction: bool):
     required_ops = {}
     op_type_usage_manager = OperatorTypeUsageManager() if enable_type_reduction else None
 
@@ -20,40 +20,43 @@
             raise ValueError(f"Path is not a file: '{model_file}'")
         model_processor = OrtFormatModelProcessor(str(model_file), required_ops, op_type_usage_manager)
         model_processor.process()  # this updates required_ops and op_type_processors
 
     return required_ops, op_type_usage_manager
 
 
-def create_config_from_models(model_files: typing.Iterable[pathlib.Path], output_file: pathlib.Path,
-                              enable_type_reduction: bool):
-    '''
+def create_config_from_models(
+    model_files: typing.Iterable[pathlib.Path], output_file: pathlib.Path, enable_type_reduction: bool
+):
+    """
     Create a configuration file with required operators and optionally required types.
     :param model_files: Model files to use to generate the configuration file.
     :param output_file: File to write configuration to.
     :param enable_type_reduction: Include required type information for individual operators in the configuration.
-    '''
+    """
 
     required_ops, op_type_processors = _extract_ops_and_types_from_ort_models(model_files, enable_type_reduction)
 
     output_file.parent.mkdir(parents=True, exist_ok=True)
 
-    with open(output_file, 'w') as out:
+    with open(output_file, "w") as out:
         out.write("# Generated from model/s:\n")
         for model_file in sorted(model_files):
             out.write(f"# - {model_file}\n")
 
         for domain in sorted(required_ops.keys()):
             for opset in sorted(required_ops[domain].keys()):
                 ops = required_ops[domain][opset]
                 if ops:
-                    out.write("{};{};".format(domain, opset))
+                    out.write(f"{domain};{opset};")
                     if enable_type_reduction:
                         # type string is empty if op hasn't been seen
-                        entries = ['{}{}'.format(op, op_type_processors.get_config_entry(domain, op) or '')
-                                   for op in sorted(ops)]
+                        entries = [
+                            "{}{}".format(op, op_type_processors.get_config_entry(domain, op) or "")
+                            for op in sorted(ops)
+                        ]
                     else:
                         entries = sorted(ops)
 
-                    out.write("{}\n".format(','.join(entries)))
+                    out.write("{}\n".format(",".join(entries)))
 
     log.info("Created config in %s", output_file)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py`

 * *Files 8% similar despite different names*

```diff
@@ -39,22 +39,22 @@
             from ort_flatbuffers_py.fbs.Model import Model
             obj = Model()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
     # InferenceSession
-    def SessionState(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
+    def KernelTypeStrResolver(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
         if o != 0:
             x = self._tab.Indirect(o + self._tab.Pos)
-            from ort_flatbuffers_py.fbs.SessionState import SessionState
-            obj = SessionState()
+            from ort_flatbuffers_py.fbs.KernelTypeStrResolver import KernelTypeStrResolver
+            obj = KernelTypeStrResolver()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-def InferenceSessionStart(builder): builder.StartObject(3)
+def InferenceSessionStart(builder): builder.StartObject(4)
 def InferenceSessionAddOrtVersion(builder, ortVersion): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(ortVersion), 0)
 def InferenceSessionAddModel(builder, model): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(model), 0)
-def InferenceSessionAddSessionState(builder, sessionState): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(sessionState), 0)
+def InferenceSessionAddKernelTypeStrResolver(builder, kernelTypeStrResolver): builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(kernelTypeStrResolver), 0)
 def InferenceSessionEnd(builder): return builder.EndObject()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelCreateInfos.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py`

 * *Files 26% similar despite different names*

```diff
@@ -2,85 +2,80 @@
 
 # namespace: fbs
 
 import flatbuffers
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
-class KernelCreateInfos(object):
+class SparseTensor(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsKernelCreateInfos(cls, buf, offset):
+    def GetRootAsSparseTensor(cls, buf, offset):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
-        x = KernelCreateInfos()
+        x = SparseTensor()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
-    def KernelCreateInfosBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
+    def SparseTensorBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
-    # KernelCreateInfos
+    # SparseTensor
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
-    # KernelCreateInfos
-    def NodeIndices(self, j):
+    # SparseTensor
+    def Values(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
-            a = self._tab.Vector(o)
-            return self._tab.Get(flatbuffers.number_types.Uint32Flags, a + flatbuffers.number_types.UOffsetTFlags.py_type(j * 4))
-        return 0
-
-    # KernelCreateInfos
-    def NodeIndicesAsNumpy(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
-        if o != 0:
-            return self._tab.GetVectorAsNumpy(flatbuffers.number_types.Uint32Flags, o)
-        return 0
-
-    # KernelCreateInfos
-    def NodeIndicesLength(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
-        if o != 0:
-            return self._tab.VectorLen(o)
-        return 0
-
-    # KernelCreateInfos
-    def NodeIndicesIsNone(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
-        return o == 0
+            x = self._tab.Indirect(o + self._tab.Pos)
+            from ort_flatbuffers_py.fbs.Tensor import Tensor
+            obj = Tensor()
+            obj.Init(self._tab.Bytes, x)
+            return obj
+        return None
 
-    # KernelCreateInfos
-    def KernelDefHashes(self, j):
+    # SparseTensor
+    def Indices(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
+            x = self._tab.Indirect(o + self._tab.Pos)
+            from ort_flatbuffers_py.fbs.Tensor import Tensor
+            obj = Tensor()
+            obj.Init(self._tab.Bytes, x)
+            return obj
+        return None
+
+    # SparseTensor
+    def Dims(self, j):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
+        if o != 0:
             a = self._tab.Vector(o)
-            return self._tab.Get(flatbuffers.number_types.Uint64Flags, a + flatbuffers.number_types.UOffsetTFlags.py_type(j * 8))
+            return self._tab.Get(flatbuffers.number_types.Int64Flags, a + flatbuffers.number_types.UOffsetTFlags.py_type(j * 8))
         return 0
 
-    # KernelCreateInfos
-    def KernelDefHashesAsNumpy(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
+    # SparseTensor
+    def DimsAsNumpy(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
         if o != 0:
-            return self._tab.GetVectorAsNumpy(flatbuffers.number_types.Uint64Flags, o)
+            return self._tab.GetVectorAsNumpy(flatbuffers.number_types.Int64Flags, o)
         return 0
 
-    # KernelCreateInfos
-    def KernelDefHashesLength(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
+    # SparseTensor
+    def DimsLength(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
         if o != 0:
             return self._tab.VectorLen(o)
         return 0
 
-    # KernelCreateInfos
-    def KernelDefHashesIsNone(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
+    # SparseTensor
+    def DimsIsNone(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
         return o == 0
 
-def KernelCreateInfosStart(builder): builder.StartObject(2)
-def KernelCreateInfosAddNodeIndices(builder, nodeIndices): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(nodeIndices), 0)
-def KernelCreateInfosStartNodeIndicesVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def KernelCreateInfosAddKernelDefHashes(builder, kernelDefHashes): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(kernelDefHashes), 0)
-def KernelCreateInfosStartKernelDefHashesVector(builder, numElems): return builder.StartVector(8, numElems, 8)
-def KernelCreateInfosEnd(builder): return builder.EndObject()
+def SparseTensorStart(builder): builder.StartObject(3)
+def SparseTensorAddValues(builder, values): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(values), 0)
+def SparseTensorAddIndices(builder, indices): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(indices), 0)
+def SparseTensorAddDims(builder, dims): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(dims), 0)
+def SparseTensorStartDimsVector(builder, numElems): return builder.StartVector(8, numElems, 8)
+def SparseTensorEnd(builder): return builder.EndObject()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py`

 * *Files 19% similar despite different names*

```diff
@@ -2,47 +2,39 @@
 
 # namespace: fbs
 
 import flatbuffers
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
-class MapType(object):
+class SequenceType(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsMapType(cls, buf, offset):
+    def GetRootAsSequenceType(cls, buf, offset):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
-        x = MapType()
+        x = SequenceType()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
-    def MapTypeBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
+    def SequenceTypeBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
-    # MapType
+    # SequenceType
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
-    # MapType
-    def KeyType(self):
+    # SequenceType
+    def ElemType(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
-            return self._tab.Get(flatbuffers.number_types.Int32Flags, o + self._tab.Pos)
-        return 0
-
-    # MapType
-    def ValueType(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
-        if o != 0:
             x = self._tab.Indirect(o + self._tab.Pos)
             from ort_flatbuffers_py.fbs.TypeInfo import TypeInfo
             obj = TypeInfo()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-def MapTypeStart(builder): builder.StartObject(2)
-def MapTypeAddKeyType(builder, keyType): builder.PrependInt32Slot(0, keyType, 0)
-def MapTypeAddValueType(builder, valueType): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(valueType), 0)
-def MapTypeEnd(builder): return builder.EndObject()
+def SequenceTypeStart(builder): builder.StartObject(1)
+def SequenceTypeAddElemType(builder, elemType): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(elemType), 0)
+def SequenceTypeEnd(builder): return builder.EndObject()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeIndexAndKernelDefHash.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedNodeIndexAndKernelDefHash.py`

 * *Files 16% similar despite different names*

```diff
@@ -2,43 +2,44 @@
 
 # namespace: fbs
 
 import flatbuffers
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
-class NodeIndexAndKernelDefHash(object):
+# deprecated: no longer using kernel def hashes
+class DeprecatedNodeIndexAndKernelDefHash(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsNodeIndexAndKernelDefHash(cls, buf, offset):
+    def GetRootAsDeprecatedNodeIndexAndKernelDefHash(cls, buf, offset):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
-        x = NodeIndexAndKernelDefHash()
+        x = DeprecatedNodeIndexAndKernelDefHash()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
-    def NodeIndexAndKernelDefHashBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
+    def DeprecatedNodeIndexAndKernelDefHashBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
-    # NodeIndexAndKernelDefHash
+    # DeprecatedNodeIndexAndKernelDefHash
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
-    # NodeIndexAndKernelDefHash
+    # DeprecatedNodeIndexAndKernelDefHash
     def NodeIndex(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
             return self._tab.Get(flatbuffers.number_types.Uint32Flags, o + self._tab.Pos)
         return 0
 
-    # NodeIndexAndKernelDefHash
+    # DeprecatedNodeIndexAndKernelDefHash
     def KernelDefHash(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
             return self._tab.Get(flatbuffers.number_types.Uint64Flags, o + self._tab.Pos)
         return 0
 
-def NodeIndexAndKernelDefHashStart(builder): builder.StartObject(2)
-def NodeIndexAndKernelDefHashAddNodeIndex(builder, nodeIndex): builder.PrependUint32Slot(0, nodeIndex, 0)
-def NodeIndexAndKernelDefHashAddKernelDefHash(builder, kernelDefHash): builder.PrependUint64Slot(1, kernelDefHash, 0)
-def NodeIndexAndKernelDefHashEnd(builder): return builder.EndObject()
+def DeprecatedNodeIndexAndKernelDefHashStart(builder): builder.StartObject(2)
+def DeprecatedNodeIndexAndKernelDefHashAddNodeIndex(builder, nodeIndex): builder.PrependUint32Slot(0, nodeIndex, 0)
+def DeprecatedNodeIndexAndKernelDefHashAddKernelDefHash(builder, kernelDefHash): builder.PrependUint64Slot(1, kernelDefHash, 0)
+def DeprecatedNodeIndexAndKernelDefHashEnd(builder): return builder.EndObject()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py`

 * *Files 18% similar despite different names*

```diff
@@ -41,37 +41,32 @@
             from ort_flatbuffers_py.fbs.NodesToOptimizeIndices import NodesToOptimizeIndices
             obj = NodesToOptimizeIndices()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
     # RuntimeOptimizationRecord
-    def ProducedNodes(self, j):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
+    def ProducedOpIds(self, j):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
         if o != 0:
-            x = self._tab.Vector(o)
-            x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4
-            x = self._tab.Indirect(x)
-            from ort_flatbuffers_py.fbs.NodeIndexAndKernelDefHash import NodeIndexAndKernelDefHash
-            obj = NodeIndexAndKernelDefHash()
-            obj.Init(self._tab.Bytes, x)
-            return obj
-        return None
+            a = self._tab.Vector(o)
+            return self._tab.String(a + flatbuffers.number_types.UOffsetTFlags.py_type(j * 4))
+        return ""
 
     # RuntimeOptimizationRecord
-    def ProducedNodesLength(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
+    def ProducedOpIdsLength(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
         if o != 0:
             return self._tab.VectorLen(o)
         return 0
 
     # RuntimeOptimizationRecord
-    def ProducedNodesIsNone(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
+    def ProducedOpIdsIsNone(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
         return o == 0
 
-def RuntimeOptimizationRecordStart(builder): builder.StartObject(3)
+def RuntimeOptimizationRecordStart(builder): builder.StartObject(4)
 def RuntimeOptimizationRecordAddActionId(builder, actionId): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(actionId), 0)
 def RuntimeOptimizationRecordAddNodesToOptimizeIndices(builder, nodesToOptimizeIndices): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(nodesToOptimizeIndices), 0)
-def RuntimeOptimizationRecordAddProducedNodes(builder, producedNodes): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(producedNodes), 0)
-def RuntimeOptimizationRecordStartProducedNodesVector(builder, numElems): return builder.StartVector(4, numElems, 4)
+def RuntimeOptimizationRecordAddProducedOpIds(builder, producedOpIds): builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(producedOpIds), 0)
+def RuntimeOptimizationRecordStartProducedOpIdsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
 def RuntimeOptimizationRecordEnd(builder): return builder.EndObject()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py`

 * *Files 24% similar despite different names*

```diff
@@ -2,39 +2,55 @@
 
 # namespace: fbs
 
 import flatbuffers
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
-class SequenceType(object):
+class ValueInfo(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsSequenceType(cls, buf, offset):
+    def GetRootAsValueInfo(cls, buf, offset):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
-        x = SequenceType()
+        x = ValueInfo()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
-    def SequenceTypeBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
+    def ValueInfoBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
-    # SequenceType
+    # ValueInfo
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
-    # SequenceType
-    def ElemType(self):
+    # ValueInfo
+    def Name(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
+            return self._tab.String(o + self._tab.Pos)
+        return None
+
+    # ValueInfo
+    def DocString(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
+        if o != 0:
+            return self._tab.String(o + self._tab.Pos)
+        return None
+
+    # ValueInfo
+    def Type(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
+        if o != 0:
             x = self._tab.Indirect(o + self._tab.Pos)
             from ort_flatbuffers_py.fbs.TypeInfo import TypeInfo
             obj = TypeInfo()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-def SequenceTypeStart(builder): builder.StartObject(1)
-def SequenceTypeAddElemType(builder, elemType): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(elemType), 0)
-def SequenceTypeEnd(builder): return builder.EndObject()
+def ValueInfoStart(builder): builder.StartObject(3)
+def ValueInfoAddName(builder, name): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
+def ValueInfoAddDocString(builder, docString): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(docString), 0)
+def ValueInfoAddType(builder, type): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(type), 0)
+def ValueInfoEnd(builder): return builder.EndObject()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SessionState.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSubGraphSessionState.py`

 * *Files 14% similar despite different names*

```diff
@@ -2,66 +2,48 @@
 
 # namespace: fbs
 
 import flatbuffers
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
-class SessionState(object):
+# deprecated: no longer using kernel def hashes
+class DeprecatedSubGraphSessionState(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsSessionState(cls, buf, offset):
+    def GetRootAsDeprecatedSubGraphSessionState(cls, buf, offset):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
-        x = SessionState()
+        x = DeprecatedSubGraphSessionState()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
-    def SessionStateBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
+    def DeprecatedSubGraphSessionStateBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
-    # SessionState
+    # DeprecatedSubGraphSessionState
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
-    # SessionState
-    def Kernels(self):
+    # DeprecatedSubGraphSessionState
+    def GraphId(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
-            x = self._tab.Indirect(o + self._tab.Pos)
-            from ort_flatbuffers_py.fbs.KernelCreateInfos import KernelCreateInfos
-            obj = KernelCreateInfos()
-            obj.Init(self._tab.Bytes, x)
-            return obj
+            return self._tab.String(o + self._tab.Pos)
         return None
 
-    # SessionState
-    def SubGraphSessionStates(self, j):
+    # DeprecatedSubGraphSessionState
+    def SessionState(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
-            x = self._tab.Vector(o)
-            x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4
-            x = self._tab.Indirect(x)
-            from ort_flatbuffers_py.fbs.SubGraphSessionState import SubGraphSessionState
-            obj = SubGraphSessionState()
+            x = self._tab.Indirect(o + self._tab.Pos)
+            from ort_flatbuffers_py.fbs.DeprecatedSessionState import DeprecatedSessionState
+            obj = DeprecatedSessionState()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-    # SessionState
-    def SubGraphSessionStatesLength(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
-        if o != 0:
-            return self._tab.VectorLen(o)
-        return 0
-
-    # SessionState
-    def SubGraphSessionStatesIsNone(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
-        return o == 0
-
-def SessionStateStart(builder): builder.StartObject(2)
-def SessionStateAddKernels(builder, kernels): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(kernels), 0)
-def SessionStateAddSubGraphSessionStates(builder, subGraphSessionStates): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(subGraphSessionStates), 0)
-def SessionStateStartSubGraphSessionStatesVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def SessionStateEnd(builder): return builder.EndObject()
+def DeprecatedSubGraphSessionStateStart(builder): builder.StartObject(2)
+def DeprecatedSubGraphSessionStateAddGraphId(builder, graphId): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(graphId), 0)
+def DeprecatedSubGraphSessionStateAddSessionState(builder, sessionState): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(sessionState), 0)
+def DeprecatedSubGraphSessionStateEnd(builder): return builder.EndObject()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OpIdKernelTypeStrArgsEntry.py`

 * *Files 20% similar despite different names*

```diff
@@ -2,80 +2,62 @@
 
 # namespace: fbs
 
 import flatbuffers
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
-class SparseTensor(object):
+class OpIdKernelTypeStrArgsEntry(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsSparseTensor(cls, buf, offset):
+    def GetRootAsOpIdKernelTypeStrArgsEntry(cls, buf, offset):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
-        x = SparseTensor()
+        x = OpIdKernelTypeStrArgsEntry()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
-    def SparseTensorBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
+    def OpIdKernelTypeStrArgsEntryBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
-    # SparseTensor
+    # OpIdKernelTypeStrArgsEntry
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
-    # SparseTensor
-    def Values(self):
+    # OpIdKernelTypeStrArgsEntry
+    def OpId(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
-            x = self._tab.Indirect(o + self._tab.Pos)
-            from ort_flatbuffers_py.fbs.Tensor import Tensor
-            obj = Tensor()
-            obj.Init(self._tab.Bytes, x)
-            return obj
+            return self._tab.String(o + self._tab.Pos)
         return None
 
-    # SparseTensor
-    def Indices(self):
+    # OpIdKernelTypeStrArgsEntry
+    def KernelTypeStrArgs(self, j):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
-            x = self._tab.Indirect(o + self._tab.Pos)
-            from ort_flatbuffers_py.fbs.Tensor import Tensor
-            obj = Tensor()
+            x = self._tab.Vector(o)
+            x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4
+            x = self._tab.Indirect(x)
+            from ort_flatbuffers_py.fbs.KernelTypeStrArgsEntry import KernelTypeStrArgsEntry
+            obj = KernelTypeStrArgsEntry()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-    # SparseTensor
-    def Dims(self, j):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
-        if o != 0:
-            a = self._tab.Vector(o)
-            return self._tab.Get(flatbuffers.number_types.Int64Flags, a + flatbuffers.number_types.UOffsetTFlags.py_type(j * 8))
-        return 0
-
-    # SparseTensor
-    def DimsAsNumpy(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
-        if o != 0:
-            return self._tab.GetVectorAsNumpy(flatbuffers.number_types.Int64Flags, o)
-        return 0
-
-    # SparseTensor
-    def DimsLength(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
+    # OpIdKernelTypeStrArgsEntry
+    def KernelTypeStrArgsLength(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
             return self._tab.VectorLen(o)
         return 0
 
-    # SparseTensor
-    def DimsIsNone(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
+    # OpIdKernelTypeStrArgsEntry
+    def KernelTypeStrArgsIsNone(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         return o == 0
 
-def SparseTensorStart(builder): builder.StartObject(3)
-def SparseTensorAddValues(builder, values): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(values), 0)
-def SparseTensorAddIndices(builder, indices): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(indices), 0)
-def SparseTensorAddDims(builder, dims): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(dims), 0)
-def SparseTensorStartDimsVector(builder, numElems): return builder.StartVector(8, numElems, 8)
-def SparseTensorEnd(builder): return builder.EndObject()
+def OpIdKernelTypeStrArgsEntryStart(builder): builder.StartObject(2)
+def OpIdKernelTypeStrArgsEntryAddOpId(builder, opId): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(opId), 0)
+def OpIdKernelTypeStrArgsEntryAddKernelTypeStrArgs(builder, kernelTypeStrArgs): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(kernelTypeStrArgs), 0)
+def OpIdKernelTypeStrArgsEntryStartKernelTypeStrArgsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
+def OpIdKernelTypeStrArgsEntryEnd(builder): return builder.EndObject()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SubGraphSessionState.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSessionState.py`

 * *Files 17% similar despite different names*

```diff
@@ -2,47 +2,67 @@
 
 # namespace: fbs
 
 import flatbuffers
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
-class SubGraphSessionState(object):
+# deprecated: no longer using kernel def hashes
+class DeprecatedSessionState(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsSubGraphSessionState(cls, buf, offset):
+    def GetRootAsDeprecatedSessionState(cls, buf, offset):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
-        x = SubGraphSessionState()
+        x = DeprecatedSessionState()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
-    def SubGraphSessionStateBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
+    def DeprecatedSessionStateBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
-    # SubGraphSessionState
+    # DeprecatedSessionState
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
-    # SubGraphSessionState
-    def GraphId(self):
+    # DeprecatedSessionState
+    def Kernels(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
-            return self._tab.String(o + self._tab.Pos)
+            x = self._tab.Indirect(o + self._tab.Pos)
+            from ort_flatbuffers_py.fbs.DeprecatedKernelCreateInfos import DeprecatedKernelCreateInfos
+            obj = DeprecatedKernelCreateInfos()
+            obj.Init(self._tab.Bytes, x)
+            return obj
         return None
 
-    # SubGraphSessionState
-    def SessionState(self):
+    # DeprecatedSessionState
+    def SubGraphSessionStates(self, j):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
-            x = self._tab.Indirect(o + self._tab.Pos)
-            from ort_flatbuffers_py.fbs.SessionState import SessionState
-            obj = SessionState()
+            x = self._tab.Vector(o)
+            x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4
+            x = self._tab.Indirect(x)
+            from ort_flatbuffers_py.fbs.DeprecatedSubGraphSessionState import DeprecatedSubGraphSessionState
+            obj = DeprecatedSubGraphSessionState()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-def SubGraphSessionStateStart(builder): builder.StartObject(2)
-def SubGraphSessionStateAddGraphId(builder, graphId): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(graphId), 0)
-def SubGraphSessionStateAddSessionState(builder, sessionState): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(sessionState), 0)
-def SubGraphSessionStateEnd(builder): return builder.EndObject()
+    # DeprecatedSessionState
+    def SubGraphSessionStatesLength(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
+        if o != 0:
+            return self._tab.VectorLen(o)
+        return 0
+
+    # DeprecatedSessionState
+    def SubGraphSessionStatesIsNone(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
+        return o == 0
+
+def DeprecatedSessionStateStart(builder): builder.StartObject(2)
+def DeprecatedSessionStateAddKernels(builder, kernels): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(kernels), 0)
+def DeprecatedSessionStateAddSubGraphSessionStates(builder, subGraphSessionStates): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(subGraphSessionStates), 0)
+def DeprecatedSessionStateStartSubGraphSessionStatesVector(builder, numElems): return builder.StartVector(4, numElems, 4)
+def DeprecatedSessionStateEnd(builder): return builder.EndObject()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py`

 * *Files identical despite different names*

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py` & `onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrArgsEntry.py`

 * *Files 18% similar despite different names*

```diff
@@ -2,54 +2,62 @@
 
 # namespace: fbs
 
 import flatbuffers
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
-class TypeInfo(object):
+class KernelTypeStrArgsEntry(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsTypeInfo(cls, buf, offset):
+    def GetRootAsKernelTypeStrArgsEntry(cls, buf, offset):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
-        x = TypeInfo()
+        x = KernelTypeStrArgsEntry()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
-    def TypeInfoBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
+    def KernelTypeStrArgsEntryBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
-    # TypeInfo
+    # KernelTypeStrArgsEntry
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
-    # TypeInfo
-    def Denotation(self):
+    # KernelTypeStrArgsEntry
+    def KernelTypeStr(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
             return self._tab.String(o + self._tab.Pos)
         return None
 
-    # TypeInfo
-    def ValueType(self):
+    # KernelTypeStrArgsEntry
+    def Args(self, j):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
-            return self._tab.Get(flatbuffers.number_types.Uint8Flags, o + self._tab.Pos)
-        return 0
-
-    # TypeInfo
-    def Value(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
-        if o != 0:
-            from flatbuffers.table import Table
-            obj = Table(bytearray(), 0)
-            self._tab.Union(obj, o)
+            x = self._tab.Vector(o)
+            x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4
+            x = self._tab.Indirect(x)
+            from ort_flatbuffers_py.fbs.ArgTypeAndIndex import ArgTypeAndIndex
+            obj = ArgTypeAndIndex()
+            obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-def TypeInfoStart(builder): builder.StartObject(3)
-def TypeInfoAddDenotation(builder, denotation): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(denotation), 0)
-def TypeInfoAddValueType(builder, valueType): builder.PrependUint8Slot(1, valueType, 0)
-def TypeInfoAddValue(builder, value): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(value), 0)
-def TypeInfoEnd(builder): return builder.EndObject()
+    # KernelTypeStrArgsEntry
+    def ArgsLength(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
+        if o != 0:
+            return self._tab.VectorLen(o)
+        return 0
+
+    # KernelTypeStrArgsEntry
+    def ArgsIsNone(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
+        return o == 0
+
+def KernelTypeStrArgsEntryStart(builder): builder.StartObject(2)
+def KernelTypeStrArgsEntryAddKernelTypeStr(builder, kernelTypeStr): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(kernelTypeStr), 0)
+def KernelTypeStrArgsEntryAddArgs(builder, args): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(args), 0)
+def KernelTypeStrArgsEntryStartArgsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
+def KernelTypeStrArgsEntryEnd(builder): return builder.EndObject()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py` & `onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,56 +1,78 @@
 # automatically generated by the FlatBuffers compiler, do not modify
 
-# namespace: fbs
+# namespace: CalTableFlatBuffers
 
 import flatbuffers
 from flatbuffers.compat import import_numpy
+
 np = import_numpy()
 
-class ValueInfo(object):
-    __slots__ = ['_tab']
+
+class KeyValue:
+    __slots__ = ["_tab"]
 
     @classmethod
-    def GetRootAsValueInfo(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):  # noqa: N802
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
-        x = ValueInfo()
+        x = KeyValue()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
-    def ValueInfoBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
-        return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
+    def GetRootAsKeyValue(cls, buf, offset=0):  # noqa: N802
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
 
-    # ValueInfo
-    def Init(self, buf, pos):
+    # KeyValue
+    def Init(self, buf, pos):  # noqa: N802
         self._tab = flatbuffers.table.Table(buf, pos)
 
-    # ValueInfo
-    def Name(self):
+    # KeyValue
+    def Key(self):  # noqa: N802
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
             return self._tab.String(o + self._tab.Pos)
         return None
 
-    # ValueInfo
-    def DocString(self):
+    # KeyValue
+    def Value(self):  # noqa: N802
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
             return self._tab.String(o + self._tab.Pos)
         return None
 
-    # ValueInfo
-    def Type(self):
-        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
-        if o != 0:
-            x = self._tab.Indirect(o + self._tab.Pos)
-            from ort_flatbuffers_py.fbs.TypeInfo import TypeInfo
-            obj = TypeInfo()
-            obj.Init(self._tab.Bytes, x)
-            return obj
-        return None
 
-def ValueInfoStart(builder): builder.StartObject(3)
-def ValueInfoAddName(builder, name): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
-def ValueInfoAddDocString(builder, docString): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(docString), 0)
-def ValueInfoAddType(builder, type): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(type), 0)
-def ValueInfoEnd(builder): return builder.EndObject()
+def Start(builder):  # noqa: N802
+    builder.StartObject(2)
+
+
+def KeyValueStart(builder):  # noqa: N802
+    """This method is deprecated. Please switch to Start."""
+    return Start(builder)
+
+
+def AddKey(builder, key):  # noqa: N802
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(key), 0)
+
+
+def KeyValueAddKey(builder, key):  # noqa: N802
+    """This method is deprecated. Please switch to AddKey."""
+    return AddKey(builder, key)
+
+
+def AddValue(builder, value):  # noqa: N802
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(value), 0)
+
+
+def KeyValueAddValue(builder, value):  # noqa: N802
+    """This method is deprecated. Please switch to AddValue."""
+    return AddValue(builder, value)
+
+
+def End(builder):  # noqa: N802
+    return builder.EndObject()
+
+
+def KeyValueEnd(builder):  # noqa: N802
+    """This method is deprecated. Please switch to End."""
+    return End(builder)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/affinity_helper.py` & `onnxruntime/transformers/affinity_helper.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,36 +1,40 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 # Get/Set cpu affinity. Currently only support part of Unix system
 import logging
 import os
 
 logger = logging.getLogger(__name__)
 
 
-class AffinitySetting():
+class AffinitySetting:
     def __init__(self):
         self.pid = os.getpid()
         self.affinity = None
-        self.is_os_supported = hasattr(os, 'sched_getaffinity') and hasattr(os, 'sched_setaffinity')
+        self.is_os_supported = hasattr(os, "sched_getaffinity") and hasattr(os, "sched_setaffinity")
         if not self.is_os_supported:
             logger.warning("Current OS does not support os.get_affinity() and os.set_affinity()")
 
     def get_affinity(self):
         if self.is_os_supported:
             self.affinity = os.sched_getaffinity(self.pid)
 
     def set_affinity(self):
         if self.is_os_supported:
             current_affinity = os.sched_getaffinity(self.pid)
-            if (self.affinity != current_affinity):
-                logger.warning("Replacing affinity setting %s with %s", str(current_affinity), str(self.affinity))
+            if self.affinity != current_affinity:
+                logger.warning(
+                    "Replacing affinity setting %s with %s",
+                    str(current_affinity),
+                    str(self.affinity),
+                )
                 os.sched_setaffinity(self.pid, self.affinity)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     affi_helper = AffinitySetting()
     affi_helper.get_affinity()
     affi_helper.set_affinity()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/benchmark.py` & `onnxruntime/transformers/benchmark.py`

 * *Files 6% similar despite different names*

```diff
@@ -38,122 +38,217 @@
             python benchmark.py -g -m bert-base-cased --provider rocm --optimizer_info by_script --disable_embed_layer_norm
 
     It is recommended to use run_benchmark.sh to launch benchmark.
 """
 
 import argparse
 import logging
+import os
 import timeit
 from datetime import datetime
-import numpy
+from enum import Enum  # noqa: F401
 
-import os
+import numpy
+import onnx  # noqa: F401
 import psutil
-import onnx
-from enum import Enum
-from benchmark_helper import (OptimizerInfo, create_onnxruntime_session, Precision, setup_logger, get_latency_result,
-                              output_details, output_summary, output_fusion_statistics, inference_ort,
-                              inference_ort_with_io_binding, allocateOutputBuffers, ConfigModifier)
+from benchmark_helper import allocateOutputBuffers  # noqa: F401
+from benchmark_helper import (
+    ConfigModifier,
+    OptimizerInfo,
+    Precision,
+    create_onnxruntime_session,
+    get_latency_result,
+    inference_ort,
+    inference_ort_with_io_binding,
+    output_details,
+    output_fusion_statistics,
+    output_summary,
+    setup_logger,
+)
 from fusion_options import FusionOptions
+from onnx_exporter import (
+    create_onnxruntime_input,
+    export_onnx_model_from_pt,
+    export_onnx_model_from_tf,
+    load_pretrained_model,
+)
+from packaging import version
 from quantize_helper import QuantizeHelper
-from onnx_exporter import create_onnxruntime_input, load_pretrained_model, export_onnx_model_from_pt, export_onnx_model_from_tf
 
-logger = logging.getLogger('')
+logger = logging.getLogger("")
 
-from huggingface_models import MODELS, MODEL_CLASSES
+from huggingface_models import MODEL_CLASSES, MODELS  # noqa: E402
 
 cpu_count = psutil.cpu_count(logical=False)
 
 # Set OMP environment variable before importing onnxruntime or torch.
 if "OMP_NUM_THREADS" not in os.environ:
     os.environ["OMP_NUM_THREADS"] = str(cpu_count)
 
-import torch
-from transformers import (AutoConfig, AutoTokenizer, AutoModel, GPT2Model, LxmertConfig)
+import torch  # noqa: E402
+from transformers import AutoConfig, AutoModel, AutoTokenizer, GPT2Model, LxmertConfig  # noqa: E402, F401
 
 
-def run_onnxruntime(use_gpu, provider, model_names, model_class, config_modifier, precision, num_threads, batch_sizes,
-                    sequence_lengths, repeat_times, input_counts, optimizer_info, validate_onnx, cache_dir, onnx_dir,
-                    verbose, overwrite, disable_ort_io_binding, use_raw_attention_mask, model_fusion_statistics,
-                    model_source, args):
+def run_onnxruntime(
+    use_gpu,
+    provider,
+    model_names,
+    model_class,
+    config_modifier,
+    precision,
+    num_threads,
+    batch_sizes,
+    sequence_lengths,
+    repeat_times,
+    input_counts,
+    optimizer_info,
+    validate_onnx,
+    cache_dir,
+    onnx_dir,
+    verbose,
+    overwrite,
+    disable_ort_io_binding,
+    use_raw_attention_mask,
+    model_fusion_statistics,
+    model_source,
+    args,
+):
     import onnxruntime
 
     results = []
-    if (use_gpu and ('CUDAExecutionProvider' not in onnxruntime.get_available_providers())
-            and ('ROCMExecutionProvider' not in onnxruntime.get_available_providers())):
+    if (
+        use_gpu
+        and ("CUDAExecutionProvider" not in onnxruntime.get_available_providers())
+        and ("ROCMExecutionProvider" not in onnxruntime.get_available_providers())
+        and ("DmlExecutionProvider" not in onnxruntime.get_available_providers())
+    ):
         logger.error(
-            "Please install onnxruntime-gpu package instead of onnxruntime, and use a machine with GPU for testing gpu performance."
+            "Please install onnxruntime-gpu or onnxruntime-directml package instead of onnxruntime, and use a machine with GPU for testing gpu performance."
         )
         return results
 
     warm_up_repeat = 0
-    if provider == 'tensorrt':
+    if provider == "tensorrt":
         optimizer_info = OptimizerInfo.NOOPT
         warm_up_repeat = 5
-        if 'TensorrtExecutionProvider' not in onnxruntime.get_available_providers():
+        if "TensorrtExecutionProvider" not in onnxruntime.get_available_providers():
             logger.error(
                 "Please install onnxruntime-gpu-tensorrt package, and use a machine with GPU for testing gpu performance."
             )
             return results
 
     if optimizer_info == OptimizerInfo.NOOPT:
-        logger.warning(f"OptimizerInfo is set to {optimizer_info}, graph optimizations specified in FusionOptions are not applied.")
+        logger.warning(
+            f"OptimizerInfo is set to {optimizer_info}, graph optimizations specified in FusionOptions are not applied."
+        )
 
     for model_name in model_names:
         all_input_names = MODELS[model_name][0]
         for num_inputs in input_counts:
             if num_inputs > len(all_input_names):
                 break
 
             input_names = all_input_names[:num_inputs]
             args.model_type = MODELS[model_name][3]
             fusion_options = FusionOptions.parse(args)
 
-            if 'pt' in model_source:
+            if "pt" in model_source:
                 with torch.no_grad():
-                    onnx_model_file, is_valid_onnx_model, vocab_size, max_sequence_length = export_onnx_model_from_pt(
-                        model_name, MODELS[model_name][1], MODELS[model_name][2], MODELS[model_name][3], model_class,
-                        config_modifier, cache_dir, onnx_dir, input_names, use_gpu, precision, optimizer_info,
-                        validate_onnx, use_raw_attention_mask, overwrite, model_fusion_statistics, fusion_options)
-            if 'tf' in model_source:
-                onnx_model_file, is_valid_onnx_model, vocab_size, max_sequence_length = export_onnx_model_from_tf(
-                    model_name, MODELS[model_name][1], MODELS[model_name][2], MODELS[model_name][3], model_class,
-                    config_modifier, cache_dir, onnx_dir, input_names, use_gpu, precision, optimizer_info,
-                    validate_onnx, use_raw_attention_mask, overwrite, model_fusion_statistics, fusion_options)
+                    (
+                        onnx_model_file,
+                        is_valid_onnx_model,
+                        vocab_size,
+                        max_sequence_length,
+                    ) = export_onnx_model_from_pt(
+                        model_name,
+                        MODELS[model_name][1],
+                        MODELS[model_name][2],
+                        MODELS[model_name][3],
+                        model_class,
+                        config_modifier,
+                        cache_dir,
+                        onnx_dir,
+                        input_names,
+                        use_gpu,
+                        precision,
+                        optimizer_info,
+                        validate_onnx,
+                        use_raw_attention_mask,
+                        overwrite,
+                        model_fusion_statistics,
+                        fusion_options,
+                    )
+            if "tf" in model_source:
+                (
+                    onnx_model_file,
+                    is_valid_onnx_model,
+                    vocab_size,
+                    max_sequence_length,
+                ) = export_onnx_model_from_tf(
+                    model_name,
+                    MODELS[model_name][1],
+                    MODELS[model_name][2],
+                    MODELS[model_name][3],
+                    model_class,
+                    config_modifier,
+                    cache_dir,
+                    onnx_dir,
+                    input_names,
+                    use_gpu,
+                    precision,
+                    optimizer_info,
+                    validate_onnx,
+                    use_raw_attention_mask,
+                    overwrite,
+                    model_fusion_statistics,
+                    fusion_options,
+                )
 
             if not is_valid_onnx_model:
                 continue
 
-            ort_session = create_onnxruntime_session(onnx_model_file,
-                                                     use_gpu,
-                                                     provider,
-                                                     enable_all_optimization=True,
-                                                     num_threads=num_threads,
-                                                     verbose=verbose)
+            ort_session = create_onnxruntime_session(
+                onnx_model_file,
+                use_gpu,
+                provider,
+                enable_all_optimization=True,
+                num_threads=num_threads,
+                verbose=verbose,
+            )
             if ort_session is None:
                 continue
 
             ort_output_names = [node_arg.name for node_arg in ort_session.get_outputs()]
             output_buffers = []
             device = "cuda" if use_gpu else "cpu"
             config = AutoConfig.from_pretrained(model_name, cache_dir=cache_dir)
             max_last_state_size = numpy.prod(
-                [max(batch_sizes), max(sequence_lengths),
-                 max(vocab_size, config.hidden_size)])
+                [
+                    max(batch_sizes),
+                    max(sequence_lengths),
+                    max(vocab_size, config.hidden_size),
+                ]
+            )
             max_pooler_size = numpy.prod([max(batch_sizes), config.hidden_size])
             for batch_size in batch_sizes:
                 if batch_size <= 0:
                     continue
                 for sequence_length in sequence_lengths:
                     if max_sequence_length is not None and sequence_length > max_sequence_length:
                         continue
 
-                    input_value_type = numpy.int64 if 'pt' in model_source else numpy.int32
-                    ort_inputs = create_onnxruntime_input(vocab_size, batch_size, sequence_length, input_names, config,
-                                                          input_value_type)
+                    input_value_type = numpy.int64 if "pt" in model_source else numpy.int32
+                    ort_inputs = create_onnxruntime_input(
+                        vocab_size,
+                        batch_size,
+                        sequence_length,
+                        input_names,
+                        config,
+                        input_value_type,
+                    )
                     result_template = {
                         "engine": "onnxruntime",
                         "version": onnxruntime.__version__,
                         "providers": provider,
                         "device": device,
                         "optimizer": optimizer_info,
                         "precision": precision,
@@ -163,59 +258,103 @@
                         "threads": num_threads,
                         "batch_size": batch_size,
                         "sequence_length": sequence_length,
                         "custom_layer_num": config_modifier.get_layer_num(),
                         "datetime": str(datetime.now()),
                     }
 
-                    logger.info("Run onnxruntime on {} with input shape {}".format(model_name,
-                                                                                   [batch_size, sequence_length]))
+                    if config.model_type in ["vit", "swin"]:
+                        logger.info(
+                            f"Run onnxruntime on {model_name} with input shape {[batch_size, 3, config.image_size, config.image_size]}"
+                        )
+                    else:
+                        logger.info(f"Run onnxruntime on {model_name} with input shape {[batch_size, sequence_length]}")
 
                     if disable_ort_io_binding:
-                        result = inference_ort(ort_session, ort_inputs, result_template, repeat_times, batch_size,
-                                               warm_up_repeat)
+                        result = inference_ort(
+                            ort_session,
+                            ort_inputs,
+                            result_template,
+                            repeat_times,
+                            batch_size,
+                            warm_up_repeat,
+                        )
                     else:
                         # Get output sizes from a dummy ort run
                         ort_outputs = ort_session.run(ort_output_names, ort_inputs)
                         output_buffer_max_sizes = [max_last_state_size]
                         for i in range(len(ort_outputs)):
                             if i == 2 and MODELS[model_name][3] == "gpt":
                                 # past state output max size
                                 output_buffer_max_sizes.append(max_pooler_size)
                             else:
                                 output_buffer_max_sizes.append(max_last_state_size)
 
-                        data_type = numpy.longlong if 'pt' in model_source else numpy.intc
-                        result = inference_ort_with_io_binding(ort_session, ort_inputs, result_template, repeat_times,
-                                                               ort_output_names, ort_outputs, output_buffers,
-                                                               output_buffer_max_sizes, batch_size, device, data_type,
-                                                               warm_up_repeat)
+                        data_type = numpy.longlong if "pt" in model_source else numpy.intc
+                        result = inference_ort_with_io_binding(
+                            ort_session,
+                            ort_inputs,
+                            result_template,
+                            repeat_times,
+                            ort_output_names,
+                            ort_outputs,
+                            output_buffers,
+                            output_buffer_max_sizes,
+                            batch_size,
+                            device,
+                            data_type,
+                            warm_up_repeat,
+                        )
                     logger.info(result)
                     results.append(result)
 
     return results
 
 
-def run_pytorch(use_gpu, model_names, model_class, config_modifier, precision, num_threads, batch_sizes,
-                sequence_lengths, repeat_times, torchscript, cache_dir, verbose):
+def run_pytorch(
+    use_gpu,
+    model_names,
+    model_class,
+    config_modifier,
+    precision,
+    num_threads,
+    batch_sizes,
+    sequence_lengths,
+    repeat_times,
+    torchscript,
+    torch2,
+    cache_dir,
+    verbose,
+):
     results = []
     if use_gpu and not torch.cuda.is_available():
         logger.error("Please install PyTorch with Cuda, and use a machine with GPU for testing gpu performance.")
         return results
 
     torch.set_grad_enabled(False)
 
     for model_name in model_names:
         config = AutoConfig.from_pretrained(model_name, torchscript=torchscript, cache_dir=cache_dir)
         config_modifier.modify(config)
-        model = load_pretrained_model(model_name, config=config, cache_dir=cache_dir, custom_model_class=model_class)
-        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
+        model = load_pretrained_model(
+            model_name,
+            config=config,
+            cache_dir=cache_dir,
+            custom_model_class=model_class,
+        )
 
-        max_input_size = tokenizer.max_model_input_sizes[
-            model_name] if model_name in tokenizer.max_model_input_sizes else 1024
+        if config.model_type in ["vit", "swin"]:
+            # These models don't use sequence lengths, so just pick the first sequence length so that the summary still works
+            sequence_lengths = [sequence_lengths[0]]
+        else:
+            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
+
+            max_input_size = (
+                tokenizer.max_model_input_sizes[model_name] if model_name in tokenizer.max_model_input_sizes else 1024
+            )
 
         logger.debug(f"Model {model}")
         logger.debug(f"Number of parameters {model.num_parameters()}")
 
         if precision == Precision.FLOAT16:
             model.half()
 
@@ -226,31 +365,45 @@
             model = QuantizeHelper.quantize_torch_model(model)
 
         for batch_size in batch_sizes:
             if batch_size <= 0:
                 continue
 
             for sequence_length in sequence_lengths:
-                if max_input_size is not None and sequence_length > max_input_size:
-                    continue
+                if config.model_type in ["vit", "swin"]:
+                    logger.info(
+                        f"Run PyTorch on {model_name} with input shape {[batch_size, 3, config.image_size, config.image_size]}"
+                    )
+                    input_ids = torch.randn(
+                        size=(batch_size, 3, config.image_size, config.image_size),
+                        dtype=torch.float16 if precision == Precision.FLOAT16 else torch.float32,
+                        device=device,
+                    )
+                else:
+                    if max_input_size is not None and sequence_length > max_input_size:
+                        continue
 
-                logger.info("Run PyTorch on {} with input shape {}".format(model_name, [batch_size, sequence_length]))
-                input_ids = torch.randint(low=0,
-                                          high=config.vocab_size - 1,
-                                          size=(batch_size, sequence_length),
-                                          dtype=torch.long,
-                                          device=device)
+                    logger.info(f"Run PyTorch on {model_name} with input shape {[batch_size, sequence_length]}")
+                    input_ids = torch.randint(
+                        low=0,
+                        high=config.vocab_size - 1,
+                        size=(batch_size, sequence_length),
+                        dtype=torch.long,
+                        device=device,
+                    )
                 try:
-                    inference = torch.jit.trace(model, input_ids) if torchscript else model
+                    inference = (
+                        torch.jit.trace(model, input_ids) if torchscript else torch.compile(model) if torch2 else model
+                    )
                     inference(input_ids)
 
-                    runtimes = timeit.repeat(lambda: inference(input_ids), repeat=repeat_times, number=1)
+                    runtimes = timeit.repeat(lambda: inference(input_ids), repeat=repeat_times, number=1)  # noqa: B023
 
                     result = {
-                        "engine": "torchscript" if torchscript else "torch",
+                        "engine": "torchscript" if torchscript else "torch2" if torch2 else "torch",
                         "version": torch.__version__,
                         "providers": "NA",
                         "device": "cuda" if use_gpu else "cpu",
                         "optimizer": "",
                         "precision": precision,
                         "io_binding": "",
                         "model_name": model_name,
@@ -268,17 +421,18 @@
                     logger.exception(e)
                     torch.cuda.empty_cache()
 
     return results
 
 
 def run_with_tf_optimizations(do_eager_mode: bool, use_xla: bool):
-    import tensorflow as tf
     from functools import wraps
 
+    import tensorflow as tf
+
     def run_func(func):
         @wraps(func)
         def run_in_eager_mode(*args, **kwargs):
             return func(*args, **kwargs)
 
         @wraps(func)
         @tf.function(experimental_compile=use_xla)
@@ -292,96 +446,116 @@
             return run_in_eager_mode
         else:
             return run_in_graph_mode
 
     return run_func
 
 
-def run_tensorflow(use_gpu, model_names, model_class, config_modifier, precision, num_threads, batch_sizes,
-                   sequence_lengths, repeat_times, cache_dir, verbose):
+def run_tensorflow(
+    use_gpu,
+    model_names,
+    model_class,
+    config_modifier,
+    precision,
+    num_threads,
+    batch_sizes,
+    sequence_lengths,
+    repeat_times,
+    cache_dir,
+    verbose,
+):
     results = []
 
     import tensorflow as tf
+
     tf.config.threading.set_intra_op_parallelism_threads(num_threads)
 
     if not use_gpu:
-        tf.config.set_visible_devices([], 'GPU')
+        tf.config.set_visible_devices([], "GPU")
 
     if use_gpu and not tf.test.is_built_with_cuda():
         logger.error("Please install Tensorflow-gpu, and use a machine with GPU for testing gpu performance.")
         return results
 
     if use_gpu:  # Restrict TensorFlow to only use the first GPU
-        physical_devices = tf.config.list_physical_devices('GPU')
+        physical_devices = tf.config.list_physical_devices("GPU")
         try:
-            tf.config.set_visible_devices(physical_devices[0], 'GPU')
+            tf.config.set_visible_devices(physical_devices[0], "GPU")
             tf.config.experimental.set_memory_growth(physical_devices[0], True)
-            tf.distribute.OneDeviceStrategy(device='/gpu:0')
+            tf.distribute.OneDeviceStrategy(device="/gpu:0")
         except RuntimeError as e:
             logger.exception(e)
 
     if precision == Precision.FLOAT16 or precision == Precision.INT8:
         raise NotImplementedError("Mixed precision is currently not supported.")
 
     for model_name in model_names:
         config = AutoConfig.from_pretrained(model_name, cache_dir=cache_dir)
         config_modifier.modify(config)
 
-        model = load_pretrained_model(model_name,
-                                      config=config,
-                                      cache_dir=cache_dir,
-                                      custom_model_class=model_class,
-                                      is_tf_model=True)
+        model = load_pretrained_model(
+            model_name,
+            config=config,
+            cache_dir=cache_dir,
+            custom_model_class=model_class,
+            is_tf_model=True,
+        )
 
         tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
 
-        max_input_size = tokenizer.max_model_input_sizes[
-            model_name] if model_name in tokenizer.max_model_input_sizes else 1024
+        max_input_size = (
+            tokenizer.max_model_input_sizes[model_name] if model_name in tokenizer.max_model_input_sizes else 1024
+        )
 
         for batch_size in batch_sizes:
             if batch_size <= 0:
                 continue
 
             for sequence_length in sequence_lengths:
                 if max_input_size is not None and sequence_length > max_input_size:
                     continue
 
-                logger.info("Run Tensorflow on {} with input shape {}".format(model_name,
-                                                                              [batch_size, sequence_length]))
+                logger.info(f"Run Tensorflow on {model_name} with input shape {[batch_size, sequence_length]}")
 
                 import random
+
                 rng = random.Random()
                 values = [rng.randint(0, config.vocab_size - 1) for i in range(batch_size * sequence_length)]
                 input_ids = tf.constant(values, shape=(batch_size, sequence_length), dtype=tf.int32)
 
                 try:
                     # Disable both for better inference perf
                     @run_with_tf_optimizations(do_eager_mode=False, use_xla=False)
                     def encoder_forward():
-                        return model(input_ids, training=False)
+                        return model(input_ids, training=False)  # noqa: B023
 
                     @run_with_tf_optimizations(do_eager_mode=False, use_xla=False)
                     def encoder_decoder_forward():
-                        return model(input_ids, decoder_input_ids=input_ids, training=False)
+                        return model(input_ids, decoder_input_ids=input_ids, training=False)  # noqa: B023
 
                     @run_with_tf_optimizations(do_eager_mode=False, use_xla=False)
                     def lxmert_forward():
-                        feats = tf.random.normal([1, 1, config.visual_feat_dim])
-                        pos = tf.random.normal([1, 1, config.visual_pos_dim])
-                        return model(input_ids, visual_feats=feats, visual_pos=pos, training=False)
+                        feats = tf.random.normal([1, 1, config.visual_feat_dim])  # noqa: B023
+                        pos = tf.random.normal([1, 1, config.visual_pos_dim])  # noqa: B023
+                        return model(  # noqa: B023
+                            input_ids,  # noqa: B023
+                            visual_feats=feats,
+                            visual_pos=pos,
+                            training=False,
+                        )
 
                     inference = encoder_forward
                     if config.is_encoder_decoder:
                         inference = encoder_decoder_forward
                     elif isinstance(config, LxmertConfig):
                         inference = lxmert_forward
 
                     inference()
 
-                    runtimes = timeit.repeat(lambda: inference(), repeat=repeat_times, number=1)
+                    runtimes = timeit.repeat(lambda: inference(), repeat=repeat_times, number=1)  # noqa: B023
 
                     result = {
                         "engine": "tensorflow",
                         "version": tf.__version__,
                         "providers": "NA",
                         "device": "cuda" if use_gpu else "cpu",
                         "optimizer": "",
@@ -397,139 +571,206 @@
                     }
                     result.update(get_latency_result(runtimes, batch_size))
                     logger.info(result)
                     results.append(result)
                 except RuntimeError as e:
                     logger.exception(e)
                     from numba import cuda
+
                     device = cuda.get_current_device()
                     device.reset()
 
     return results
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
 
-    parser.add_argument("-m",
-                        "--models",
-                        required=False,
-                        nargs="+",
-                        type=str,
-                        default=["bert-base-cased", "roberta-base", "gpt2"],
-                        choices=list(MODELS.keys()),
-                        help="Pre-trained models in the list: " + ", ".join(MODELS.keys()))
-
-    parser.add_argument("--model_source",
-                        required=False,
-                        nargs=1,
-                        type=str,
-                        default='pt',
-                        choices=['pt', 'tf'],
-                        help="Export onnx from pt or tf")
-
-    parser.add_argument('--model_class',
-                        required=False,
-                        type=str,
-                        default=None,
-                        choices=list(MODEL_CLASSES),
-                        help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES))
-
-    parser.add_argument("-e",
-                        "--engines",
-                        required=False,
-                        nargs="+",
-                        type=str,
-                        default=['onnxruntime'],
-                        choices=['onnxruntime', 'torch', 'torchscript', 'tensorflow'],
-                        help="Engines to benchmark")
-
-    parser.add_argument("-c",
-                        "--cache_dir",
-                        required=False,
-                        type=str,
-                        default=os.path.join('.', 'cache_models'),
-                        help="Directory to cache pre-trained models")
-
-    parser.add_argument("--onnx_dir",
-                        required=False,
-                        type=str,
-                        default=os.path.join('.', 'onnx_models'),
-                        help="Directory to store onnx models")
+    parser.add_argument(
+        "-m",
+        "--models",
+        required=False,
+        nargs="+",
+        type=str,
+        default=["bert-base-cased", "roberta-base", "gpt2"],
+        choices=list(MODELS.keys()),
+        help="Pre-trained models in the list: " + ", ".join(MODELS.keys()),
+    )
+
+    parser.add_argument(
+        "--model_source",
+        required=False,
+        nargs=1,
+        type=str,
+        default="pt",
+        choices=["pt", "tf"],
+        help="Export onnx from pt or tf",
+    )
+
+    parser.add_argument(
+        "--model_class",
+        required=False,
+        type=str,
+        default=None,
+        choices=list(MODEL_CLASSES),
+        help="Model type selected in the list: " + ", ".join(MODEL_CLASSES),
+    )
+
+    parser.add_argument(
+        "-e",
+        "--engines",
+        required=False,
+        nargs="+",
+        type=str,
+        default=["onnxruntime"],
+        choices=["onnxruntime", "torch", "torch2", "torchscript", "tensorflow"],
+        help="Engines to benchmark",
+    )
+
+    parser.add_argument(
+        "-c",
+        "--cache_dir",
+        required=False,
+        type=str,
+        default=os.path.join(".", "cache_models"),
+        help="Directory to cache pre-trained models",
+    )
+
+    parser.add_argument(
+        "--onnx_dir",
+        required=False,
+        type=str,
+        default=os.path.join(".", "onnx_models"),
+        help="Directory to store onnx models",
+    )
 
     parser.add_argument("-g", "--use_gpu", required=False, action="store_true", help="Run on gpu device")
 
-    parser.add_argument("--provider", required=False, type=str, default=None, help="Execution provider to use")
+    parser.add_argument(
+        "--provider",
+        required=False,
+        type=str,
+        default=None,
+        help="Execution provider to use",
+    )
 
     parser.add_argument(
         "-p",
         "--precision",
         type=Precision,
         default=Precision.FLOAT32,
         choices=list(Precision),
-        help="Precision of model to run. fp32 for full precision, fp16 for half precision, and int8 for quantization")
+        help="Precision of model to run. fp32 for full precision, fp16 for half precision, and int8 for quantization",
+    )
 
     parser.add_argument("--verbose", required=False, action="store_true", help="Print more information")
 
-    parser.add_argument("--overwrite", required=False, action="store_true", help="Overwrite existing models")
+    parser.add_argument(
+        "--overwrite",
+        required=False,
+        action="store_true",
+        help="Overwrite existing models",
+    )
 
     parser.add_argument(
         "-o",
         "--optimizer_info",
         type=OptimizerInfo,
         default=OptimizerInfo.BYSCRIPT,
         choices=list(OptimizerInfo),
-        help="Optimizer info: Use optimizer.py to optimize onnx model as default. Can also choose from by_ort and no_opt"
+        help="Optimizer info: Use optimizer.py to optimize onnx model as default. Can also choose from by_ort and no_opt",
     )
 
-    parser.add_argument("-v", "--validate_onnx", required=False, action="store_true", help="Validate ONNX model")
+    parser.add_argument(
+        "-v",
+        "--validate_onnx",
+        required=False,
+        action="store_true",
+        help="Validate ONNX model",
+    )
 
-    parser.add_argument("-f",
-                        "--fusion_csv",
-                        required=False,
-                        default=None,
-                        help="CSV file for saving summary results of graph optimization.")
-
-    parser.add_argument("-d", "--detail_csv", required=False, default=None, help="CSV file for saving detail results.")
-
-    parser.add_argument("-r", "--result_csv", required=False, default=None, help="CSV file for saving summary results.")
-
-    parser.add_argument("-i",
-                        "--input_counts",
-                        required=False,
-                        nargs="+",
-                        default=[1],
-                        type=int,
-                        choices=[1, 2, 3],
-                        help="Number of ONNX model inputs. Please use 1 for fair comparison with Torch or TorchScript.")
-
-    parser.add_argument("-t",
-                        "--test_times",
-                        required=False,
-                        default=100,
-                        type=int,
-                        help="Number of repeat times to get average inference latency.")
+    parser.add_argument(
+        "-f",
+        "--fusion_csv",
+        required=False,
+        default=None,
+        help="CSV file for saving summary results of graph optimization.",
+    )
+
+    parser.add_argument(
+        "-d",
+        "--detail_csv",
+        required=False,
+        default=None,
+        help="CSV file for saving detail results.",
+    )
+
+    parser.add_argument(
+        "-r",
+        "--result_csv",
+        required=False,
+        default=None,
+        help="CSV file for saving summary results.",
+    )
+
+    parser.add_argument(
+        "-i",
+        "--input_counts",
+        required=False,
+        nargs="+",
+        default=[1],
+        type=int,
+        choices=[1, 2, 3],
+        help="Number of ONNX model inputs. Please use 1 for fair comparison with Torch or TorchScript.",
+    )
+
+    parser.add_argument(
+        "-t",
+        "--test_times",
+        required=False,
+        default=100,
+        type=int,
+        help="Number of repeat times to get average inference latency.",
+    )
 
     parser.add_argument("-b", "--batch_sizes", nargs="+", type=int, default=[1])
 
-    parser.add_argument("-s", "--sequence_lengths", nargs="+", type=int, default=[4, 8, 16, 32, 64, 128, 256])
+    parser.add_argument(
+        "-s",
+        "--sequence_lengths",
+        nargs="+",
+        type=int,
+        default=[4, 8, 16, 32, 64, 128, 256],
+    )
 
-    parser.add_argument('--disable_ort_io_binding',
-                        required=False,
-                        action='store_true',
-                        help='Disable running ONNX Runtime with binded inputs and outputs. ')
+    parser.add_argument(
+        "--disable_ort_io_binding",
+        required=False,
+        action="store_true",
+        help="Disable running ONNX Runtime with binded inputs and outputs. ",
+    )
     parser.set_defaults(disable_ort_io_binding=False)
 
-    parser.add_argument("-n", "--num_threads", required=False, nargs="+", type=int, default=[0], help="Threads to use")
+    parser.add_argument(
+        "-n",
+        "--num_threads",
+        required=False,
+        nargs="+",
+        type=int,
+        default=[0],
+        help="Threads to use",
+    )
 
-    parser.add_argument("--force_num_layers",
-                        required=False,
-                        type=int,
-                        default=None,
-                        help="Manually set the model's layer number")
+    parser.add_argument(
+        "--force_num_layers",
+        required=False,
+        type=int,
+        default=None,
+        help="Manually set the model's layer number",
+    )
 
     FusionOptions.add_arguments(parser)
 
     args = parser.parse_args()
     return args
 
 
@@ -542,67 +783,144 @@
         logger.error("fp16 is for GPU only")
         return
 
     if args.precision == Precision.INT8 and args.use_gpu:
         logger.error("int8 is for CPU only")
         return
 
-    args.num_threads = sorted(set(cpu_count if x <= 0 else x for x in args.num_threads))
+    if len(args.models) == 1 and MODELS[args.models[0]][3] in ["vit", "swim"]:
+        args.sequence_lengths = [""]
+
+    args.num_threads = sorted({cpu_count if x <= 0 else x for x in args.num_threads})
 
     logger.info(f"Arguments: {args}")
 
     if not os.path.exists(args.cache_dir):
         try:
             os.mkdir(args.cache_dir)
         except OSError:
             logger.error("Creation of the directory %s failed" % args.cache_dir)
 
     enable_torch = "torch" in args.engines
+    enable_torch2 = "torch2" in args.engines
     enable_torchscript = "torchscript" in args.engines
     enable_onnxruntime = "onnxruntime" in args.engines
     enable_tensorflow = "tensorflow" in args.engines
 
+    if enable_torch2 and version.parse(torch.__version__) < version.parse("2.0.0"):
+        logger.error(f"PyTorch version must be >=2.0.0 and you are using {torch.__version__}")
+        return
+
     config_modifier = ConfigModifier(args.force_num_layers)
 
     results = []
 
     for num_threads in args.num_threads:
         torch.set_num_threads(num_threads)
         logger.debug(torch.__config__.parallel_info())
-        if enable_torch or enable_torchscript:
+        if enable_torch or enable_torch2 or enable_torchscript:
             if args.input_counts != [1]:
                 logger.warning("--input_counts is not implemented for torch or torchscript engine.")
 
             if enable_torchscript:
-                results += run_pytorch(args.use_gpu, args.models, args.model_class, config_modifier, args.precision,
-                                       num_threads, args.batch_sizes, args.sequence_lengths, args.test_times, True,
-                                       args.cache_dir, args.verbose)
+                results += run_pytorch(
+                    args.use_gpu,
+                    args.models,
+                    args.model_class,
+                    config_modifier,
+                    args.precision,
+                    num_threads,
+                    args.batch_sizes,
+                    args.sequence_lengths,
+                    args.test_times,
+                    True,
+                    False,
+                    args.cache_dir,
+                    args.verbose,
+                )
 
             if enable_torch:
-                results += run_pytorch(args.use_gpu, args.models, args.model_class, config_modifier, args.precision,
-                                       num_threads, args.batch_sizes, args.sequence_lengths, args.test_times, False,
-                                       args.cache_dir, args.verbose)
+                results += run_pytorch(
+                    args.use_gpu,
+                    args.models,
+                    args.model_class,
+                    config_modifier,
+                    args.precision,
+                    num_threads,
+                    args.batch_sizes,
+                    args.sequence_lengths,
+                    args.test_times,
+                    False,
+                    False,
+                    args.cache_dir,
+                    args.verbose,
+                )
+
+            if enable_torch2:
+                results += run_pytorch(
+                    args.use_gpu,
+                    args.models,
+                    args.model_class,
+                    config_modifier,
+                    args.precision,
+                    num_threads,
+                    args.batch_sizes,
+                    args.sequence_lengths,
+                    args.test_times,
+                    False,
+                    True,
+                    args.cache_dir,
+                    args.verbose,
+                )
 
         if enable_tensorflow:
-            results += run_tensorflow(args.use_gpu, args.models, args.model_class, config_modifier, args.precision,
-                                      num_threads, args.batch_sizes, args.sequence_lengths, args.test_times,
-                                      args.cache_dir, args.verbose)
+            results += run_tensorflow(
+                args.use_gpu,
+                args.models,
+                args.model_class,
+                config_modifier,
+                args.precision,
+                num_threads,
+                args.batch_sizes,
+                args.sequence_lengths,
+                args.test_times,
+                args.cache_dir,
+                args.verbose,
+            )
 
         model_fusion_statistics = {}
         if enable_onnxruntime:
             try:
-                use_raw_attention_mask = True
-                results += run_onnxruntime(args.use_gpu, args.provider, args.models, args.model_class, config_modifier,
-                                           args.precision, num_threads, args.batch_sizes, args.sequence_lengths,
-                                           args.test_times, args.input_counts, args.optimizer_info, args.validate_onnx,
-                                           args.cache_dir, args.onnx_dir, args.verbose, args.overwrite,
-                                           args.disable_ort_io_binding, use_raw_attention_mask, model_fusion_statistics,
-                                           args.model_source, args)
-            except:
-                logger.error(f"Exception", exc_info=True)
+                use_raw_attention_mask = not args.use_mask_index
+                results += run_onnxruntime(
+                    args.use_gpu,
+                    args.provider,
+                    args.models,
+                    args.model_class,
+                    config_modifier,
+                    args.precision,
+                    num_threads,
+                    args.batch_sizes,
+                    args.sequence_lengths,
+                    args.test_times,
+                    args.input_counts,
+                    args.optimizer_info,
+                    args.validate_onnx,
+                    args.cache_dir,
+                    args.onnx_dir,
+                    args.verbose,
+                    args.overwrite,
+                    args.disable_ort_io_binding,
+                    use_raw_attention_mask,
+                    model_fusion_statistics,
+                    args.model_source,
+                    args,
+                )
+            except Exception:
+                logger.error("Exception", exc_info=True)
 
     time_stamp = datetime.now().strftime("%Y%m%d-%H%M%S")
     if model_fusion_statistics:
         csv_filename = args.fusion_csv or f"benchmark_fusion_{time_stamp}.csv"
         output_fusion_statistics(model_fusion_statistics, csv_filename)
 
     if len(results) == 0:
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/benchmark_gpt2.py` & `onnxruntime/transformers/models/gpt2/benchmark_gpt2.py`

 * *Files 20% similar despite different names*

```diff
@@ -2,379 +2,417 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 # This script benchmarks gpt2 model with past state.
 # For gpt2 model without past state, use benchmark.py to measure performance.
 
+import argparse
+import csv
+import logging
 import os
 import sys
-import numpy
-import csv
 from datetime import datetime
+
 import psutil
-import argparse
-import logging
 import torch
-import onnx
+from gpt2_helper import DEFAULT_TOLERANCE, MODEL_CLASSES, PRETRAINED_GPT2_MODELS, Gpt2Helper
 from packaging import version
 from transformers import AutoConfig
-from gpt2_helper import Gpt2Helper, DEFAULT_TOLERANCE, PRETRAINED_GPT2_MODELS
-from gpt2_beamsearch_helper import Gpt2HelperFactory, MODEL_CLASSES
-from quantize_helper import QuantizeHelper
-from benchmark_helper import create_onnxruntime_session, setup_logger, prepare_environment, Precision
 
-logger = logging.getLogger('')
+sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
+
+from benchmark_helper import (  # noqa: E402
+    Precision,
+    create_onnxruntime_session,
+    get_ort_environment_variables,
+    prepare_environment,
+    setup_logger,
+)
+from quantize_helper import QuantizeHelper  # noqa: E402
+
+logger = logging.getLogger("")
 
 
 def parse_arguments(argv=None):
     parser = argparse.ArgumentParser()
 
-    parser.add_argument('-m',
-                        '--model_name_or_path',
-                        required=True,
-                        type=str,
-                        help='Model path, or pretrained model name selected in the list: ' +
-                        ', '.join(PRETRAINED_GPT2_MODELS))
-
-    parser.add_argument('--model_class',
-                        required=False,
-                        type=str,
-                        default='GPT2LMHeadModel',
-                        choices=list(MODEL_CLASSES.keys()),
-                        help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))
-
-    parser.add_argument('--cache_dir',
-                        required=False,
-                        type=str,
-                        default=os.path.join('.', 'cache_models'),
-                        help='Directory to cache pre-trained models')
-
-    parser.add_argument('--onnx_dir',
-                        required=False,
-                        type=str,
-                        default=os.path.join('.', 'onnx_models'),
-                        help='Directory to store onnx models')
-
-    parser.add_argument('--test_times',
-                        required=False,
-                        default=100,
-                        type=int,
-                        help='Number of repeat times to get average inference latency.')
-
-    parser.add_argument('-v', '--validate_onnx', required=False, action='store_true', help='Validate ONNX model')
-
-    parser.add_argument('-o',
-                        '--optimize_onnx',
-                        required=False,
-                        action='store_true',
-                        help='Use optimizer.py to optimize onnx model')
+    parser.add_argument(
+        "-m",
+        "--model_name_or_path",
+        required=True,
+        type=str,
+        help="Model path, or pretrained model name selected in the list: " + ", ".join(PRETRAINED_GPT2_MODELS),
+    )
+
+    parser.add_argument(
+        "--model_class",
+        required=False,
+        type=str,
+        default="GPT2LMHeadModel",
+        choices=list(MODEL_CLASSES.keys()),
+        help="Model type selected in the list: " + ", ".join(MODEL_CLASSES.keys()),
+    )
+
+    parser.add_argument(
+        "--cache_dir",
+        required=False,
+        type=str,
+        default=os.path.join(".", "cache_models"),
+        help="Directory to cache pre-trained models",
+    )
+
+    parser.add_argument(
+        "--onnx_dir",
+        required=False,
+        type=str,
+        default=os.path.join(".", "onnx_models"),
+        help="Directory to store onnx models",
+    )
+
+    parser.add_argument(
+        "--test_times",
+        required=False,
+        default=100,
+        type=int,
+        help="Number of repeat times to get average inference latency.",
+    )
+
+    parser.add_argument(
+        "-v",
+        "--validate_onnx",
+        required=False,
+        action="store_true",
+        help="Validate ONNX model",
+    )
+
+    parser.add_argument(
+        "-o",
+        "--optimize_onnx",
+        required=False,
+        action="store_true",
+        help="Use optimizer.py to optimize onnx model",
+    )
     parser.set_defaults(optimize_onnx=False)
 
-    parser.add_argument('--use_gpu', required=False, action='store_true', help="use GPU for inference")
+    parser.add_argument(
+        "--stage",
+        type=int,
+        default=0,
+        required=False,
+        choices=[0, 1, 2],
+        help="Stage in generation: 1 (initial decoder), 2 (decoder), 0 (both). "
+        "1 - decode the first token when past_sequence_length is zero; "
+        "2 - decode the remaining tokens when past_sequence_length is not zero; "
+        "0 - one onnx model for both stages 1 and 2. "
+        "Note that we will optimize 1 and 2 differently for best performance.",
+    )
+
+    parser.add_argument("--use_gpu", required=False, action="store_true", help="use GPU for inference")
     parser.set_defaults(use_gpu=False)
 
     parser.add_argument(
         "-p",
         "--precision",
         type=Precision,
         default=Precision.FLOAT32,
         choices=list(Precision),
-        help="Precision of model to run. fp32 for full precision, fp16 for half precision, and int8 for quantization")
+        help="Precision of model to run. fp32 for full precision, fp16 for half precision, and int8 for quantization",
+    )
 
-    parser.add_argument('--torchscript', required=False, action='store_true', help="use Torchscript")
+    parser.add_argument("--torchscript", required=False, action="store_true", help="use Torchscript")
     parser.set_defaults(torchscript=False)
 
-    parser.add_argument('-b', '--batch_sizes', nargs='+', type=int, default=[1], help="batch size")
-    parser.add_argument('--beam_size', type=int, default=4, help='Beam size if greedy/top-p/top-k sampling is needed')
+    parser.add_argument("-b", "--batch_sizes", nargs="+", type=int, default=[1], help="batch size")
 
-    parser.add_argument('--sequence_lengths',
-                        nargs='+',
-                        type=int,
-                        default=[1],
-                        help="sequence lengths (excluding past)")
-
-    parser.add_argument('-s',
-                        '--past_sequence_lengths',
-                        nargs='+',
-                        type=int,
-                        default=[8, 16, 32, 64, 128, 256],
-                        help="past sequence lengths")
+    parser.add_argument(
+        "--sequence_lengths",
+        nargs="+",
+        type=int,
+        default=[1],
+        help="sequence lengths (excluding past)",
+    )
 
-    parser.add_argument("-r", "--result_csv", required=False, default=None, help="CSV file for saving summary results.")
+    parser.add_argument(
+        "-s",
+        "--past_sequence_lengths",
+        nargs="+",
+        type=int,
+        default=[8, 16, 32, 64, 128, 256],
+        help="past sequence lengths",
+    )
+
+    parser.add_argument(
+        "-r",
+        "--result_csv",
+        required=False,
+        default=None,
+        help="CSV file for saving summary results.",
+    )
 
     parser.add_argument("--thread_num", required=False, type=int, default=-1, help="Threads to use")
 
-    parser.add_argument('--include_copy_output_latency', required=False, action='store_true')
+    parser.add_argument("--include_copy_output_latency", required=False, action="store_true")
     parser.set_defaults(include_copy_output_latency=False)
 
-    parser.add_argument('--verbose', required=False, action='store_true')
+    parser.add_argument("--verbose", required=False, action="store_true")
     parser.set_defaults(verbose=False)
 
-    search_option_group = parser.add_argument_group("configurable one step search options")
+    parser.add_argument("--output_torch_latency", required=False, action="store_true")
+    parser.set_defaults(output_torch_latency=False)
 
-    search_option_group.add_argument('--ignore_eos',
-                                     type=bool,
-                                     default=False,
-                                     help='If ignore end of sentence token in model inference.')
-    search_option_group.add_argument('--repetition_penalty',
-                                     type=float,
-                                     default=1,
-                                     help='Positive. >1 to penalize and <1 to encorage.')
-    search_option_group.add_argument('--temperature',
-                                     type=float,
-                                     default=1,
-                                     help='Softmax temperature for output logits.')
-    search_option_group.add_argument('--excluded_token_ids',
-                                     required=False,
-                                     nargs='+',
-                                     type=float,
-                                     help='A list of token ids to be excluded in inference.')
-    search_option_group.add_argument('--length_penalty',
-                                     type=float,
-                                     default=1,
-                                     help='Positive. >1 to penalize and <1 to encorage short sentence.')
-
-    sampling_option_group = parser.add_argument_group("one step sampling options")
-    sampling_option_group.add_argument('--do_sample',
-                                       action='store_true',
-                                       help='If to do sampling instead of beam search or greedy.')
-    sampling_option_group.add_argument('--do_sample_top_p',
-                                       type=float,
-                                       default=0.95,
-                                       help='Nuclear/top-p sampling accumulation probability.')
-    sampling_option_group.add_argument('--do_sample_top_k', type=int, default=0, help='Use top-k if non-zero.')
+    parser.add_argument("--disable_io_binding", required=False, action="store_true")
+    parser.set_defaults(disable_io_binding=False)
 
     args = parser.parse_args(argv)
 
     return args
 
 
 def main(args):
     from transformers import __version__ as transformers_version
+
     if version.parse(transformers_version) < version.parse(
-            "3.1.0"):  # past_key_values name does not exist in 3.0.2 or older
+        "3.1.0"
+    ):  # past_key_values name does not exist in 3.0.2 or older
         raise RuntimeError("This tool requires transformers 3.1.0 or later.")
 
     logger.info(f"Arguments:{args}")
     if args.precision == Precision.FLOAT16:
         assert args.optimize_onnx and args.use_gpu, "fp16 requires --optimize_onnx --use_gpu"
 
     if args.precision == Precision.INT8:
         assert not args.use_gpu, "quantization only supports CPU"
 
+    if args.stage == 1:
+        assert args.past_sequence_lengths == [0], "past_sequence_lengths shall be 0 for stage==1 (init decoder)"
+
     torch.set_num_threads(psutil.cpu_count(logical=True) if args.thread_num <= 0 else args.thread_num)
     print(torch.__config__.parallel_info())
 
     cache_dir = args.cache_dir
     output_dir = args.onnx_dir
     prepare_environment(cache_dir, output_dir, args.use_gpu)
 
     model_class = MODEL_CLASSES[args.model_class][0]
-    if args.model_class == "GPT2LMHeadModel_BeamSearchStep":
-        model_type = "beam_search_step"
-    elif args.model_class == "GPT2LMHeadModel_ConfigurableOneStepSearch":
-        model_type = "configurable_one_step_search"
-    else:
-        model_type = "default"
-
-    gpt2helper = Gpt2HelperFactory.create_helper(model_type)
+    gpt2helper = Gpt2Helper
     config = AutoConfig.from_pretrained(args.model_name_or_path, torchscript=args.torchscript, cache_dir=cache_dir)
-    if model_type == 'beam_search_step':
-        model = model_class.from_pretrained(args.model_name_or_path,
-                                            config=config,
-                                            batch_size=1,
-                                            beam_size=args.beam_size,
-                                            cache_dir=cache_dir)
-    elif model_type == 'configurable_one_step_search':
-        model = model_class.from_pretrained(args.model_name_or_path,
-                                            config=config,
-                                            batch_size=1,
-                                            beam_size=args.beam_size,
-                                            ignore_eos=args.ignore_eos,
-                                            temperature=args.temperature,
-                                            repetition_penalty=args.repetition_penalty,
-                                            excluded_token_ids=args.excluded_token_ids,
-                                            length_penalty=args.length_penalty,
-                                            do_sample=args.do_sample,
-                                            do_sample_top_p=args.do_sample_top_p,
-                                            do_sample_top_k=args.do_sample_top_k,
-                                            cache_dir=cache_dir)
-    else:
-        model = model_class.from_pretrained(args.model_name_or_path, config=config, cache_dir=cache_dir)
+    model = model_class.from_pretrained(args.model_name_or_path, config=config, cache_dir=cache_dir)
 
     # This scirpt does not support float16 for PyTorch.
-    #if args.float16:
+    # if args.float16:
     #    model.half()
 
     device = torch.device("cuda:0" if args.use_gpu else "cpu")
     model.to(device)
-    use_external_data_format = (config.n_layer > 24)  #TODO: find a way to check model size > 2GB
-    onnx_model_paths = gpt2helper.get_onnx_paths(output_dir,
-                                                 args.model_name_or_path,
-                                                 args.model_class,
-                                                 has_past=True,
-                                                 new_folder=use_external_data_format)
+    use_external_data_format = config.n_layer > 24  # TODO: find a way to check model size > 2GB
+    onnx_model_paths = gpt2helper.get_onnx_paths(
+        output_dir,
+        args.model_name_or_path,
+        args.model_class,
+        has_past=True,
+        new_folder=use_external_data_format,
+    )
 
     onnx_model_path = onnx_model_paths["raw"]
     use_padding = MODEL_CLASSES[args.model_class][2]
-    gpt2helper.export_onnx(model,
-                           device,
-                           onnx_model_path,
-                           args.verbose,
-                           use_external_data_format,
-                           has_position_ids=use_padding,
-                           has_attention_mask=use_padding)
+    gpt2helper.export_onnx(
+        model,
+        device,
+        onnx_model_path,
+        args.verbose,
+        use_external_data_format,
+        has_position_ids=use_padding,
+        has_attention_mask=use_padding,
+    )
 
     if args.optimize_onnx or args.precision != Precision.FLOAT32:
-        onnx_model_path = onnx_model_paths[str(args.precision) if args.precision != Precision.INT8 else 'fp32']
-        gpt2helper.optimize_onnx(onnx_model_paths["raw"], onnx_model_path, args.precision == Precision.FLOAT16,
-                                 model.config.num_attention_heads, model.config.hidden_size, use_external_data_format)
+        onnx_model_path = onnx_model_paths[str(args.precision) if args.precision != Precision.INT8 else "fp32"]
+        gpt2helper.optimize_onnx(
+            onnx_model_paths["raw"],
+            onnx_model_path,
+            args.precision == Precision.FLOAT16,
+            model.config.num_attention_heads,
+            model.config.hidden_size,
+            use_external_data_format,
+            auto_mixed_precision=True,
+            stage=args.stage,
+        )
 
         if args.precision == Precision.INT8:
             logger.info("quantizing model...")
             QuantizeHelper.quantize_onnx_model(onnx_model_path, onnx_model_paths["int8"], use_external_data_format)
             model = QuantizeHelper.quantize_torch_model(model)
             logger.info("finished quantizing model")
             onnx_model_path = onnx_model_paths["int8"]
 
     if args.torchscript:
-        model = gpt2helper.torchscript(model,
-                                       config,
-                                       device,
-                                       has_position_ids=use_padding,
-                                       has_attention_mask=use_padding)
-
-    session = create_onnxruntime_session(onnx_model_path,
-                                         args.use_gpu,
-                                         enable_all_optimization=False,
-                                         num_threads=args.thread_num,
-                                         verbose=args.verbose)
+        model = gpt2helper.torchscript(
+            model,
+            config,
+            device,
+            has_position_ids=use_padding,
+            has_attention_mask=use_padding,
+        )
+
+    session = create_onnxruntime_session(
+        onnx_model_path,
+        args.use_gpu,
+        enable_all_optimization=False,
+        num_threads=args.thread_num,
+        verbose=args.verbose,
+    )
     if session is None:
         return
 
     # Allocate output buffers for IO Binding
-    if model_type == 'beam_search_step' or model_type == 'configurable_one_step_search':
-        max_output_shapes = gpt2helper.get_output_shapes(max(args.batch_sizes), max(args.past_sequence_lengths),
-                                                         max(args.past_sequence_lengths), max(args.sequence_lengths), 4,
-                                                         0, config, args.model_class)
-        output_buffers = gpt2helper.get_output_buffers(max_output_shapes, device, args.precision == Precision.FLOAT16)
-
-    else:
-        max_output_shapes = gpt2helper.get_output_shapes(max(args.batch_sizes), max(args.past_sequence_lengths),
-                                                         max(args.sequence_lengths), config, args.model_class)
-        output_buffers = gpt2helper.get_output_buffers(max_output_shapes, device, args.precision == Precision.FLOAT16)
+    max_output_shapes = gpt2helper.get_output_shapes(
+        max(args.batch_sizes),
+        max(args.past_sequence_lengths),
+        max(args.sequence_lengths),
+        config,
+        args.model_class,
+    )
+    output_buffers = gpt2helper.get_output_buffers(max_output_shapes, device, args.precision == Precision.FLOAT16)
 
     csv_filename = args.result_csv or "benchmark_result_{}.csv".format(datetime.now().strftime("%Y%m%d-%H%M%S"))
-    with open(csv_filename, mode="a", newline='') as csv_file:
+    with open(csv_filename, mode="a", newline="") as csv_file:
         column_names = [
-            "model_name", "model_class", "gpu", "precision", "optimizer", "torchscript", "batch_size",
-            "sequence_length", "past_sequence_length", "torch_latency", "onnxruntime_latency",
-            "onnxruntime_io_binding_latency"
+            "model_name",
+            "model_class",
+            "stage",
+            "environment_variables",
+            "gpu",
+            "precision",
+            "optimizer",
+            "torchscript",
+            "batch_size",
+            "sequence_length",
+            "past_sequence_length",
+            "disable_io_binding",
+            "torch_latency",
+            "onnxruntime_latency",
         ]
         csv_writer = csv.DictWriter(csv_file, fieldnames=column_names)
         csv_writer.writeheader()
 
         for batch_size in args.batch_sizes:
             for sequence_length in args.sequence_lengths:
                 for past_sequence_length in args.past_sequence_lengths:
                     assert batch_size > 0 and sequence_length > 0 and past_sequence_length >= 0
                     logger.debug(
-                        f"Running test for batch_size={batch_size} sequence_length={sequence_length} past_sequence_length={past_sequence_length}..."
+                        "Running test for batch_size=%d sequence_length=%d past_sequence_length=%d ...",
+                        batch_size,
+                        sequence_length,
+                        past_sequence_length,
+                    )
+
+                    dummy_inputs = gpt2helper.get_dummy_inputs(
+                        batch_size,
+                        past_sequence_length,
+                        sequence_length,
+                        config.num_attention_heads,
+                        config.hidden_size,
+                        config.n_layer,
+                        config.vocab_size,
+                        device,
+                        float16=(args.precision == Precision.FLOAT16),
+                        has_position_ids=use_padding,
+                        has_attention_mask=use_padding,
+                    )
+                    output_shapes = gpt2helper.get_output_shapes(
+                        batch_size,
+                        past_sequence_length,
+                        sequence_length,
+                        config,
+                        args.model_class,
                     )
-                    if model_type == 'beam_search_step' or model_type == 'configurable_one_step_search':
-                        dummy_inputs = gpt2helper.get_dummy_inputs(batch_size,
-                                                                   past_sequence_length,
-                                                                   sequence_length,
-                                                                   config.num_attention_heads,
-                                                                   config.hidden_size,
-                                                                   config.n_layer,
-                                                                   config.vocab_size,
-                                                                   device,
-                                                                   float16=(args.precision == Precision.FLOAT16),
-                                                                   has_position_ids=use_padding,
-                                                                   has_attention_mask=use_padding)
-                        output_shapes = gpt2helper.get_output_shapes(batch_size, past_sequence_length,
-                                                                     past_sequence_length, sequence_length, 4, 0,
-                                                                     config, args.model_class)
-                    else:
-                        dummy_inputs = gpt2helper.get_dummy_inputs(batch_size,
-                                                                   past_sequence_length,
-                                                                   sequence_length,
-                                                                   config.num_attention_heads,
-                                                                   config.hidden_size,
-                                                                   config.n_layer,
-                                                                   config.vocab_size,
-                                                                   device,
-                                                                   float16=(args.precision == Precision.FLOAT16),
-                                                                   has_position_ids=use_padding,
-                                                                   has_attention_mask=use_padding)
-                        output_shapes = gpt2helper.get_output_shapes(batch_size, past_sequence_length, sequence_length,
-                                                                     config, args.model_class)
 
                     try:
-                        outputs, torch_latency = Gpt2Helper.pytorch_inference(model, dummy_inputs, args.test_times)
-                        ort_outputs, ort_latency = Gpt2Helper.onnxruntime_inference(session, dummy_inputs,
-                                                                                    args.test_times)
-                        ort_io_outputs, ort_io_latency = gpt2helper.onnxruntime_inference_with_binded_io(
-                            session,
-                            dummy_inputs,
-                            output_buffers,
-                            output_shapes,
-                            args.test_times,
-                            return_numpy=False,
-                            include_copy_output_latency=args.include_copy_output_latency)
+                        if args.validate_onnx or args.output_torch_latency:
+                            outputs, torch_latency = gpt2helper.pytorch_inference(model, dummy_inputs, args.test_times)
 
-                        if args.validate_onnx:
-                            if gpt2helper.compare_outputs(outputs,
-                                                          ort_outputs,
-                                                          model_class,
-                                                          rtol=DEFAULT_TOLERANCE[args.precision],
-                                                          atol=DEFAULT_TOLERANCE[args.precision]):
-                                logger.info(
-                                    f'Pytorch and ONNX Runtime outputs are all close (tolerance={DEFAULT_TOLERANCE[args.precision]}).'
-                                )
+                            # Dump Torch output shape
+                            for i, value in enumerate(outputs):
+                                if isinstance(value, tuple):
+                                    logger.debug(
+                                        f"torch output {i} is tuple of size {len(value)}, shape {value[0].shape}"
+                                    )
+                                else:
+                                    logger.debug(f"torch output {i} shape {value.shape}")
+                        else:
+                            outputs = None
+                            torch_latency = None
+
+                        if args.disable_io_binding:
+                            ort_outputs, ort_latency = gpt2helper.onnxruntime_inference(
+                                session, dummy_inputs, args.test_times
+                            )
+                        else:
+                            ort_outputs, ort_latency = gpt2helper.onnxruntime_inference_with_binded_io(
+                                session,
+                                dummy_inputs,
+                                output_buffers,
+                                output_shapes,
+                                args.test_times,
+                                return_numpy=False,
+                                include_copy_output_latency=args.include_copy_output_latency,
+                            )
 
-                            # Results of IO binding might be in GPU. Copy outputs to CPU for comparison.
-                            copy_outputs = []
-                            for output in ort_io_outputs:
-                                copy_outputs.append(output.cpu().numpy())
-
-                            if gpt2helper.compare_outputs(outputs,
-                                                          copy_outputs,
-                                                          model_class,
-                                                          rtol=DEFAULT_TOLERANCE[args.precision],
-                                                          atol=DEFAULT_TOLERANCE[args.precision]):
+                        if args.validate_onnx:
+                            copy_outputs = ort_outputs
+                            if not args.disable_io_binding:
+                                # Results of IO binding might be in GPU. Copy outputs to CPU for comparison.
+                                copy_outputs = []
+                                for output in ort_outputs:
+                                    copy_outputs.append(output.cpu().numpy())
+
+                            if gpt2helper.compare_outputs(
+                                outputs,
+                                copy_outputs,
+                                model_class=args.model_class,
+                                rtol=DEFAULT_TOLERANCE[args.precision],
+                                atol=DEFAULT_TOLERANCE[args.precision],
+                            ):
                                 logger.info(
-                                    f'Pytorch and ONNX Runtime IO Binding outputs are all close (tolerance={DEFAULT_TOLERANCE[args.precision]}).'
+                                    f"Pytorch and ONNX Runtime outputs are all close (tolerance={DEFAULT_TOLERANCE[args.precision]})."
                                 )
 
                         logger.info(
-                            f"batch_size={batch_size}, sequence_length={sequence_length}, past_sequence_length={past_sequence_length}, torch_latency={torch_latency:.2f}, onnxruntime_latency={ort_latency:.2f}, onnxruntime_io_binding_latency={ort_io_latency:.2f}"
+                            "batch_size=%d, sequence_length=%d, past_sequence_length=%d, onnxruntime_latency=%.2f %s %s",
+                            batch_size,
+                            sequence_length,
+                            past_sequence_length,
+                            ort_latency,
+                            "(disable_io_binding)" if args.disable_io_binding else "",
+                            ", torch_latency={torch_latency}" if torch_latency else "",
                         )
 
                         row = {
                             "model_name": args.model_name_or_path,
                             "model_class": args.model_class,
+                            "stage": args.stage,
+                            "environment_variables": get_ort_environment_variables(),
                             "gpu": args.use_gpu,
                             "precision": args.precision,
                             "optimizer": args.optimize_onnx,
                             "torchscript": args.torchscript,
                             "batch_size": batch_size,
                             "sequence_length": sequence_length,
                             "past_sequence_length": past_sequence_length,
-                            "torch_latency": f"{torch_latency:.2f}",
+                            "disable_io_binding": args.disable_io_binding,
+                            "torch_latency": f"{torch_latency:.2f}" if torch_latency else "None",
                             "onnxruntime_latency": f"{ort_latency:.2f}",
-                            "onnxruntime_io_binding_latency": f"{ort_io_latency:.2f}"
                         }
                         csv_writer.writerow(row)
-                    except:
-                        logger.error(f"Exception", exc_info=True)
+                    except Exception:
+                        logger.error("Exception", exc_info=True)
+                        return None
 
     logger.info(f"Results are saved to file {csv_filename}")
     return csv_filename
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     args = parse_arguments()
     setup_logger(args.verbose)
     main(args)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/benchmark_helper.py` & `onnxruntime/transformers/benchmark_helper.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,94 +1,100 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
-import os
-import sys
 import csv
-import numpy
-import time
+import logging
+import os
+import random
 import timeit
+from concurrent.futures import ThreadPoolExecutor
 from datetime import datetime
-import argparse
-import logging
+from enum import Enum
+from time import sleep
+from typing import Any, Dict, List, Optional
+
 import coloredlogs
+import numpy
 import torch
-import onnx
-from enum import Enum
+import transformers
 from packaging import version
 
+import onnxruntime
+
 logger = logging.getLogger(__name__)
 
 
 class Precision(Enum):
-    FLOAT32 = 'fp32'
-    FLOAT16 = 'fp16'
-    INT8 = 'int8'
+    FLOAT32 = "fp32"
+    FLOAT16 = "fp16"
+    INT8 = "int8"
 
     def __str__(self):
         return self.value
 
 
 class OptimizerInfo(Enum):
     # no_opt means using the raw ONNX model, but OnnxRuntime might still apply optimization as long as
     # graph optimization level is not 0 (disable all).
-    NOOPT = 'no_opt'
-    BYORT = 'by_ort'
-    BYSCRIPT = 'by_script'
+    NOOPT = "no_opt"
+    BYORT = "by_ort"
+    BYSCRIPT = "by_script"
 
     def __str__(self):
         return self.value
 
 
-class ConfigModifier():
+class ConfigModifier:
     def __init__(self, num_layers):
         self.num_layers = num_layers
 
     def modify(self, config):
         if self.num_layers is None:
             return
-        if hasattr(config, 'num_hidden_layers'):
+        if hasattr(config, "num_hidden_layers"):
             config.num_hidden_layers = self.num_layers
             logger.info(f"Modifying pytorch model's number of hidden layers to: {self.num_layers}")
-        if hasattr(config, 'encoder_layers'):
+        if hasattr(config, "encoder_layers"):
             config.encoder_layers = self.num_layers
             logger.info(f"Modifying pytorch model's number of encoder layers to: {self.num_layers}")
-        if hasattr(config, 'decoder_layers '):
+        if hasattr(config, "decoder_layers "):
             config.decoder_layers = self.num_layers
             logger.info(f"Modifying pytorch model's number of decoder layers to: {self.num_layers}")
 
     def get_layer_num(self):
         return self.num_layers
 
 
 IO_BINDING_DATA_TYPE_MAP = {
     "float32": numpy.float32,
     # TODO: Add more.
 }
 
 
-def create_onnxruntime_session(onnx_model_path,
-                               use_gpu,
-                               provider=None,
-                               enable_all_optimization=True,
-                               num_threads=-1,
-                               enable_profiling=False,
-                               verbose=False):
+def create_onnxruntime_session(
+    onnx_model_path,
+    use_gpu,
+    provider=None,
+    enable_all_optimization=True,
+    num_threads=-1,
+    enable_profiling=False,
+    verbose=False,
+    provider_options={},  # map execution provider name to its option  # noqa: B006
+):
     session = None
     try:
-        from onnxruntime import SessionOptions, InferenceSession, GraphOptimizationLevel, __version__ as onnxruntime_version
-        sess_options = SessionOptions()
+        sess_options = onnxruntime.SessionOptions()
 
         if enable_all_optimization:
-            sess_options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL
+            sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
         else:
-            sess_options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_BASIC
+            sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_BASIC
 
         if enable_profiling:
             sess_options.enable_profiling = True
 
         if num_threads > 0:
             sess_options.intra_op_num_threads = num_threads
             logger.debug(f"Session option: intra_op_num_threads={sess_options.intra_op_num_threads}")
@@ -96,293 +102,470 @@
         if verbose:
             sess_options.log_severity_level = 0
         else:
             sess_options.log_severity_level = 4
 
         logger.debug(f"Create session for onnx model: {onnx_model_path}")
         if use_gpu:
-            if provider == 'dml':
-                execution_providers = ['DmlExecutionProvider', 'CPUExecutionProvider']
-            elif provider == 'rocm':
-                execution_providers = ['ROCMExecutionProvider', 'CPUExecutionProvider']
-            elif provider == 'migraphx':
-                execution_providers = ['MIGraphXExecutionProvider', 'ROCMExecutionProvider', 'CPUExecutionProvider']
-            elif provider == 'cuda':
-                execution_providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
-            elif provider == 'tensorrt':
-                execution_providers = ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
+            if provider == "dml":
+                providers = ["DmlExecutionProvider", "CPUExecutionProvider"]
+            elif provider == "rocm":
+                providers = ["ROCMExecutionProvider", "CPUExecutionProvider"]
+            elif provider == "migraphx":
+                providers = [
+                    "MIGraphXExecutionProvider",
+                    "ROCMExecutionProvider",
+                    "CPUExecutionProvider",
+                ]
+            elif provider == "cuda":
+                providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
+            elif provider == "tensorrt":
+                providers = [
+                    "TensorrtExecutionProvider",
+                    "CUDAExecutionProvider",
+                    "CPUExecutionProvider",
+                ]
             else:
-                execution_providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
+                providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
         else:
-            execution_providers = ['CPUExecutionProvider']
-        session = InferenceSession(onnx_model_path, sess_options, providers=execution_providers)
-    except:
-        logger.error(f"Exception", exc_info=True)
+            providers = ["CPUExecutionProvider"]
+
+        if provider_options:
+            providers = [(name, provider_options[name]) if name in provider_options else name for name in providers]
+
+        session = onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=providers)
+    except Exception:
+        logger.error("Exception", exc_info=True)
 
     return session
 
 
 def setup_logger(verbose=True):
     if verbose:
-        coloredlogs.install(level='DEBUG', fmt='[%(filename)s:%(lineno)s - %(funcName)20s()] %(message)s')
+        coloredlogs.install(
+            level="DEBUG",
+            fmt="[%(filename)s:%(lineno)s - %(funcName)20s()] %(message)s",
+        )
     else:
-        coloredlogs.install(fmt='%(message)s')
+        coloredlogs.install(fmt="%(message)s")
         logging.getLogger("transformers").setLevel(logging.WARNING)
 
 
 def prepare_environment(cache_dir, output_dir, use_gpu, provider=None):
     if cache_dir and not os.path.exists(cache_dir):
         os.makedirs(cache_dir)
 
     if output_dir and not os.path.exists(output_dir):
         os.makedirs(output_dir)
 
-    import onnxruntime
     if use_gpu:
-        if provider == 'dml':
-            assert 'DmlExecutionProvider' in onnxruntime.get_available_providers(
+        if provider == "dml":
+            assert (
+                "DmlExecutionProvider" in onnxruntime.get_available_providers()
             ), "Please install onnxruntime-directml package to test GPU inference."
 
         else:
-            assert 'CUDAExecutionProvider' in onnxruntime.get_available_providers(
-            ), "Please install onnxruntime-gpu package to test GPU inference."
-
-    import transformers
-    logger.info(f'PyTorch Version:{torch.__version__}')
-    logger.info(f'Transformers Version:{transformers.__version__}')
-    logger.info(f'Onnxruntime Version:{onnxruntime.__version__}')
-
-    # Support three major versions of PyTorch and OnnxRuntime, and up to 6 months of transformers.
-    from packaging import version
-    assert version.parse(torch.__version__) >= version.parse('1.5.0')
-    assert version.parse(transformers.__version__) >= version.parse('3.0.0')
-    assert version.parse(onnxruntime.__version__) >= version.parse('1.4.0')
-
-
-def get_latency_result(runtimes, batch_size):
-    latency_ms = sum(runtimes) / float(len(runtimes)) * 1000.0
-    latency_variance = numpy.var(runtimes, dtype=numpy.float64) * 1000.0
+            assert not set(onnxruntime.get_available_providers()).isdisjoint(
+                ["CUDAExecutionProvider", "ROCMExecutionProvider", "MIGraphXExecutionProvider"]
+            ), "Please install onnxruntime-gpu package, or install ROCm support, to test GPU inference."
+
+    logger.info(f"PyTorch Version:{torch.__version__}")
+    logger.info(f"Transformers Version:{transformers.__version__}")
+    logger.info(f"Onnxruntime Version:{onnxruntime.__version__}")
+
+    # Support three major versions of PyTorch and OnnxRuntime, and up to 9 months of transformers.
+    assert version.parse(torch.__version__) >= version.parse("1.10.0")
+    assert version.parse(transformers.__version__) >= version.parse("4.12.0")
+    assert version.parse(onnxruntime.__version__) >= version.parse("1.10.0")
+
+
+def get_latency_result(latency_list, batch_size):
+    latency_ms = sum(latency_list) / float(len(latency_list)) * 1000.0
+    latency_variance = numpy.var(latency_list, dtype=numpy.float64) * 1000.0
     throughput = batch_size * (1000.0 / latency_ms)
 
     return {
-        "test_times": len(runtimes),
-        "latency_variance": "{:.2f}".format(latency_variance),
-        "latency_90_percentile": "{:.2f}".format(numpy.percentile(runtimes, 90) * 1000.0),
-        "latency_95_percentile": "{:.2f}".format(numpy.percentile(runtimes, 95) * 1000.0),
-        "latency_99_percentile": "{:.2f}".format(numpy.percentile(runtimes, 99) * 1000.0),
-        "average_latency_ms": "{:.2f}".format(latency_ms),
-        "QPS": "{:.2f}".format(throughput),
+        "test_times": len(latency_list),
+        "latency_variance": f"{latency_variance:.2f}",
+        "latency_90_percentile": f"{numpy.percentile(latency_list, 90) * 1000.0:.2f}",
+        "latency_95_percentile": f"{numpy.percentile(latency_list, 95) * 1000.0:.2f}",
+        "latency_99_percentile": f"{numpy.percentile(latency_list, 99) * 1000.0:.2f}",
+        "average_latency_ms": f"{latency_ms:.2f}",
+        "QPS": f"{throughput:.2f}",
     }
 
 
 def output_details(results, csv_filename):
-    with open(csv_filename, mode="a", newline='') as csv_file:
+    with open(csv_filename, mode="a", newline="", encoding="ascii") as csv_file:
         column_names = [
-            "engine", "version", "providers", "device", "precision", "optimizer", "io_binding", "model_name", "inputs",
-            "threads", "batch_size", "sequence_length", "custom_layer_num", "datetime", "test_times", "QPS",
-            "average_latency_ms", "latency_variance", "latency_90_percentile", "latency_95_percentile",
-            "latency_99_percentile"
+            "engine",
+            "version",
+            "providers",
+            "device",
+            "precision",
+            "optimizer",
+            "io_binding",
+            "model_name",
+            "inputs",
+            "threads",
+            "batch_size",
+            "sequence_length",
+            "custom_layer_num",
+            "datetime",
+            "test_times",
+            "QPS",
+            "average_latency_ms",
+            "latency_variance",
+            "latency_90_percentile",
+            "latency_95_percentile",
+            "latency_99_percentile",
         ]
 
         csv_writer = csv.DictWriter(csv_file, fieldnames=column_names)
         csv_writer.writeheader()
         for result in results:
             csv_writer.writerow(result)
 
     logger.info(f"Detail results are saved to csv file: {csv_filename}")
 
 
 def output_summary(results, csv_filename, args):
-    with open(csv_filename, mode="a", newline='') as csv_file:
+    with open(csv_filename, mode="a", newline="", encoding="ascii") as csv_file:
         header_names = [
-            "model_name", "inputs", "custom_layer_num", "engine", "version", "providers", "device", "precision",
-            "optimizer", "io_binding", "threads"
+            "model_name",
+            "inputs",
+            "custom_layer_num",
+            "engine",
+            "version",
+            "providers",
+            "device",
+            "precision",
+            "optimizer",
+            "io_binding",
+            "threads",
         ]
         data_names = []
         for batch_size in args.batch_sizes:
-            for sequence_length in args.sequence_lengths:
-                data_names.append(f"b{batch_size}_s{sequence_length}")
+            if args.sequence_lengths == [""]:
+                data_names.append(f"b{batch_size}")
+            else:
+                for sequence_length in args.sequence_lengths:
+                    data_names.append(f"b{batch_size}_s{sequence_length}")
 
         csv_writer = csv.DictWriter(csv_file, fieldnames=header_names + data_names)
         csv_writer.writeheader()
         for model_name in args.models:
             for input_count in [1, 2, 3]:
                 for engine_name in args.engines:
                     for io_binding in [True, False, ""]:
                         for threads in args.num_threads:
                             row = {}
                             for result in results:
-                                if result["model_name"] == model_name and result["inputs"] == input_count and result[
-                                        "engine"] == engine_name and result["io_binding"] == io_binding and result[
-                                            "threads"] == threads:
+                                if (
+                                    result["model_name"] == model_name
+                                    and result["inputs"] == input_count
+                                    and result["engine"] == engine_name
+                                    and result["io_binding"] == io_binding
+                                    and result["threads"] == threads
+                                ):
                                     headers = {k: v for k, v in result.items() if k in header_names}
                                     if not row:
                                         row.update(headers)
                                         row.update({k: "" for k in data_names})
                                     else:
                                         for k in header_names:
                                             assert row[k] == headers[k]
                                     b = result["batch_size"]
                                     s = result["sequence_length"]
-                                    row[f"b{b}_s{s}"] = result["average_latency_ms"]
+                                    if s:
+                                        row[f"b{b}_s{s}"] = result["average_latency_ms"]
+                                    else:
+                                        row[f"b{b}"] = result["average_latency_ms"]
                             if row:
                                 csv_writer.writerow(row)
 
     logger.info(f"Summary results are saved to csv file: {csv_filename}")
 
 
 def output_fusion_statistics(model_fusion_statistics, csv_filename):
-    from transformers import __version__ as transformers_version
-    with open(csv_filename, mode="a", newline='') as csv_file:
-        column_names = ["model_filename", "datetime", "transformers", "torch"] + list(
-            next(iter(model_fusion_statistics.values())).keys())
+    with open(csv_filename, mode="a", newline="", encoding="ascii") as csv_file:
+        column_names = [
+            "model_filename",
+            "datetime",
+            "transformers",
+            "torch",
+            *list(next(iter(model_fusion_statistics.values())).keys()),
+        ]
         csv_writer = csv.DictWriter(csv_file, fieldnames=column_names)
         csv_writer.writeheader()
-        for key in model_fusion_statistics.keys():
+        for key in model_fusion_statistics:
             model_fusion_statistics[key]["datetime"] = str(datetime.now())
-            model_fusion_statistics[key]["transformers"] = transformers_version
+            model_fusion_statistics[key]["transformers"] = transformers.__version__
             model_fusion_statistics[key]["torch"] = torch.__version__
             model_fusion_statistics[key]["model_filename"] = key
             csv_writer.writerow(model_fusion_statistics[key])
     logger.info(f"Fusion statistics is saved to csv file: {csv_filename}")
 
 
 def inference_ort(ort_session, ort_inputs, result_template, repeat_times, batch_size, warm_up_repeat=0):
     result = {}
     timeit.repeat(lambda: ort_session.run(None, ort_inputs), number=1, repeat=warm_up_repeat)  # Dry run
-    runtimes = timeit.repeat(lambda: ort_session.run(None, ort_inputs), number=1, repeat=repeat_times)
+    latency_list = timeit.repeat(lambda: ort_session.run(None, ort_inputs), number=1, repeat=repeat_times)
     result.update(result_template)
     result.update({"io_binding": False})
-    result.update(get_latency_result(runtimes, batch_size))
+    result.update(get_latency_result(latency_list, batch_size))
     return result
 
 
-def inference_ort_with_io_binding(ort_session,
-                                  ort_inputs,
-                                  result_template,
-                                  repeat_times,
-                                  ort_output_names,
-                                  ort_outputs,
-                                  output_buffers,
-                                  output_buffer_max_sizes,
-                                  batch_size,
-                                  device,
-                                  data_type=numpy.longlong,
-                                  warm_up_repeat=0):
+def inference_ort_with_io_binding(
+    ort_session,
+    ort_inputs,
+    result_template,
+    repeat_times,
+    ort_output_names,
+    ort_outputs,
+    output_buffers,
+    output_buffer_max_sizes,
+    batch_size,
+    device,
+    data_type=numpy.longlong,
+    warm_up_repeat=0,
+):
     result = {}
 
     # Bind inputs and outputs to onnxruntime session
     io_binding = ort_session.io_binding()
     # Bind inputs to device
-    for name in ort_inputs.keys():
+    for name in ort_inputs:
         np_input = torch.from_numpy(ort_inputs[name]).to(device)
-        input_type = IO_BINDING_DATA_TYPE_MAP[str(ort_inputs[name].dtype)] if str(
-            ort_inputs[name].dtype) in IO_BINDING_DATA_TYPE_MAP else data_type
-        io_binding.bind_input(name, np_input.device.type, 0, input_type, np_input.shape, np_input.data_ptr())
+        input_type = (
+            IO_BINDING_DATA_TYPE_MAP[str(ort_inputs[name].dtype)]
+            if str(ort_inputs[name].dtype) in IO_BINDING_DATA_TYPE_MAP
+            else data_type
+        )
+        io_binding.bind_input(
+            name,
+            np_input.device.type,
+            0,
+            input_type,
+            np_input.shape,
+            np_input.data_ptr(),
+        )
     # Bind outputs buffers with the sizes needed if not allocated already
     if len(output_buffers) == 0:
         allocateOutputBuffers(output_buffers, output_buffer_max_sizes, device)
 
-    for i in range(len(ort_output_names)):
-        io_binding.bind_output(ort_output_names[i], output_buffers[i].device.type, 0, numpy.float32,
-                               ort_outputs[i].shape, output_buffers[i].data_ptr())
-    timeit.repeat(lambda: ort_session.run_with_iobinding(io_binding), number=1, repeat=warm_up_repeat)  # Dry run
-    runtimes = timeit.repeat(lambda: ort_session.run_with_iobinding(io_binding), number=1, repeat=repeat_times)
+    for i, ort_output_name in enumerate(ort_output_names):
+        io_binding.bind_output(
+            ort_output_name,
+            output_buffers[i].device.type,
+            0,
+            numpy.float32,
+            ort_outputs[i].shape,
+            output_buffers[i].data_ptr(),
+        )
+
+    timeit.repeat(
+        lambda: ort_session.run_with_iobinding(io_binding),
+        number=1,
+        repeat=warm_up_repeat,
+    )  # Dry run
+
+    latency_list = timeit.repeat(
+        lambda: ort_session.run_with_iobinding(io_binding),
+        number=1,
+        repeat=repeat_times,
+    )
     result.update(result_template)
     result.update({"io_binding": True})
-    result.update(get_latency_result(runtimes, batch_size))
+    result.update(get_latency_result(latency_list, batch_size))
     return result
 
 
-def allocateOutputBuffers(output_buffers, output_buffer_max_sizes, device):
+def allocateOutputBuffers(output_buffers, output_buffer_max_sizes, device):  # noqa: N802
     # Allocate output tensors with the largest test size needed. So the allocated memory can be reused
     # for each test run.
 
     for i in output_buffer_max_sizes:
         output_buffers.append(torch.empty(i, dtype=torch.float32, device=device))
 
 
 def set_random_seed(seed=123):
-    """Set random seed manully to get deterministic results"""
-    import random
+    """Set random seed manually to get deterministic results"""
     random.seed(seed)
     numpy.random.seed(seed)
     torch.manual_seed(seed)
     torch.cuda.manual_seed(seed)
     torch.cuda.manual_seed_all(seed)
-    #torch.backends.cudnn.enabled = False
-    #torch.backends.cudnn.benchmark = False
-    #torch.backends.cudnn.deterministic = True
+    # torch.backends.cudnn.enabled = False
+    # torch.backends.cudnn.benchmark = False
+    # torch.backends.cudnn.deterministic = True
 
 
-def measure_memory(is_gpu, func):
-    import os
-    import psutil
-    from time import sleep
+def get_gpu_info() -> Optional[List[Dict[str, Any]]]:
+    from py3nvml.py3nvml import (
+        NVMLError,
+        nvmlDeviceGetCount,
+        nvmlDeviceGetHandleByIndex,
+        nvmlDeviceGetMemoryInfo,
+        nvmlDeviceGetName,
+        nvmlInit,
+        nvmlShutdown,
+    )
+
+    try:
+        nvmlInit()
+        result = []
+        device_count = nvmlDeviceGetCount()
+        if not isinstance(device_count, int):
+            return None
+
+        for i in range(device_count):
+            info = nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i))
+            if isinstance(info, str):
+                return None
+            result.append(
+                {
+                    "id": i,
+                    "name": nvmlDeviceGetName(nvmlDeviceGetHandleByIndex(i)),
+                    "total": info.total,
+                    "free": info.free,
+                    "used": info.used,
+                }
+            )
+        nvmlShutdown()
+        return result
+    except NVMLError as error:
+        print("Error fetching GPU information using nvml: %s", error)
+        return None
+
 
+def measure_memory(is_gpu, func):
     class MemoryMonitor:
         def __init__(self, keep_measuring=True):
             self.keep_measuring = keep_measuring
 
         def measure_cpu_usage(self):
+            import psutil
+
             max_usage = 0
             while True:
                 max_usage = max(max_usage, psutil.Process(os.getpid()).memory_info().rss / 1024**2)
                 sleep(0.005)  # 5ms
                 if not self.keep_measuring:
                     break
             return max_usage
 
-        def measure_gpu_usage(self):
-            from py3nvml.py3nvml import nvmlInit, nvmlDeviceGetCount, nvmlDeviceGetHandleByIndex, \
-                                 nvmlDeviceGetMemoryInfo, nvmlDeviceGetName, nvmlShutdown, NVMLError
+        def measure_gpu_usage(self) -> Optional[List[Dict[str, Any]]]:
+            from py3nvml.py3nvml import (
+                NVMLError,
+                nvmlDeviceGetCount,
+                nvmlDeviceGetHandleByIndex,
+                nvmlDeviceGetMemoryInfo,
+                nvmlDeviceGetName,
+                nvmlInit,
+                nvmlShutdown,
+            )
+
             max_gpu_usage = []
             gpu_name = []
             try:
                 nvmlInit()
-                deviceCount = nvmlDeviceGetCount()
-                max_gpu_usage = [0 for i in range(deviceCount)]
-                gpu_name = [nvmlDeviceGetName(nvmlDeviceGetHandleByIndex(i)) for i in range(deviceCount)]
+                device_count = nvmlDeviceGetCount()
+                if not isinstance(device_count, int):
+                    logger.error(f"nvmlDeviceGetCount result is not integer: {device_count}")
+                    return None
+
+                max_gpu_usage = [0 for i in range(device_count)]
+                gpu_name = [nvmlDeviceGetName(nvmlDeviceGetHandleByIndex(i)) for i in range(device_count)]
                 while True:
-                    for i in range(deviceCount):
+                    for i in range(device_count):
                         info = nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i))
+                        if isinstance(info, str):
+                            logger.error(f"nvmlDeviceGetMemoryInfo returns str: {info}")
+                            return None
                         max_gpu_usage[i] = max(max_gpu_usage[i], info.used / 1024**2)
                     sleep(0.005)  # 5ms
                     if not self.keep_measuring:
                         break
                 nvmlShutdown()
-                return [{
-                    "device_id": i,
-                    "name": gpu_name[i],
-                    "max_used_MB": max_gpu_usage[i]
-                } for i in range(deviceCount)]
+                return [
+                    {
+                        "device_id": i,
+                        "name": gpu_name[i],
+                        "max_used_MB": max_gpu_usage[i],
+                    }
+                    for i in range(device_count)
+                ]
             except NVMLError as error:
-                if not self.silent:
-                    self.logger.error("Error fetching GPU information using nvml: %s", error)
+                logger.error("Error fetching GPU information using nvml: %s", error)
                 return None
 
     monitor = MemoryMonitor(False)
 
-    memory_before_test = monitor.measure_gpu_usage() if is_gpu else monitor.measure_cpu_usage()
+    if is_gpu:
+        memory_before_test = monitor.measure_gpu_usage()
+        if memory_before_test is None:
+            return None
+
+        with ThreadPoolExecutor() as executor:
+            monitor = MemoryMonitor()
+            mem_thread = executor.submit(monitor.measure_gpu_usage)
+            try:
+                fn_thread = executor.submit(func)
+                _ = fn_thread.result()
+            finally:
+                monitor.keep_measuring = False
+                max_usage = mem_thread.result()
+
+            if max_usage is None:
+                return None
+
+            print(f"GPU memory usage: before={memory_before_test}  peak={max_usage}")
+            if len(memory_before_test) >= 1 and len(max_usage) >= 1 and len(memory_before_test) == len(max_usage):
+                # When there are multiple GPUs, we will check the one with maximum usage.
+                max_used = 0
+                for i, memory_before in enumerate(memory_before_test):
+                    before = memory_before["max_used_MB"]
+                    after = max_usage[i]["max_used_MB"]
+                    used = after - before
+                    max_used = max(max_used, used)
+                return max_used
+        return None
+
+    # CPU memory
+    memory_before_test = monitor.measure_cpu_usage()
 
-    from concurrent.futures import ThreadPoolExecutor
     with ThreadPoolExecutor() as executor:
         monitor = MemoryMonitor()
-        mem_thread = executor.submit(monitor.measure_gpu_usage if is_gpu else monitor.measure_cpu_usage)
+        mem_thread = executor.submit(monitor.measure_cpu_usage)
         try:
             fn_thread = executor.submit(func)
-            result = fn_thread.result()
+            _ = fn_thread.result()
         finally:
             monitor.keep_measuring = False
             max_usage = mem_thread.result()
 
-        if is_gpu:
-            print(f"GPU memory usage: before={memory_before_test}  peak={max_usage}")
-            if len(memory_before_test) >= 1 and len(max_usage) >= 1:
-                before = memory_before_test[0]["max_used_MB"]
-                after = max_usage[0]["max_used_MB"]
-                return after - before
-            else:
-                return None
-        else:
-            print(f"CPU memory usage: before={memory_before_test:.1f} MB, peak={max_usage:.1f} MB")
-            return max_usage - memory_before_test
+        print(f"CPU memory usage: before={memory_before_test:.1f} MB, peak={max_usage:.1f} MB")
+        return max_usage - memory_before_test
+
+
+def get_ort_environment_variables():
+    # Environment variables might impact ORT performance on transformer models. Note that they are for testing only.
+    env_names = [
+        "ORT_DISABLE_FUSED_ATTENTION",
+        "ORT_ENABLE_FUSED_CAUSAL_ATTENTION",
+        "ORT_DISABLE_FUSED_CROSS_ATTENTION",
+        "ORT_DISABLE_TRT_FLASH_ATTENTION",
+        "ORT_DISABLE_MEMORY_EFFICIENT_ATTENTION",
+        "ORT_TRANSFORMER_OPTIONS",
+        "ORT_CUDA_GEMM_OPTIONS",
+    ]
+    env = ""
+    for name in env_names:
+        value = os.getenv(name)
+        if value is None:
+            continue
+        if env:
+            env += ","
+        env += f"{name}={value}"
+    return env
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/bert_perf_test.py` & `onnxruntime/transformers/bert_perf_test.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,161 +1,199 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 # This tool measures the inference performance of onnxruntime or onnxruntime-gpu python package on Bert model.
 
 # The input model shall have exactly three inputs. The model is either fully optimized (with EmbedLayerNormalization node),
 # or with reasonable input names (one input name has 'mask' substring, another has 'token' or 'segment' substring).
 # See get_bert_inputs function in bert_test_data.py for more information.
 
 # Example command to run test on batch_size 1 and 2 for a model on GPU:
 #   python bert_perf_test.py --model bert.onnx --batch_size 1 2 --sequence_length 128 --use_gpu --samples 1000 --test_times 1
 
-import sys
 import argparse
-import os
-from pathlib import Path
-import timeit
-import statistics
-import psutil
 import csv
-import numpy as np
-import torch
+import json
+import multiprocessing
+import os
 import random
+import statistics
+import timeit
+from dataclasses import dataclass
 from datetime import datetime
-import multiprocessing
-from bert_test_data import get_bert_inputs, generate_test_data
+from pathlib import Path
+from typing import Optional
 
-from dataclasses import dataclass
+import numpy as np
+import psutil
+import torch
+from bert_test_data import generate_test_data, get_bert_inputs
 
 
 @dataclass
 class TestSetting:
     batch_size: int
     sequence_length: int
     test_cases: int
     test_times: int
     use_gpu: bool
     use_io_binding: bool
     provider: str
     intra_op_num_threads: int
     seed: int
     verbose: bool
+    log_severity: int
 
 
 @dataclass
 class ModelSetting:
     model_path: str
     input_ids_name: str
     segment_ids_name: str
     input_mask_name: str
     opt_level: int
+    input_tuning_results: Optional[str]
+    output_tuning_results: Optional[str]
 
 
-def create_session(model_path, use_gpu, provider, intra_op_num_threads, graph_optimization_level=None):
+def create_session(
+    model_path,
+    use_gpu,
+    provider,
+    intra_op_num_threads,
+    graph_optimization_level=None,
+    log_severity=2,
+    tuning_results_path=None,
+):
     import onnxruntime
 
-    if use_gpu and ('CUDAExecutionProvider' not in onnxruntime.get_available_providers()):
+    onnxruntime.set_default_logger_severity(log_severity)
+
+    if use_gpu and ("CUDAExecutionProvider" not in onnxruntime.get_available_providers()):
         print(
             "Warning: Please install onnxruntime-gpu package instead of onnxruntime, and use a machine with GPU for testing gpu performance."
         )
 
-    if intra_op_num_threads is None and graph_optimization_level is None:
-        session = onnxruntime.InferenceSession(model_path)
-    else:
-        if use_gpu:
-            if provider == 'dml':
-                execution_providers = ['DmlExecutionProvider', 'CPUExecutionProvider']
-            elif provider == 'rocm':
-                execution_providers = ['ROCMExecutionProvider', 'CPUExecutionProvider']
-            elif provider == 'migraphx':
-                execution_providers = ['MIGraphXExecutionProvider', 'ROCMExecutionProvider', 'CPUExecutionProvider']
-            elif provider == 'cuda':
-                execution_providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
-            elif provider == 'tensorrt':
-                execution_providers = ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
-            else:
-                execution_providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
+    if use_gpu:
+        if provider == "dml":
+            execution_providers = ["DmlExecutionProvider", "CPUExecutionProvider"]
+        elif provider == "rocm":
+            execution_providers = ["ROCMExecutionProvider", "CPUExecutionProvider"]
+        elif provider == "migraphx":
+            execution_providers = [
+                "MIGraphXExecutionProvider",
+                "ROCMExecutionProvider",
+                "CPUExecutionProvider",
+            ]
+        elif provider == "cuda":
+            execution_providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
+        elif provider == "tensorrt":
+            execution_providers = [
+                "TensorrtExecutionProvider",
+                "CUDAExecutionProvider",
+                "CPUExecutionProvider",
+            ]
         else:
-            execution_providers = ['CPUExecutionProvider']
-
-        sess_options = onnxruntime.SessionOptions()
-        sess_options.execution_mode = onnxruntime.ExecutionMode.ORT_SEQUENTIAL
+            execution_providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
+    else:
+        execution_providers = ["CPUExecutionProvider"]
 
-        if graph_optimization_level is None:
-            sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
-        elif graph_optimization_level == 0:
-            sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL
-        elif graph_optimization_level == 1:
-            sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_BASIC
-        elif graph_optimization_level == 2:
-            sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_EXTENDED
-        elif graph_optimization_level == 99:
-            sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
-        else:
-            sess_options.graph_optimization_level = graph_optimization_level
+    sess_options = onnxruntime.SessionOptions()
+    sess_options.log_severity_level = log_severity
+    sess_options.execution_mode = onnxruntime.ExecutionMode.ORT_SEQUENTIAL
+
+    if graph_optimization_level is None:
+        sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
+    elif graph_optimization_level == 0:
+        sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL
+    elif graph_optimization_level == 1:
+        sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_BASIC
+    elif graph_optimization_level == 2:
+        sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_EXTENDED
+    elif graph_optimization_level == 99:
+        sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
+    else:
+        sess_options.graph_optimization_level = graph_optimization_level
 
-        if intra_op_num_threads is not None:
-            sess_options.intra_op_num_threads = intra_op_num_threads
+    if intra_op_num_threads is not None:
+        sess_options.intra_op_num_threads = intra_op_num_threads
 
-        session = onnxruntime.InferenceSession(model_path, sess_options, providers=execution_providers)
+    session = onnxruntime.InferenceSession(model_path, sess_options, providers=execution_providers)
 
     if use_gpu:
-        if provider == 'dml':
-            assert 'DmlExecutionProvider' in session.get_providers()
-        elif provider == 'rocm':
-            assert 'ROCMExecutionProvider' in session.get_providers()
-        elif provider == 'migraphx':
-            assert 'MIGraphXExecutionProvider' in session.get_providers()
-            assert 'ROCMExecutionProvider' in session.get_providers()
-        elif provider == 'cuda':
-            assert 'CUDAExecutionProvider' in session.get_providers()
-        elif provider == 'tensorrt':
-            assert 'TensorrtExecutionProvider' in session.get_providers()
-            assert 'CUDAExecutionProvider' in session.get_providers()
+        if provider == "dml":
+            assert "DmlExecutionProvider" in session.get_providers()
+        elif provider == "rocm":
+            assert "ROCMExecutionProvider" in session.get_providers()
+        elif provider == "migraphx":
+            assert "MIGraphXExecutionProvider" in session.get_providers()
+            assert "ROCMExecutionProvider" in session.get_providers()
+        elif provider == "cuda":
+            assert "CUDAExecutionProvider" in session.get_providers()
+        elif provider == "tensorrt":
+            assert "TensorrtExecutionProvider" in session.get_providers()
+            assert "CUDAExecutionProvider" in session.get_providers()
         else:
-            assert 'CUDAExecutionProvider' in session.get_providers()
+            assert "CUDAExecutionProvider" in session.get_providers()
     else:
-        assert 'CPUExecutionProvider' in session.get_providers()
+        assert "CPUExecutionProvider" in session.get_providers()
+
+    if tuning_results_path is not None:
+        with open(tuning_results_path) as f:
+            session.set_tuning_results(json.load(f))
 
     return session
 
+
 def numpy_type(torch_type):
-    type_map = {torch.float32: np.float32,
-                torch.float16: np.float16,
-                torch.int32: np.int32,
-                torch.int64: np.longlong}
+    type_map = {
+        torch.float32: np.float32,
+        torch.float16: np.float16,
+        torch.int32: np.int32,
+        torch.int64: np.longlong,
+    }
     return type_map[torch_type]
 
+
 def create_input_output_tensors(inputs, outputs, device):
-    input_tensors = {name: torch.from_numpy(array).to(device)
-                        for name, array in inputs.items()}
-    output_tensors = {name: torch.from_numpy(array).to(device)
-                        for name, array in outputs.items()}
+    input_tensors = {name: torch.from_numpy(array).to(device) for name, array in inputs.items()}
+    output_tensors = {name: torch.from_numpy(array).to(device) for name, array in outputs.items()}
     return input_tensors, output_tensors
 
+
 def create_io_binding(sess, input_tensors, output_tensors):
     io_binding = sess.io_binding()
     for name, tensor in input_tensors.items():
-        io_binding.bind_input(name, tensor.device.type, 0,
-                                numpy_type(tensor.dtype), tensor.shape,
-                                tensor.data_ptr())
+        io_binding.bind_input(
+            name,
+            tensor.device.type,
+            0,
+            numpy_type(tensor.dtype),
+            tensor.shape,
+            tensor.data_ptr(),
+        )
     for name, tensor in output_tensors.items():
-        io_binding.bind_output(name, tensor.device.type, 0,
-                                numpy_type(tensor.dtype), tensor.shape,
-                                tensor.data_ptr())
+        io_binding.bind_output(
+            name,
+            tensor.device.type,
+            0,
+            numpy_type(tensor.dtype),
+            tensor.shape,
+            tensor.data_ptr(),
+        )
     return io_binding
 
+
 def onnxruntime_inference_with_io_binding(session, all_inputs, output_names, test_setting):
     results = []
     latency_list = []
-    device = 'cuda' if test_setting.use_gpu else 'cpu'
-    for test_case_id, inputs in enumerate(all_inputs):
+    device = "cuda" if test_setting.use_gpu else "cpu"
+    for _test_case_id, inputs in enumerate(all_inputs):
         result = session.run(output_names, inputs)
         results.append(result)
         outputs = {}
         for i in range(len(output_names)):
             outputs[output_names[i]] = result[i]
 
         input_tensors, output_tensors = create_input_output_tensors(inputs, outputs, device)
@@ -167,190 +205,305 @@
         start_time = timeit.default_timer()
         session.run_with_iobinding(io_binding)
         latency = timeit.default_timer() - start_time
         latency_list.append(latency)
 
     return results, latency_list
 
+
 def onnxruntime_inference(session, all_inputs, output_names):
     if len(all_inputs) > 0:
         # Use a random input as warm up.
         session.run(output_names, random.choice(all_inputs))
 
     results = []
     latency_list = []
-    for test_case_id, inputs in enumerate(all_inputs):
+    for _test_case_id, inputs in enumerate(all_inputs):
         start_time = timeit.default_timer()
         result = session.run(output_names, inputs)
         latency = timeit.default_timer() - start_time
         results.append(result)
         latency_list.append(latency)
     return results, latency_list
 
+
 def to_string(model_path, session, test_setting):
     sess_options = session.get_session_options()
-    option = "model={},".format(os.path.basename(model_path))
-    option += "graph_optimization_level={},intra_op_num_threads={},".format(sess_options.graph_optimization_level,
-                                                                            sess_options.intra_op_num_threads).replace(
-                                                                                'GraphOptimizationLevel.ORT_', '')
+    option = f"model={os.path.basename(model_path)},"
+    option += "graph_optimization_level={},intra_op_num_threads={},".format(
+        sess_options.graph_optimization_level, sess_options.intra_op_num_threads
+    ).replace("GraphOptimizationLevel.ORT_", "")
     option += f"batch_size={test_setting.batch_size},sequence_length={test_setting.sequence_length},test_cases={test_setting.test_cases},test_times={test_setting.test_times},use_gpu={test_setting.use_gpu}"
     return option
 
 
 def run_one_test(model_setting, test_setting, perf_results, all_inputs, intra_op_num_threads):
-    session = create_session(model_setting.model_path, test_setting.use_gpu, test_setting.provider, intra_op_num_threads,
-                             model_setting.opt_level)
+    session = create_session(
+        model_setting.model_path,
+        test_setting.use_gpu,
+        test_setting.provider,
+        intra_op_num_threads,
+        model_setting.opt_level,
+        log_severity=test_setting.log_severity,
+        tuning_results_path=model_setting.input_tuning_results,
+    )
     output_names = [output.name for output in session.get_outputs()]
 
     key = to_string(model_setting.model_path, session, test_setting)
     if key in perf_results:
         print("skip duplicated test:", key)
         return
 
     print("Running test:", key)
 
     all_latency_list = []
     if test_setting.use_io_binding:
-        for i in range(test_setting.test_times):
-            results, latency_list = onnxruntime_inference_with_io_binding(session, all_inputs, output_names, test_setting)
+        for _i in range(test_setting.test_times):
+            results, latency_list = onnxruntime_inference_with_io_binding(
+                session, all_inputs, output_names, test_setting
+            )
             all_latency_list.extend(latency_list)
     else:
-        for i in range(test_setting.test_times):
+        for _i in range(test_setting.test_times):
             results, latency_list = onnxruntime_inference(session, all_inputs, output_names)
             all_latency_list.extend(latency_list)
 
     # latency in miliseconds
     latency_ms = np.array(all_latency_list) * 1000
 
     average_latency = statistics.mean(latency_ms)
     latency_50 = np.percentile(latency_ms, 50)
     latency_75 = np.percentile(latency_ms, 75)
     latency_90 = np.percentile(latency_ms, 90)
     latency_95 = np.percentile(latency_ms, 95)
     latency_99 = np.percentile(latency_ms, 99)
     throughput = test_setting.batch_size * (1000.0 / average_latency)
 
-    perf_results[key] = (average_latency, latency_50, latency_75, latency_90, latency_95, latency_99, throughput)
+    perf_results[key] = (
+        average_latency,
+        latency_50,
+        latency_75,
+        latency_90,
+        latency_95,
+        latency_99,
+        throughput,
+    )
+
+    print(
+        "Average latency = {} ms, Throughput = {} QPS".format(format(average_latency, ".2f"), format(throughput, ".2f"))
+    )
 
-    print("Average latency = {} ms, Throughput = {} QPS".format(format(average_latency, '.2f'),
-                                                                format(throughput, '.2f')))
+    if model_setting.output_tuning_results:
+        output_path = os.path.abspath(model_setting.output_tuning_results)
+        if os.path.exists(output_path):
+            old_output_path = output_path
+            output_path = f"""{output_path.rsplit(".json", 1)[0]}.{datetime.now().timestamp()}.json"""
+            print("WARNING:", old_output_path, "exists, will write to", output_path, "instead.")
+
+        trs = session.get_tuning_results()
+        with open(output_path, "w") as f:
+            json.dump(trs, f)
+        print("Tuning results is saved to", output_path)
 
 
 def launch_test(model_setting, test_setting, perf_results, all_inputs, intra_op_num_threads):
-    process = multiprocessing.Process(target=run_one_test,
-                                      args=(model_setting, test_setting, perf_results, all_inputs,
-                                            intra_op_num_threads))
+    process = multiprocessing.Process(
+        target=run_one_test,
+        args=(
+            model_setting,
+            test_setting,
+            perf_results,
+            all_inputs,
+            intra_op_num_threads,
+        ),
+    )
     process.start()
     process.join()
 
 
 def run_perf_tests(model_setting, test_setting, perf_results, all_inputs):
-    if (test_setting.intra_op_num_threads is not None):
-        launch_test(model_setting, test_setting, perf_results, all_inputs, test_setting.intra_op_num_threads)
+    if test_setting.intra_op_num_threads is not None:
+        launch_test(
+            model_setting,
+            test_setting,
+            perf_results,
+            all_inputs,
+            test_setting.intra_op_num_threads,
+        )
         return
 
     cpu_count = psutil.cpu_count(logical=False)
     logical_cores = psutil.cpu_count(logical=True)
 
-    candidate_threads = list(set([logical_cores, cpu_count]))
+    candidate_threads = list({logical_cores, cpu_count})
     for i in range(1, min(16, logical_cores)):
         if i not in candidate_threads:
             candidate_threads.append(i)
     candidate_threads.sort(reverse=True)
 
     for intra_op_num_threads in candidate_threads:
         launch_test(model_setting, test_setting, perf_results, all_inputs, intra_op_num_threads)
 
 
 def run_performance(model_setting, test_setting, perf_results):
-    input_ids, segment_ids, input_mask = get_bert_inputs(model_setting.model_path, model_setting.input_ids_name,
-                                                         model_setting.segment_ids_name, model_setting.input_mask_name)
+    input_ids, segment_ids, input_mask = get_bert_inputs(
+        model_setting.model_path,
+        model_setting.input_ids_name,
+        model_setting.segment_ids_name,
+        model_setting.input_mask_name,
+    )
 
     # Do not generate random mask for performance test.
     print(
         f"Generating {test_setting.test_cases} samples for batch_size={test_setting.batch_size} sequence_length={test_setting.sequence_length}"
     )
-    all_inputs = generate_test_data(test_setting.batch_size,
-                                    test_setting.sequence_length,
-                                    test_setting.test_cases,
-                                    test_setting.seed,
-                                    test_setting.verbose,
-                                    input_ids,
-                                    segment_ids,
-                                    input_mask,
-                                    random_mask_length=False)
+    all_inputs = generate_test_data(
+        test_setting.batch_size,
+        test_setting.sequence_length,
+        test_setting.test_cases,
+        test_setting.seed,
+        test_setting.verbose,
+        input_ids,
+        segment_ids,
+        input_mask,
+        random_mask_length=False,
+    )
 
     run_perf_tests(model_setting, test_setting, perf_results, all_inputs)
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
-    parser.add_argument('--model', required=True, type=str, help="bert onnx model path")
+    parser.add_argument("--model", required=True, type=str, help="bert onnx model path")
 
-    parser.add_argument('-b',
-                        '--batch_size',
-                        required=True,
-                        type=int,
-                        nargs="+",
-                        help="batch size of input. Allow one or multiple values in the range of [1, 128].")
+    parser.add_argument(
+        "-b",
+        "--batch_size",
+        required=True,
+        type=int,
+        nargs="+",
+        help="batch size of input. Allow one or multiple values in the range of [1, 128].",
+    )
 
-    parser.add_argument('-s', '--sequence_length', required=True, type=int, help="maximum sequence length of input")
+    parser.add_argument(
+        "-s",
+        "--sequence_length",
+        required=True,
+        type=int,
+        help="maximum sequence length of input",
+    )
 
-    parser.add_argument('--samples', required=False, type=int, default=10, help="number of samples to be generated")
+    parser.add_argument(
+        "--samples",
+        required=False,
+        type=int,
+        default=10,
+        help="number of samples to be generated",
+    )
 
-    parser.add_argument('-t',
-                        '--test_times',
-                        required=False,
-                        type=int,
-                        default=0,
-                        help="number of times to run per sample. By default, the value is 1000 / samples")
+    parser.add_argument(
+        "-t",
+        "--test_times",
+        required=False,
+        type=int,
+        default=0,
+        help="number of times to run per sample. By default, the value is 1000 / samples",
+    )
 
     parser.add_argument(
-        '--opt_level',
+        "--opt_level",
         required=False,
         type=int,
         choices=[0, 1, 2, 99],
         default=99,
-        help="onnxruntime optimization level: 0 - disable all, 1 - basic, 2 - extended, 99 - enable all.")
+        help="onnxruntime optimization level: 0 - disable all, 1 - basic, 2 - extended, 99 - enable all.",
+    )
 
-    parser.add_argument('--seed',
-                        required=False,
-                        type=int,
-                        default=3,
-                        help="random seed. Use the same seed to make sure test data is same in multiple tests.")
+    parser.add_argument(
+        "--seed",
+        required=False,
+        type=int,
+        default=3,
+        help="random seed. Use the same seed to make sure test data is same in multiple tests.",
+    )
 
-    parser.add_argument('--verbose', required=False, action='store_true', help="print verbose information")
+    parser.add_argument(
+        "--verbose",
+        required=False,
+        action="store_true",
+        help="print verbose information",
+    )
     parser.set_defaults(verbose=False)
 
-    parser.add_argument('--use_gpu', required=False, action='store_true', help="use GPU")
+    parser.add_argument(
+        "--log_severity",
+        required=False,
+        type=int,
+        default=2,
+        choices=[0, 1, 2, 3, 4],
+        help="0:Verbose, 1:Info, 2:Warning, 3:Error, 4:Fatal",
+    )
+
+    parser.add_argument("--use_gpu", required=False, action="store_true", help="use GPU")
     parser.set_defaults(use_gpu=False)
 
-    parser.add_argument('--use_io_binding', required=False, action='store_true', help="use io_binding")
+    parser.add_argument("--use_io_binding", required=False, action="store_true", help="use io_binding")
     parser.set_defaults(use_io_binding=False)
 
-    parser.add_argument("--provider",
-                        required=False,
-                        type=str,
-                        default=None,
-                        help="Execution provider to use")
-
-    parser.add_argument('-n',
-                        '--intra_op_num_threads',
-                        required=False,
-                        type=int,
-                        default=None,
-                        help=">=0, set intra_op_num_threads")
-
-    parser.add_argument('--input_ids_name', required=False, type=str, default=None, help="input name for input ids")
-    parser.add_argument('--segment_ids_name', required=False, type=str, default=None, help="input name for segment ids")
-    parser.add_argument('--input_mask_name',
-                        required=False,
-                        type=str,
-                        default=None,
-                        help="input name for attention mask")
+    parser.add_argument(
+        "--provider",
+        required=False,
+        type=str,
+        default=None,
+        help="Execution provider to use",
+    )
+
+    parser.add_argument(
+        "-n",
+        "--intra_op_num_threads",
+        required=False,
+        type=int,
+        default=None,
+        help=">=0, set intra_op_num_threads",
+    )
+
+    parser.add_argument(
+        "--input_ids_name",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for input ids",
+    )
+    parser.add_argument(
+        "--segment_ids_name",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for segment ids",
+    )
+    parser.add_argument(
+        "--input_mask_name",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for attention mask",
+    )
+
+    parser.add_argument(
+        "--input_tuning_results",
+        default=None,
+        type=str,
+        help="tuning results (json) to be loaded before benchmark",
+    )
+    parser.add_argument(
+        "--output_tuning_results",
+        default=None,
+        type=str,
+        help="tuning results (json) to be saved after benchmark",
+    )
 
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_arguments()
@@ -361,48 +514,74 @@
     manager = multiprocessing.Manager()
     perf_results = manager.dict()
 
     batch_size_set = set(args.batch_size)
     if not min(batch_size_set) >= 1 and max(batch_size_set) <= 128:
         raise Exception("batch_size not in range [1, 128]")
 
-    model_setting = ModelSetting(args.model, args.input_ids_name, args.segment_ids_name, args.input_mask_name,
-                                 args.opt_level)
+    model_setting = ModelSetting(
+        args.model,
+        args.input_ids_name,
+        args.segment_ids_name,
+        args.input_mask_name,
+        args.opt_level,
+        args.input_tuning_results,
+        args.output_tuning_results,
+    )
 
     for batch_size in batch_size_set:
-        test_setting = TestSetting(batch_size, args.sequence_length, args.samples, args.test_times, args.use_gpu, args.use_io_binding,
-                                   args.provider, args.intra_op_num_threads, args.seed, args.verbose)
+        test_setting = TestSetting(
+            batch_size,
+            args.sequence_length,
+            args.samples,
+            args.test_times,
+            args.use_gpu,
+            args.use_io_binding,
+            args.provider,
+            args.intra_op_num_threads,
+            args.seed,
+            args.verbose,
+            args.log_severity,
+        )
 
         print("test setting", test_setting)
         run_performance(model_setting, test_setting, perf_results)
 
     # Sort the results so that the first one has smallest latency.
     sorted_results = sorted(perf_results.items(), reverse=False, key=lambda x: x[1])
 
     summary_file = os.path.join(
         Path(args.model).parent,
-        "perf_results_{}_B{}_S{}_{}.txt".format('GPU' if args.use_gpu else 'CPU',
-                                                "-".join([str(x) for x in sorted(list(batch_size_set))]),
-                                                args.sequence_length,
-                                                datetime.now().strftime("%Y%m%d-%H%M%S")))
-    with open(summary_file, 'w+', newline='') as tsv_file:
-        tsv_writer = csv.writer(tsv_file, delimiter='\t', lineterminator='\n')
+        "perf_results_{}_B{}_S{}_{}.txt".format(
+            "GPU" if args.use_gpu else "CPU",
+            "-".join([str(x) for x in sorted(list(batch_size_set))]),
+            args.sequence_length,
+            datetime.now().strftime("%Y%m%d-%H%M%S"),
+        ),
+    )
+    with open(summary_file, "w+", newline="") as tsv_file:
+        tsv_writer = csv.writer(tsv_file, delimiter="\t", lineterminator="\n")
         headers = None
-        for (key, perf_result) in sorted_results:
-            params = key.split(',')
+        for key, perf_result in sorted_results:
+            params = key.split(",")
             if headers is None:
                 headers = [
-                    "Latency(ms)", "Latency_P50", "Latency_P75", "Latency_P90", "Latency_P95", "Latency_P99",
-                    "Throughput(QPS)"
+                    "Latency(ms)",
+                    "Latency_P50",
+                    "Latency_P75",
+                    "Latency_P90",
+                    "Latency_P95",
+                    "Latency_P99",
+                    "Throughput(QPS)",
                 ]
-                headers.extend([x.split('=')[0] for x in params])
+                headers.extend([x.split("=")[0] for x in params])
                 tsv_writer.writerow(headers)
 
-            values = [format(x, '.2f') for x in perf_result]
-            values.extend([x.split('=')[1] for x in params])
+            values = [format(x, ".2f") for x in perf_result]
+            values.extend([x.split("=")[1] for x in params])
             tsv_writer.writerow(values)
 
     print("Test summary is saved to", summary_file)
 
 
 if __name__ == "__main__":
     # work around for AnaConda Jupyter. See https://stackoverflow.com/questions/45720153/python-multiprocessing-error-attributeerror-module-main-has-no-attribute
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/bert_test_data.py` & `onnxruntime/transformers/bert_test_data.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,158 +1,185 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 # It is a tool to generate test data for a bert model.
 # The test data can be used by onnxruntime_perf_test tool to evaluate the inference latency.
 
-import sys
 import argparse
-import numpy as np
 import os
 import random
 from pathlib import Path
-from typing import List, Dict, Tuple, Union
+from typing import Dict, Optional, Tuple
+
+import numpy as np
 from onnx import ModelProto, TensorProto, numpy_helper
 from onnx_model import OnnxModel
 
 
-def fake_input_ids_data(input_ids: TensorProto, batch_size: int, sequence_length: int,
-                        dictionary_size: int) -> np.ndarray:
+def fake_input_ids_data(
+    input_ids: TensorProto, batch_size: int, sequence_length: int, dictionary_size: int
+) -> np.ndarray:
     """Create input tensor based on the graph input of input_ids
 
     Args:
         input_ids (TensorProto): graph input of the input_ids input tensor
         batch_size (int): batch size
         sequence_length (int): sequence length
-        dictionary_size (int): vacaburary size of dictionary
+        dictionary_size (int): vocabulary size of dictionary
 
     Returns:
         np.ndarray: the input tensor created
     """
-    assert input_ids.type.tensor_type.elem_type in [TensorProto.FLOAT, TensorProto.INT32, TensorProto.INT64]
+    assert input_ids.type.tensor_type.elem_type in [
+        TensorProto.FLOAT,
+        TensorProto.INT32,
+        TensorProto.INT64,
+    ]
 
     data = np.random.randint(dictionary_size, size=(batch_size, sequence_length), dtype=np.int32)
 
     if input_ids.type.tensor_type.elem_type == TensorProto.FLOAT:
         data = np.float32(data)
     elif input_ids.type.tensor_type.elem_type == TensorProto.INT64:
         data = np.int64(data)
 
     return data
 
 
 def fake_segment_ids_data(segment_ids: TensorProto, batch_size: int, sequence_length: int) -> np.ndarray:
-    """Create input tensor based on the graph input of segment_ids 
+    """Create input tensor based on the graph input of segment_ids
 
     Args:
         segment_ids (TensorProto): graph input of the token_type_ids input tensor
         batch_size (int): batch size
         sequence_length (int): sequence length
 
     Returns:
         np.ndarray: the input tensor created
     """
-    assert segment_ids.type.tensor_type.elem_type in [TensorProto.FLOAT, TensorProto.INT32, TensorProto.INT64]
+    assert segment_ids.type.tensor_type.elem_type in [
+        TensorProto.FLOAT,
+        TensorProto.INT32,
+        TensorProto.INT64,
+    ]
 
     data = np.zeros((batch_size, sequence_length), dtype=np.int32)
 
     if segment_ids.type.tensor_type.elem_type == TensorProto.FLOAT:
         data = np.float32(data)
     elif segment_ids.type.tensor_type.elem_type == TensorProto.INT64:
         data = np.int64(data)
 
     return data
 
 
-def fake_input_mask_data(input_mask: TensorProto, batch_size: int, sequence_length: int,
-                         random_mask_length: bool) -> np.ndarray:
+def fake_input_mask_data(
+    input_mask: TensorProto,
+    batch_size: int,
+    sequence_length: int,
+    random_mask_length: bool,
+) -> np.ndarray:
     """Create input tensor based on the graph input of segment_ids.
 
     Args:
         input_mask (TensorProto): graph input of the attention mask input tensor
         batch_size (int): batch size
         sequence_length (int): sequence length
         random_mask_length (bool): whether mask according to random padding length
 
     Returns:
         np.ndarray: the input tensor created
     """
 
-    assert input_mask.type.tensor_type.elem_type in [TensorProto.FLOAT, TensorProto.INT32, TensorProto.INT64]
+    assert input_mask.type.tensor_type.elem_type in [
+        TensorProto.FLOAT,
+        TensorProto.INT32,
+        TensorProto.INT64,
+    ]
 
     if random_mask_length:
         actual_seq_len = random.randint(int(sequence_length * 2 / 3), sequence_length)
         data = np.zeros((batch_size, sequence_length), dtype=np.int32)
         temp = np.ones((batch_size, actual_seq_len), dtype=np.int32)
-        data[:temp.shape[0], :temp.shape[1]] = temp
+        data[: temp.shape[0], : temp.shape[1]] = temp
     else:
         data = np.ones((batch_size, sequence_length), dtype=np.int32)
 
     if input_mask.type.tensor_type.elem_type == TensorProto.FLOAT:
         data = np.float32(data)
     elif input_mask.type.tensor_type.elem_type == TensorProto.INT64:
         data = np.int64(data)
 
     return data
 
 
-def output_test_data(dir: str, inputs: np.ndarray):
+def output_test_data(directory: str, inputs: Dict[str, np.ndarray]):
     """Output input tensors of test data to a directory
 
     Args:
-        dir (str): path of a directory
-        inputs (numpy.ndarray): numpy array
+        directory (str): path of a directory
+        inputs (Dict[str, np.ndarray]): map from input name to value
     """
-    if not os.path.exists(dir):
+    if not os.path.exists(directory):
         try:
-            os.mkdir(dir)
+            os.mkdir(directory)
         except OSError:
-            print("Creation of the directory %s failed" % dir)
+            print("Creation of the directory %s failed" % directory)
         else:
-            print("Successfully created the directory %s " % dir)
+            print("Successfully created the directory %s " % directory)
     else:
-        print("Warning: directory %s existed. Files will be overwritten." % dir)
+        print("Warning: directory %s existed. Files will be overwritten." % directory)
 
     index = 0
     for name, data in inputs.items():
         tensor = numpy_helper.from_array(data, name)
-        with open(os.path.join(dir, 'input_{}.pb'.format(index)), 'wb') as f:
-            f.write(tensor.SerializeToString())
+        with open(os.path.join(directory, f"input_{index}.pb"), "wb") as file:
+            file.write(tensor.SerializeToString())
         index += 1
 
 
-def fake_test_data(batch_size: int, sequence_length: int, test_cases: int, dictionary_size: int, verbose: bool,
-                   random_seed: int, input_ids: TensorProto, segment_ids: TensorProto, input_mask: TensorProto,
-                   random_mask_length: bool):
+def fake_test_data(
+    batch_size: int,
+    sequence_length: int,
+    test_cases: int,
+    dictionary_size: int,
+    verbose: bool,
+    random_seed: int,
+    input_ids: TensorProto,
+    segment_ids: TensorProto,
+    input_mask: TensorProto,
+    random_mask_length: bool,
+):
     """Create given number of input data for testing
 
     Args:
         batch_size (int): batch size
         sequence_length (int): sequence length
         test_cases (int): number of test cases
-        dictionary_size (int): vocaburary size of dictionary for input_ids
+        dictionary_size (int): vocabulary size of dictionary for input_ids
         verbose (bool): print more information or not
         random_seed (int): random seed
         input_ids (TensorProto): graph input of input IDs
         segment_ids (TensorProto): graph input of token type IDs
         input_mask (TensorProto): graph input of attention mask
         random_mask_length (bool): whether mask random number of words at the end
 
     Returns:
-        List[Dict[str,numpy.ndarray]]: list of test cases, where each test case is a dictonary with input name as key and a tensor as value
+        List[Dict[str,numpy.ndarray]]: list of test cases, where each test case is a dictionary
+                                       with input name as key and a tensor as value
     """
     assert input_ids is not None
 
     np.random.seed(random_seed)
     random.seed(random_seed)
 
     all_inputs = []
-    for test_case in range(test_cases):
+    for _test_case in range(test_cases):
         input_1 = fake_input_ids_data(input_ids, batch_size, sequence_length, dictionary_size)
         inputs = {input_ids.name: input_1}
 
         if segment_ids:
             inputs[segment_ids.name] = fake_segment_ids_data(segment_ids, batch_size, sequence_length)
 
         if input_mask:
@@ -160,74 +187,97 @@
 
         if verbose and len(all_inputs) == 0:
             print("Example inputs", inputs)
         all_inputs.append(inputs)
     return all_inputs
 
 
-def generate_test_data(batch_size: int, sequence_length: int, test_cases: int, seed: int, verbose: bool,
-                       input_ids: TensorProto, segment_ids: TensorProto, input_mask: TensorProto,
-                       random_mask_length: bool):
-    """Create given number of minput data for testing
+def generate_test_data(
+    batch_size: int,
+    sequence_length: int,
+    test_cases: int,
+    seed: int,
+    verbose: bool,
+    input_ids: TensorProto,
+    segment_ids: TensorProto,
+    input_mask: TensorProto,
+    random_mask_length: bool,
+):
+    """Create given number of input data for testing
 
     Args:
         batch_size (int): batch size
         sequence_length (int): sequence length
         test_cases (int): number of test cases
         seed (int): random seed
         verbose (bool): print more information or not
         input_ids (TensorProto): graph input of input IDs
         segment_ids (TensorProto): graph input of token type IDs
         input_mask (TensorProto): graph input of attention mask
         random_mask_length (bool): whether mask random number of words at the end
 
     Returns:
-        List[Dict[str,numpy.ndarray]]: list of test cases, where each test case is a dictonary with input name as key and a tensor as value
+        List[Dict[str,numpy.ndarray]]: list of test cases, where each test case is a dictionary
+                                       with input name as key and a tensor as value
     """
     dictionary_size = 10000
-    all_inputs = fake_test_data(batch_size, sequence_length, test_cases, dictionary_size, verbose, seed, input_ids,
-                                segment_ids, input_mask, random_mask_length)
+    all_inputs = fake_test_data(
+        batch_size,
+        sequence_length,
+        test_cases,
+        dictionary_size,
+        verbose,
+        seed,
+        input_ids,
+        segment_ids,
+        input_mask,
+        random_mask_length,
+    )
     if len(all_inputs) != test_cases:
         print("Failed to create test data for test.")
     return all_inputs
 
 
 def get_graph_input_from_embed_node(onnx_model, embed_node, input_index):
     if input_index >= len(embed_node.input):
         return None
 
     input = embed_node.input[input_index]
     graph_input = onnx_model.find_graph_input(input)
     if graph_input is None:
         parent_node = onnx_model.get_parent(embed_node, input_index)
-        if parent_node is not None and parent_node.op_type == 'Cast':
+        if parent_node is not None and parent_node.op_type == "Cast":
             graph_input = onnx_model.find_graph_input(parent_node.input[0])
     return graph_input
 
 
-def find_bert_inputs(onnx_model: OnnxModel,
-                     input_ids_name: str = None,
-                     segment_ids_name: str = None,
-                     input_mask_name: str = None
-                     ) -> Tuple[Union[None, np.ndarray], Union[None, np.ndarray], Union[None, np.ndarray]]:
+def find_bert_inputs(
+    onnx_model: OnnxModel,
+    input_ids_name: Optional[str] = None,
+    segment_ids_name: Optional[str] = None,
+    input_mask_name: Optional[str] = None,
+) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray]]:
     """Find graph inputs for BERT model.
-    First, we will deduce inputs from EmbedLayerNormalization node. If not found, we will guess the meaning of graph inputs based on naming.
+    First, we will deduce inputs from EmbedLayerNormalization node.
+    If not found, we will guess the meaning of graph inputs based on naming.
 
     Args:
         onnx_model (OnnxModel): onnx model object
         input_ids_name (str, optional): Name of graph input for input IDs. Defaults to None.
         segment_ids_name (str, optional): Name of graph input for segment IDs. Defaults to None.
         input_mask_name (str, optional): Name of graph input for attention mask. Defaults to None.
 
     Raises:
         ValueError: Graph does not have input named of input_ids_name or segment_ids_name or input_mask_name
-        ValueError: Exptected graph input number does not match with specifeid input_ids_name, segment_ids_name and input_mask_name
+        ValueError: Expected graph input number does not match with specified input_ids_name, segment_ids_name
+                    and input_mask_name
 
     Returns:
-        Tuple[Union[None, np.ndarray], Union[None, np.ndarray], Union[None, np.ndarray]]: input tensors of input_ids, segment_ids and input_mask
+        Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray]]: input tensors of input_ids,
+                                                                                 segment_ids and input_mask
     """
 
     graph_inputs = onnx_model.get_graph_inputs_excluding_initializers()
 
     if input_ids_name is not None:
         input_ids = onnx_model.find_graph_input(input_ids_name)
         if input_ids is None:
@@ -248,188 +298,248 @@
         expected_inputs = 1 + (1 if segment_ids else 0) + (1 if input_mask else 0)
         if len(graph_inputs) != expected_inputs:
             raise ValueError(f"Expect the graph to have {expected_inputs} inputs. Got {len(graph_inputs)}")
 
         return input_ids, segment_ids, input_mask
 
     if len(graph_inputs) != 3:
-        raise ValueError("Expect the graph to have 3 inputs. Got {}".format(len(graph_inputs)))
+        raise ValueError(f"Expect the graph to have 3 inputs. Got {len(graph_inputs)}")
 
-    embed_nodes = onnx_model.get_nodes_by_op_type('EmbedLayerNormalization')
+    embed_nodes = onnx_model.get_nodes_by_op_type("EmbedLayerNormalization")
     if len(embed_nodes) == 1:
         embed_node = embed_nodes[0]
         input_ids = get_graph_input_from_embed_node(onnx_model, embed_node, 0)
         segment_ids = get_graph_input_from_embed_node(onnx_model, embed_node, 1)
         input_mask = get_graph_input_from_embed_node(onnx_model, embed_node, 7)
 
         if input_mask is None:
             for input in graph_inputs:
                 input_name_lower = input.name.lower()
                 if "mask" in input_name_lower:
                     input_mask = input
         if input_mask is None:
-            raise ValueError(f"Failed to find attention mask input")
+            raise ValueError("Failed to find attention mask input")
 
         return input_ids, segment_ids, input_mask
 
     # Try guess the inputs based on naming.
     input_ids = None
     segment_ids = None
     input_mask = None
     for input in graph_inputs:
         input_name_lower = input.name.lower()
         if "mask" in input_name_lower:  # matches input with name like "attention_mask" or "input_mask"
             input_mask = input
-        elif "token" in input_name_lower or "segment" in input_name_lower:  # matches input with name like "segment_ids" or "token_type_ids"
+        elif (
+            "token" in input_name_lower or "segment" in input_name_lower
+        ):  # matches input with name like "segment_ids" or "token_type_ids"
             segment_ids = input
         else:
             input_ids = input
 
     if input_ids and segment_ids and input_mask:
         return input_ids, segment_ids, input_mask
 
     raise ValueError("Fail to assign 3 inputs. You might try rename the graph inputs.")
 
 
-def get_bert_inputs(onnx_file: str,
-                    input_ids_name: str = None,
-                    segment_ids_name: str = None,
-                    input_mask_name: str = None):
+def get_bert_inputs(
+    onnx_file: str,
+    input_ids_name: Optional[str] = None,
+    segment_ids_name: Optional[str] = None,
+    input_mask_name: Optional[str] = None,
+) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray]]:
     """Find graph inputs for BERT model.
-    First, we will deduce inputs from EmbedLayerNormalization node. If not found, we will guess the meaning of graph inputs based on naming.
+    First, we will deduce inputs from EmbedLayerNormalization node.
+    If not found, we will guess the meaning of graph inputs based on naming.
 
     Args:
         onnx_file (str): onnx model path
         input_ids_name (str, optional): Name of graph input for input IDs. Defaults to None.
         segment_ids_name (str, optional): Name of graph input for segment IDs. Defaults to None.
         input_mask_name (str, optional): Name of graph input for attention mask. Defaults to None.
 
     Returns:
-        Tuple[Union[None, np.ndarray], Union[None, np.ndarray], Union[None, np.ndarray]]: input tensors of input_ids, segment_ids and input_mask
+        Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray]]: input tensors of input_ids,
+                                                                                 segment_ids and input_mask
     """
     model = ModelProto()
-    with open(onnx_file, "rb") as f:
-        model.ParseFromString(f.read())
+    with open(onnx_file, "rb") as file:
+        model.ParseFromString(file.read())
 
     onnx_model = OnnxModel(model)
     return find_bert_inputs(onnx_model, input_ids_name, segment_ids_name, input_mask_name)
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
 
-    parser.add_argument('--model', required=True, type=str, help="bert onnx model path.")
+    parser.add_argument("--model", required=True, type=str, help="bert onnx model path.")
 
-    parser.add_argument('--output_dir',
-                        required=False,
-                        type=str,
-                        default=None,
-                        help="output test data path. Default is current directory.")
-
-    parser.add_argument('--batch_size', required=False, type=int, default=1, help="batch size of input")
-
-    parser.add_argument('--sequence_length',
-                        required=False,
-                        type=int,
-                        default=128,
-                        help="maximum sequence length of input")
-
-    parser.add_argument('--input_ids_name', required=False, type=str, default=None, help="input name for input ids")
-    parser.add_argument('--segment_ids_name', required=False, type=str, default=None, help="input name for segment ids")
-    parser.add_argument('--input_mask_name',
-                        required=False,
-                        type=str,
-                        default=None,
-                        help="input name for attention mask")
-
-    parser.add_argument('--samples', required=False, type=int, default=1, help="number of test cases to be generated")
-
-    parser.add_argument('--seed', required=False, type=int, default=3, help="random seed")
-
-    parser.add_argument('--verbose', required=False, action='store_true', help="print verbose information")
+    parser.add_argument(
+        "--output_dir",
+        required=False,
+        type=str,
+        default=None,
+        help="output test data path. Default is current directory.",
+    )
+
+    parser.add_argument("--batch_size", required=False, type=int, default=1, help="batch size of input")
+
+    parser.add_argument(
+        "--sequence_length",
+        required=False,
+        type=int,
+        default=128,
+        help="maximum sequence length of input",
+    )
+
+    parser.add_argument(
+        "--input_ids_name",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for input ids",
+    )
+    parser.add_argument(
+        "--segment_ids_name",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for segment ids",
+    )
+    parser.add_argument(
+        "--input_mask_name",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for attention mask",
+    )
+
+    parser.add_argument(
+        "--samples",
+        required=False,
+        type=int,
+        default=1,
+        help="number of test cases to be generated",
+    )
+
+    parser.add_argument("--seed", required=False, type=int, default=3, help="random seed")
+
+    parser.add_argument(
+        "--verbose",
+        required=False,
+        action="store_true",
+        help="print verbose information",
+    )
     parser.set_defaults(verbose=False)
 
-    parser.add_argument('--only_input_tensors',
-                        required=False,
-                        action='store_true',
-                        help="only save input tensors and no output tensors")
+    parser.add_argument(
+        "--only_input_tensors",
+        required=False,
+        action="store_true",
+        help="only save input tensors and no output tensors",
+    )
     parser.set_defaults(only_input_tensors=False)
 
     args = parser.parse_args()
     return args
 
 
-def create_and_save_test_data(model: str, output_dir: str, batch_size: int, sequence_length: int, test_cases: int,
-                              seed: int, verbose: bool, input_ids_name: str, segment_ids_name: str,
-                              input_mask_name: str, only_input_tensors: bool):
+def create_and_save_test_data(
+    model: str,
+    output_dir: str,
+    batch_size: int,
+    sequence_length: int,
+    test_cases: int,
+    seed: int,
+    verbose: bool,
+    input_ids_name: Optional[str],
+    segment_ids_name: Optional[str],
+    input_mask_name: Optional[str],
+    only_input_tensors: bool,
+):
     """Create test data for a model, and save test data to a directory.
 
     Args:
-        model (str): path of ONNX bert model 
+        model (str): path of ONNX bert model
         output_dir (str): output directory
         batch_size (int): batch size
         sequence_length (int): sequence length
         test_cases (int): number of test cases
         seed (int): random seed
         verbose (bool): whether print more information
         input_ids_name (str): graph input name of input_ids
         segment_ids_name (str): graph input name of segment_ids
         input_mask_name (str): graph input name of input_mask
         only_input_tensors (bool): only save input tensors
     """
     input_ids, segment_ids, input_mask = get_bert_inputs(model, input_ids_name, segment_ids_name, input_mask_name)
 
-    all_inputs = generate_test_data(batch_size,
-                                    sequence_length,
-                                    test_cases,
-                                    seed,
-                                    verbose,
-                                    input_ids,
-                                    segment_ids,
-                                    input_mask,
-                                    random_mask_length=False)
+    all_inputs = generate_test_data(
+        batch_size,
+        sequence_length,
+        test_cases,
+        seed,
+        verbose,
+        input_ids,
+        segment_ids,
+        input_mask,
+        random_mask_length=False,
+    )
 
     for i, inputs in enumerate(all_inputs):
-        dir = os.path.join(output_dir, 'test_data_set_' + str(i))
-        output_test_data(dir, inputs)
+        directory = os.path.join(output_dir, "test_data_set_" + str(i))
+        output_test_data(directory, inputs)
 
     if only_input_tensors:
         return
 
     import onnxruntime
-    sess = onnxruntime.InferenceSession(model)
-    output_names = [output.name for output in sess.get_outputs()]
+
+    session = onnxruntime.InferenceSession(model)
+    output_names = [output.name for output in session.get_outputs()]
 
     for i, inputs in enumerate(all_inputs):
-        dir = os.path.join(output_dir, 'test_data_set_' + str(i))
-        result = sess.run(output_names, inputs)
-        for i, output_name in enumerate(output_names):
-            tensor_result = numpy_helper.from_array(np.asarray(result[i]), output_names[i])
-            with open(os.path.join(dir, 'output_{}.pb'.format(i)), 'wb') as f:
-                f.write(tensor_result.SerializeToString())
+        directory = os.path.join(output_dir, "test_data_set_" + str(i))
+        result = session.run(output_names, inputs)
+        for i, output_name in enumerate(output_names):  # noqa: PLW2901
+            tensor_result = numpy_helper.from_array(np.asarray(result[i]), output_name)
+            with open(os.path.join(directory, f"output_{i}.pb"), "wb") as file:
+                file.write(tensor_result.SerializeToString())
 
 
 def main():
     args = parse_arguments()
 
     output_dir = args.output_dir
     if output_dir is None:
         # Default output directory is a sub-directory under the directory of model.
         p = Path(args.model)
-        output_dir = os.path.join(p.parent, "batch_{}_seq_{}".format(args.batch_size, args.sequence_length))
+        output_dir = os.path.join(p.parent, f"batch_{args.batch_size}_seq_{args.sequence_length}")
 
     if output_dir is not None:
         # create the output directory if not existed
         path = Path(output_dir)
         path.mkdir(parents=True, exist_ok=True)
     else:
         print("Directory existed. test data files will be overwritten.")
 
-    create_and_save_test_data(args.model, output_dir, args.batch_size, args.sequence_length, args.samples, args.seed,
-                              args.verbose, args.input_ids_name, args.segment_ids_name, args.input_mask_name,
-                              args.only_input_tensors)
+    create_and_save_test_data(
+        args.model,
+        output_dir,
+        args.batch_size,
+        args.sequence_length,
+        args.samples,
+        args.seed,
+        args.verbose,
+        args.input_ids_name,
+        args.segment_ids_name,
+        args.input_mask_name,
+        args.only_input_tensors,
+    )
 
     print("Test data is saved to directory:", output_dir)
 
 
 if __name__ == "__main__":
     main()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/compare_bert_results.py` & `onnxruntime/transformers/compare_bert_results.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,43 +1,36 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 # It is a tool to compare the inference results of the original model and optimized model.
 
-import sys
 import argparse
-import numpy as np
-import os
-import random
-from pathlib import Path
 import statistics
-import onnx
-import onnx.utils
+from pathlib import Path
+
+import numpy as np
 import psutil
-import csv
-import timeit
-from datetime import datetime
-from onnx import ModelProto, TensorProto, numpy_helper
-from onnx_model import OnnxModel
-from bert_test_data import get_bert_inputs, generate_test_data, output_test_data
 from bert_perf_test import create_session, onnxruntime_inference
+from bert_test_data import generate_test_data, get_bert_inputs, output_test_data
 
 
 def run_model(model_path, all_inputs, use_gpu, disable_optimization):
     import onnxruntime
 
     graph_optimization_level = None
     if disable_optimization:
         graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL
 
     intra_op_num_threads = psutil.cpu_count(logical=False)
 
-    session = create_session(model_path, use_gpu, intra_op_num_threads, graph_optimization_level)
+    session = create_session(
+        model_path, use_gpu, "cuda" if use_gpu else "cpu", intra_op_num_threads, graph_optimization_level
+    )
 
     output_names = [output.name for output in session.get_outputs()]
     results, latency_list = onnxruntime_inference(session, all_inputs, output_names)
     return results, latency_list, output_names
 
 
 def compare(baseline_results, treatment_results, verbose, rtol=1e-3, atol=1e-4):
@@ -55,121 +48,195 @@
             max_abs_diff = max(max_abs_diff, abs_diff)
             if not np.allclose(results[i].tolist(), treatment_output.tolist(), rtol=rtol, atol=atol):
                 if case_passed:
                     case_passed = False
                     diff_count += 1
 
                     if verbose:
-                        print("case {} output {}".format(test_case_id, i))
-                        print("baseline={}\ntreatment={}".format(results[i].tolist(), treatment_output))
-                        print("rel_diff={} abs_diff={}".format(rel_diff, abs_diff))
+                        print(f"case {test_case_id} output {i}")
+                        print(f"baseline={results[i].tolist()}\ntreatment={treatment_output}")
+                        print(f"rel_diff={rel_diff} abs_diff={abs_diff}")
 
     if diff_count == 0:
-        print("100% passed for {} random inputs given thresholds (rtol={}, atol={}).".format(
-            len(baseline_results), rtol, atol))
+        print(
+            "100% passed for {} random inputs given thresholds (rtol={}, atol={}).".format(
+                len(baseline_results), rtol, atol
+            )
+        )
     else:
-        print("WARNING: {} out of {} results NOT passed for thresholds (rtol={}, atol={}).".format(
-            diff_count, len(baseline_results), rtol, atol))
-
-    print("maximum absolute difference={}".format(max_abs_diff))
-
-    print("maximum relative difference={}".format(max_rel_diff))
-
-
-def run_test(baseline_model, optimized_model, output_dir, batch_size, sequence_length, use_gpu, test_cases, seed,
-             verbose, rtol, atol, input_ids_name, segment_ids_name, input_mask_name):
-
+        print(
+            "WARNING: {} out of {} results NOT passed for thresholds (rtol={}, atol={}).".format(
+                diff_count, len(baseline_results), rtol, atol
+            )
+        )
+
+    print(f"maximum absolute difference={max_abs_diff}")
+
+    print(f"maximum relative difference={max_rel_diff}")
+
+
+def run_test(
+    baseline_model,
+    optimized_model,
+    output_dir,
+    batch_size,
+    sequence_length,
+    use_gpu,
+    test_cases,
+    seed,
+    verbose,
+    rtol,
+    atol,
+    input_ids_name,
+    segment_ids_name,
+    input_mask_name,
+):
     # Try deduce input names from optimized model.
-    input_ids, segment_ids, input_mask = get_bert_inputs(optimized_model, input_ids_name, segment_ids_name,
-                                                         input_mask_name)
+    input_ids, segment_ids, input_mask = get_bert_inputs(
+        optimized_model, input_ids_name, segment_ids_name, input_mask_name
+    )
 
     # Use random mask length for accuracy test. It might introduce slight inflation in latency reported in this script.
-    all_inputs = generate_test_data(batch_size,
-                                    sequence_length,
-                                    test_cases,
-                                    seed,
-                                    verbose,
-                                    input_ids,
-                                    segment_ids,
-                                    input_mask,
-                                    random_mask_length=True)
-
-    baseline_results, baseline_latency, output_names = run_model(baseline_model,
-                                                                 all_inputs,
-                                                                 use_gpu,
-                                                                 disable_optimization=True)
+    all_inputs = generate_test_data(
+        batch_size,
+        sequence_length,
+        test_cases,
+        seed,
+        verbose,
+        input_ids,
+        segment_ids,
+        input_mask,
+        random_mask_length=True,
+    )
+
+    baseline_results, baseline_latency, output_names = run_model(
+        baseline_model, all_inputs, use_gpu, disable_optimization=True
+    )
     if verbose:
-        print("baseline average latency (all optimizations disabled): {} ms".format(
-            statistics.mean(baseline_latency) * 1000))
+        print(
+            "baseline average latency (all optimizations disabled): {} ms".format(
+                statistics.mean(baseline_latency) * 1000
+            )
+        )
 
     if output_dir is not None:
         for i, inputs in enumerate(all_inputs):
             output_test_data(output_dir, i, inputs)
 
-    treatment_results, treatment_latency, treatment_output_names = run_model(optimized_model,
-                                                                             all_inputs,
-                                                                             use_gpu,
-                                                                             disable_optimization=False)
+    treatment_results, treatment_latency, treatment_output_names = run_model(
+        optimized_model, all_inputs, use_gpu, disable_optimization=False
+    )
     if verbose:
-        print("treatment average latency: {} ms".format(statistics.mean(treatment_latency) * 1000))
+        print(f"treatment average latency: {statistics.mean(treatment_latency) * 1000} ms")
 
     # Validate the output of baseline and treatment, to make sure the results are similar.
     compare(baseline_results, treatment_results, verbose, rtol, atol)
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
-    parser.add_argument('--baseline_model', required=True, type=str, help="baseline onnx model path.")
-
-    parser.add_argument('--optimized_model',
-                        required=True,
-                        type=str,
-                        default=None,
-                        help="path of the optimized model. It shall have same inputs as the baseline model.")
-
-    parser.add_argument('--output_dir',
-                        required=False,
-                        type=str,
-                        default=None,
-                        help="output test data path. If not specified, test data will not be saved.")
-
-    parser.add_argument('--batch_size', required=True, type=int, help="batch size of input")
-
-    parser.add_argument('--sequence_length', required=True, type=int, help="maximum sequence length of input")
-
-    parser.add_argument('--rtol', required=False, type=float, default=1e-3, help="relative tolerance")
-
-    parser.add_argument('--atol', required=False, type=float, default=1e-4, help="absolute tolerance")
+    parser.add_argument("--baseline_model", required=True, type=str, help="baseline onnx model path.")
 
-    parser.add_argument('--samples', required=False, type=int, default=100, help="number of test cases to be generated")
+    parser.add_argument(
+        "--optimized_model",
+        required=True,
+        type=str,
+        default=None,
+        help="path of the optimized model. It shall have same inputs as the baseline model.",
+    )
+
+    parser.add_argument(
+        "--output_dir",
+        required=False,
+        type=str,
+        default=None,
+        help="output test data path. If not specified, test data will not be saved.",
+    )
+
+    parser.add_argument("--batch_size", required=True, type=int, help="batch size of input")
+
+    parser.add_argument(
+        "--sequence_length",
+        required=True,
+        type=int,
+        help="maximum sequence length of input",
+    )
+
+    parser.add_argument("--rtol", required=False, type=float, default=1e-3, help="relative tolerance")
+
+    parser.add_argument("--atol", required=False, type=float, default=1e-4, help="absolute tolerance")
+
+    parser.add_argument(
+        "--samples",
+        required=False,
+        type=int,
+        default=100,
+        help="number of test cases to be generated",
+    )
 
-    parser.add_argument('--seed', required=False, type=int, default=3, help="random seed")
+    parser.add_argument("--seed", required=False, type=int, default=3, help="random seed")
 
-    parser.add_argument('--use_gpu', required=False, action='store_true', help="use GPU")
+    parser.add_argument("--use_gpu", required=False, action="store_true", help="use GPU")
     parser.set_defaults(use_gpu=False)
 
-    parser.add_argument('--verbose', required=False, action='store_true', help="print verbose information")
+    parser.add_argument(
+        "--verbose",
+        required=False,
+        action="store_true",
+        help="print verbose information",
+    )
     parser.set_defaults(verbose=False)
 
-    parser.add_argument('--input_ids', required=False, type=str, default=None, help="input name for input ids")
-    parser.add_argument('--segment_ids', required=False, type=str, default=None, help="input name for segment ids")
-    parser.add_argument('--input_mask', required=False, type=str, default=None, help="input name for attention mask")
+    parser.add_argument(
+        "--input_ids",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for input ids",
+    )
+    parser.add_argument(
+        "--segment_ids",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for segment ids",
+    )
+    parser.add_argument(
+        "--input_mask",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for attention mask",
+    )
 
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_arguments()
 
     if args.output_dir is not None:
         # create the output directory if not existed
         path = Path(args.output_dir)
         path.mkdir(parents=True, exist_ok=True)
 
-    run_test(args.baseline_model, args.optimized_model, args.output_dir, args.batch_size, args.sequence_length,
-             args.use_gpu, args.samples, args.seed, args.verbose, args.rtol, args.atol, args.input_ids,
-             args.segment_ids, args.input_mask)
+    run_test(
+        args.baseline_model,
+        args.optimized_model,
+        args.output_dir,
+        args.batch_size,
+        args.sequence_length,
+        args.use_gpu,
+        args.samples,
+        args.seed,
+        args.verbose,
+        args.rtol,
+        args.atol,
+        args.input_ids,
+        args.segment_ids,
+        args.input_mask,
+    )
 
 
 if __name__ == "__main__":
     main()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/convert_beam_search.py` & `onnxruntime/transformers/models/longformer/benchmark_longformer.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,631 +1,825 @@
-#-------------------------------------------------------------------------
-# Copyright (c) Microsoft Corporation. All rights reserved.
-# Licensed under the MIT License.
-#-------------------------------------------------------------------------
-
-"""
-This converts GPT2 or T5 model to onnx with beam search operator.
-
-Example 1: convert gpt2 model with beam search:
-   python convert_beam_search.py -m gpt2 --decoder_onnx .\onnx_models\gpt2_past_fp32.onnx --output .\onnx_models\gpt2_beam_search.onnx --output_sequences_scores
-   
-Example 2: convert T5 model with beam search:
-   python ./models/t5/convert_to_onnx.py -m t5-small -s
-   python convert_beam_search.py -m t5-small --model_type t5 --decoder_onnx ./onnx_models/t5-small_decoder.onnx --encoder_decoder_init_onnx ./onnx_models/t5-small_encoder_decoder_init.onnx --output ./onnx_models/t5_small_beam_search.onnx
-"""
+# -------------------------------------------------------------------------
+# Copyright (c) Microsoft Corporation.  All rights reserved.
+# Licensed under the MIT License.  See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+#
+# This script run benchmark of latency or peak memory usage of Longformer model inference.
+# Please run convert_to_onnx.py to get onnx model before running benchmark.
+#
+# It is tested with python 3.8, onnxruntime-gpu 1.11.0, PyTorch 1.11.0, transformers 4.18.0, CUDA 11.3 like:
+#   conda create -n gpu_env python=3.8
+#   conda activate gpu_env
+#   pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113
+#   pip3 install onnx transformers onnxruntime-gpu numpy sympy coloredlogs psutil py3nvml
+#   python benchmark_longformer.py
+#
+# When there is no parameter, pre-defined tests will run on the longformer-base-4096 model.
+
+# Benchmark the latency:
+#   python benchmark_longformer.py --model longformer-base-4096 --batch_sizes 1 --sequence_lengths 512 1024 2048 4096 \
+#          --global_lengths 8 --onnx ./longformer-base-4096_fp16.onnx -t 100
+#
+# Benchmark GPU peak memory:
+#   export ORT_LONGFORMER_COMPACT_MEMORY=0
+#   python benchmark_longformer.py --model longformer-base-4096 --batch_sizes 1 --sequence_lengths 4096 \
+#          --global_lengths 8 --onnx ./longformer-base-4096_fp32.onnx --memory -t 10 --engine onnxruntime
+#   export ORT_LONGFORMER_COMPACT_MEMORY=1
+#   python benchmark_longformer.py --model longformer-base-4096 --batch_sizes 1 --sequence_lengths 4096 \
+#          --global_lengths 8 --onnx ./longformer-base-4096_fp32.onnx --memory -t 10 --engine onnxruntime
+#
+# By default, compact memory kernel is enabled. To disable it, set environment variable ORT_LONGFORMER_COMPACT_MEMORY=0.
 
-import os
-import time
-import onnx
-import logging
 import argparse
-from pathlib import Path
-from onnx import helper
+import csv
+import logging
+import math
+import os
+import re
+import sys
+import timeit
+import traceback
+from concurrent.futures import ProcessPoolExecutor
+from datetime import datetime
+from typing import Any, Dict, List
+
 import numpy as np
-from typing import List, Union
 import torch
-from packaging import version
-from transformers import GPT2Config, T5Config
-from gpt2_helper import PRETRAINED_GPT2_MODELS
-from convert_to_onnx import main as convert_gpt2_to_onnx
-from benchmark_helper import Precision
-from onnx import onnx_pb as onnx_proto
-
-
-config: Union[GPT2Config, T5Config] = None
-
-logger = logging.getLogger('')
-
-
-def parse_arguments(argv=None):
-    parser = argparse.ArgumentParser()
-
-    parser.add_argument('-m',
-                        '--model_name_or_path',
-                        required=True,
-                        type=str,
-                        help='Model path, or pretrained model name in the list: ' + ', '.join(PRETRAINED_GPT2_MODELS))
-
-    parser.add_argument('--model_type',
-                        required=False,
-                        type=str,
-                        default="gpt2",
-                        choices=["gpt2", "t5"],
-                        help='Model type in the list: ' + ', '.join(["gpt2", "t5"]))
-
-    parser.add_argument('--cache_dir',
-                        required=False,
-                        type=str,
-                        default=os.path.join('.', 'cache_models'),
-                        help='Directory to cache pre-trained models')
-
-    parser.add_argument('--decoder_onnx',
-                        required=True,
-                        type=str,
-                        help='Output directory for decoder onnx model, or model path ends with .onnx')
-
-    parser.add_argument('--encoder_decoder_init_onnx',
-                        required=False,
-                        type=str,
-                        default="",
-                        help='path of ONNX model for encoder and decoder initialization. Required for t5 model type.')
-
-    parser.add_argument('--output',
-                        required=False,
-                        type=str,
-                        help='Output directory for beam search model, or model path ends with .onnx')
-
-    parser.add_argument("-p",
-                        "--precision",
-                        required=False,
-                        type=Precision,
-                        default=Precision.FLOAT32,
-                        choices=[Precision.FLOAT32, Precision.FLOAT16],
-                        help="Precision of model to run. fp32 for full precision, fp16 for half or mixed precision")
-
-    parser.add_argument('--use_gpu', required=False, action='store_true', help="use GPU for inference")
-    parser.set_defaults(use_gpu=False)
+from longformer_helper import PRETRAINED_LONGFORMER_MODELS, LongformerHelper, LongformerInputs
+from transformers import LongformerModel
 
-    parser.add_argument('-e', '--use_external_data_format', required=False, action='store_true')
-    parser.set_defaults(use_external_data_format=False)
+import onnxruntime
 
-    parser.add_argument('--disable_parity', required=False, action='store_true', help="do not run parity test")
-    parser.set_defaults(disable_parity=False)
-
-    parser.add_argument('--torch_performance', required=False, action='store_true', help="test PyTorch performance")
-    parser.set_defaults(torch_performance=False)
+sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
+import benchmark_helper  # noqa: E402
 
-    parser.add_argument('--total_runs',
-                        required=False,
-                        type=int,
-                        default=1,
-                        help='Number of times of inference for latency measurement')
-
-    beam_search_group = parser.add_argument_group("beam search options")
-
-    beam_search_group.add_argument('--output_sequences_scores',
-                                   required=False,
-                                   action='store_true',
-                                   help="output sequences scores")
-    beam_search_group.set_defaults(output_sequences_scores=False)
-
-    beam_search_group.add_argument('--output_token_scores',
-                                   required=False,
-                                   action='store_true',
-                                   help="output token scores")
-    beam_search_group.set_defaults(output_token_scores=False)
-
-    beam_search_group.add_argument('--early_stopping', required=False, action='store_true')
-    beam_search_group.set_defaults(early_stopping=False)
-
-    beam_search_group.add_argument('--min_length', type=int, required=False, default=1, help='Min sequence length')
-
-    beam_search_group.add_argument('--max_length', type=int, required=False, default=50, help='Max sequence length')
-
-    beam_search_group.add_argument('--no_repeat_ngram_size',
-                                   type=int,
-                                   required=False,
-                                   default=0,
-                                   help='No repeat ngram size')
-
-    beam_search_group.add_argument('--num_beams', type=int, required=False, default=4, help='Beam size')
-
-    beam_search_group.add_argument('--num_return_sequences',
-                                   type=int,
-                                   required=False,
-                                   default=1,
-                                   help='Number of return sequence <= num_beams')
-
-    beam_search_group.add_argument('--temperature',
-                                   type=float,
-                                   required=False,
-                                   default=1,
-                                   help='Softmax temperature for output logits.')
-
-    beam_search_group.add_argument('--length_penalty',
-                                   type=float,
-                                   required=False,
-                                   default=1,
-                                   help='Positive. >1 to penalize and <1 to encorage short sentence.')
-
-    beam_search_group.add_argument('--repetition_penalty',
-                                   type=float,
-                                   required=False,
-                                   default=1,
-                                   help='Positive. >1 to penalize and <1 to encorage.')
-
-    beam_search_group.add_argument('--vocab_size',
-                                   type=int,
-                                   required=False,
-                                   default=-1,
-                                   help="Vocab_size of the underlying model")
+logger = logging.getLogger("")
+
+
+def test_torch_latency(
+    device,
+    model,
+    model_name,
+    batch_sizes,
+    sequence_lengths,
+    global_lengths,
+    test_times,
+    num_threads,
+) -> List[Dict[str, Any]]:
+    if num_threads > 0:
+        torch.set_num_threads(num_threads)
+
+    results = []
+    for batch_size in batch_sizes:
+        for sequence_length in sequence_lengths:
+            for global_length in global_lengths:
+                logger.info(f"batch_size={batch_size} sequence_length={sequence_length} global_length={global_length}")
+                inputs: LongformerInputs = LongformerHelper.get_dummy_inputs(
+                    batch_size, sequence_length, global_length, device
+                )
+                input_list = inputs.to_list()
+
+                _ = model(*input_list)
+                runtimes = timeit.repeat(lambda: model(*input_list), repeat=test_times, number=1)  # noqa: B023
+                result = {
+                    "engine": "torch",  # TODO: test torchscript
+                    "version": torch.__version__,
+                    "device": "cuda",
+                    "optimizer": "",
+                    "precision": "fp32",
+                    "io_binding": "",
+                    "model_name": model_name,
+                    "description": model_name + " [torch]",
+                    "inputs": 3,
+                    "threads": num_threads,
+                    "batch_size": batch_size,
+                    "sequence_length": sequence_length,
+                    "global_length": global_length,
+                    "datetime": str(datetime.now()),
+                    "memory": "NA",
+                    "diff_max": 0,
+                    "diff_90_percentile": 0,
+                    "diff_95_percentile": 0,
+                    "diff_99_percentile": 0,
+                    "use_compact_memory": "NA",
+                }
+                result.update(benchmark_helper.get_latency_result(runtimes, batch_size))
+                logger.info("%s", result)
+                results.append(result)
+    return results
+
+
+def test_parity(device, model, ort_session, batch_size, sequence_length, global_length, verbose=True):
+    parameters = f"batch_size={batch_size} sequence_length={sequence_length} global_length={global_length}"
+    logger.info(f"Comparing Torch and ORT outputs for {parameters}...")
+    dummy_inputs: LongformerInputs = LongformerHelper.get_dummy_inputs(
+        batch_size, sequence_length, global_length, device
+    )
+    ort_inputs = dummy_inputs.get_ort_inputs()
+    ort_outputs = ort_session.run(None, ort_inputs)
+    input_list = dummy_inputs.to_list()
+    torch_outputs = model(*input_list)
+    max_diff = np.amax(torch_outputs[0].cpu().numpy() - ort_outputs[0])
+    logger.info(f"last_state max diff = {max_diff}")
+    if verbose and (math.isnan(max_diff) or max_diff > 0.001):
+        print("torch last_state:", torch_outputs[0])
+        print("ort last_state:", ort_outputs[0])
+    return float(max_diff)
+
+
+def test_ort_latency(
+    device,
+    model,
+    model_name,
+    description,
+    ort_session,
+    batch_sizes,
+    sequence_lengths,
+    global_lengths,
+    test_times,
+    num_threads,
+    optimizer=False,
+    precision="fp32",
+    disable_io_binding=False,
+    verbose=True,
+    use_compact_memory=False,
+    use_half4=False,
+    disable_parity=False,
+) -> List[Dict[str, Any]]:
+    results = []
+    for batch_size in batch_sizes:
+        for sequence_length in sequence_lengths:
+            for global_length in global_lengths:
+                assert (
+                    global_length <= model.config.attention_window[0]
+                ), "Limitation of current implementation: number of global token <= attention_window"
+
+                logger.info(
+                    f"Testing batch_size={batch_size} sequence_length={sequence_length} global_length={global_length} "
+                    f"optimizer={optimizer}, precision={precision} io_binding={not disable_io_binding}..."
+                )
+                dummy_inputs: LongformerInputs = LongformerHelper.get_dummy_inputs(
+                    batch_size, sequence_length, global_length, device
+                )
+
+                # Run OnnxRuntime
+                ort_inputs = dummy_inputs.get_ort_inputs()
+
+                if verbose:
+                    print(ort_inputs)
+
+                # run one query for warm up
+                ort_outputs = ort_session.run(None, ort_inputs)
+
+                result_template = {
+                    "model_name": model_name,
+                    "description": description,
+                    "inputs": 3,
+                    "engine": "OnnxRuntime",
+                    "version": str(onnxruntime.__version__),
+                    "device": "cuda",
+                    "precision": str(precision),
+                    "optimizer": int(optimizer),
+                    "threads": int(num_threads),
+                    "batch_size": int(batch_size),
+                    "sequence_length": int(sequence_length),
+                    "global_length": int(global_length),
+                    "test_times": int(test_times),
+                    "datetime": str(datetime.now()),
+                    "memory": "",
+                    "diff_max": None,
+                    "diff_90_percentile": None,
+                    "diff_95_percentile": None,
+                    "diff_99_percentile": None,
+                    "use_compact_memory": use_compact_memory,
+                    "use_half4": use_half4,
+                }
+
+                if not disable_io_binding:
+                    max_last_state_size = max(batch_sizes) * max(sequence_lengths) * model.config.hidden_size
+                    max_pooler_size = max(batch_sizes) * max(sequence_lengths)
+                    result = benchmark_helper.inference_ort_with_io_binding(
+                        ort_session,
+                        ort_inputs,
+                        result_template=result_template,
+                        repeat_times=test_times,
+                        ort_output_names=["last_state", "pooler"],
+                        ort_outputs=ort_outputs,
+                        output_buffers=[],
+                        output_buffer_max_sizes=[max_last_state_size, max_pooler_size],
+                        batch_size=batch_size,
+                        device=device,
+                        data_type=np.longlong,  # input data type
+                    )
+                else:
+                    result = benchmark_helper.inference_ort(
+                        ort_session,
+                        ort_inputs,
+                        result_template=result_template,
+                        repeat_times=test_times,
+                        batch_size=batch_size,
+                    )
+
+                # measure result difference between PyTorch and OnnxRuntime
+                if not disable_parity:
+                    diff_results = [
+                        test_parity(
+                            device,
+                            model,
+                            ort_session,
+                            batch_size,
+                            sequence_length,
+                            global_length,
+                            verbose,
+                        )
+                        for _ in range(test_times)
+                    ]
+
+                    result["diff_max"] = max(diff_results)
+                    result["diff_90_percentile"] = np.percentile(diff_results, 90)
+                    result["diff_95_percentile"] = np.percentile(diff_results, 95)
+                    result["diff_99_percentile"] = np.percentile(diff_results, 99)
+
+                results.append(result)
+    return results
+
+
+def test_ort_memory(
+    device,
+    onnx_model_path,
+    batch_size,
+    sequence_length,
+    global_length,
+    test_times,
+    num_threads,
+) -> Dict[str, Any]:
+    logger.info(
+        f"Testing memory for model={onnx_model_path}, batch_size={batch_size}, sequence_length={sequence_length}, "
+        f"global_length={global_length}, test_times={test_times}, num_threads={num_threads}"
+    )
+
+    def inference():
+        # Update Arena strategy so that we can measure the minimum memory required
+        cuda_provider_options = {"arena_extend_strategy": "kSameAsRequested"}
+        provider_options = {"CUDAExecutionProvider": cuda_provider_options}
+        session = benchmark_helper.create_onnxruntime_session(
+            onnx_model_path,
+            use_gpu=True,
+            enable_all_optimization=True,
+            num_threads=num_threads,
+            provider_options=provider_options,
+        )
 
-    beam_search_group.add_argument(
-        '--prefix_vocab_mask',
-        required=False,
-        action='store_true',
-        help="This vocab mask applies only to first iteration, enable if last word in query might need auto complete")
-    beam_search_group.set_defaults(prefix_vocab_mask=False)
+        dummy_inputs: LongformerInputs = LongformerHelper.get_dummy_inputs(
+            batch_size, sequence_length, global_length, device
+        )
+        ort_inputs = dummy_inputs.get_ort_inputs()
+        for _ in range(test_times):
+            _ = session.run(None, ort_inputs)
+
+    memory_used = benchmark_helper.measure_memory(is_gpu=True, func=inference)
+
+    return {
+        "onnx_model": onnx_model_path,
+        "batch_size": batch_size,
+        "sequence_length": sequence_length,
+        "global_length": global_length,
+        "test_times": test_times,
+        "num_threads": num_threads,
+        "memory": memory_used,
+    }
 
-    args = parser.parse_args(argv)
 
-    return args
+def load_torch_model(model_name, device):
+    torch_model_name_or_dir = (
+        PRETRAINED_LONGFORMER_MODELS[model_name] if model_name in PRETRAINED_LONGFORMER_MODELS else model_name
+    )
+    model = LongformerModel.from_pretrained(torch_model_name_or_dir)
+    model.to(device)
+    return model
 
 
-def gpt2_to_onnx(args):
-    model_name = args.model_name_or_path
+def find_onnx_model(model_name, onnx_dir="."):
+    # Search onnx model in the following order: optimized fp16 model, optimized fp32 model, raw model
+    onnx_model_path = os.path.join(onnx_dir, model_name + ".onnx")
+    optimized_fp32_model = os.path.join(onnx_dir, model_name + "_fp32.onnx")
+    optimized_fp16_model = os.path.join(onnx_dir, model_name + "_fp16.onnx")
+    if os.path.isfile(optimized_fp16_model):
+        onnx_model_path = optimized_fp16_model
+    elif os.path.isfile(optimized_fp32_model):
+        onnx_model_path = optimized_fp32_model
+    return onnx_model_path
+
+
+def test_memory(args, device) -> Dict[str, Any]:
+    if len(args.batch_sizes) > 1:
+        raise RuntimeError("For memory test, only one batch_size (-b) is allowed.")
+    if len(args.sequence_lengths) > 1:
+        raise RuntimeError("For memory test, only one sequence_length (-s) is allowed.")
+    if len(args.global_lengths) > 1:
+        raise RuntimeError("For memory test, only one global_length (-g) is allowed.")
+
+    model_name = args.model
+    onnx_model_path = find_onnx_model(model_name) if not args.onnx else args.onnx
+
+    torch.cuda.empty_cache()
+    return test_ort_memory(
+        device,
+        onnx_model_path,
+        args.batch_sizes[0],
+        args.sequence_lengths[0],
+        args.global_lengths[0],
+        args.test_times,
+        args.num_threads,
+    )
+
+
+def test_ort(args, device) -> List[Dict[str, Any]]:
+    model_name = args.model
+
+    onnx_model_path = find_onnx_model(model_name) if not args.onnx else args.onnx
+
+    optimized = onnx_model_path.endswith("_fp16.onnx") or onnx_model_path.endswith("_fp32.onnx")
+    precision = "fp32" if not onnx_model_path.endswith("_fp16.onnx") else "fp16"
+
+    model = load_torch_model(model_name, device)
+
+    num_threads = args.num_threads
+
+    cuda_provider_options = {"arena_extend_strategy": "kSameAsRequested"}
+    provider_options = {"CUDAExecutionProvider": cuda_provider_options}
+    session = benchmark_helper.create_onnxruntime_session(
+        onnx_model_path,
+        use_gpu=True,
+        enable_all_optimization=True,
+        num_threads=num_threads,
+        provider_options=provider_options,
+    )
+    if session is None:
+        raise RuntimeError(f"Failed to create ORT session from ONNX file {onnx_model_path}")
+
+    use_compact_memory = os.environ.get("ORT_LONGFORMER_COMPACT_MEMORY", "1") == "1"
+    description = onnx_model_path
+    if not use_compact_memory:
+        description += "[non_compact_memory]"
 
-    print(f"use convert_to_onnx.py to convert model {model_name} to onnx {args.decoder_onnx} ...")
-    arguments = [
-        '--model_name_or_path',
-        model_name,
-        '--output',
-        args.decoder_onnx,
-        '--optimize_onnx',
-        '--precision',
-        'fp32' if args.precision == Precision.FLOAT32 else 'fp16',
-        '--test_runs',
-        '1',
-        '--test_cases',
-        '10',
-        '--use_int32_inputs'  # BeamSearch requires to use int32 for input_ids, postion_ids and attention_mask
-    ]
-    if args.use_gpu:
-        arguments.append('--use_gpu')
-    if args.use_external_data_format:
-        arguments.append('--use_external_data_format')
-
-    if args.precision == Precision.FLOAT16:
-        assert args.use_gpu, "fp16 or mixed precision model cannot run in CPU. Please add --use_gpu"
-        # TODO: Use auto mixed precision for fp16 conversion: arguments.append('--auto_mixed_precision')
-        #       Need change cuda kernel to support a combination of fp32 logits and fp16 past state.
-        #       Currently logits and past state shall be same data type.
-        arguments.extend(['--op_block_list', 'Add', 'LayerNormalization', 'FastGelu'])
-    convert_gpt2_to_onnx(arguments)
-
-
-def shape_inference(decoder_onnx_path):
-    if version.parse(onnx.__version__) >= version.parse('1.11.0'):
-        logger.warn("SymbolicShapeInference might fail using onnx version 1.11. Please install 1.10.0 for now.")
-
-    # Run symbolic shape inference to walk around ORT shape inference issue for subgraph.
-    from onnxruntime.tools.symbolic_shape_infer import SymbolicShapeInference
-    out = SymbolicShapeInference.infer_shapes(onnx.load(decoder_onnx_path), auto_merge=True, guess_output_rank=False)
-    if out:
-        # TODO: Use external format if input has extra data.
-        onnx.save(out, decoder_onnx_path)
+    if args.use_half4:
+        description += "[half4]" if precision == "fp16" else "[float4]"
     else:
-        print("Failed to run symbolic shape inference on the model.")
-
-
-def create_ort_session(model_path, use_gpu):
-    from onnxruntime import SessionOptions, InferenceSession, __version__ as ort_version, GraphOptimizationLevel, get_available_providers
-    sess_options = SessionOptions()
-    sess_options.graph_optimization_level = GraphOptimizationLevel.ORT_DISABLE_ALL
-    execution_providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if use_gpu else ['CPUExecutionProvider']
-    if use_gpu:
-        if 'CUDAExecutionProvider' not in get_available_providers():
-            raise RuntimeError("CUDAExecutionProvider is not avaiable for --use_gpu!")
-        else:
-            print("use CUDAExecutionProvider")
-
-    ort_session = InferenceSession(model_path, sess_options, providers=execution_providers)
-    return ort_session
-
+        description += "[half2]" if precision == "fp16" else "[float4]"
 
-def verify_gpt2_subgraph(graph, precision):
-    is_float16 = (Precision.FLOAT16 == precision)
-
-    input_count = len(graph.input)
-    layer_count = input_count - 3
-
-    expected_inputs = ['input_ids', 'position_ids', 'attention_mask'] + [f"past_{i}" for i in range(layer_count)]
-    if len(graph.input) != len(expected_inputs):
-        raise ValueError(f"Number of inputs expected to be {len(expected_inputs)}. Got {len(graph.input)}")
-
-    for i, expected_input in enumerate(expected_inputs):
-        if graph.input[i].name != expected_input:
-            raise ValueError(f"Input {i} is expected to be {expected_input}. Got {graph.input[i].name}")
-
-        expected_type = onnx_proto.TensorProto.INT32
-        if i >= 3:
-            expected_type = onnx_proto.TensorProto.FLOAT16 if is_float16 else onnx_proto.TensorProto.FLOAT
-
-        if graph.input[i].type.tensor_type.elem_type != expected_type:
-            raise ValueError(
-                f"Input {i} is expected to have onnx data type {expected_type}. Got {graph.input[i].type.tensor_type.elem_type}"
-            )
-    print("Verifying GPT-2 graph inputs: name and data type are good.")
-
-    expected_outputs = ['logits'] + [f"present_{i}" for i in range(layer_count)]
-    if len(graph.output) != len(expected_outputs):
-        raise ValueError(f"Number of outputs expected to be {len(expected_outputs)}. Got {len(graph.output)}")
+    return test_ort_latency(
+        device,
+        model,
+        model_name,
+        description,
+        session,
+        args.batch_sizes,
+        args.sequence_lengths,
+        args.global_lengths,
+        args.test_times,
+        num_threads,
+        optimized,
+        precision,
+        args.disable_io_binding,
+        args.verbose,
+        use_compact_memory,
+        args.use_half4,
+        args.disable_parity,
+    )
+
+
+def test_torch(args, device) -> List[Dict[str, Any]]:
+    model = load_torch_model(args.model, device)
+    return test_torch_latency(
+        device,
+        model,
+        args.model,
+        args.batch_sizes,
+        args.sequence_lengths,
+        args.global_lengths,
+        args.test_times,
+        args.num_threads,
+    )
+
+
+def test_latency(args, device) -> List[Dict[str, Any]]:
+    if args.engine == "onnxruntime":
+        return test_ort(args, device)
 
-    for i, expected_output in enumerate(expected_outputs):
-        if graph.output[i].name != expected_output:
-            raise ValueError(f"Output {i} is expected to be {expected_output}. Got {graph.output[i].name}")
+    return test_torch(args, device)
 
-        expected_type = onnx_proto.TensorProto.FLOAT16 if is_float16 else onnx_proto.TensorProto.FLOAT
-        if graph.output[i].type.tensor_type.elem_type != expected_type:
-            raise ValueError(
-                f"Input {i} is expected to have onnx data type {expected_type}. Got {graph.output[i].type.tensor_type.elem_type}"
-            )
-    print("Verifying GPT-2 graph outputs: name and data type are good.")
 
-    # TODO: verify shapes of inputs and outputs.
-    return
+def parse_arguments(argv=None):
+    parser = argparse.ArgumentParser()
 
+    parser.add_argument(
+        "-m",
+        "--model",
+        required=False,
+        type=str,
+        default="longformer-base-4096",
+        help="Checkpoint directory or pre-trained model names in the list: "
+        + ", ".join(PRETRAINED_LONGFORMER_MODELS.keys()),
+    )
+
+    parser.add_argument(
+        "-e",
+        "--engine",
+        required=False,
+        type=str,
+        default="onnxruntime",
+        choices=["onnxruntime", "torch"],
+        help="Engine to benchmark.",
+    )
+
+    parser.add_argument(
+        "-t",
+        "--test_times",
+        required=False,
+        default=1000,
+        type=int,
+        help="Number of repeat times to get average inference latency.",
+    )
+
+    parser.add_argument("-b", "--batch_sizes", nargs="+", type=int, default=[1])
+
+    # If --export_padding is not used in exporting onnx model, there is no padding in ONNX model,
+    # and you will need padding inputs by yourself before running onnx model.
+    # Here, we only test sequence length that is multiple of attention window size.
+    parser.add_argument(
+        "-s",
+        "--sequence_lengths",
+        nargs="+",
+        type=int,
+        default=[512, 1024, 2048, 4096],
+        help="Sequence lengths. It could have multiple values in latency test."
+        "If --export_padding is not used, sequence length shall be multiple of window size.",
+    )
+
+    parser.add_argument("--onnx", required=False, type=str, default=None, help="Onnx model path")
+
+    parser.add_argument(
+        "-g",
+        "--global_lengths",
+        nargs="+",
+        type=int,
+        default=[0],
+        help="Number of global tokens. It could have multiple values in latency test.",
+    )
+
+    parser.add_argument(
+        "-n",
+        "--num_threads",
+        required=False,
+        type=int,
+        default=0,
+        help="Threads to use.",
+    )
 
-def verify_t5_decoder_subgraph(graph, precision):
-    # TODO: implement it
-    pass
+    parser.add_argument(
+        "--disable_io_binding",
+        required=False,
+        action="store_true",
+        help="Do not use IO Binding.",
+    )
 
+    parser.add_argument(
+        "--memory",
+        required=False,
+        action="store_true",
+        help="Test memory usage instead of latency.",
+    )
 
-def verify_t5_encoder_decoder_init_subgraph(graph, precision):
-    # TODO: implement it
-    pass
+    parser.add_argument("--verbose", required=False, action="store_true", help="Print more information.")
+    parser.set_defaults(verbose=False)
 
+    parser.add_argument("--use_half4", required=False, action="store_true", help="Use half4 kernel.")
+    parser.set_defaults(use_half4=False)
 
-def convert_model(args):
-    if os.path.exists(args.decoder_onnx):
-        print(f"skip convert_to_onnx since path existed: {args.decoder_onnx}")
-    else:
-        assert args.model_type == "gpt2", "please have onnx model ready for model type that is not gpt2"
-        gpt2_to_onnx(args)
+    parser.add_argument("--disable_parity", required=False, action="store_true", help="Do not run parity test.")
+    parser.set_defaults(disable_parity=False)
 
-    # TODO: fix shape inference for T5. Currently symbolic shape inference on T5 is broken.
-    enable_shape_inference = args.model_type == "gpt2"
-    
-    if enable_shape_inference:
-        print(f"Run symbolic shape inference on {args.decoder_onnx}. The file will be overwritten.")
-        shape_inference(args.decoder_onnx)
-        
-    global config
-    if args.model_type == "gpt2":
-        config = GPT2Config.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)
-    else:
-        config = T5Config.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)
-    print(config)
+    args = parser.parse_args(argv)
 
-    eos_token_id = config.eos_token_id
-    pad_token_id = config.eos_token_id
-    vocab_size = config.vocab_size
-
-    # if vocab_size is given in parameters use that.
-    if args.vocab_size != -1:
-        vocab_size = args.vocab_size
+    return args
 
-    model = onnx.load(args.decoder_onnx)
-    model.graph.name = f"{args.model_type} decoder subgraph"
 
-    if args.model_type == "gpt2":
-        verify_gpt2_subgraph(model.graph, args.precision)
-    else:
-        verify_t5_decoder_subgraph(model.graph, args.precision)
+def output_details(results, csv_filename):
+    latency_results = [result for result in results if "average_latency_ms" in result]
+    if len(latency_results) == 0:
+        print("No latency results for output.")
+        return
+
+    with open(csv_filename, mode="a", newline="", encoding="ascii") as csv_file:
+        column_names = [
+            "engine",
+            "version",
+            "device",
+            "precision",
+            "optimizer",
+            "io_binding",
+            "model_name",
+            "inputs",
+            "threads",
+            "datetime",
+            "test_times",
+            "description",
+            "batch_size",
+            "sequence_length",
+            "global_length",
+            "use_compact_memory",
+            "use_half4",
+            "diff_max",
+            "diff_90_percentile",
+            "diff_95_percentile",
+            "diff_99_percentile",
+            "memory",
+            "QPS",
+            "average_latency_ms",
+            "latency_variance",
+            "latency_90_percentile",
+            "latency_95_percentile",
+            "latency_99_percentile",
+        ]
+
+        csv_writer = csv.DictWriter(csv_file, fieldnames=column_names)
+        csv_writer.writeheader()
+        for result in latency_results:
+            print(result)
+            csv_writer.writerow(result)
 
-    inputs = [
-        "input_ids", "max_length", "min_length", "num_beams", "num_return_sequences", "temperature", "length_penalty",
-        "repetition_penalty", "vocab_mask"
-    ]
-    if args.prefix_vocab_mask:
-        inputs.append("prefix_vocab_mask")
-
-    outputs = ["sequences"]
-    if args.output_sequences_scores:
-        outputs.append("sequences_scores")
-
-    if args.output_token_scores:
-        assert args.output_sequences_scores, "--output_token_scores requires --output_sequences_scores"
-        outputs.append("scores")
-
-    node = helper.make_node('BeamSearch', inputs=inputs, outputs=outputs, name=f'BeamSearch_{args.model_type}')
-    node.domain = "com.microsoft"
-    node.attribute.extend([
-        helper.make_attribute("eos_token_id", eos_token_id),
-        helper.make_attribute("pad_token_id", pad_token_id),
-        helper.make_attribute("no_repeat_ngram_size", args.no_repeat_ngram_size),
-        helper.make_attribute("early_stopping", 1 if args.early_stopping else 0),
-        helper.make_attribute("model_type", 0 if args.model_type == "gpt2" else 1),
-        helper.make_attribute("decoder", model.graph),
-    ])
-
-    if args.model_type == "t5":
-        if enable_shape_inference:
-            print(f"Run symbolic shape inference on {args.encoder_decoder_init_onnx}. The file will be overwritten.")
-            shape_inference(args.encoder_decoder_init_onnx)
-        init_model = onnx.load(args.encoder_decoder_init_onnx)
-        init_model.graph.name = f"{args.model_type} encoder decoder init subgraph"
-        verify_t5_encoder_decoder_init_subgraph(init_model.graph, args.precision)
-        node.attribute.extend([
-            helper.make_attribute("encoder_decoder_init", init_model.graph),
-        ])
-
-    from onnx import TensorProto
-
-    # graph inputs
-    input_ids = helper.make_tensor_value_info('input_ids', TensorProto.INT32, ['batch_size', 'sequence_length'])
-    max_length = helper.make_tensor_value_info('max_length', TensorProto.INT32, [1])
-    min_length = helper.make_tensor_value_info('min_length', TensorProto.INT32, [1])
-    num_beams = helper.make_tensor_value_info('num_beams', TensorProto.INT32, [1])
-    num_return_sequences = helper.make_tensor_value_info('num_return_sequences', TensorProto.INT32, [1])
-    temperature = helper.make_tensor_value_info('temperature', TensorProto.FLOAT, [1])
-    length_penalty = helper.make_tensor_value_info('length_penalty', TensorProto.FLOAT, [1])
-    repetition_penalty = helper.make_tensor_value_info('repetition_penalty', TensorProto.FLOAT, [1])
-    vocab_mask = helper.make_tensor_value_info('vocab_mask', TensorProto.INT32, [vocab_size])
-
-    graph_inputs = [
-        input_ids, max_length, min_length, num_beams, num_return_sequences, temperature, length_penalty,
-        repetition_penalty, vocab_mask
-    ]
-
-    if args.prefix_vocab_mask:
-        prefix_vocab_mask = helper.make_tensor_value_info('prefix_vocab_mask', TensorProto.INT32,
-                                                          ['batch_size', vocab_size])
-        graph_inputs.append(prefix_vocab_mask)
-
-    # graph outputs
-    sequences = helper.make_tensor_value_info('sequences', TensorProto.INT32,
-                                              ['batch_size', 'num_return_sequences', 'max_length'])
-
-    sequences_scores = helper.make_tensor_value_info('sequences_scores', TensorProto.FLOAT,
-                                                     ['batch_size', 'num_return_sequences'])
-
-    scores = helper.make_tensor_value_info('scores', TensorProto.FLOAT,
-                                           ['max_length - sequence_length', 'batch_size', 'num_beams', vocab_size])
-
-    initializers = []
-
-    graph_outputs = [sequences]
-
-    if args.output_sequences_scores:
-        graph_outputs.append(sequences_scores)
-
-    if args.output_token_scores:
-        graph_outputs.append(scores)
-
-    new_graph = helper.make_graph([node], f'{args.model_type}-beam-search', graph_inputs, graph_outputs, initializers)
-
-    # Create the model
-    new_model = helper.make_model(new_graph, producer_name='onnxruntime.transformers', opset_imports=model.opset_import)
-    onnx.save(new_model, args.output)
-
-
-def test_torch_performance(args, model, input_ids, attention_mask, eos_token_id, pad_token_id, bad_words_ids):
-    if args.use_gpu and not torch.cuda.is_available():
-        logger.error("Please install PyTorch with Cuda, and use a machine with GPU for testing gpu performance.")
-        return None
+        csv_file.flush()
 
-    if args.precision == Precision.FLOAT16:
-        model.half()
+    print(f"Detail results are saved to csv file: {csv_filename}")
 
-    device = torch.device("cuda:0" if args.use_gpu else "cpu")
-    model.to(device)
 
+def run(args) -> List[Dict[str, Any]]:
     torch.set_grad_enabled(False)
-    input_ids = input_ids.to(device)
-    attention_mask = attention_mask.to(device)
 
-    torch_latency = []
-    for _ in range(args.total_runs):
-        start = time.time()
-        _ = model.generate(input_ids=input_ids,
-                           attention_mask=attention_mask,
-                           max_length=args.max_length,
-                           min_length=args.min_length,
-                           num_beams=args.num_beams,
-                           early_stopping=args.early_stopping,
-                           no_repeat_ngram_size=args.no_repeat_ngram_size,
-                           eos_token_id=eos_token_id,
-                           pad_token_id=pad_token_id,
-                           num_return_sequences=args.num_return_sequences,
-                           temperature=args.temperature,
-                           length_penalty=args.length_penalty,
-                           repetition_penalty=args.repetition_penalty,
-                           bad_words_ids=bad_words_ids,
-                           return_dict_in_generate=True,
-                           output_scores=args.output_sequences_scores or args.output_token_scores)
-        torch_latency.append(time.time() - start)
-    batch_size = input_ids.shape[0]
-    from benchmark_helper import get_latency_result
-    return get_latency_result(torch_latency, batch_size)
-
-
-def test_model(args, use_vocab_mask: bool = False, sentences: List[str] = None):
-    if args.model_type != "gpt2":
-        print(
-            f"Skipping parity test since the support for model type {args.model_type} is not implemented in OnnxRuntime"
-        )
-        return True
+    # set random seed manually to get deterministic results
+    benchmark_helper.set_random_seed(123)
 
-    if args.temperature != 1.0:
-        # TODO: implement temperature in BeamSearch operator.
-        print("Skipping parity test as temperature is not implemented in BeamSearch operator")
-        return True
-
-    if args.prefix_vocab_mask:
-        print("Skipping parity test as prefix vocab mask is not implemented by Hugging Face")
-        return True
-
-    from transformers import GPT2Tokenizer, GPT2LMHeadModel
-
-    tokenizer = GPT2Tokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)
-    tokenizer.padding_side = "left"
-    tokenizer.pad_token = tokenizer.eos_token
-
-    model = GPT2LMHeadModel.from_pretrained(args.model_name_or_path,
-                                            cache_dir=args.cache_dir,
-                                            pad_token_id=tokenizer.eos_token_id)
-
-    # Use different length sentences to test batching
-    if sentences is None:
-        sentences = ["The product is released", "I enjoy walking in the park", "Test best way to invest"]
-
-    inputs = tokenizer(sentences, return_tensors='pt', padding=True)
-    input_ids = inputs["input_ids"]
-    attention_mask = inputs["attention_mask"]
-
-    bad_words = "walk in park"
-    bad_words_ids = tokenizer.encode(bad_words, add_prefix_space=True)
-    bad_words_ids = [[word_id] for word_id in bad_words_ids]  # Convert to list of list
-    if use_vocab_mask:
-        print("bad_words_ids", bad_words_ids)
-    else:
-        bad_words_ids = None
+    # Currently, the longformer attention operator could only run in GPU (no CPU implementation yet).
+    device = torch.device("cuda:0")
 
-    global config
-    config = model.config
-    eos_token_id = config.eos_token_id
-    pad_token_id = config.eos_token_id
-    vocab_size = config.vocab_size
-
-    torch_decoded_sequences = []
-    if not args.disable_parity:
-        print('-' * 50)
-        print("Test PyTorch model and beam search with huggingface transformers...")
-        beam_outputs = model.generate(input_ids=input_ids,
-                                      attention_mask=attention_mask,
-                                      max_length=args.max_length,
-                                      min_length=args.min_length,
-                                      num_beams=args.num_beams,
-                                      early_stopping=args.early_stopping,
-                                      no_repeat_ngram_size=args.no_repeat_ngram_size,
-                                      eos_token_id=eos_token_id,
-                                      pad_token_id=pad_token_id,
-                                      num_return_sequences=args.num_return_sequences,
-                                      temperature=args.temperature,
-                                      length_penalty=args.length_penalty,
-                                      repetition_penalty=args.repetition_penalty,
-                                      bad_words_ids=bad_words_ids,
-                                      return_dict_in_generate=True,
-                                      output_scores=args.output_sequences_scores or args.output_token_scores)
-        print("input_ids", input_ids)
-        print("huggingface transformers outputs:")
-        print("sequences", beam_outputs.sequences)
-        if args.output_sequences_scores:
-            print("sequences_scores", beam_outputs.sequences_scores)
-        if args.output_token_scores:
-            print("scores", beam_outputs.scores)
-        for i, sequence in enumerate(beam_outputs.sequences):
-            decoded_sequence = tokenizer.decode(sequence, skip_special_tokens=True)
-            torch_decoded_sequences.append(decoded_sequence)
-            print("{}: {}".format(i, decoded_sequence))
-
-    print('-' * 50)
-    print("Test ONNX model and bream search with onnxruntime...")
-
-    ort_session = create_ort_session(args.output, args.use_gpu)
-
-    vocab_mask = np.ones((vocab_size), dtype=np.int32)
-    if use_vocab_mask:
-        for bad_word_id in bad_words_ids:
-            vocab_mask[bad_word_id] = 0
-
-    inputs = {
-        "input_ids": input_ids.cpu().numpy().astype(np.int32),
-        "max_length": np.array([args.max_length], dtype=np.int32),
-        "min_length": np.array([args.min_length], dtype=np.int32),
-        "num_beams": np.array([args.num_beams], dtype=np.int32),
-        "num_return_sequences": np.array([args.num_return_sequences], dtype=np.int32),
-        "temperature": np.array([args.temperature], dtype=np.float32),
-        "length_penalty": np.array([args.length_penalty], dtype=np.float32),
-        "repetition_penalty": np.array([args.repetition_penalty], dtype=np.float32),
-        "vocab_mask": vocab_mask
-    }
+    if args.memory:
+        return [test_memory(args, device)]  # Convert to List so that return type is same as test_latency
 
-    test_data_dir = Path(args.output).parent.as_posix()
-    print("test_data_dir", test_data_dir)
-    from bert_test_data import output_test_data
-    all_inputs = [inputs]
-    for i, inputs in enumerate(all_inputs):
-        dir = os.path.join(test_data_dir, 'test_data_set_' + str(i))
-        output_test_data(dir, inputs)
-
-    print("inputs", inputs)
-
-    # Test performance
-    latency = []
-    for _ in range(args.total_runs):
-        start = time.time()
-        result = ort_session.run(None, inputs)
-        latency.append(time.time() - start)
-    batch_size = input_ids.shape[0]
-    from benchmark_helper import get_latency_result
-    output = get_latency_result(latency, batch_size)
-
-    print("ORT outputs:")
-    sequences = result[0]
-    print("sequences", sequences)
-    if args.output_sequences_scores:
-        print("sequences_scores", result[1])
-    if args.output_token_scores:
-        print("scores", result[2])
-
-    (batch_size, num_sequences, max_length) = sequences.shape
-    ort_decoded_sequences = []
-    for i in range(batch_size):
-        for j in range(num_sequences):
-            decoded_sequence = tokenizer.decode(sequences[i][j], skip_special_tokens=True)
-            ort_decoded_sequences.append(decoded_sequence)
-            print(f"batch {i} sequence {j}: {decoded_sequence}")
-
-    if not args.disable_parity:
-        torch_sequences = beam_outputs.sequences.reshape(batch_size, args.num_return_sequences, -1)
-        ort_sequences = torch.LongTensor(sequences)
-        print("-" * 50)
-        print("Torch Sequences:")
-        print(torch_sequences)
-        print(torch_decoded_sequences)
-        print("-" * 50)
-        print("ORT Sequences:")
-        print(ort_sequences)
-        print(ort_decoded_sequences)
-        print("-" * 50)
-        # Compare the generated text instead of word IDs since ORT pads to max sequence length but Torch not.
-        is_same = (torch_decoded_sequences == ort_decoded_sequences)
-        print("Torch and ORT result is ", "same" if is_same else "different")
-        output["parity"] = is_same
-
-    if args.torch_performance:
-        torch_latency_output = test_torch_performance(args, model, input_ids, attention_mask, eos_token_id,
-                                                      pad_token_id, bad_words_ids)
-        print("Torch Latency", torch_latency_output)
-
-    print("ORT", output)
-    return output
-
-
-def main(argv=None, sentences=None):
-    args = parse_arguments(argv)
-    if args.model_type == "t5":
-        assert args.encoder_decoder_init_onnx, "please export t5 to onnx models before using this tool"
+    return test_latency(args, device)
+
+
+def launch_test(arguments) -> List[Dict[str, Any]]:
+    if not torch.cuda.is_available():
+        raise RuntimeError("Please install PyTorch with Cuda, and use a machine with GPU for testing gpu performance.")
+
+    with ProcessPoolExecutor() as executor:
+        results = list(executor.map(run, [arguments]))
+        assert len(results) == 1
+        return results[0]
+
+
+def run_tests(
+    use_compact_memory=True,
+    run_torch=False,
+    run_memory=True,
+    use_io_binding=True,
+    use_fp16=True,
+    use_merged_qkv_weights=True,
+    use_half4=True,
+    batch_size=1,
+):
+    compact_memory = "1" if use_compact_memory else "0"
+    os.environ["ORT_LONGFORMER_COMPACT_MEMORY"] = compact_memory
+    logger.info(f"ORT_LONGFORMER_COMPACT_MEMORY={compact_memory}")
+
+    os.environ["ORT_LONGFORMER_USE_HALF4"] = "1" if use_half4 else "0"
+    logger.info("ORT_LONGFORMER_USE_HALF4={}".format("1" if use_half4 else "0"))
+
+    results = []
+    test_times = 1000
+    sequence_lengths = [4096, 2048, 1024, 512]
+    batch_sizes = [batch_size]
+    for model_name in ["longformer-base-4096"]:
+        for batch_size in batch_sizes:
+            for sequence_length in sequence_lengths:
+                for global_length in [16]:
+                    if run_torch:
+                        engine_name = "torch"
+                        args = parse_arguments(
+                            f"-e {engine_name} -t {test_times} -b {batch_size} -s {sequence_length} -g {global_length} "
+                            f"-t {test_times} -m {model_name}".split(" ")
+                        )
+                        results += run(args)
+
+                    engine_name = "onnxruntime"
+                    file_format = 1 if use_merged_qkv_weights else 0
+                    onnx_path = (
+                        f"{model_name}_f{file_format}_fp16.onnx"
+                        if use_fp16
+                        else f"{model_name}_f{file_format}_fp32.onnx"
+                    )
+                    if not os.path.exists(onnx_path):
+                        raise RuntimeError(f"onnx file not exists:{onnx_path}")
+
+                    arguments = (
+                        f"-e {engine_name} --onnx {onnx_path} "
+                        f"-b {batch_size} -s {sequence_length} -g {global_length} -m {model_name}"
+                    )
+
+                    if not use_io_binding:
+                        arguments += " --disable_io_binding"
+
+                    if use_half4:
+                        arguments += " --use_half4"
+
+                    # Disable parity test to avoid out of memory for large batch size
+                    if batch_size >= 4:
+                        arguments += " --disable_parity"
+
+                    memory_results = None
+                    try:
+                        if run_memory:
+                            args = parse_arguments(f"{arguments} -t 10 --memory".split(" "))
+                            memory_results = launch_test(args)
+
+                        args = parse_arguments(f"{arguments} -t {test_times}".split(" "))
+                        latency_results = launch_test(args)
+                    except KeyboardInterrupt as exc:
+                        raise RuntimeError("Keyboard Interrupted") from exc
+                    except Exception:
+                        traceback.print_exc()
+                        continue
+
+                    if len(latency_results) == 1:
+                        latency_results[0]["memory"] = memory_results[0]["memory"] if memory_results else "N/A"
+                    else:
+                        raise RuntimeError("length of latency_results should be 1")
+
+                    logger.info("%s", latency_results)
+
+                    results += latency_results
+    return results
+
+
+def output_summary(results, csv_filename, data_field="average_latency_ms"):
+    with open(csv_filename, mode="a", newline="", encoding="ascii") as csv_file:
+        header_names = [
+            "model_name",
+            "precision",
+            "engine",
+            "version",
+            "global_length",
+            "use_compact_memory",
+            "use_half4",
+            "description",
+        ]
+
+        description_list = list({result["description"] for result in results})
+        description_list.sort()
+
+        batch_sizes = list({result["batch_size"] for result in results})
+        batch_sizes.sort()
+
+        sequence_lengths = list({result["sequence_length"] for result in results})
+        sequence_lengths.sort()
+
+        data_names = []
+        for sequence_length in sequence_lengths:
+            for batch_size in batch_sizes:
+                data_names.append(f"b{batch_size}_s{sequence_length}")
+
+        csv_writer = csv.DictWriter(csv_file, fieldnames=header_names + data_names)
+        csv_writer.writeheader()
+
+        for description in description_list:
+            row = {}
+
+            sum_latency = {}
+            sum_latency.update({k: 0 for k in data_names})
+
+            count_latency = {}
+            count_latency.update({k: 0 for k in data_names})
+
+            for result in results:
+                if result["description"] == description and result[data_field]:
+                    headers = {k: v for k, v in result.items() if k in header_names}
+                    if not row:
+                        row.update(headers)
+                    else:
+                        for k in header_names:
+                            if row[k] != headers[k]:
+                                raise RuntimeError("Description shall be unique")
+
+                    batch_size = result["batch_size"]
+                    sequence_length = result["sequence_length"]
+                    key = f"b{batch_size}_s{sequence_length}"
+
+                    try:
+                        latency = float(result[data_field])
+                    except ValueError:
+                        continue
+
+                    sum_latency[key] += latency
+                    count_latency[key] += 1
+
+            if row:
+                for key in data_names:
+                    if key in count_latency and count_latency[key] > 0:
+                        row[key] = sum_latency[key] / count_latency[key]
+                    else:
+                        row[key] = ""
+
+                csv_writer.writerow(row)
+
+        csv_file.flush()
+
+
+def run_experiments(use_fp16, batch_size, is_baseline=False):
+    """Run experiments to compare different algorithms on one batch size"""
+    test_results = run_tests(
+        use_fp16=use_fp16,
+        use_merged_qkv_weights=True,
+        use_half4=False,
+        batch_size=batch_size,
+    )
+
+    if is_baseline:
+        return test_results
+
+    if use_fp16:
+        test_results += run_tests(
+            use_fp16=use_fp16,
+            use_merged_qkv_weights=True,
+            use_half4=True,
+            batch_size=batch_size,
+        )
 
-    if os.path.exists(args.output):
-        print(f"skip conversion since path existed: {args.output}")
-    else:
-        convert_model(args)
+        test_results += run_tests(
+            use_fp16=use_fp16,
+            use_merged_qkv_weights=False,
+            use_half4=True,
+            batch_size=batch_size,
+        )
 
-    return test_model(args, use_vocab_mask=True, sentences=sentences)
+    test_results += run_tests(
+        use_fp16=use_fp16,
+        use_merged_qkv_weights=False,
+        use_half4=False,
+        batch_size=batch_size,
+    )
+
+    return test_results
+
+
+def main():
+    torch.multiprocessing.set_start_method("spawn")
+
+    args = parse_arguments()
+
+    benchmark_helper.setup_logger(args.verbose)
+
+    if len(sys.argv) > 1:
+        test_results = launch_test(args)
+        time_stamp = datetime.now().strftime("%Y%m%d-%H%M%S")
+        csv_filename = f"benchmark_detail_{time_stamp}.csv"
+        output_details(test_results, csv_filename)
+        return
+
+    gpu_list = benchmark_helper.get_gpu_info()
+    logger.info("GPU info: %s", gpu_list)
+    fp16_batch_sizes = [16, 8, 4, 2, 1]
+    fp32_batch_sizes = [4, 2, 1]
+    if gpu_list and gpu_list[0]["total"] >= 32 * 1024 * 1024 * 1024:  # 32 GB
+        fp16_batch_sizes = [64, 32, 16, 8, 4, 2, 1]
+        fp32_batch_sizes = [16, 8, 4, 2, 1]
+
+    gpu_name = re.sub(r"(?u)[^-\w.]", "_", gpu_list[0]["name"]) if gpu_list else "gpu"
+    is_baseline = os.environ.get("ORT_LONGFORMER_BASELINE", "0") == "1"
+    experiment_name = f"longformer_base_{gpu_name}" + ("_baseline" if is_baseline else "")
+    logger.info(
+        f"experiment_name={experiment_name}, fp16_batch_sizes={fp16_batch_sizes}, fp32_batch_sizes={fp32_batch_sizes}"
+    )
+
+    total_runs = 1
+    all_results = []
+    for _ in range(total_runs):
+        for batch_size in fp16_batch_sizes:
+            fp16_results = run_experiments(use_fp16=True, batch_size=batch_size, is_baseline=is_baseline)
+            output_details(fp16_results, "longformer_base_fp16.csv")
+            all_results += fp16_results
+    for metric_name in ["average_latency_ms", "QPS", "memory", "diff_90_percentile"]:
+        output_summary(all_results, f"{experiment_name}_{metric_name}.csv", metric_name)
+
+    all_results = []
+    for _ in range(total_runs):
+        for batch_size in fp32_batch_sizes:
+            fp32_results = run_experiments(use_fp16=False, batch_size=batch_size, is_baseline=is_baseline)
+            output_details(fp32_results, "longformer_base_fp32.csv")
+            all_results += fp32_results
+    for metric_name in ["average_latency_ms", "QPS", "memory", "diff_90_percentile"]:
+        output_summary(all_results, f"{experiment_name}_{metric_name}.csv", metric_name)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/convert_tf_models_to_pytorch.py` & `onnxruntime/transformers/convert_tf_models_to_pytorch.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,107 +1,149 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 import glob
 import os
+
 import requests
 
 TFMODELS = {
-    "bert-base-uncased":
-    ("bert", "BertConfig", "", "https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"),
-    "bert-base-cased":
-    ("bert", "BertConfig", "", "https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip"),
-    "bert-large-uncased":
-    ("bert", "BertConfig", "", "https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip"),
-    "albert-base": ("albert", "AlbertConfig", "", "https://storage.googleapis.com/albert_models/albert_base_v1.tar.gz"),
-    "albert-large":
-    ("albert", "AlbertConfig", "", "https://storage.googleapis.com/albert_models/albert_large_v1.tar.gz"),
-    "gpt-2-117M": ("gpt2", "GPT2Config", "GPT2Model", "https://storage.googleapis.com/gpt-2/models/117M"),
-    "gpt-2-124M": ("gpt2", "GPT2Config", "GPT2Model", "https://storage.googleapis.com/gpt-2/models/124M")
+    "bert-base-uncased": (
+        "bert",
+        "BertConfig",
+        "",
+        "https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip",
+    ),
+    "bert-base-cased": (
+        "bert",
+        "BertConfig",
+        "",
+        "https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip",
+    ),
+    "bert-large-uncased": (
+        "bert",
+        "BertConfig",
+        "",
+        "https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip",
+    ),
+    "albert-base": (
+        "albert",
+        "AlbertConfig",
+        "",
+        "https://storage.googleapis.com/albert_models/albert_base_v1.tar.gz",
+    ),
+    "albert-large": (
+        "albert",
+        "AlbertConfig",
+        "",
+        "https://storage.googleapis.com/albert_models/albert_large_v1.tar.gz",
+    ),
+    "gpt-2-117M": (
+        "gpt2",
+        "GPT2Config",
+        "GPT2Model",
+        "https://storage.googleapis.com/gpt-2/models/117M",
+    ),
+    "gpt-2-124M": (
+        "gpt2",
+        "GPT2Config",
+        "GPT2Model",
+        "https://storage.googleapis.com/gpt-2/models/124M",
+    ),
 }
 
 
 def download_compressed_file(tf_ckpt_url, ckpt_dir):
     r = requests.get(tf_ckpt_url)
     compressed_file_name = tf_ckpt_url.split("/")[-1]
     compressed_file_dir = os.path.join(ckpt_dir, compressed_file_name)
-    with open(compressed_file_dir, 'wb') as f:
+    with open(compressed_file_dir, "wb") as f:
         f.write(r.content)
     return compressed_file_dir
 
 
 def get_ckpt_prefix_path(ckpt_dir):
     # get prefix
     sub_folder_dir = None
     for o in os.listdir(ckpt_dir):
         sub_folder_dir = os.path.join(ckpt_dir, o)
         break
     if os.path.isfile(sub_folder_dir):
         sub_folder_dir = ckpt_dir
     unique_file_name = str(glob.glob(sub_folder_dir + "/*data-00000-of-00001"))
-    prefix = (unique_file_name.rpartition('.')[0]).split("/")[-1]
+    prefix = (unique_file_name.rpartition(".")[0]).split("/")[-1]
 
     return os.path.join(sub_folder_dir, prefix)
 
 
 def download_tf_checkpoint(model_name, tf_models_dir="tf_models"):
     import pathlib
+
     base_dir = os.path.join(pathlib.Path(__file__).parent.absolute(), tf_models_dir)
     ckpt_dir = os.path.join(base_dir, model_name)
 
     if not os.path.exists(ckpt_dir):
         os.makedirs(ckpt_dir)
 
     tf_ckpt_url = TFMODELS[model_name][3]
 
     import re
-    if (re.search('.zip$', tf_ckpt_url) != None):
+
+    if re.search(".zip$", tf_ckpt_url) is not None:
         zip_dir = download_compressed_file(tf_ckpt_url, ckpt_dir)
 
         # unzip file
         import zipfile
-        with zipfile.ZipFile(zip_dir, 'r') as zip_ref:
+
+        with zipfile.ZipFile(zip_dir, "r") as zip_ref:
             zip_ref.extractall(ckpt_dir)
             os.remove(zip_dir)
 
         return get_ckpt_prefix_path(ckpt_dir)
 
-    elif (re.search('.tar.gz$', tf_ckpt_url) != None):
+    elif re.search(".tar.gz$", tf_ckpt_url) is not None:
         tar_dir = download_compressed_file(tf_ckpt_url, ckpt_dir)
 
         # untar file
         import tarfile
-        with tarfile.open(tar_dir, 'r') as tar_ref:
+
+        with tarfile.open(tar_dir, "r") as tar_ref:
             tar_ref.extractall(ckpt_dir)
             os.remove(tar_dir)
 
         return get_ckpt_prefix_path(ckpt_dir)
 
     else:
-        for filename in ['checkpoint', 'model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta']:
+        for filename in [
+            "checkpoint",
+            "model.ckpt.data-00000-of-00001",
+            "model.ckpt.index",
+            "model.ckpt.meta",
+        ]:
             r = requests.get(tf_ckpt_url + "/" + filename)
-            with open(os.path.join(ckpt_dir, filename), 'wb') as f:
+            with open(os.path.join(ckpt_dir, filename), "wb") as f:
                 f.write(r.content)
 
         return get_ckpt_prefix_path(ckpt_dir)
 
 
 def init_pytorch_model(model_name, tf_checkpoint_path):
     config_name = TFMODELS[model_name][1]
     config_module = __import__("transformers", fromlist=[config_name])
     model_config = getattr(config_module, config_name)
 
-    parent_path = tf_checkpoint_path.rpartition('/')[0]
+    parent_path = tf_checkpoint_path.rpartition("/")[0]
     config_path = glob.glob(parent_path + "/*config.json")
     config = model_config() if len(config_path) == 0 else model_config.from_json_file(str(config_path[0]))
 
-    if TFMODELS[model_name][2] == "":
+    if not TFMODELS[model_name][2]:
         from transformers import AutoModelForPreTraining
+
         init_model = AutoModelForPreTraining.from_config(config)
     else:
         model_categroy_name = TFMODELS[model_name][2]
         module = __import__("transformers", fromlist=[model_categroy_name])
         model_categroy = getattr(module, model_categroy_name)
         init_model = model_categroy(config)
     return config, init_model
@@ -114,19 +156,23 @@
 
     if is_tf2 is False:
         load_tf_weight_func = getattr(module, load_tf_weight_func_name)
     else:
         if TFMODELS[model_name][0] != "bert":
             raise NotImplementedError("Only support tf2 ckeckpoint for Bert model")
         from transformers import convert_bert_original_tf2_checkpoint_to_pytorch
+
         load_tf_weight_func = convert_bert_original_tf2_checkpoint_to_pytorch.load_tf2_weights_in_bert
 
     # Expect transformers team will unify the order of signature in the future
-    model = load_tf_weight_func(init_model, config, tf_checkpoint_path) if is_tf2 is False else load_tf_weight_func(
-        init_model, tf_checkpoint_path, config)
+    model = (
+        load_tf_weight_func(init_model, config, tf_checkpoint_path)
+        if is_tf2 is False
+        else load_tf_weight_func(init_model, tf_checkpoint_path, config)
+    )
     model.eval()
     return model
 
 
 def tf2pt_pipeline(model_name, is_tf2=False):
     if model_name not in TFMODELS:
         raise NotImplementedError(model_name + " not implemented")
@@ -136,22 +182,24 @@
     # Could then use the model in Benchmark
     return config, model
 
 
 def tf2pt_pipeline_test():
     # For test on linux only
     import logging
+
     import torch
-    logger = logging.getLogger('')
-    for model_name in TFMODELS.keys():
+
+    logger = logging.getLogger("")
+    for model_name in TFMODELS:
         config, model = tf2pt_pipeline(model_name)
-        assert (config.model_type is TFMODELS[model_name][0])
+        assert config.model_type is TFMODELS[model_name][0]
 
         input = torch.randint(low=0, high=config.vocab_size - 1, size=(4, 128), dtype=torch.long)
         try:
             model(input)
         except RuntimeError as e:
             logger.exception(e)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     tf2pt_pipeline_test()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/convert_to_onnx.py` & `onnxruntime/transformers/models/gpt2/convert_to_onnx.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 # -------------------------------------------------------------------------
-# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 """
 This converts GPT2 model to onnx. Examples:
 (1) Convert pretrained model 'gpt2' to ONNX
@@ -11,224 +10,255 @@
 (2) Convert pretrained model 'distilgpt2' to ONNX, and use optimizer to get float16 model.
    python convert_to_onnx.py -m distilgpt2 --output distilgpt2_fp16.onnx -o -p fp16
 (3) Convert a model check point to ONNX, and run optimization and int8 quantization
    python convert_to_onnx.py -m ./my_model_checkpoint/ --output my_model_int8.onnx -o -p int8
 
 """
 
-import os
 import argparse
-import logging
-import torch
-import numpy
 import json
+import logging
+import os
+import sys
 from pathlib import Path
+
+import numpy
+import torch
+from gpt2_helper import DEFAULT_TOLERANCE, MODEL_CLASSES, PRETRAINED_GPT2_MODELS, Gpt2Helper
+from gpt2_tester import Gpt2Tester
 from packaging import version
 from transformers import AutoConfig
-from gpt2_helper import DEFAULT_TOLERANCE, PRETRAINED_GPT2_MODELS
-from gpt2_beamsearch_helper import Gpt2HelperFactory, MODEL_CLASSES
-from gpt2_beamsearch_tester import Gpt2TesterFactory
-from quantize_helper import QuantizeHelper
-from benchmark_helper import create_onnxruntime_session, setup_logger, prepare_environment, Precision
 
-logger = logging.getLogger('')
+sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
+
+from benchmark_helper import (  # noqa: E402
+    Precision,
+    create_onnxruntime_session,
+    get_ort_environment_variables,
+    prepare_environment,
+    setup_logger,
+)
+from quantize_helper import QuantizeHelper  # noqa: E402
+
+logger = logging.getLogger("")
 
 
 def parse_arguments(argv=None):
     parser = argparse.ArgumentParser()
 
-    parser.add_argument('-m',
-                        '--model_name_or_path',
-                        required=True,
-                        type=str,
-                        help='Model path, or pretrained model name in the list: ' + ', '.join(PRETRAINED_GPT2_MODELS))
-
-    parser.add_argument('--model_class',
-                        required=False,
-                        type=str,
-                        default='GPT2LMHeadModel',
-                        choices=list(MODEL_CLASSES.keys()),
-                        help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))
-
-    parser.add_argument('--cache_dir',
-                        required=False,
-                        type=str,
-                        default=os.path.join('.', 'cache_models'),
-                        help='Directory to cache pre-trained models')
-
-    parser.add_argument('--output',
-                        required=False,
-                        type=str,
-                        default=os.path.join('.', 'onnx_models'),
-                        help='Output directory, or model path ends with .onnx')
-
-    parser.add_argument('-o',
-                        '--optimize_onnx',
-                        required=False,
-                        action='store_true',
-                        help='Use optimizer.py to optimize onnx model')
+    parser.add_argument(
+        "-m",
+        "--model_name_or_path",
+        required=True,
+        type=str,
+        help="Model path, or pretrained model name in the list: " + ", ".join(PRETRAINED_GPT2_MODELS),
+    )
+
+    parser.add_argument(
+        "--model_class",
+        required=False,
+        type=str,
+        default="GPT2LMHeadModel",
+        choices=list(MODEL_CLASSES.keys()),
+        help="Model type selected in the list: " + ", ".join(MODEL_CLASSES.keys()),
+    )
+
+    parser.add_argument(
+        "--cache_dir",
+        required=False,
+        type=str,
+        default=os.path.join(".", "cache_models"),
+        help="Directory to cache pre-trained models",
+    )
+
+    parser.add_argument(
+        "--output",
+        required=False,
+        type=str,
+        default=os.path.join(".", "onnx_models"),
+        help="Output directory, or model path ends with .onnx",
+    )
+
+    parser.add_argument(
+        "-o",
+        "--optimize_onnx",
+        required=False,
+        action="store_true",
+        help="Use optimizer.py to optimize onnx model",
+    )
     parser.set_defaults(optimize_onnx=False)
 
-    parser.add_argument('--use_gpu', required=False, action='store_true', help="use GPU for inference")
+    parser.add_argument("--use_gpu", required=False, action="store_true", help="use GPU for inference")
     parser.set_defaults(use_gpu=False)
 
-    parser.add_argument('--tolerance',
-                        required=False,
-                        type=float,
-                        default=0,
-                        help="the aboslute and relative tolerance for parity verification")
-
-    parser.add_argument('--input_test_file',
-                        '-i',
-                        required=False,
-                        type=str,
-                        default='',
-                        help='Path to the file with inputs to test with')
+    parser.add_argument(
+        "--provider",
+        required=False,
+        default=None,
+        choices=["dml", "rocm", "migraphx", "cuda", "tensorrt"],
+        help="use dml, rocm, cuda, tensorrt or migraphx for respective backend",
+    )
+
+    parser.add_argument(
+        "--tolerance",
+        required=False,
+        type=float,
+        default=0,
+        help="the aboslute and relative tolerance for parity verification",
+    )
+
+    parser.add_argument(
+        "--input_test_file",
+        "-i",
+        required=False,
+        type=str,
+        default="",
+        help="Path to the file with inputs to test with",
+    )
 
     parser.add_argument(
         "-p",
         "--precision",
         required=False,
         type=Precision,
         default=Precision.FLOAT32,
         choices=list(Precision),
-        help=
-        "Precision of model to run. fp32 for full precision, fp16 for half or mixed precision, and int8 for quantization"
+        help="Precision of model to run. fp32 for full precision, fp16 for half or mixed precision, and int8 for quantization",
     )
 
-    parser.add_argument("-t",
-                        "--test_cases",
-                        required=False,
-                        type=int,
-                        default=1000,
-                        help="Number of test cases per run for parity")
-    parser.add_argument("-r",
-                        "--test_runs",
-                        required=False,
-                        type=int,
-                        default=10,
-                        help="Number of runs for parity. It is used for significance test.")
+    parser.add_argument(
+        "-t",
+        "--test_cases",
+        required=False,
+        type=int,
+        default=1000,
+        help="Number of test cases per run for parity",
+    )
+    parser.add_argument(
+        "-r",
+        "--test_runs",
+        required=False,
+        type=int,
+        default=10,
+        help="Number of runs for parity. It is used for significance test.",
+    )
 
-    parser.add_argument('--verbose', required=False, action='store_true')
+    parser.add_argument("--verbose", required=False, action="store_true")
     parser.set_defaults(verbose=False)
 
-    parser.add_argument('-e', '--use_external_data_format', required=False, action='store_true')
+    parser.add_argument("-e", "--use_external_data_format", required=False, action="store_true")
     parser.set_defaults(use_external_data_format=False)
 
-    parser.add_argument('--use_int32_inputs',
-                        required=False,
-                        action='store_true',
-                        help='Use int32 instead of int64 for input_ids, position_ids and attention_mask.')
-    parser.set_defaults(use_int32_inputs=False)
-
-    parser.add_argument('--beam_size', type=int, default=4, help='Beam size if greedy/top-p/top-k sampling is needed')
-
-    search_option_group = parser.add_argument_group("configurable one step search options")
-
-    search_option_group.add_argument('--ignore_eos',
-                                     type=bool,
-                                     default=False,
-                                     help='If ignore end of sentence token in model inference.')
-    search_option_group.add_argument('--repetition_penalty',
-                                     type=float,
-                                     default=1,
-                                     help='Positive. >1 to penalize and <1 to encorage.')
-    search_option_group.add_argument('--temperature',
-                                     type=float,
-                                     default=1,
-                                     help='Softmax temperature for output logits.')
-    search_option_group.add_argument('--excluded_token_ids',
-                                     required=False,
-                                     nargs='+',
-                                     type=float,
-                                     help='A list of token ids to be excluded in inference.')
-    search_option_group.add_argument('--length_penalty',
-                                     type=float,
-                                     default=1,
-                                     help='Positive. >1 to penalize and <1 to encorage short sentence.')
-
-    sampling_option_group = parser.add_argument_group("one step sampling options")
-    sampling_option_group.add_argument('--do_sample',
-                                       action='store_true',
-                                       help='If to do sampling instead of beam search or greedy.')
-    sampling_option_group.add_argument('--do_sample_top_p',
-                                       type=float,
-                                       default=0.95,
-                                       help='Nuclear/top-p sampling accumulation probability.')
-    sampling_option_group.add_argument('--do_sample_top_k', type=int, default=0, help='Use top-k if non-zero.')
+    parser.add_argument("--overwrite", required=False, action="store_true")
+    parser.set_defaults(overwrite=False)
+
+    parser.add_argument(
+        "--use_int64_inputs",
+        required=False,
+        action="store_true",
+        help="Use int32 instead of int64 for input_ids, position_ids and attention_mask.",
+    )
+    parser.set_defaults(use_int64_inputs=False)
+
+    parser.add_argument(
+        "-s",
+        "--stage",
+        type=int,
+        default=0,
+        required=False,
+        choices=[0, 1, 2],
+        help="Stage in generation: 1 (initial decoder), 2 (decoder), 0 (both). "
+        "1 - decode the first token when past_sequence_length is zero; "
+        "2 - decode the remaining tokens when past_sequence_length is not zero; "
+        "0 - one onnx model for both stages 1 and 2. "
+        "Note that we will optimize 1 and 2 differently for best performance.",
+    )
 
     fp16_option_group = parser.add_argument_group(
-        "float to float16 conversion parameters that works when \"--precision fp16\" is specified")
+        'float to float16 conversion parameters that works when "--precision fp16" is specified'
+    )
 
     fp16_option_group.add_argument(
-        '-a',
-        '--auto_mixed_precision',
+        "-a",
+        "--auto_mixed_precision",
         required=False,
-        action='store_true',
-        help='Convert to mixed precision automatically. Other float16 conversion parameters will be ignored.')
+        action="store_true",
+        help="Convert to mixed precision automatically. Other float16 conversion parameters will be ignored.",
+    )
     fp16_option_group.set_defaults(auto_mixed_precision=False)
 
-    fp16_option_group.add_argument('--keep_io_types',
-                                   required=False,
-                                   action='store_true',
-                                   help='Use float32 for past inputs, present and logits outputs.')
+    fp16_option_group.add_argument(
+        "--keep_io_types",
+        required=False,
+        action="store_true",
+        help="Use float32 for past inputs, present and logits outputs.",
+    )
     fp16_option_group.set_defaults(keep_io_types=False)
 
-    fp16_option_group.add_argument('--io_block_list',
-                                   nargs='+',
-                                   default=[],
-                                   help='List of inputs or outputs in float32 instead of float16')
+    fp16_option_group.add_argument(
+        "--io_block_list",
+        nargs="+",
+        default=[],
+        help="List of inputs or outputs in float32 instead of float16",
+    )
 
     fp16_option_group.add_argument(
-        '--op_block_list',
-        nargs='+',
+        "--op_block_list",
+        nargs="+",
         default=[],
-        help=
-        'List of operators (like Attention Gather Add LayerNormalization FastGelu MatMul) to compute in float32 instead of float16.'
+        help="List of operators (like Add LayerNormalization SkipLayerNormalization EmbedLayerNormalization FastGelu) "
+        "to compute in float32 instead of float16.",
     )
 
-    fp16_option_group.add_argument('--node_block_list',
-                                   nargs='+',
-                                   default=[],
-                                   help='List of node names to compute in float32 instead of float16.')
-
-    fp16_option_group.add_argument('--force_fp16_initializers',
-                                   required=False,
-                                   action='store_true',
-                                   help='Convert all float initializers to float16.')
+    fp16_option_group.add_argument(
+        "--node_block_list",
+        nargs="+",
+        default=[],
+        help="List of node names to compute in float32 instead of float16.",
+    )
+
+    fp16_option_group.add_argument(
+        "--force_fp16_initializers",
+        required=False,
+        action="store_true",
+        help="Convert all float initializers to float16.",
+    )
     fp16_option_group.set_defaults(force_fp16_initializers=False)
 
     args = parser.parse_args(argv)
 
     return args
 
 
 def get_onnx_model_size(onnx_path: str, use_external_data_format: bool):
     if not use_external_data_format:
         return os.path.getsize(onnx_path)
     else:
-        return sum([f.stat().st_size for f in Path(onnx_path).parent.rglob('*')])
+        return sum([f.stat().st_size for f in Path(onnx_path).parent.rglob("*")])
 
 
-def get_latency_name():
-    return "average_latency(batch_size=8,sequence_length=1,past_sequence_length=32)"
+def get_latency_name(batch_size, sequence_length, past_sequence_length):
+    return f"average_latency(batch_size={batch_size},sequence_length={sequence_length},past_sequence_length={past_sequence_length})"
 
 
-def main(argv=None, experiment_name="", run_id=0, csv_filename="gpt2_parity_results.csv"):
+def main(argv=None, experiment_name: str = "", run_id: str = "0", csv_filename: str = "gpt2_parity_results.csv"):
     result = {}
     from transformers import __version__ as transformers_version
+
     if version.parse(transformers_version) < version.parse(
-            "3.1.0"):  # past_key_values name does not exist in 3.0.2 or older
+        "3.1.0"
+    ):  # past_key_values name does not exist in 3.0.2 or older
         raise RuntimeError("This tool requires transformers 3.1.0 or later.")
 
     args = parse_arguments(argv)
     setup_logger(args.verbose)
 
     if not experiment_name:
         import sys
+
         experiment_name = " ".join(argv if argv else sys.argv[1:])
 
     if args.tolerance == 0:
         args.tolerance = DEFAULT_TOLERANCE[args.precision]
 
     logger.info(f"Arguments:{args}")
 
@@ -241,275 +271,295 @@
 
     if args.precision == Precision.FLOAT16:
         assert args.use_gpu, "fp16 requires --use_gpu"
 
     if args.precision == Precision.INT8:
         assert not args.use_gpu, "quantization only supports CPU"
 
-    if args.use_external_data_format:
-        assert not args.output.endswith('.onnx'), "output shall be a directory for --use_external_data_format"
-
     model_class = MODEL_CLASSES[args.model_class][0]
     use_padding = MODEL_CLASSES[args.model_class][2]
 
-    if args.model_class == "GPT2LMHeadModel_BeamSearchStep":
-        model_type = "beam_search_step"
-    elif args.model_class == "GPT2LMHeadModel_ConfigurableOneStepSearch":
-        model_type = "configurable_one_step_search"
-    else:
-        model_type = "default"
-
-    gpt2helper = Gpt2HelperFactory.create_helper(model_type)
-    gpt2tester = Gpt2TesterFactory.create_tester(model_type)
+    gpt2helper = Gpt2Helper
     config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=cache_dir)
-    if model_type == 'beam_search_step':
-        model = model_class.from_pretrained(args.model_name_or_path,
-                                            config=config,
-                                            batch_size=1,
-                                            beam_size=args.beam_size,
-                                            cache_dir=cache_dir)
-    elif model_type == 'configurable_one_step_search':
-        model = model_class.from_pretrained(args.model_name_or_path,
-                                            config=config,
-                                            batch_size=1,
-                                            beam_size=args.beam_size,
-                                            ignore_eos=args.ignore_eos,
-                                            temperature=args.temperature,
-                                            repetition_penalty=args.repetition_penalty,
-                                            excluded_token_ids=args.excluded_token_ids,
-                                            length_penalty=args.length_penalty,
-                                            do_sample=args.do_sample,
-                                            do_sample_top_p=args.do_sample_top_p,
-                                            do_sample_top_k=args.do_sample_top_k,
-                                            cache_dir=cache_dir)
-    else:
-        model = model_class.from_pretrained(args.model_name_or_path, config=config, cache_dir=cache_dir)
+    model = model_class.from_pretrained(args.model_name_or_path, config=config, cache_dir=cache_dir)
 
     device = torch.device("cuda:0" if args.use_gpu else "cpu")
     model.eval().to(device)
 
     if (not args.use_external_data_format) and (config.n_layer > 24):
-        logger.info(f"Try --use_external_data_format when model size > 2GB")
+        logger.info("Try --use_external_data_format when model size > 2GB")
 
     onnx_model_paths = gpt2helper.get_onnx_paths(
         output_dir,
         args.model_name_or_path,
         args.model_class,
-        new_folder=args.use_external_data_format,
-        remove_existing=["fp32", "fp16", "int8"])  # Do not remove raw model to save time in parity test
+        new_folder=(args.precision == Precision.INT8),
+        remove_existing=["fp32", "fp16", "int8"],
+    )  # Do not remove raw model to save time in parity test
 
     raw_onnx_model = onnx_model_paths["raw"]
 
-    if os.path.exists(raw_onnx_model):
+    int_data_type = torch.int64 if args.use_int64_inputs else torch.int32
+
+    if os.path.exists(raw_onnx_model) and not args.overwrite:
         logger.warning(f"Skip exporting ONNX model since it existed: {raw_onnx_model}")
     else:
         logger.info(f"Exporting ONNX model to {raw_onnx_model}")
-        gpt2helper.export_onnx(model,
-                               device,
-                               raw_onnx_model,
-                               args.verbose,
-                               args.use_external_data_format,
-                               has_position_ids=use_padding,
-                               has_attention_mask=use_padding,
-                               input_ids_dtype=torch.int32 if args.use_int32_inputs else torch.int64,
-                               position_ids_dtype=torch.int32 if args.use_int32_inputs else torch.int64,
-                               attention_mask_dtype=torch.int32 if args.use_int32_inputs else torch.int64)
+        gpt2helper.export_onnx(
+            model,
+            device,
+            raw_onnx_model,
+            args.verbose,
+            args.use_external_data_format,
+            has_position_ids=use_padding,
+            has_attention_mask=use_padding,
+            input_ids_dtype=int_data_type,
+            position_ids_dtype=int_data_type,
+            attention_mask_dtype=int_data_type,
+        )
 
     fp16_params = {"keep_io_types": args.keep_io_types}
     if args.io_block_list:
         fp16_params["keep_io_types"] = args.io_block_list
     if args.node_block_list:
         fp16_params["node_block_list"] = args.node_block_list
     if args.op_block_list:
         fp16_params["op_block_list"] = args.op_block_list
     if args.force_fp16_initializers:
         fp16_params["force_fp16_initializers"] = args.force_fp16_initializers
 
-    is_io_float16 = (args.precision == Precision.FLOAT16 and not args.keep_io_types)
+    is_io_float16 = args.precision == Precision.FLOAT16 and not args.keep_io_types
 
+    optimized_ops = ""
+    all_ops = ""
     if args.optimize_onnx or args.precision != Precision.FLOAT32:
-        output_path = onnx_model_paths[str(args.precision) if args.precision != Precision.INT8 else 'fp32']
+        output_path = onnx_model_paths[str(args.precision) if args.precision != Precision.INT8 else "fp32"]
 
         logger.info(f"Optimizing model to {output_path}")
-        gpt2helper.optimize_onnx(raw_onnx_model,
-                                 output_path,
-                                 args.precision == Precision.FLOAT16,
-                                 model.config.num_attention_heads,
-                                 model.config.hidden_size,
-                                 args.use_external_data_format,
-                                 auto_mixed_precision=args.auto_mixed_precision,
-                                 **fp16_params)
+        m = gpt2helper.optimize_onnx(
+            raw_onnx_model,
+            output_path,
+            args.precision == Precision.FLOAT16,
+            model.config.num_attention_heads,
+            model.config.hidden_size,
+            args.use_external_data_format,
+            auto_mixed_precision=args.auto_mixed_precision,
+            stage=args.stage,
+            **fp16_params,
+        )
+
+        nodes = m.nodes()
+        op_list = {node.op_type for node in nodes}
+        all_ops = ",".join(op_list)
+
+        # print optimized operators
+        optimized_op_counter = m.get_fused_operator_statistics()
+        if optimized_op_counter:
+            optimized_ops = ",".join([key for key in optimized_op_counter if optimized_op_counter[key] > 0])
     else:
         output_path = raw_onnx_model
 
     if args.precision == Precision.INT8:
         logger.info("quantizing model...")
-        QuantizeHelper.quantize_onnx_model(output_path, onnx_model_paths['int8'], args.use_external_data_format)
+        QuantizeHelper.quantize_onnx_model(output_path, onnx_model_paths["int8"], args.use_external_data_format)
         model = QuantizeHelper.quantize_torch_model(model)
         logger.info("finished quantizing model")
-        output_path = onnx_model_paths['int8']
+        output_path = onnx_model_paths["int8"]
 
-    if args.output.endswith('.onnx') and output_path != args.output and not args.use_external_data_format:
+    if args.output.endswith(".onnx") and output_path != args.output and not args.use_external_data_format:
         import shutil
+
         shutil.move(output_path, args.output)
         output_path = args.output
 
     logger.info(f"Output path: {output_path}")
-    model_size_in_MB = int(get_onnx_model_size(output_path, args.use_external_data_format) / 1024 / 1024)
+    model_size_in_MB = int(get_onnx_model_size(output_path, args.use_external_data_format) / 1024 / 1024)  # noqa: N806
 
-    session = create_onnxruntime_session(output_path, args.use_gpu, enable_all_optimization=True, verbose=args.verbose)
+    session = create_onnxruntime_session(
+        output_path, args.use_gpu, args.provider, enable_all_optimization=True, verbose=args.verbose
+    )
     if args.model_class == "GPT2LMHeadModel" and session is not None:
         parity_result = gpt2helper.test_parity(
             session,
             model,
             device,
             is_io_float16,
             rtol=args.tolerance,
             atol=args.tolerance,
             model_class=args.model_class,
             has_position_ids=use_padding,
             has_attention_mask=use_padding,
-            input_ids_dtype=torch.int32 if args.use_int32_inputs else torch.int64,
-            position_ids_dtype=torch.int32 if args.use_int32_inputs else torch.int64,
-            attention_mask_dtype=torch.int32 if args.use_int32_inputs else torch.int64,
+            input_ids_dtype=int_data_type,
+            position_ids_dtype=int_data_type,
+            attention_mask_dtype=int_data_type,
             test_cases_per_run=args.test_cases,
             total_runs=args.test_runs,
-            verbose=args.verbose)
+            stage=args.stage,
+            verbose=args.verbose,
+        )
+
+        # An example configuration for testing performance
+        batch_size = 8
+        sequence_length = 32 if args.stage == 1 else 1
+        past_sequence_length = 0 if args.stage == 1 else 32
 
         latency = gpt2helper.test_performance(
             session,
             model,
             device,
             is_io_float16,
             total_runs=100,
             use_io_binding=True,
             model_class=args.model_class,
             has_position_ids=use_padding,
             has_attention_mask=use_padding,
-            input_ids_dtype=torch.int32 if args.use_int32_inputs else torch.int64,
-            position_ids_dtype=torch.int32 if args.use_int32_inputs else torch.int64,
-            attention_mask_dtype=torch.int32 if args.use_int32_inputs else torch.int64,
-            batch_size=8,
-            sequence_length=1,
-            past_sequence_length=32)
+            input_ids_dtype=int_data_type,
+            position_ids_dtype=int_data_type,
+            attention_mask_dtype=int_data_type,
+            batch_size=batch_size,
+            sequence_length=sequence_length,
+            past_sequence_length=past_sequence_length,
+        )
 
         if args.precision == Precision.FLOAT16:
             logger.info(f"fp16 conversion parameters:{fp16_params}")
 
         # Write results to file
         import csv
+
         from onnxruntime import __version__ as ort_version
-        latency_name = get_latency_name()
+
+        latency_name = get_latency_name(batch_size, sequence_length, past_sequence_length)
         csv_file_existed = os.path.exists(csv_filename)
-        with open(csv_filename, mode="a", newline='') as csv_file:
+        with open(csv_filename, mode="a", newline="") as csv_file:
             column_names = [
-                "experiment", "run_id", "model_name", "model_class", "gpu", "precision", "optimizer", "test_cases",
-                "runs", "keep_io_types", "io_block_list", "op_block_list", "node_block_list", "force_fp16_initializers",
-                "auto_mixed_precision", "ORT_TRANSFORMER_OPTIONS", "ORT_CUDA_GEMM_OPTIONS", "onnxruntime", latency_name,
-                "top1_match_rate", "onnx_size_in_MB", "diff_50_percentile", "diff_90_percentile", "diff_95_percentile",
-                "diff_99_percentile", "diff_pass_rate", "nan_rate", "top1_match_rate_per_run"
+                "experiment",
+                "run_id",
+                "model_name",
+                "model_class",
+                "stage",
+                "gpu",
+                "precision",
+                "optimizer",
+                "test_cases",
+                "runs",
+                "keep_io_types",
+                "io_block_list",
+                "op_block_list",
+                "node_block_list",
+                "force_fp16_initializers",
+                "auto_mixed_precision",
+                "optimized_operators",
+                "operators",
+                "environment_variables",
+                "onnxruntime",
+                latency_name,
+                "top1_match_rate",
+                "onnx_size_in_MB",
+                "diff_50_percentile",
+                "diff_90_percentile",
+                "diff_95_percentile",
+                "diff_99_percentile",
+                "diff_pass_rate",
+                "nan_rate",
+                "top1_match_rate_per_run",
             ]
             csv_writer = csv.DictWriter(csv_file, fieldnames=column_names)
             if not csv_file_existed:
                 csv_writer.writeheader()
             row = {
                 "experiment": experiment_name,
                 "run_id": run_id,
                 "model_name": args.model_name_or_path,
                 "model_class": args.model_class,
+                "stage": args.stage,
                 "gpu": args.use_gpu,
                 "precision": args.precision,
                 "optimizer": args.optimize_onnx,
                 "test_cases": args.test_cases,
                 "runs": args.test_runs,
                 "keep_io_types": args.keep_io_types,
                 "io_block_list": args.io_block_list,
                 "op_block_list": args.op_block_list,
                 "node_block_list": args.node_block_list,
                 "force_fp16_initializers": args.force_fp16_initializers,
                 "auto_mixed_precision": args.auto_mixed_precision,
-                "ORT_TRANSFORMER_OPTIONS": os.getenv('ORT_TRANSFORMER_OPTIONS'),
-                "ORT_CUDA_GEMM_OPTIONS": os.getenv('ORT_CUDA_GEMM_OPTIONS'),
+                "optimized_operators": optimized_ops,
+                "operators": all_ops,
+                "environment_variables": get_ort_environment_variables(),
                 "onnxruntime": ort_version,
                 latency_name: f"{latency:.2f}",
                 "diff_50_percentile": parity_result["max_diff_percentile_50"],
                 "diff_90_percentile": parity_result["max_diff_percentile_90"],
                 "diff_95_percentile": parity_result["max_diff_percentile_95"],
                 "diff_99_percentile": parity_result["max_diff_percentile_99"],
                 "diff_pass_rate": parity_result["diff_pass_rate"],
                 "nan_rate": parity_result["nan_rate"],
                 "top1_match_rate": parity_result["top1_match_rate"],
                 "top1_match_rate_per_run": parity_result["top1_match_rate_per_run"],
-                "onnx_size_in_MB": "{}".format(model_size_in_MB),
+                "onnx_size_in_MB": f"{model_size_in_MB}",
             }
             logger.info(f"result: {row}")
             result.update(row)
             csv_writer.writerow(row)
 
     if args.input_test_file:
         test_inputs = []
         # Each line of test file is a JSON string like:
         # {"input_ids": [[14698, 257, 1310, 13688, 319, 326]]}
         with open(args.input_test_file) as read_f:
             for _, line in enumerate(read_f):
-                line = line.rstrip()
+                line = line.rstrip()  # noqa: PLW2901
                 data = json.loads(line)
                 input_ids = torch.from_numpy(numpy.asarray(data["input_ids"], dtype=numpy.int64)).to(device)
 
                 if use_padding:
                     if "attention_mask" in data:
                         numpy_float = numpy.float16 if is_io_float16 else numpy.float32
-                        attention_mask = torch.from_numpy(numpy.asarray(data["attention_mask"],
-                                                                        dtype=numpy_float)).to(device)
+                        attention_mask = torch.from_numpy(numpy.asarray(data["attention_mask"], dtype=numpy_float)).to(
+                            device
+                        )
                     else:
                         padding = -1
                         attention_mask = (input_ids != padding).type(torch.float16 if is_io_float16 else torch.float32)
                         input_ids.masked_fill_(input_ids == padding, 0)
 
                     if "position_ids" in data:
-                        position_ids = torch.from_numpy(numpy.asarray(data["position_ids"],
-                                                                      dtype=numpy.int64)).to(device)
+                        position_ids = torch.from_numpy(numpy.asarray(data["position_ids"], dtype=numpy.int64)).to(
+                            device
+                        )
                     else:
-                        position_ids = (attention_mask.long().cumsum(-1) - 1)
+                        position_ids = attention_mask.long().cumsum(-1) - 1
                         position_ids.masked_fill_(position_ids < 0, 0)
 
                     inputs = {
-                        "input_ids": input_ids.to(torch.int32) if args.use_int32_inputs else input_ids,
-                        "position_ids": position_ids.to(torch.int32) if args.use_int32_inputs else position_ids,
-                        "attention_mask": attention_mask.to(torch.int32) if args.use_int32_inputs else attention_mask
+                        "input_ids": input_ids.to(int_data_type),
+                        "position_ids": position_ids.to(int_data_type),
+                        "attention_mask": attention_mask.to(int_data_type),
                     }
                 else:
-                    inputs = {"input_ids": input_ids.to(torch.int32) if args.use_int32_inputs else input_ids}
-
-                if model_type == "beam_search_step" or model_type == "configurable_one_step_search":
-                    beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()
-
-                    input_log_probs = torch.zeros([input_ids.shape[0], 1])
-                    input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)
-                    inputs.update({
-                        "beam_select_idx": beam_select_idx,
-                        "input_log_probs": input_log_probs,
-                        "input_unfinished_sents": input_unfinished_sents,
-                    })
+                    inputs = {"input_ids": input_ids.to(int_data_type)}
 
                 test_inputs.append(inputs)
 
-        gpt2tester.test_generation(session,
-                                   model,
-                                   device,
-                                   test_inputs,
-                                   precision=args.precision,
-                                   model_class=args.model_class,
-                                   top_k=20,
-                                   top_k_no_order=True,
-                                   max_steps=24,
-                                   max_inputs=0,
-                                   verbose=args.verbose,
-                                   save_test_data=3,
-                                   save_test_data_dir=Path(output_path).parent)
+        Gpt2Tester.test_generation(
+            session,
+            model,
+            device,
+            test_inputs,
+            precision=args.precision,
+            model_class=args.model_class,
+            top_k=20,
+            top_k_no_order=True,
+            max_steps=24,
+            max_inputs=0,
+            verbose=args.verbose,
+            save_test_data=3,
+            save_test_data_dir=Path(output_path).parent,
+        )
 
     logger.info(f"Done. Output model: {output_path}")
     return result
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/float16.py` & `onnxruntime/transformers/float16.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,52 +1,78 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 # This file is modified from https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/float16.py
-# Modifications: keep_io_types can be list of names; convert initializers if needed to preserve precision; add force_fp16_initializers option.
+# Modifications:
+# (1) Update default value of min_positive_val and max_finite_val
+# (2) keep_io_types can be list of names
+# (3) convert initializers if needed to preserve precision
+# (4) add force_fp16_initializers option
+# (5) handle Resize and GroupNorm with mixed float inputs
+# (6) allow convert_float_to_float16 to accept model path
 
 import itertools
+import logging
+import os
+import tempfile
+from typing import Dict
+
 import numpy as np
 import onnx
 from onnx import helper, numpy_helper
 from onnx import onnx_pb as onnx_proto
-from typing import List, Dict
-
-import logging
+from onnx.shape_inference import infer_shapes, infer_shapes_path
+from packaging import version
 
 logger = logging.getLogger(__name__)
 
 
 def _npfloat16_to_int(np_list):
-    '''
+    """
     Convert numpy float16 to python int.
 
     :param np_list: numpy float16 list
     :return int_list: python int list
-    '''
-    return [int(bin(_.view('H'))[2:].zfill(16), 2) for _ in np_list]
+    """
+    return [int(bin(_.view("H"))[2:].zfill(16), 2) for _ in np_list]
 
 
 def convert_np_to_float16(np_array, min_positive_val=5.96e-08, max_finite_val=65504.0):
-    '''
+    """
     Convert float32 numpy array to float16 without changing sign or finiteness.
     Positive values less than min_positive_val are mapped to min_positive_val.
     Positive finite values greater than max_finite_val are mapped to max_finite_val.
     Similar for negative values. NaN, 0, inf, and -inf are unchanged.
-    '''
+    """
 
     def between(a, b, c):
         return np.logical_and(a < b, b < c)
 
+    if np_array[np.where(np_array > 0)].shape[0] > 0:
+        positive_max = np_array[np.where(np_array > 0)].max()
+        positive_min = np_array[np.where(np_array > 0)].min()
+        if positive_max >= max_finite_val:
+            logger.debug(f"the float32 number {positive_max} will be truncated to {max_finite_val}")
+        if positive_min <= min_positive_val:
+            logger.debug(f"the float32 number {positive_min} will be truncated to {min_positive_val}")
+
+    if np_array[np.where(np_array < 0)].shape[0] > 0:
+        negative_max = np_array[np.where(np_array < 0)].max()
+        negative_min = np_array[np.where(np_array < 0)].min()
+        if negative_min <= -max_finite_val:
+            logger.debug(f"the float32 number {negative_min} will be truncated to {-max_finite_val}")
+        if negative_max >= -min_positive_val:
+            logger.debug(f"the float32 number {negative_max} will be truncated to {-min_positive_val}")
+
     np_array = np.where(between(0, np_array, min_positive_val), min_positive_val, np_array)
     np_array = np.where(between(-min_positive_val, np_array, 0), -min_positive_val, np_array)
-    np_array = np.where(between(max_finite_val, np_array, float('inf')), max_finite_val, np_array)
-    np_array = np.where(between(float('-inf'), np_array, -max_finite_val), -max_finite_val, np_array)
+    np_array = np.where(between(max_finite_val, np_array, float("inf")), max_finite_val, np_array)
+    np_array = np.where(between(float("-inf"), np_array, -max_finite_val), -max_finite_val, np_array)
     return np.float16(np_array)
 
 
 def convert_tensor_float_to_float16(tensor, min_positive_val=5.96e-08, max_finite_val=65504.0):
     """Convert tensor float to float16.
 
     Args:
@@ -58,48 +84,75 @@
         ValueError: input type is not TensorProto.
 
     Returns:
         TensorProto: the converted tensor.
     """
 
     if not isinstance(tensor, onnx_proto.TensorProto):
-        raise ValueError('Expected input type is an ONNX TensorProto but got %s' % type(tensor))
+        raise ValueError(f"Expected input type is an ONNX TensorProto but got {type(tensor)}")
 
     if tensor.data_type == onnx_proto.TensorProto.FLOAT:
         tensor.data_type = onnx_proto.TensorProto.FLOAT16
         # convert float_data (float type) to float16 and write to int32_data
         if tensor.float_data:
             float16_data = convert_np_to_float16(np.array(tensor.float_data), min_positive_val, max_finite_val)
             int_list = _npfloat16_to_int(float16_data)
             tensor.int32_data[:] = int_list
             tensor.float_data[:] = []
         # convert raw_data (bytes type)
         if tensor.raw_data:
             # convert n.raw_data to float
-            float32_list = np.fromstring(tensor.raw_data, dtype='float32')
+            float32_list = np.frombuffer(tensor.raw_data, dtype="float32")
             # convert float to float16
             float16_list = convert_np_to_float16(float32_list, min_positive_val, max_finite_val)
             # convert float16 to bytes and write back to raw_data
-            tensor.raw_data = float16_list.tostring()
+            tensor.raw_data = float16_list.tobytes()
     return tensor
 
 
 def make_value_info_from_tensor(tensor):
     shape = numpy_helper.to_array(tensor).shape
     return helper.make_tensor_value_info(tensor.name, tensor.data_type, shape)
 
 
 DEFAULT_OP_BLOCK_LIST = [
-    'ArrayFeatureExtractor', 'Binarizer', 'CastMap', 'CategoryMapper', 'DictVectorizer', 'FeatureVectorizer', 'Imputer',
-    'LabelEncoder', 'LinearClassifier', 'LinearRegressor', 'Normalizer', 'OneHotEncoder', 'SVMClassifier',
-    'SVMRegressor', 'Scaler', 'TreeEnsembleClassifier', 'TreeEnsembleRegressor', 'ZipMap', 'NonMaxSuppression', 'TopK',
-    'RoiAlign', 'Resize', 'Range', 'CumSum', 'Min', 'Max', 'Upsample'
+    "ArrayFeatureExtractor",
+    "Binarizer",
+    "CastMap",
+    "CategoryMapper",
+    "DictVectorizer",
+    "FeatureVectorizer",
+    "Imputer",
+    "LabelEncoder",
+    "LinearClassifier",
+    "LinearRegressor",
+    "Normalizer",
+    "OneHotEncoder",
+    "RandomUniformLike",
+    "SVMClassifier",
+    "SVMRegressor",
+    "Scaler",
+    "TreeEnsembleClassifier",
+    "TreeEnsembleRegressor",
+    "ZipMap",
+    "NonMaxSuppression",
+    "TopK",
+    "RoiAlign",
+    "Range",
+    "CumSum",
+    "Min",
+    "Max",
+    "Upsample",
 ]
 
 
+# Some operators has data type fixed as float for some inputs. Key is op_type, value is list of input indices
+ALWAYS_FLOAT_INPUTS = {"Resize": [2], "GroupNorm": [1, 2]}
+
+
 class InitializerTracker:
     """Class for keeping track of initializer."""
 
     def __init__(self, initializer: onnx_proto.TensorProto):
         self.initializer = initializer
         self.fp32_nodes = []
         self.fp16_nodes = []
@@ -107,56 +160,74 @@
     def add_node(self, node: onnx_proto.NodeProto, is_node_blocked):
         if is_node_blocked:
             self.fp32_nodes.append(node)
         else:
             self.fp16_nodes.append(node)
 
 
-def convert_float_to_float16(model,
-                             min_positive_val=5.96e-08,
-                             max_finite_val=65504.0,
-                             keep_io_types=False,
-                             disable_shape_infer=False,
-                             op_block_list=None,
-                             node_block_list=None,
-                             force_fp16_initializers=False):
-    """Convert model tensor float type in the ONNX ModelProto input to tensor float16.
+def convert_float_to_float16(
+    model,
+    min_positive_val=5.96e-08,
+    max_finite_val=65504.0,
+    keep_io_types=False,
+    disable_shape_infer=False,
+    op_block_list=None,
+    node_block_list=None,
+    force_fp16_initializers=False,
+):
+    """Convert tensor float type in the input ONNX model to tensor float16.
 
     Args:
-        model (ModelProto): The ONNX model to convert.
+        model (ModelProto or str): The ONNX model or path of the model to convert.
         min_positive_val (float, optional): minimal positive value. Defaults to 5.96e-08.
         max_finite_val (float, optional): maximal finite value of float16. Defaults to 65504.
         keep_io_types (Union[bool, List[str]], optional): It could be boolean or a list of float32 input/output names.
-                                                          If True, model inputs/outputs should be left as float32. Defaults to False.
-        disable_shape_infer (bool, optional): Skips running onnx shape/type inference. Useful if shape inference has been done. Defaults to False.
+                                                          If True, model inputs/outputs should be left as float32.
+                                                          Defaults to False.
+        disable_shape_infer (bool, optional): Skips running onnx shape/type inference.
+                                              Useful if shape inference has been done. Defaults to False.
         op_block_list (List[str], optional): List of op types to leave as float32.
-                                             Defaults to None, which will use `float16.DEFAULT_OP_BLOCK_LIST` as default.
+                                             Defaults to None, which will use `float16.DEFAULT_OP_BLOCK_LIST`.
         node_block_list (List[str], optional): List of node names to leave as float32. Defaults to None.
         force_fp16_initializers(bool): force converting all float initializers to float16.
                                        Default to false, which will convert only the one needed to avoid precision loss.
     Raises:
         ValueError: input type is not ModelProto.
 
     Returns:
         ModelProto: converted model.
     """
-    assert min_positive_val >= 5.96e-08, "invalid min_positive_val. smallest positive float16 value: subnormal 5.96e-08, and normalized 6.104e-05"
+    assert (
+        min_positive_val >= 5.96e-08
+    ), "invalid min_positive_val. smallest positive float16 value: subnormal 5.96e-08, and normalized 6.104e-05"
     assert max_finite_val <= float(np.finfo(np.float16).max), "invalid max_finite_val. largest float16 value: 65504"
 
+    if isinstance(model, str):
+        model_path = model
+        if version.parse(onnx.__version__) >= version.parse("1.8.0") and not disable_shape_infer:
+            # shape_infer_model_path should be in the same folder of model_path
+            with tempfile.NamedTemporaryFile(dir=os.path.dirname(model_path)) as tmpfile:
+                shape_infer_model_path = tmpfile.name
+                # infer_shapes_path can be used for model >2GB, and infer_shapes cannot.
+                infer_shapes_path(model_path, shape_infer_model_path)
+                model = onnx.load(shape_infer_model_path)
+                disable_shape_infer = True
+        else:
+            model = onnx.load(model_path)
+
+    if not isinstance(model, onnx_proto.ModelProto):
+        raise ValueError(f"Expected an ONNX ModelProto but got {type(model)}")
+
     func_infer_shape = None
-    if not disable_shape_infer and onnx.__version__ >= '1.2':
+    if not disable_shape_infer and version.parse(onnx.__version__) >= version.parse("1.2.0"):
         try:
-            from onnx.shape_inference import infer_shapes
             func_infer_shape = infer_shapes
         finally:
             pass
 
-    if not isinstance(model, onnx_proto.ModelProto):
-        raise ValueError('Expected model type is an ONNX ModelProto but got %s' % type(model))
-
     # create blocklists
     if op_block_list is None:
         op_block_list = DEFAULT_OP_BLOCK_LIST
     if node_block_list is None:
         node_block_list = []
     op_block_list = set(op_block_list)
     node_block_list = set(node_block_list)
@@ -165,14 +236,20 @@
         f"fp16 parameters: min_positive_val={min_positive_val} max_finite_val={max_finite_val} keep_io_types={keep_io_types} disable_shape_infer={disable_shape_infer} op_block_list={op_block_list} node_block_list={node_block_list} force_fp16_initializers={force_fp16_initializers}"
     )
 
     # create a queue for BFS
     queue = []
     value_info_list = []
     node_list = []
+
+    # Some operators (Like Resize or GroupNorm) have data type fixed as float for some input.
+    # When it is converted to float16, there are mixed types: some inputs are float32 and some are float16.
+    # This list keeps track of such nodes that are not in block list.
+    mixed_float_type_node_list = []
+
     # type inference on input model
     if func_infer_shape is not None:
         model = func_infer_shape(model)
     queue.append(model)
     name_mapping = {}
     graph_io_to_skip = set()
     io_casts = set()
@@ -184,42 +261,42 @@
         fp32_outputs = [n for n in fp32_outputs if n in keep_io_types]
     elif not keep_io_types:
         fp32_inputs = []
         fp32_outputs = []
 
     for i, n in enumerate(model.graph.input):
         if n.name in fp32_inputs:
-            output_name = 'graph_input_cast_' + str(i)
+            output_name = "graph_input_cast_" + str(i)
             name_mapping[n.name] = output_name
             graph_io_to_skip.add(n.name)
 
-            node_name = 'graph_input_cast' + str(i)
+            node_name = "graph_input_cast" + str(i)
             new_value_info = model.graph.value_info.add()
             new_value_info.CopyFrom(n)
             new_value_info.name = output_name
             new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT16
             # add Cast node (from tensor(float) to tensor(float16) after graph input
-            new_node = [helper.make_node('Cast', [n.name], [output_name], to=10, name=node_name)]
+            new_node = [helper.make_node("Cast", [n.name], [output_name], to=10, name=node_name)]
             model.graph.node.extend(new_node)
             value_info_list.append(new_value_info)
             io_casts.add(node_name)
 
     for i, n in enumerate(model.graph.output):
         if n.name in fp32_outputs:
-            input_name = 'graph_output_cast_' + str(i)
+            input_name = "graph_output_cast_" + str(i)
             name_mapping[n.name] = input_name
             graph_io_to_skip.add(n.name)
 
-            node_name = 'graph_output_cast' + str(i)
+            node_name = "graph_output_cast" + str(i)
             # add Cast node (from tensor(float16) to tensor(float) before graph output
             new_value_info = model.graph.value_info.add()
             new_value_info.CopyFrom(n)
             new_value_info.name = input_name
             new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT16
-            new_node = [helper.make_node('Cast', [input_name], [n.name], to=1, name=node_name)]
+            new_node = [helper.make_node("Cast", [input_name], [n.name], to=1, name=node_name)]
             model.graph.node.extend(new_node)
             value_info_list.append(new_value_info)
             io_casts.add(node_name)
 
     fp32_initializers: Dict[str, InitializerTracker] = {}
     while queue:
         next_level = []
@@ -243,117 +320,152 @@
                         if n.input[i] in name_mapping:
                             n.input[i] = name_mapping[n.input[i]]
                     for i in range(len(n.output)):
                         if n.output[i] in name_mapping:
                             n.output[i] = name_mapping[n.output[i]]
 
                     is_node_blocked = n.op_type in op_block_list or n.name in node_block_list
-                    for input in n.input:
-                        if input in fp32_initializers:
-                            fp32_initializers[input].add_node(n, is_node_blocked)
+                    for i, input_name in enumerate(n.input):
+                        if input_name in fp32_initializers:
+                            # For Resize/GroupNorm, only the first input can be float16
+                            use_fp32_weight = is_node_blocked or (n.op_type in ["Resize", "GroupNorm"] and i != 0)
+                            fp32_initializers[input_name].add_node(n, use_fp32_weight)
 
                     if is_node_blocked:
                         node_list.append(n)
                     else:
-                        if n.op_type == 'Cast':
+                        if n.op_type == "Cast":
                             for attr in n.attribute:
-                                if attr.name == 'to' and attr.i == 1:
+                                if attr.name == "to" and attr.i == 1:
                                     attr.i = 10
                                     break
-                        for attr in n.attribute:
-                            next_level.append(attr)
+
+                        # For Resize/GroupNorm, attribute data type cannot be changed
+                        if n.op_type not in ["Resize", "GroupNorm"]:
+                            for attr in n.attribute:
+                                next_level.append(attr)
+                        else:
+                            mixed_float_type_node_list.append(n)
+
             # if q is model.graph.node.attribute, push q.g and q.graphs (GraphProto)
             # and process node.attribute.t and node.attribute.tensors (TensorProto)
             if isinstance(q, onnx_proto.AttributeProto):
                 next_level.append(q.g)
                 for n in q.graphs:
                     next_level.append(n)
                 q.t.CopyFrom(convert_tensor_float_to_float16(q.t, min_positive_val, max_finite_val))
                 for n in q.tensors:
-                    n = convert_tensor_float_to_float16(n, min_positive_val, max_finite_val)
+                    n = convert_tensor_float_to_float16(n, min_positive_val, max_finite_val)  # noqa: PLW2901
             # if q is graph, process input, output and value_info (ValueInfoProto)
             if isinstance(q, onnx_proto.GraphProto):
                 # Note that float initializers tracked by fp32_initializers will be processed later.
                 # for all ValueInfoProto with tensor(float) type in input, output and value_info, convert them to
                 # tensor(float16) except map and seq(map). And save them in value_info_list for further processing
                 for n in itertools.chain(q.input, q.output, q.value_info):
                     if n.type.tensor_type.elem_type == onnx_proto.TensorProto.FLOAT:
                         if n.name not in graph_io_to_skip:
                             n.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT16
                             value_info_list.append(n)
-                    if n.type.HasField('sequence_type'):
+                    if n.type.HasField("sequence_type"):
                         if n.type.sequence_type.elem_type.tensor_type.elem_type == onnx_proto.TensorProto.FLOAT:
                             if n.name not in graph_io_to_skip:
                                 n.type.sequence_type.elem_type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT16
                                 value_info_list.append(n)
-                            
+
         queue = next_level
 
-    for key, value in fp32_initializers.items():
+    for _key, value in fp32_initializers.items():
         # By default, to avoid precision loss, do not convert an initializer to fp16 when it is used only by fp32 nodes.
         if force_fp16_initializers or value.fp16_nodes:
             value.initializer = convert_tensor_float_to_float16(value.initializer, min_positive_val, max_finite_val)
             value_info_list.append(make_value_info_from_tensor(value.initializer))
             if value.fp32_nodes and not force_fp16_initializers:
                 logger.info(
                     "initializer is used by both fp32 and fp16 nodes. Consider add these nodes to block list:{}".format(
-                        value.fp16_nodes))
+                        value.fp16_nodes
+                    )
+                )
+
+    # Some operators have data type fixed as float for some input. Add a float16 to float cast for those inputs.
+    for node in mixed_float_type_node_list:
+        for i, input_name in enumerate(node.input):
+            if i not in ALWAYS_FLOAT_INPUTS[node.op_type]:
+                continue
+            for value_info in value_info_list:
+                if input_name == value_info.name:
+                    # create new value_info for current node's new input name
+                    new_value_info = model.graph.value_info.add()
+                    new_value_info.CopyFrom(value_info)
+                    output_name = node.name + "_input_cast_" + str(i)
+                    new_value_info.name = output_name
+                    new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT
+                    # add Cast node (from tensor(float16) to tensor(float) before current node
+                    node_name = node.name + "_input_cast" + str(i)
+                    new_node = [helper.make_node("Cast", [input_name], [output_name], to=1, name=node_name)]
+                    model.graph.node.extend(new_node)
+                    # change current node's input name
+                    node.input[i] = output_name
+                    break
 
     # process the nodes in block list that doesn't support tensor(float16)
     for node in node_list:
         # if input's name is in the value_info_list meaning input is tensor(float16) type,
         # insert a float16 to float Cast node before the node,
         # change current node's input name and create new value_info for the new name
         for i in range(len(node.input)):
-            input = node.input[i]
+            input_name = node.input[i]
             for value_info in value_info_list:
-                if input == value_info.name:
+                if input_name == value_info.name:
                     # create new value_info for current node's new input name
                     new_value_info = model.graph.value_info.add()
                     new_value_info.CopyFrom(value_info)
-                    output_name = node.name + '_input_cast_' + str(i)
+                    output_name = node.name + "_input_cast_" + str(i)
                     new_value_info.name = output_name
                     new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT
                     # add Cast node (from tensor(float16) to tensor(float) before current node
-                    node_name = node.name + '_input_cast' + str(i)
-                    new_node = [helper.make_node('Cast', [input], [output_name], to=1, name=node_name)]
+                    node_name = node.name + "_input_cast" + str(i)
+                    new_node = [helper.make_node("Cast", [input_name], [output_name], to=1, name=node_name)]
                     model.graph.node.extend(new_node)
                     # change current node's input name
                     node.input[i] = output_name
                     break
         # if output's name is in the value_info_list meaning output is tensor(float16) type, insert a float to
         # float16 Cast node after the node, change current node's output name and create new value_info for the new name
         for i in range(len(node.output)):
             output = node.output[i]
             for value_info in value_info_list:
                 if output == value_info.name:
                     # create new value_info for current node's new output
                     new_value_info = model.graph.value_info.add()
                     new_value_info.CopyFrom(value_info)
-                    input_name = node.name + '_output_cast_' + str(i)
+                    input_name = node.name + "_output_cast_" + str(i)
                     new_value_info.name = input_name
                     new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT
                     # add Cast node (from tensor(float) to tensor(float16) after current node
-                    node_name = node.name + '_output_cast' + str(i)
-                    new_node = [helper.make_node('Cast', [input_name], [output], to=10, name=node_name)]
+                    node_name = node.name + "_output_cast" + str(i)
+                    new_node = [helper.make_node("Cast", [input_name], [output], to=10, name=node_name)]
                     model.graph.node.extend(new_node)
                     # change current node's input name
                     node.output[i] = input_name
                     break
     return model
 
 
 def float_to_float16_max_diff(tensor, min_positive_val=5.96e-08, max_finite_val=65504.0):
     """Measure the maximum absolute difference after converting a float tensor to float16."""
     if not isinstance(tensor, onnx_proto.TensorProto):
-        raise ValueError('Expected input type is an ONNX TensorProto but got %s' % type(tensor))
+        raise ValueError(f"Expected input type is an ONNX TensorProto but got {type(tensor)}")
     if tensor.data_type != onnx_proto.TensorProto.FLOAT:
-        raise ValueError('Expected tensor data type is float.')
+        raise ValueError("Expected tensor data type is float.")
 
+    float32_data = None
     if tensor.float_data:
         float32_data = np.array(tensor.float_data)
 
     if tensor.raw_data:
-        float32_data = np.fromstring(tensor.raw_data, dtype='float32')
+        float32_data = np.frombuffer(tensor.raw_data, dtype="float32")
+
+    if float32_data is None:
+        raise RuntimeError("external data not loaded!")
 
     float16_data = convert_np_to_float16(float32_data, min_positive_val, max_finite_val)
     return np.amax(np.abs(float32_data - np.float32(float16_data)))
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_attention.py` & `onnxruntime/transformers/fusion_attention_unet.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,487 +1,439 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
-from os import name
-from sys import path
-import numpy as np
+# --------------------------------------------------------------------------
 from logging import getLogger
-from enum import Enum
 from typing import Tuple, Union
-from onnx import helper, numpy_helper, TensorProto, NodeProto
-from onnx_model import OnnxModel
+
+import numpy as np
 from fusion_base import Fusion
-from fusion_utils import FusionUtils, NumpyHelper
-from fusion_options import AttentionMaskFormat
-from shape_infer_helper import SymbolicShapeInferenceHelper, get_shape_from_type_proto
+from fusion_utils import NumpyHelper
+from onnx import NodeProto, TensorProto, helper
+from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
-class AttentionMask():
+class FusionAttentionUnet(Fusion):
     """
-    Fuse Attention subgraph into one Attention node.
+    Fuse Attention subgraph of UNet into one Attention node.
     """
-    def __init__(self, model: OnnxModel):
-        self.model = model
-        # A lookup table with mask input as key, and mask index output as value
-        self.mask_indice = {}
-        # A lookup table with mask input as key, and cast (to int32) output as value
-        self.mask_casted = {}
-        self.utils = FusionUtils(model)
-        self.mask_format = AttentionMaskFormat.MaskIndexEnd
-
-    def set_mask_format(self, mask_format: AttentionMaskFormat):
-        self.mask_format = mask_format
-
-    def set_mask_indice(self, mask, mask_index):
-        if mask in self.mask_indice:
-            assert mask_index == self.mask_indice[mask]
-        self.mask_indice[mask] = mask_index
-
-    def get_first_mask(self):
-        assert len(self.mask_indice) > 0
-        return next(iter(self.mask_indice))
-
-    def process_mask(self, input: str) -> str:
-        if self.mask_format == AttentionMaskFormat.NoMask:
-            return None
-
-        if input in self.mask_indice:
-            return self.mask_indice[input]
-
-        # Add cast to convert int64 to int32
-        if self.model.find_graph_input(input):
-            casted, input_name = self.utils.cast_graph_input_to_int32(input)
-        else:
-            input_name, cast_node = self.utils.cast_input_to_int32(input)
-            casted = True
 
-        if casted:
-            self.mask_casted[input] = input_name
-
-        # Attention supports int32 attention mask (2D) since 1.4.0
-        if self.mask_format == AttentionMaskFormat.AttentionMask:
-            self.mask_indice[input] = input_name
-            return input_name
-
-        # Add a mask processing node to convert attention mask to mask index (1D)
-        output_name = self.model.create_node_name('mask_index')
-        mask_index_node = helper.make_node('ReduceSum',
-                                           inputs=[input_name],
-                                           outputs=[output_name],
-                                           name=self.model.create_node_name('ReduceSum', 'MaskReduceSum'))
-        mask_index_node.attribute.extend([helper.make_attribute("axes", [1]), helper.make_attribute("keepdims", 0)])
-        self.model.add_node(mask_index_node)
-
-        self.mask_indice[input] = output_name
-        return output_name
-
-
-class FusionAttention(Fusion):
-    """
-    Fuse Attention subgraph into one Attention node.
-    """
-    def __init__(self, model: OnnxModel, hidden_size: int, num_heads: int, attention_mask: AttentionMask):
-        super().__init__(model, "Attention", ["SkipLayerNormalization", "LayerNormalization"])
+    def __init__(
+        self,
+        model: OnnxModel,
+        hidden_size: int,
+        num_heads: int,
+        is_cross_attention: bool,
+        enable_packed_qkv: bool,
+        enable_packed_kv: bool,
+    ):
+        super().__init__(model, "MultiHeadAttention" if is_cross_attention else "Attention", ["LayerNormalization"])
         self.hidden_size = hidden_size
         self.num_heads = num_heads
-        self.attention_mask = attention_mask
+        self.is_cross_attention = is_cross_attention
+        self.enable_packed_qkv = enable_packed_qkv
+        self.enable_packed_kv = enable_packed_kv
 
         # Flags to show warning only once
         self.num_heads_warning = True
         self.hidden_size_warning = True
 
-    def get_num_heads_and_hidden_size(self, reshape_q: NodeProto) -> Tuple[int, int]:
-        """ Detect num_heads and hidden_size from a reshape node.
+    def get_num_heads_and_hidden_size(self, reshape_q: NodeProto, layernorm_node: NodeProto) -> Tuple[int, int]:
+        """Detect num_heads and hidden_size from a reshape node.
 
         Args:
             reshape_q (NodeProto): reshape node for Q
+            add_q (NodeProto): add node for Q
 
         Returns:
             Tuple[int, int]: num_heads and hidden_size
         """
 
         # we assume that reshape fusion has done, so the shape is a tensor like [0, 0, num_heads, head_size]
-        q_shape = self.model.get_initializer(reshape_q.input[1])
-        if q_shape is None:
-            logger.debug(f"{reshape_q.input[1]} is not initializer.")
+        q_shape_value = self.model.get_constant_value(reshape_q.input[1])
+        if q_shape_value is None:
+            logger.debug(f"{reshape_q.input[1]} is not constant.")
             return self.num_heads, self.hidden_size  # Fall back to user specified value
 
-        q_shape_value = NumpyHelper.to_array(q_shape)
-        if len(q_shape_value) != 4 or (q_shape_value[2] <= 0 or q_shape_value[3] <= 0):
-            logger.debug(f"q_shape_value={q_shape_value}. Expected value are like [0, 0, num_heads, head_size].")
+        if len(q_shape_value) != 4 or q_shape_value[2] <= 0:
+            logger.debug(f"q_shape_value={q_shape_value}. Expected value are like [0, 0, num_heads, -1].")
             return self.num_heads, self.hidden_size  # Fall back to user specified value
 
         num_heads = q_shape_value[2]
-        head_size = q_shape_value[3]
-        hidden_size = num_heads * head_size
+
+        layernorm_bias = self.model.get_initializer(layernorm_node.input[1])
+        if layernorm_bias is None:
+            logger.debug(f"{layernorm_node.input[1]} is not initializer.")
+            return self.num_heads, self.hidden_size  # Fall back to user specified value
+
+        hidden_size = NumpyHelper.to_array(layernorm_bias).shape[0]
 
         if self.num_heads > 0 and num_heads != self.num_heads:
             if self.num_heads_warning:
                 logger.warning(f"--num_heads is {self.num_heads}. Detected value is {num_heads}. Using detected value.")
                 self.num_heads_warning = False  # Do not show the warning more than once
 
         if self.hidden_size > 0 and hidden_size != self.hidden_size:
             if self.hidden_size_warning:
                 logger.warning(
-                    f"--hidden_size is {self.hidden_size}. Detected value is {hidden_size}. Using detected value.")
+                    f"--hidden_size is {self.hidden_size}. Detected value is {hidden_size}. Using detected value."
+                )
                 self.hidden_size_warning = False  # Do not show the warning more than once
 
         return num_heads, hidden_size
 
-    def get_add_qk_str(self, add_qk: NodeProto):
-        shape_infer = self.model.infer_runtime_shape(update=True)
-        if shape_infer is None:
-            return
-
-        input_0_shape = shape_infer.get_edge_shape(add_qk.input[0])
-        input_1_shape = shape_infer.get_edge_shape(add_qk.input[1])
-
-        if input_0_shape is None or input_1_shape is None:
-            logger.debug(f"one of the inputs of {add_qk} is None")
-            return None
-
-        if input_0_shape != input_1_shape:
-            logger.debug(f"the shape of two inputs of {add_qk} is not same")
-            return None
-
-        return add_qk.input[1]
-
-    def create_attention_node(self, mask_index: str, q_matmul: NodeProto, k_matmul: NodeProto, v_matmul: NodeProto,
-                              q_add: NodeProto, k_add: NodeProto, v_add: NodeProto, num_heads: int, hidden_size: int,
-                              input: str, output: str, add_qk_str: str) -> Union[NodeProto, None]:
-        """ Create an Attention node.
+    def create_attention_node(
+        self,
+        q_matmul: NodeProto,
+        k_matmul: NodeProto,
+        v_matmul: NodeProto,
+        num_heads: int,
+        hidden_size: int,
+        input: str,
+        output: str,
+    ) -> Union[NodeProto, None]:
+        """Create an Attention node.
 
         Args:
-            mask_index (str): mask input
             q_matmul (NodeProto): MatMul node in fully connection for Q
-            k_matmul (NodeProto): MatMul node in fully connection for  K
-            v_matmul (NodeProto): MatMul node in fully connection for  V
-            q_add (NodeProto): Add bias node in fully connection for Q
-            k_add (NodeProto): Add bias node in fully connection for K
-            v_add (NodeProto): Add bias node in fully connection for V
+            k_matmul (NodeProto): MatMul node in fully connection for K
+            v_matmul (NodeProto): MatMul node in fully connection for V
             num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
             hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
             input (str): input name
             output (str): output name
 
         Returns:
             Union[NodeProto, None]: the node created or None if failed.
         """
-        assert num_heads > 0
+        is_self_attention = not self.is_cross_attention
+
+        if is_self_attention:
+            if q_matmul.input[0] != input or k_matmul.input[0] != input or v_matmul.input[0] != input:
+                logger.debug(
+                    "For self attention, input hidden state for q and k/v shall be same. Got %s, %s, %s",
+                    q_matmul.input[0],
+                    k_matmul.input[0],
+                    v_matmul.input[0],
+                )
+                return None
+        else:
+            if q_matmul.input[0] != input or (k_matmul.input[0] != v_matmul.input[0]) or (k_matmul.input[0] == input):
+                logger.debug(
+                    "For cross attention, input hidden state for q and k/v shall be different. Got %s, %s, %s",
+                    q_matmul.input[0],
+                    k_matmul.input[0],
+                    v_matmul.input[0],
+                )
+                return None
 
         if hidden_size > 0 and (hidden_size % num_heads) != 0:
             logger.debug(f"input hidden size {hidden_size} is not a multiple of num of heads {num_heads}")
             return None
 
         q_weight = self.model.get_initializer(q_matmul.input[1])
         k_weight = self.model.get_initializer(k_matmul.input[1])
         v_weight = self.model.get_initializer(v_matmul.input[1])
-        q_bias = self.model.get_initializer(q_add.input[1]) or self.model.get_initializer(q_add.input[0])
-        k_bias = self.model.get_initializer(k_add.input[1]) or self.model.get_initializer(k_add.input[0])
-        v_bias = self.model.get_initializer(v_add.input[1]) or self.model.get_initializer(v_add.input[0])
-
-        if q_weight is None:
-            print(f"{q_matmul.input[1]} is not initializer. Please set do_constant_folding=True in torch.onnx.export")
+        if not (q_weight and k_weight and v_weight):
             return None
-        if not (k_weight and v_weight and q_bias and k_bias):
+
+        # Sometimes weights are stored in fp16
+        if q_weight.data_type == 10:
+            logger.debug("weights are in fp16. Please run fp16 conversion after optimization")
             return None
 
         qw = NumpyHelper.to_array(q_weight)
         kw = NumpyHelper.to_array(k_weight)
         vw = NumpyHelper.to_array(v_weight)
+        logger.debug(f"qw={qw.shape} kw={kw.shape} vw={vw.shape} hidden_size={hidden_size}")
 
         # assert q and k have same shape as expected
-        assert qw.shape == kw.shape
-
-        qw_in_size = qw.shape[0]
-        kw_in_size = kw.shape[0]
-        vw_in_size = vw.shape[0]
-
-        assert qw_in_size == kw_in_size == vw_in_size
-
-        if hidden_size > 0 and hidden_size != qw_in_size:
-            logger.warning(
-                f"Input hidden size {hidden_size} is not same as weight matrix dimension of q,k,v paths {qw_in_size}, provide correct input hidden size or pass 0"
-            )
-
-        is_qkv_diff_dims = False
-        if qw.shape != vw.shape:
-            is_qkv_diff_dims = True
-
-        # All the matrices can have the same shape or q, k matrics can have the same shape with v being different
-        # For 2d weights, the shapes would be [in_size, out_size].
-        # For 3d weights, shape would be [in_size, a, b] where a*b = out_size
-        qw_out_size = np.prod(qw.shape[1:])
-        kw_out_size = np.prod(qw.shape[1:])
-        vw_out_size = np.prod(vw.shape[1:])
-
-        qkv_weight_dim = 0
-        if is_qkv_diff_dims:
-            qkv_weight = np.concatenate((qw, kw, vw), axis=1)
-            qkv_weight_dim = qw_out_size + kw_out_size + vw_out_size
-        else:
-            qkv_weight = np.stack((qw, kw, vw), axis=1)
-            qkv_weight_dim = 3 * qw_out_size
+        if is_self_attention:
+            if qw.shape != kw.shape or qw.shape != vw.shape:
+                return None
+
+            qw_in_size = qw.shape[0]
+
+            if hidden_size > 0 and hidden_size != qw_in_size:
+                raise ValueError(
+                    f"Input hidden size ({hidden_size}) is not same as weight dimension of q,k,v ({qw_in_size}). "
+                    "Please provide a correct input hidden size or pass in 0"
+                )
+
+            # All the matrices can have the same shape or q, k matrics can have the same shape with v being different
+            # For 2d weights, the shapes would be [in_size, out_size].
+            # For 3d weights, shape would be [in_size, a, b] where a*b = out_size
+            qw_out_size = int(np.prod(qw.shape[1:]))
+
+            if self.enable_packed_qkv:
+                attention_node_name = self.model.create_node_name("MultiHeadAttention")
+
+                c = qw_in_size
+                n = num_heads
+                h = qw_out_size // num_heads
+
+                # Concat and interleave weights so that the output of fused KV GEMM has [B, S_kv, N, 3, H] shape
+                qkv_weight = np.dstack([qw.reshape(c, n, h), kw.reshape(c, n, h), vw.reshape(c, n, h)]).reshape(
+                    c, n * 3 * h
+                )
+
+                matmul_node_name = self.model.create_node_name("MatMul", name_prefix="MatMul_QKV")
+                weight = helper.make_tensor(
+                    name=matmul_node_name + "_weight",
+                    data_type=TensorProto.FLOAT,
+                    dims=[qkv_weight.shape[0], qkv_weight.shape[1]],
+                    vals=qkv_weight.flatten().tolist(),
+                )
+
+                self.model.add_initializer(weight, self.this_graph_name)
+
+                matmul_node = helper.make_node(
+                    "MatMul",
+                    inputs=[k_matmul.input[0], matmul_node_name + "_weight"],
+                    outputs=[matmul_node_name + "_out"],
+                    name=matmul_node_name,
+                )
+                self.node_name_to_graph_name[matmul_node.name] = self.this_graph_name
+
+                shape_tensor = helper.make_tensor(
+                    name=matmul_node_name + "_reshape_shape",
+                    data_type=TensorProto.INT64,
+                    dims=[5],
+                    vals=[0, 0, n, 3, h],
+                )
+                self.model.add_initializer(shape_tensor, self.this_graph_name)
 
-        qb = NumpyHelper.to_array(q_bias)
-        kb = NumpyHelper.to_array(k_bias)
-        vb = NumpyHelper.to_array(v_bias)
-
-        q_bias_shape = np.prod(qb.shape)
-        k_bias_shape = np.prod(kb.shape)
-        v_bias_shape = np.prod(vb.shape)
-
-        assert q_bias_shape == k_bias_shape == qw_out_size
-        assert v_bias_shape == vw_out_size
-
-        qkv_bias_dim = 0
-        if is_qkv_diff_dims:
-            qkv_bias = np.concatenate((qb, kb, vb), axis=0)
-            qkv_bias_dim = q_bias_shape + k_bias_shape + v_bias_shape
-        else:
-            qkv_bias = np.stack((qb, kb, vb), axis=0)
-            qkv_bias_dim = 3 * q_bias_shape
+                reshape_node = helper.make_node(
+                    "Reshape",
+                    inputs=[matmul_node_name + "_out", matmul_node_name + "_reshape_shape"],
+                    outputs=[attention_node_name + "_input"],
+                    name=matmul_node_name + "_reshape",
+                )
+                self.node_name_to_graph_name[reshape_node.name] = self.this_graph_name
+                self.nodes_to_add.extend([matmul_node, reshape_node])
+                self.nodes_to_remove.extend([q_matmul, k_matmul, v_matmul])
 
-        attention_node_name = self.model.create_node_name('Attention')
+            else:
+                qkv_weight = np.stack((qw, kw, vw), axis=1)
+                qkv_weight_dim = 3 * qw_out_size
 
-        weight = helper.make_tensor(name=attention_node_name + '_qkv_weight',
-                                    data_type=TensorProto.FLOAT,
-                                    dims=[qw_in_size, qkv_weight_dim],
-                                    vals=qkv_weight.flatten().tolist())
+                attention_node_name = self.model.create_node_name("Attention")
 
-        # Sometimes weights and bias are stored in fp16
-        if q_weight.data_type == 10:
-            weight.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(weight).astype(np.float16), weight.name))
-        self.model.add_initializer(weight, self.this_graph_name)
+                weight = helper.make_tensor(
+                    name=attention_node_name + "_qkv_weight",
+                    data_type=TensorProto.FLOAT,
+                    dims=[qw_in_size, qkv_weight_dim],
+                    vals=qkv_weight.flatten().tolist(),
+                )
+
+                self.model.add_initializer(weight, self.this_graph_name)
+        else:  # cross attention
+            attention_node_name = self.model.create_node_name("MultiHeadAttention")
+            if self.enable_packed_kv:
+                if kw.shape != vw.shape:
+                    return None
+
+                kw_in_size = kw.shape[0]
+                vw_in_size = vw.shape[0]
+                assert kw_in_size == vw_in_size
+
+                qw_out_size = qw.shape[1]
+                kw_out_size = kw.shape[1]
+                vw_out_size = vw.shape[1]
+                assert qw_out_size == vw_out_size and kw_out_size == vw_out_size
+
+                c = kw_in_size
+                n = num_heads
+                h = kw_out_size // num_heads
+
+                # Concat and interleave weights so that the output of fused KV GEMM has [B, S_kv, N, 2, H] shape
+                kv_weight = np.dstack([kw.reshape(c, n, h), vw.reshape(c, n, h)]).reshape(c, n * 2 * h)
+
+                matmul_node_name = self.model.create_node_name("MatMul", name_prefix="MatMul_KV")
+                weight = helper.make_tensor(
+                    name=matmul_node_name + "_weight",
+                    data_type=TensorProto.FLOAT,
+                    dims=[kv_weight.shape[0], kv_weight.shape[1]],
+                    vals=kv_weight.flatten().tolist(),
+                )
+
+                self.model.add_initializer(weight, self.this_graph_name)
+
+                matmul_node = helper.make_node(
+                    "MatMul",
+                    inputs=[k_matmul.input[0], matmul_node_name + "_weight"],
+                    outputs=[matmul_node_name + "_out"],
+                    name=matmul_node_name,
+                )
+                self.node_name_to_graph_name[matmul_node.name] = self.this_graph_name
+
+                shape_tensor = helper.make_tensor(
+                    name=matmul_node_name + "_reshape_shape",
+                    data_type=TensorProto.INT64,
+                    dims=[5],
+                    vals=[0, 0, n, 2, h],
+                )
+                self.model.add_initializer(shape_tensor, self.this_graph_name)
 
-        bias = helper.make_tensor(name=attention_node_name + '_qkv_bias',
-                                  data_type=TensorProto.FLOAT,
-                                  dims=[qkv_bias_dim],
-                                  vals=qkv_bias.flatten().tolist())
-        if q_bias.data_type == 10:
-            bias.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(bias).astype(np.float16), bias.name))
+                reshape_node = helper.make_node(
+                    "Reshape",
+                    inputs=[matmul_node_name + "_out", matmul_node_name + "_reshape_shape"],
+                    outputs=[k_matmul.output[0]],
+                    name=matmul_node_name + "_reshape",
+                )
+                self.node_name_to_graph_name[reshape_node.name] = self.this_graph_name
+                self.nodes_to_add.extend([matmul_node, reshape_node])
+                self.nodes_to_remove.extend([k_matmul, v_matmul])
+
+        # No bias, use zeros
+        qkv_bias = np.zeros([3, hidden_size], dtype=np.float32)
+        qkv_bias_dim = 3 * hidden_size
+
+        bias = helper.make_tensor(
+            name=attention_node_name + "_qkv_bias",
+            data_type=TensorProto.FLOAT,
+            dims=[qkv_bias_dim],
+            vals=qkv_bias.flatten().tolist(),
+        )
         self.model.add_initializer(bias, self.this_graph_name)
 
-        attention_inputs = [input, attention_node_name + '_qkv_weight', attention_node_name + '_qkv_bias']
-        if mask_index is not None:
-            attention_inputs.append(mask_index)
+        if is_self_attention:
+            if not self.enable_packed_qkv:
+                attention_inputs = [
+                    input,
+                    attention_node_name + "_qkv_weight",
+                    attention_node_name + "_qkv_bias",
+                ]
+            else:
+                attention_inputs = [attention_node_name + "_input"]
         else:
-            attention_inputs.append("")
-
-        if add_qk_str is not None:
-            attention_inputs.append("")
-            attention_inputs.append(add_qk_str)
-
-        attention_node = helper.make_node('Attention',
-                                          inputs=attention_inputs,
-                                          outputs=[output],
-                                          name=attention_node_name)
+            if not self.enable_packed_kv:
+                attention_inputs = [
+                    q_matmul.output[0],
+                    k_matmul.output[0],
+                    v_matmul.output[0],
+                    attention_node_name + "_qkv_bias",
+                ]
+            else:
+                attention_inputs = [
+                    q_matmul.output[0],
+                    k_matmul.output[0],
+                ]
+
+        attention_node = helper.make_node(
+            "Attention" if (is_self_attention and not self.enable_packed_qkv) else "MultiHeadAttention",
+            inputs=attention_inputs,
+            outputs=[output],
+            name=attention_node_name,
+        )
         attention_node.domain = "com.microsoft"
         attention_node.attribute.extend([helper.make_attribute("num_heads", num_heads)])
 
-        if is_qkv_diff_dims:
-            attention_node.attribute.extend(
-                [helper.make_attribute("qkv_hidden_sizes", [qw_out_size, kw_out_size, vw_out_size])])
-
+        counter_name = (
+            "Attention (self attention)"
+            if is_self_attention and not self.enable_packed_qkv
+            else "MultiHeadAttention ({})".format(
+                "self attention with packed qkv"
+                if self.enable_packed_qkv
+                else "cross attention with packed kv"
+                if self.enable_packed_kv
+                else "cross attention"
+            )
+        )
+        self.increase_counter(counter_name)
         return attention_node
 
     def fuse(self, normalize_node, input_name_to_nodes, output_name_to_node):
-        # Sometimes we can not fuse skiplayernormalization since the add before layernorm has an output that used by nodes outside skiplayernorm
-        # Conceptually we treat add before layernorm as skiplayernorm node since they share the same pattern
-        start_node = normalize_node
-        if normalize_node.op_type == 'LayerNormalization':
-            add_before_layernorm = self.model.match_parent(normalize_node, 'Add', 0)
-            if add_before_layernorm is not None:
-                start_node = add_before_layernorm
-            else:
-                return
+        node_before_layernorm = self.model.match_parent(normalize_node, "Add", 0)
 
-        # SkipLayerNormalization has two inputs, and one of them is the root input for attention.
-        qkv_nodes = self.model.match_parent_path(start_node, ['Add', 'MatMul', 'Reshape', 'Transpose', 'MatMul'],
-                                                 [None, None, 0, 0, 0])
-        einsum_node = None
-        if qkv_nodes is not None:
-            (_, matmul_qkv, reshape_qkv, transpose_qkv, matmul_qkv) = qkv_nodes
-        else:
-            # Match Albert
-            qkv_nodes = self.model.match_parent_path(start_node, ['Add', 'Einsum', 'Transpose', 'MatMul'],
-                                                     [1, None, 0, 0])
-            if qkv_nodes is not None:
-                (_, einsum_node, transpose_qkv, matmul_qkv) = qkv_nodes
-            else:
-                return
+        # In SD 1.5, for self attention, LayerNorm has parent Reshape
+        if node_before_layernorm is None and not self.is_cross_attention:
+            node_before_layernorm = self.model.match_parent(normalize_node, "Reshape", 0)
 
-        other_inputs = []
-        for i, input in enumerate(start_node.input):
-            if input not in output_name_to_node:
-                continue
-
-            if input == qkv_nodes[0].output[0]:
-                continue
-            other_inputs.append(input)
-        if len(other_inputs) != 1:
+        if node_before_layernorm is None:
             return
 
-        root_input = other_inputs[0]
-        """
-        Match flaubert                     Mask
-                                            |
-        Mul --> LayerNormalization -->  Attention --> MatMul --> Add
-         |                                                        |
-         |                                                        |
-         +---------------------------------------------------------
-        """
-        mul_before_layernorm = self.model.match_parent(start_node, 'Mul', 0)
-        if mul_before_layernorm is not None:
-            mul_children = input_name_to_nodes[mul_before_layernorm.output[0]]
-            if mul_children is not None and len(mul_children) == 2:
-                layernorm_node = mul_children[1]
-                if layernorm_node.op_type == 'LayerNormalization':
-                    root_input = layernorm_node.output[0]
-                else:
-                    return
-            elif mul_children is not None and len(mul_children) == 5:
-                root_input = mul_before_layernorm.output[0]
-            else:
-                return
-        elif normalize_node.op_type == 'LayerNormalization':
-            children = input_name_to_nodes[root_input]
-            for child in children:
-                if child.op_type == "LayerNormalization":
-                    root_input = child.output[0]
-
-        children = input_name_to_nodes[root_input]
-        children_types = [child.op_type for child in children]
-        if children_types.count('MatMul') != 3:
+        root_input = node_before_layernorm.output[0]
+
+        children_nodes = input_name_to_nodes[root_input]
+        skip_add = None
+        for node in children_nodes:
+            if node.op_type == "Add":  # or node.op_type == "SkipLayerNormalization":
+                skip_add = node
+                break
+        if skip_add is None:
             return
 
-        v_nodes = self.model.match_parent_path(matmul_qkv, ['Transpose', 'Reshape', 'Add', 'MatMul'], [1, 0, 0, None])
-        if v_nodes is None:
-            logger.debug("fuse_attention: failed to match v path")
+        another_input = 1 if skip_add.input[0] == root_input else 0
+        qkv_nodes = self.model.match_parent_path(
+            skip_add,
+            ["Add", "MatMul", "Reshape", "Transpose", "Reshape", "MatMul"],
+            [another_input, None, None, 0, 0, 0],
+        )
+
+        if qkv_nodes is None:
             return
-        (_, _, add_v, matmul_v) = v_nodes
 
-        is_distill = False
-        is_distill_add = False
-        qk_paths = {
-            "path1": (['Softmax', 'Add', 'Div', 'MatMul'], [0, 0, None, 0]),
-            "path2": (['Softmax', 'Add', 'Mul', 'MatMul'], [0, 0, None, 0]),
-            "path3": (['Softmax', 'Where', 'MatMul', 'Div'], [0, 0, 2, 0]),
-            "path4": (['Softmax', 'Add', 'Where', 'MatMul'], [0, 0, 0, 2])
-        }
-
-        qk_nodes = None
-        for k, v in qk_paths.items():
-            qk_nodes = self.model.match_parent_path(matmul_qkv, v[0], v[1])
-            if qk_nodes is None:
-                continue
-            if k == "path3":
-                is_distill = True
-            if k == "path4":
-                is_distill_add = True
-            break
+        (_, _, reshape_qkv, transpose_qkv, _, matmul_qkv) = qkv_nodes
 
-        if qk_nodes is None:
-            logger.debug("fuse_attention: failed to match qk path")
+        # No bias. For cross-attention, the input of the MatMul is encoder_hidden_states graph input.
+        v_nodes = self.model.match_parent_path(matmul_qkv, ["Reshape", "Transpose", "Reshape", "MatMul"], [1, 0, 0, 0])
+        if v_nodes is None:
+            logger.debug("fuse_attention: failed to match v path")
             return
+        (_, _, _, matmul_v) = v_nodes
 
-        add_qk = None
-        matmul_qk = None
-        where_qk = None
-        if is_distill:
-            (_, where_qk, matmul_qk, _) = qk_nodes
-        elif is_distill_add:
-            (_, add_qk, where_qk, matmul_qk) = qk_nodes
+        qk_nodes = self.model.match_parent_path(matmul_qkv, ["Softmax", "Mul", "MatMul"], [0, 0, 0])
+        if qk_nodes is not None:
+            (_softmax_qk, _mul_qk, matmul_qk) = qk_nodes
         else:
-            (_, add_qk, _, matmul_qk) = qk_nodes
+            qk_nodes = self.model.match_parent_path(matmul_qkv, ["Softmax", "Add", "Mul", "MatMul"], [0, 0, 0, 0])
+            if qk_nodes is not None:
+                (_softmax_qk, _add_zero, _mul_qk, matmul_qk) = qk_nodes
+            else:
+                logger.debug("fuse_attention: failed to match qk path")
+                return
 
-        q_nodes = self.model.match_parent_path(matmul_qk, ['Transpose', 'Reshape', 'Add', 'MatMul'], [0, 0, 0, None])
+        q_nodes = self.model.match_parent_path(matmul_qk, ["Reshape", "Transpose", "Reshape", "MatMul"], [0, 0, 0, 0])
         if q_nodes is None:
-            q_nodes = self.model.match_parent_path(matmul_qk, ['Div', 'Transpose', 'Reshape', 'Add', 'MatMul'],
-                                                   [0, 0, 0, 0, None])
-            if q_nodes is None:
-                logger.debug("fuse_attention: failed to match q path")
-                return
-        reshape_q = q_nodes[-3]
-        add_q = q_nodes[-2]
-        matmul_q = q_nodes[-1]
+            logger.debug("fuse_attention: failed to match q path")
+            return
+        (_, _transpose_q, reshape_q, matmul_q) = q_nodes
 
-        k_nodes = self.model.match_parent_path(matmul_qk, ['Transpose', 'Reshape', 'Add', 'MatMul'], [1, 0, 0, None])
+        k_nodes = self.model.match_parent_path(
+            matmul_qk, ["Transpose", "Reshape", "Transpose", "Reshape", "MatMul"], [1, 0, 0, 0, 0]
+        )
         if k_nodes is None:
-            k_nodes = self.model.match_parent_path(matmul_qk, ['Transpose', 'Transpose', 'Reshape', 'Add', 'MatMul'],
-                                                   [1, 0, 0, 0, None])
-            if k_nodes is None:
-                logger.debug("fuse_attention: failed to match k path")
-                return
-        add_k = k_nodes[-2]
-        matmul_k = k_nodes[-1]
-
-        # Note that Cast might be removed by OnnxRuntime so we match two patterns here.
-        mask_nodes = None
-        add_qk_str = None
-        if is_distill:
-            _, mask_nodes, _ = self.model.match_parent_paths(where_qk,
-                                                             [(['Expand', 'Reshape', 'Equal'], [0, 0, 0]),
-                                                              (['Equal', 'Unsqueeze', 'Unsqueeze'], [0, 0, 0]),
-                                                              (['Cast', 'Expand', 'Reshape', 'Equal'], [0, 0, 0, 0])],
-                                                             output_name_to_node)
-        elif is_distill_add:
-            _, mask_nodes, _ = self.model.match_parent_paths(
-                where_qk, [(['Cast', 'Equal', 'Unsqueeze', 'Unsqueeze'], [0, 0, 0, 0]),
-                           (['Equal', 'Unsqueeze', 'Unsqueeze'], [0, 0, 0])], output_name_to_node)
-            if add_qk is not None:
-                add_qk_str = self.get_add_qk_str(add_qk)
-                if add_qk_str is None:
-                    logger.debug(f"fuse_attention: failed to verify shape inference of {add_qk}")
-                    return
-        else:
-            _, mask_nodes, _ = self.model.match_parent_paths(
-                add_qk, [(['Mul', 'Sub', 'Cast', 'Unsqueeze', 'Unsqueeze'], [None, 0, 1, 0, 0]),
-                         (['Mul', 'Sub', 'Unsqueeze', 'Unsqueeze'], [None, 0, 1, 0])], output_name_to_node)
-        if mask_nodes is None:
-            logger.debug("fuse_attention: failed to match mask path")
+            logger.debug("fuse_attention: failed to match k path")
             return
 
-        if matmul_v.input[0] == root_input and matmul_q.input[0] == root_input and matmul_k.input[0] == root_input:
-            mask_index = self.attention_mask.process_mask(mask_nodes[-1].input[0])
+        (_, _, _, _, matmul_k) = k_nodes
 
-            attention_last_node = reshape_qkv if einsum_node is None else transpose_qkv
+        attention_last_node = reshape_qkv
 
-            q_num_heads, q_hidden_size = self.get_num_heads_and_hidden_size(reshape_q)
-            # number of heads are same for all the paths, hence to create attention node, we pass the q_num_heads
-            # the input_hidden_size represents the input hidden size, this is used as needed but hidden sizes for Q, K are extracted appropriately
-            new_node = self.create_attention_node(mask_index, matmul_q, matmul_k, matmul_v, add_q, add_k, add_v,
-                                                  q_num_heads, q_hidden_size, root_input, attention_last_node.output[0],
-                                                  add_qk_str)
-            if new_node is None:
-                return
+        q_num_heads, q_hidden_size = self.get_num_heads_and_hidden_size(reshape_q, normalize_node)
+        if q_num_heads <= 0:
+            logger.debug("fuse_attention: failed to detect num_heads")
+            return
 
-            self.nodes_to_add.append(new_node)
-            self.node_name_to_graph_name[new_node.name] = self.this_graph_name
+        # number of heads are same for all the paths, hence to create attention node, we pass the q_num_heads
+        new_node = self.create_attention_node(
+            matmul_q,
+            matmul_k,
+            matmul_v,
+            q_num_heads,
+            q_hidden_size,
+            input=normalize_node.output[0],
+            output=attention_last_node.output[0],
+        )
+        if new_node is None:
+            return
 
-            if einsum_node is not None:
-                unique_index = einsum_node.input[0]
-                new_edge = "edge_modified_" + unique_index
-                shape_tensor = helper.make_tensor(name="shape_modified_tensor" + unique_index,
-                                                  data_type=TensorProto.INT64,
-                                                  dims=[4],
-                                                  vals=np.int64([0, 0, q_num_heads,
-                                                                 int(q_hidden_size / q_num_heads)]).tobytes(),
-                                                  raw=True)
-                self.model.add_initializer(shape_tensor, self.this_graph_name)
-                self.model.add_node(
-                    helper.make_node("Reshape", [attention_last_node.output[0], shape_tensor.name], [new_edge],
-                                     "reshape_modified_" + unique_index), self.this_graph_name)
-                einsum_node.input[0] = new_edge
-
-            self.nodes_to_remove.extend([attention_last_node, transpose_qkv, matmul_qkv])
-            self.nodes_to_remove.extend(qk_nodes)
-            self.nodes_to_remove.extend(q_nodes)
-            self.nodes_to_remove.extend(k_nodes)
-            self.nodes_to_remove.extend(v_nodes)
-
-            # Use prune graph to remove mask nodes since they are shared by all attention nodes.
-            #self.nodes_to_remove.extend(mask_nodes)
-            self.prune_graph = True
+        self.nodes_to_add.append(new_node)
+        self.node_name_to_graph_name[new_node.name] = self.this_graph_name
+
+        self.nodes_to_remove.extend([attention_last_node, transpose_qkv])
+
+        # Use prune graph to remove nodes since they are shared by all attention nodes.
+        self.prune_graph = True
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_base.py` & `onnxruntime/transformers/fusion_base.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,36 +1,42 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
+from collections import defaultdict
 from logging import getLogger
+from typing import List, Union
+
 from onnx_model import OnnxModel
-from typing import Union, List
-from onnx import GraphProto
 
 logger = getLogger(__name__)
 
 
 class Fusion:
-    def __init__(self,
-                 model: OnnxModel,
-                 fused_op_type: str,
-                 search_op_types: Union[str, List[str]],
-                 description: str = None):
+    def __init__(
+        self,
+        model: OnnxModel,
+        fused_op_type: str,
+        search_op_types: Union[str, List[str]],
+        description: str = None,
+    ):
         self.search_op_types: List[str] = [search_op_types] if isinstance(search_op_types, str) else search_op_types
         self.fused_op_type: str = fused_op_type
         self.description: str = f"{fused_op_type}({description})" if description else fused_op_type
         self.model: OnnxModel = model
         self.nodes_to_remove: List = []
         self.nodes_to_add: List = []
         self.prune_graph: bool = False
         self.node_name_to_graph_name: dict = {}
         self.this_graph_name: str = None
         # It is optional that subclass updates fused_count since we will also check nodes_to_add to get counter.
-        self.fused_count: int = 0
+        self.fused_count: defaultdict = defaultdict(int)
+
+    def increase_counter(self, fused_op_name):
+        self.fused_count[fused_op_name] += 1
 
     def apply(self):
         logger.debug(f"start {self.description} fusion...")
         input_name_to_nodes = self.model.input_name_to_nodes()
         output_name_to_node = self.model.output_name_to_node()
 
         # This assumes that two search ops will not be fused at same time!
@@ -39,17 +45,22 @@
                 graph = self.model.get_graph_by_node(node)
                 if graph is None:
                     raise Exception("Can not find node in any graphs")
                 self.this_graph_name = graph.name
                 self.fuse(node, input_name_to_nodes, output_name_to_node)
 
         op_list = [node.op_type for node in self.nodes_to_add]
-        count = max(self.fused_count, op_list.count(self.fused_op_type))
-        if count > 0:
-            logger.info(f"Fused {self.description} count: {count}")
+        if self.fused_count:
+            for key, value in self.fused_count.items():
+                if value:
+                    logger.info(f"Fused {key}: {value}")
+        else:
+            count = op_list.count(self.fused_op_type)
+            if count > 0:
+                logger.info(f"Fused {self.description}: {count}")
 
         self.model.remove_nodes(self.nodes_to_remove)
         self.model.add_nodes(self.nodes_to_add, self.node_name_to_graph_name)
 
         if self.prune_graph:
             self.model.prune_graph()
         elif self.nodes_to_remove or self.nodes_to_add:
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_biasgelu.py` & `onnxruntime/transformers/fusion_biasgelu.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,36 +1,37 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 from logging import getLogger
-from onnx import helper
-from onnx_model import OnnxModel
+
 from fusion_base import Fusion
 from fusion_utils import NumpyHelper
+from onnx import helper
+from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class FusionBiasGelu(Fusion):
     def __init__(self, model: OnnxModel, is_fastgelu):
         if is_fastgelu:
-            super().__init__(model, 'FastGelu', 'FastGelu', 'add bias')
+            super().__init__(model, "FastGelu", "FastGelu", "add bias")
         else:
-            super().__init__(model, 'BiasGelu', 'Gelu')
+            super().__init__(model, "BiasGelu", "Gelu")
 
     def fuse(self, node, input_name_to_nodes, output_name_to_node):
         gelu_op_type = node.op_type
-        fuse_op_type = 'BiasGelu' if gelu_op_type == 'Gelu' else 'FastGelu'
+        fuse_op_type = "BiasGelu" if gelu_op_type == "Gelu" else "FastGelu"
 
         if len(node.input) != 1:
             return
 
-        nodes = self.model.match_parent_path(node, ['Add', 'MatMul'], [0, None])
+        nodes = self.model.match_parent_path(node, ["Add", "MatMul"], [0, None])
         if nodes is None:
             return
         (add, matmul) = nodes
 
         bias_weight = None
         # bias should be one dimension
         bias_index = -1
@@ -43,20 +44,23 @@
             break
         if bias_weight is None:
             return
         if len(bias_weight.shape) != 1:
             return
 
         subgraph_nodes = [node, add]
-        if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, [node.output[0]], input_name_to_nodes,
-                                                output_name_to_node):
+        if not self.model.is_safe_to_fuse_nodes(
+            subgraph_nodes, [node.output[0]], input_name_to_nodes, output_name_to_node
+        ):
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
 
-        fused_node = helper.make_node(fuse_op_type,
-                                      inputs=[matmul.output[0], add.input[bias_index]],
-                                      outputs=node.output,
-                                      name=self.model.create_node_name(fuse_op_type, gelu_op_type + "_AddBias_"))
+        fused_node = helper.make_node(
+            fuse_op_type,
+            inputs=[matmul.output[0], add.input[bias_index]],
+            outputs=node.output,
+            name=self.model.create_node_name(fuse_op_type, gelu_op_type + "_AddBias_"),
+        )
         fused_node.domain = "com.microsoft"
         self.nodes_to_add.append(fused_node)
         self.node_name_to_graph_name[fused_node.name] = self.this_graph_name
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_embedlayer.py` & `onnxruntime/transformers/fusion_embedlayer.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,96 +1,135 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
-from typing import Dict, List, Tuple, Union
 from logging import getLogger
-from onnx import helper, TensorProto, NodeProto
-from onnx_model import OnnxModel
+from typing import Dict, List, Tuple, Union
+
 from fusion_base import Fusion
 from fusion_utils import FusionUtils
+from onnx import NodeProto, TensorProto, helper
+from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class FusionEmbedLayerNoMask(Fusion):
     """
-     Fuse embedding layer into one node (EmbedLayerNormalization).
-     It supports the following model types: BERT, DistilBert, ALBert.
+    Fuse embedding layer into one node (EmbedLayerNormalization).
+    It supports the following model types: BERT, DistilBert, ALBert.
     """
-    def __init__(self, model: OnnxModel, description: str = 'no mask'):
-        super().__init__(model, "EmbedLayerNormalization", ["LayerNormalization", "SkipLayerNormalization"],
-                         description)
+
+    def __init__(self, model: OnnxModel, description: str = "no mask"):
+        super().__init__(
+            model,
+            "EmbedLayerNormalization",
+            ["LayerNormalization", "SkipLayerNormalization"],
+            description,
+        )
         self.utils = FusionUtils(model)
         self.shape_infer_helper = self.model.infer_runtime_shape({}, update=True)
         # The following will be reset in each fuse call of FusionEmbedLayerNormalization
         self.attention = None
         self.embed_node = None
 
     def match_two_gather(self, add: NodeProto) -> Union[None, Tuple[NodeProto, NodeProto]]:
-        gather_0_path = self.model.match_parent_path(add, ['Gather'], [0])
+        gather_0_path = self.model.match_parent_path(add, ["Gather"], [0])
         if gather_0_path is None:
             return None
 
-        gather_1_path = self.model.match_parent_path(add, ['Gather'], [1])
+        gather_1_path = self.model.match_parent_path(add, ["Gather"], [1])
         if gather_1_path is None:
             return None
 
         return gather_0_path[0], gather_1_path[0]
 
-    def check_attention_subgraph(self, layernorm: NodeProto, input_name_to_nodes: Dict[str, List[NodeProto]],
-                                 is_distil_bert: bool) -> bool:
+    def check_attention_subgraph(
+        self,
+        layernorm: NodeProto,
+        input_name_to_nodes: Dict[str, List[NodeProto]],
+        is_distil_bert: bool,
+    ) -> bool:
         """Check that LayerNormalization has a child of Attention node or subgraph like Attention.
 
         Args:
             layernorm (NodeProto): LayerNormalization node
             input_name_to_nodes (Dict[str, List[NodeProto]]): map from input name to nodes
             is_distil_bert (bool): whether it is DistilBert or not
 
         Returns:
             bool: whether there is Attention node or subgraph like Attention
         """
-        self.attention = self.model.find_first_child_by_type(layernorm,
-                                                             'Attention',
-                                                             input_name_to_nodes,
-                                                             recursive=False)
-        if self.attention is None:
-            # In case user disables attention fusion, check whether subgraph looks like Attention.
-            if layernorm.output[0] not in input_name_to_nodes:
-                return False
-            children = input_name_to_nodes[layernorm.output[0]]
-
-            # For Albert, there is MatMul+Add after embedding layer before attention.
-            if len(children) == 1 and children[0].op_type == "MatMul" and children[0].output[0] in input_name_to_nodes:
-                grandchildren = input_name_to_nodes[children[0].output[0]]
-                if len(grandchildren) == 1 and grandchildren[0].op_type == "Add" and grandchildren[0].output[
-                        0] in input_name_to_nodes:
-                    nodes = input_name_to_nodes[grandchildren[0].output[0]]
-                    for node in nodes:
-                        if node.op_type == "Attention":
-                            self.attention = node
-                            return True
-                    children_types = sorted([child.op_type for child in nodes])
-            else:
-                children_types = sorted([child.op_type for child in children])
+        self.attention = self.model.find_first_child_by_type(
+            layernorm, "Attention", input_name_to_nodes, recursive=False
+        )
+
+        if self.attention is not None:
+            return True
+
+        if layernorm.output[0] not in input_name_to_nodes:
+            return False
+        children = input_name_to_nodes[layernorm.output[0]]
+        children_types = sorted([child.op_type for child in children])
+
+        # Try find MultiHeadAttention
+        if children_types == ["MatMul", "MatMul", "MatMul", "SkipLayerNormalization"]:
+            for node in children:
+                if node.op_type == "SkipLayerNormalization":
+                    path1 = self.model.match_parent_path(
+                        node,
+                        ["Add", "MatMul", "MultiHeadAttention", "MatMul"],
+                        [None, None, 0, 0],
+                    )
+                    if path1 is not None and path1[-1].input[0] == layernorm.output[0]:
+                        self.cross_attention = path1[2]
+                        return True
+
+        # In case user disables attention fusion, check whether subgraph looks like Attention.
+        # For Albert, there is MatMul+Add after embedding layer before attention.
+        if len(children) == 1 and children[0].op_type == "MatMul" and children[0].output[0] in input_name_to_nodes:
+            grandchildren = input_name_to_nodes[children[0].output[0]]
+            if (
+                len(grandchildren) == 1
+                and grandchildren[0].op_type == "Add"
+                and grandchildren[0].output[0] in input_name_to_nodes
+            ):
+                nodes = input_name_to_nodes[grandchildren[0].output[0]]
+                for node in nodes:
+                    if node.op_type == "Attention":
+                        self.attention = node
+                        return True
+                children_types = sorted([child.op_type for child in nodes])
+
+        # Two Shape nodes might be merged by ORT
+        if is_distil_bert:
+            # SkipLayerNormailization might exist when model has been optimized by ORT first.
+            if (
+                children_types != ["MatMul", "MatMul", "MatMul", "Shape", "SkipLayerNormalization"]
+                and children_types != ["Add", "MatMul", "MatMul", "MatMul", "Shape", "Shape"]
+                and children_types != ["Add", "MatMul", "MatMul", "MatMul", "Shape"]
+            ):
+                logger.debug("No Attention like subgraph in children of LayerNormalization")
+                return False
+        else:
+            if children_types != [
+                "Add",
+                "MatMul",
+                "MatMul",
+                "MatMul",
+            ] and children_types != [
+                "MatMul",
+                "MatMul",
+                "MatMul",
+                "SkipLayerNormalization",
+            ]:
+                logger.debug("No Attention like subgraph in children of LayerNormalization")
+                return False
 
-            # Two Shape nodes might be merged by ORT
-            if is_distil_bert:
-                # SkipLayerNormailization might exist when model has been optimized by ORT first.
-                if children_types != ['MatMul', 'MatMul', 'MatMul', 'Shape', 'SkipLayerNormalization'] and \
-                   children_types != ['Add', 'MatMul', 'MatMul', 'MatMul', 'Shape', 'Shape'] and \
-                   children_types != ['Add', 'MatMul', 'MatMul', 'MatMul', 'Shape']:
-                    logger.debug("No Attention like subgraph in children of LayerNormalization")
-                    return False
-            else:
-                if children_types != ['Add', 'MatMul', 'MatMul', 'MatMul'] and \
-                   children_types != ['MatMul', 'MatMul', 'MatMul', 'SkipLayerNormalization']:
-                    logger.debug("No Attention like subgraph in children of LayerNormalization")
-                    return False
         return True
 
     def match_position_embedding_distilbert(self, position_embedding_gather, input_ids, output_name_to_node):
         """  Match position embedding path from input_ids to Gather for DistilBert.
 
         Pattern is like the following:
                  (input_ids)
@@ -106,58 +145,69 @@
                        |    Unsqueeze
                        |    /
                       Expand
                         |
                       Gather
         """
         # remove after tests pass
-        path1 = self.model.match_parent_path(position_embedding_gather, ['Expand', 'Shape'], [1, 1])
+        path1 = self.model.match_parent_path(position_embedding_gather, ["Expand", "Shape"], [1, 1])
         if path1 is None:
-            path1 = self.model.match_parent_path(position_embedding_gather, ['Expand', 'Where', 'Reshape', 'Shape'], [1, 1, 2, 0])
+            path1 = self.model.match_parent_path(
+                position_embedding_gather,
+                ["Expand", "Where", "Reshape", "Shape"],
+                [1, 1, 2, 0],
+            )
             if path1 is None:
                 return False
 
         expand, shape = path1[0], path1[-1]
         if shape.input[0] != input_ids:
             return False
 
-        _, path2, _ = self.model.match_parent_paths(expand, [(['Unsqueeze', 'Range', 'Cast', 'Gather', 'Shape'], [0, 0, 1, 0, 0]), \
-                                                             (['Unsqueeze', 'Range', 'Gather', 'Shape'], [0, 0, 1, 0])], output_name_to_node)
+        _, path2, _ = self.model.match_parent_paths(
+            expand,
+            [
+                (["Unsqueeze", "Range", "Cast", "Gather", "Shape"], [0, 0, 1, 0, 0]),
+                (["Unsqueeze", "Range", "Gather", "Shape"], [0, 0, 1, 0]),
+            ],
+            output_name_to_node,
+        )
         if path2 is None:
             return False
 
         range_node = path2[1]
-        if not (self.utils.check_node_input_value(range_node, 0, 0)
-                and self.utils.check_node_input_value(range_node, 2, 1)):
+        if not (
+            self.utils.check_node_input_value(range_node, 0, 0) and self.utils.check_node_input_value(range_node, 2, 1)
+        ):
             return False
 
         gather_node = path2[-2]
         if not (self.utils.check_node_input_value(gather_node, 1, 1)):
             return False
 
         shape_node = path2[-1]
         if shape_node.input[0] != input_ids:
             return False
 
         return True
 
     def match_position_embedding_roberta(self, position_embedding_gather, input_ids, output_name_to_node):
-        """  Match position embedding path from input_ids to Gather for Roberta.
+        """Match position embedding path from input_ids to Gather for Roberta.
 
-        Roberta Embedding Layer Pattern (* is optional since it might be removed by ORT, ? is the padding word id):       
+        Roberta Embedding Layer Pattern (* is optional since it might be removed by ORT, ? is the padding word id):
           (input_ids) --> Equal(B=?) -- Not -- Cast(to=6) -- CumSum(axis=1) -- Mul -- Cast(to=7) -- Add(B=1) -- Cast(to=7)* --> Gather
                                                 |                              ^
                                                 V                              |
-                                                +------------------------------+  
+                                                +------------------------------+
 
         Roberta new pattern from transformers v4.9:
            (input_ids) --> Equal(B=?) -- Not -- Cast(to=6) -- CumSum(axis=1) -- Add(B=0) -- Mul -- Cast(to=7) -- Add(B=1) --> Gather
                                                 |                                           ^
                                                 V                                           |
-                                                +-------------------------------------------+  
+                                                +-------------------------------------------+
 
         start_node = position_embedding_gather
         start_index = 1
 
         # match optional Cast node.
         parent = self.model.get_parent(start_node, start_index, output_name_to_node)
         if parent is None:
@@ -186,49 +236,57 @@
         """
 
         return False
 
     def match_position_embedding_bert(self, position_embedding_gather, input_ids, output_name_to_node):
         """  Match position embedding path from input_ids to Gather for BERT.
 
-        BERT Embedding Layer Pattern:       
+        BERT Embedding Layer Pattern:
                                     (input_ids)
                                    /         \
                                  /          Shape
                                 /              |
                               /              Gather (indices=1)
                              /                  |
                             /                  Add (optional, B=0)
                            /                    |
                         Gather (segment_ids) Unsqueeze (axes=0)
-                           \        |           |
-                            \     Gather      Slice (data[1,512], starts=0, ends=*, axes=1, steps=1)
-                              \    /            |
-                                Add          Gather 
-                                   \       /
+                           \\        |           |
+                            \\     Gather      Slice (data[1,512], starts=0, ends=*, axes=1, steps=1)
+                              \\    /            |
+                                Add          Gather
+                                   \\       /
                                       Add
                                        |
                                 LayerNormalization
         """
-        path = self.model.match_parent_path(position_embedding_gather, ['Slice', 'Unsqueeze'], [1, 2],
-                                            output_name_to_node)
+        path = self.model.match_parent_path(
+            position_embedding_gather,
+            ["Slice", "Unsqueeze"],
+            [1, 2],
+            output_name_to_node,
+        )
         if path is None:
             return False
 
         slice, unsqueeze = path
         slice_weight = self.model.get_constant_value(slice.input[0])
-        if not (slice_weight is not None  and len(slice_weight.shape) == 2 and slice_weight.shape[0] == 1 \
-                and self.utils.check_node_input_value(slice, 1, [0]) \
-                and self.utils.check_node_input_value(slice, 3, [1]) \
-                and (len(slice.input) == 4 or self.utils.check_node_input_value(slice, 4, [1]))):
+        if not (
+            slice_weight is not None
+            and len(slice_weight.shape) == 2
+            and slice_weight.shape[0] == 1
+            and self.utils.check_node_input_value(slice, 1, [0])
+            and self.utils.check_node_input_value(slice, 3, [1])
+            and (len(slice.input) == 4 or self.utils.check_node_input_value(slice, 4, [1]))
+        ):
             return False
 
         opset_version = self.model.get_opset_version()
         if opset_version < 13:
-            if not FusionUtils.check_node_attribute(unsqueeze, 'axes', [0]):
+            if not FusionUtils.check_node_attribute(unsqueeze, "axes", [0]):
                 return False
         else:
             if not self.utils.check_node_input_value(unsqueeze, 1, [0]):
                 return False
 
         node = self.model.get_parent(unsqueeze, 0, output_name_to_node)
         if node is None:
@@ -253,61 +311,74 @@
 
     def match_position_embedding(self, position_embedding_gather, input_ids, output_name_to_node):
         if self.match_position_embedding_bert(position_embedding_gather, input_ids, output_name_to_node):
             return True
 
         # TODO: Support roberta (position starts from 2 instead of 0) in EmbedLayerNormalization kernel
         #       related: https://github.com/huggingface/transformers/issues/10736
-        #if self.match_position_embedding_roberta(position_embedding_gather, input_ids, output_name_to_node):
+        # if self.match_position_embedding_roberta(position_embedding_gather, input_ids, output_name_to_node):
         #    return True
 
         if self.match_position_embedding_distilbert(position_embedding_gather, input_ids, output_name_to_node):
             return True
 
         return False
 
     def check_embedding(self, word_embedding_gather, segment_embedding_gather, position_embedding_gather):
-        """Sanity check of embedding weights, and match hidden_size of weights and shape of inputs.
-        """
+        """Sanity check of embedding weights, and match hidden_size of weights and shape of inputs."""
         input_ids = word_embedding_gather.input[1]
         segment_ids = segment_embedding_gather.input[1] if segment_embedding_gather else None
         position_ids = position_embedding_gather.input[1]
 
         if self.shape_infer_helper is not None:
             input_ids_shape = self.shape_infer_helper.get_edge_shape(input_ids)
             position_ids_shape = self.shape_infer_helper.get_edge_shape(position_ids)
             assert input_ids_shape and position_ids_shape
-            if not (len(input_ids_shape) == 2 and len(position_ids_shape) == 2
-                    and input_ids_shape[1] == position_ids_shape[1]):
+            if not (
+                len(input_ids_shape) == 2
+                and len(position_ids_shape) == 2
+                and input_ids_shape[1] == position_ids_shape[1]
+            ):
                 logger.info(
-                    "Cannot fuse EmbedLayerNormalization: input_ids and position_ids not matched in 2nd dimension: {} vs {}"
-                    .format(input_ids_shape, position_ids_shape))
+                    "Cannot fuse EmbedLayerNormalization: input_ids and position_ids not matched in 2nd dimension: {} vs {}".format(
+                        input_ids_shape, position_ids_shape
+                    )
+                )
                 return False
 
             if segment_ids and not self.shape_infer_helper.compare_shape(input_ids, segment_ids):
                 logger.info(
-                    "Cannot fuse EmbedLayerNormalization: input_ids and segment_ids does not have same shape: {} != {}".
-                    format(input_ids_shape, self.shape_infer_helper.get_edge_shape(segment_ids)))
+                    "Cannot fuse EmbedLayerNormalization: input_ids and segment_ids does not have same shape: {} != {}".format(
+                        input_ids_shape,
+                        self.shape_infer_helper.get_edge_shape(segment_ids),
+                    )
+                )
                 return False
 
         word_embedding_table = self.model.get_constant_value(word_embedding_gather.input[0])
         if word_embedding_table is None or len(word_embedding_table.shape) != 2:
             logger.info("Cannot fuse EmbedLayerNormalization: word embedding table is not expected")
             return False
 
         position_embedding_table = self.model.get_constant_value(position_embedding_gather.input[0])
-        if position_embedding_table is None or len(position_embedding_table.shape) != 2 or (
-                word_embedding_table.shape[1] != position_embedding_table.shape[1]):
+        if (
+            position_embedding_table is None
+            or len(position_embedding_table.shape) != 2
+            or (word_embedding_table.shape[1] != position_embedding_table.shape[1])
+        ):
             logger.info("Cannot fuse EmbedLayerNormalization: position embedding table is not expected")
             return False
 
         if segment_ids:
             segment_embedding_table = self.model.get_constant_value(segment_embedding_gather.input[0])
-            if segment_embedding_table is None or len(segment_embedding_table.shape) != 2 or (
-                    word_embedding_table.shape[1] != segment_embedding_table.shape[1]):
+            if (
+                segment_embedding_table is None
+                or len(segment_embedding_table.shape) != 2
+                or (word_embedding_table.shape[1] != segment_embedding_table.shape[1])
+            ):
                 logger.info("Cannot fuse EmbedLayerNormalization: segment embedding table is not expected")
                 return False
 
         # In normal case, word embeding table is the largest, and segment embedding table is the smallest, while postion embedding table is in between.
         # TODO: use other information (like initializer names) to identify different embedding weights automatically.
         if word_embedding_table.shape[0] <= position_embedding_table.shape[0]:
             logger.warning(
@@ -346,17 +417,24 @@
             else:
                 int32_output = input_name
         else:
             int32_output, input_cast_node = self.utils.cast_input_to_int32(input_name)
 
         return int32_output, input_cast_node
 
-    def create_fused_node(self, input_ids: str, layernorm: NodeProto, word_embedding_gather: NodeProto,
-                          position_embedding_gather: NodeProto, segment_embedding_gather: Union[None, NodeProto],
-                          position_ids: str = None, embedding_sum_output = False):
+    def create_fused_node(
+        self,
+        input_ids: str,
+        layernorm: NodeProto,
+        word_embedding_gather: NodeProto,
+        position_embedding_gather: NodeProto,
+        segment_embedding_gather: Union[None, NodeProto],
+        position_ids: str = None,
+        embedding_sum_output=False,
+    ):
         """Create an EmbedLayerNormalization node. Note that segment embedding is optional.
 
         Args:
             input_ids (str): input_ids for word embeddings
             layernorm (NodeProto): LayerNormalization or SkipLayerNormalization node.
             word_embedding_gather (NodeProto): the Gather node for word embedding
             position_embedding_gather (NodeProto): the Gather node for position embedding
@@ -364,62 +442,75 @@
 
         Returns:
             NodeProto: the EmbedLayerNormalization node created.
         """
         nodes_to_add = []
         input_ids, _ = self.cast_to_int32(input_ids)
 
-        node_name = self.model.create_node_name('EmbedLayerNormalization')
+        node_name = self.model.create_node_name("EmbedLayerNormalization")
 
         if layernorm.op_type == "LayerNormalization":
             gamma = layernorm.input[1]
             beta = layernorm.input[2]
         else:  # SkipLayerNormalization
             gamma = layernorm.input[2]
             beta = layernorm.input[3]
 
         embed_node_inputs = None
         if segment_embedding_gather is not None:
             segment_ids, _ = self.cast_to_int32(segment_embedding_gather.input[1])
 
             embed_node_inputs = [
-                input_ids, segment_ids, word_embedding_gather.input[0], position_embedding_gather.input[0],
-                segment_embedding_gather.input[0], gamma, beta
+                input_ids,
+                segment_ids,
+                word_embedding_gather.input[0],
+                position_embedding_gather.input[0],
+                segment_embedding_gather.input[0],
+                gamma,
+                beta,
             ]
         else:  # no segment embedding
             embed_node_inputs = [
-                input_ids, '', word_embedding_gather.input[0], position_embedding_gather.input[0], '', gamma, beta
+                input_ids,
+                "",
+                word_embedding_gather.input[0],
+                position_embedding_gather.input[0],
+                "",
+                gamma,
+                beta,
             ]
 
         if position_ids is not None:
-            #Adding an empty input for mask before position_ids
-            embed_node_inputs.append('')
+            # Adding an empty input for mask before position_ids
+            embed_node_inputs.append("")
             position_ids, _ = self.cast_to_int32(position_ids)
             embed_node_inputs.append(position_ids)
 
         embed_node_outputs = [node_name + "_output", node_name + "_dummy_mask_index"]
         if embedding_sum_output:
             embed_node_outputs.append(node_name + "_embedding_sum")
 
-        embed_node = helper.make_node('EmbedLayerNormalization',
-                                      embed_node_inputs,
-                                      outputs=embed_node_outputs,
-                                      name=node_name)
+        embed_node = helper.make_node(
+            "EmbedLayerNormalization",
+            embed_node_inputs,
+            outputs=embed_node_outputs,
+            name=node_name,
+        )
 
         embed_node.domain = "com.microsoft"
 
         # Pass attribute "epsilon" from normalize node to EmbedLayerNormalization.
         for att in layernorm.attribute:
-            if att.name == 'epsilon':
+            if att.name == "epsilon":
                 embed_node.attribute.extend([att])
 
         # Set default value to 1e-12 if no attribute is found.
         # OnnxRuntime 1.2.0 or older has no epsilon attribute. The optimized model can only work for 1.3.0 or later.
         if len(embed_node.attribute) == 0:
-            embed_node.attribute.extend([helper.make_attribute("epsilon", 1.0E-12)])
+            embed_node.attribute.extend([helper.make_attribute("epsilon", 1.0e-12)])
 
         # Make sure new EmbedLayerNormalization node is the last one in self.nodes_to_add.
         nodes_to_add.append(embed_node)
         for node in nodes_to_add:
             self.node_name_to_graph_name[node.name] = self.this_graph_name
         self.nodes_to_add.extend(nodes_to_add)
 
@@ -441,55 +532,75 @@
             bool: whether there is an extra output needed out of embed layer norm node
         """
 
         nodes = self.model.get_children(add_before_layer_norm)
 
         return len(nodes) > 1
 
-    def fuse_gpt2(self, layernorm, add_before_layernorm, input_name_to_nodes, output_name_to_node):
-        #graph checks
-        # gpt2 has no segment embedding, subgraph pattern is like
-        #     input_ids  position_ids
-        #        |        |
-        #     Gather    Gather
-        #          \   /
-        #           Add _ _ _ _ _
-        #            |           |
-        #    LayerNormalization  |
-        #            |           |
-        #         Attention      |
-        #            |           |
-        #          Matmul        |
-        #            |          /
-        #           Add        /
-        #             \       /
-        #                Add
+    def fuse_gpt2(
+        self, layernorm, add_before_layernorm, input_name_to_nodes, output_name_to_node, optional_segment_gather=None
+    ):
+        # graph checks
+        # gpt2 has optional segment embedding, subgraph pattern is like
+        #                      input_ids  position_ids
+        #                         |        |
+        #  token_ids           Gather    Gather
+        #       |                   \   /
+        #   Gather (optional)        Add _ _ _ _ _
+        #                   \         |           |
+        #                     LayerNormalization  |
+        #                             |           |
+        #                          Attention      |
+        #                             |           |
+        #                           Matmul        |
+        #                             |          /
+        #                            Add        /
+        #                              \       /
+        #                                 Add
         two_gather = self.match_two_gather(add_before_layernorm)
         if two_gather is None:
             return False
 
-        add_output = add_before_layernorm.output[0]
-
         word_embedding_gather, position_embedding_gather = two_gather
         input_ids = word_embedding_gather.input[1]
         position_ids = position_embedding_gather.input[1]
 
         if not self.check_attention_subgraph(layernorm, input_name_to_nodes, is_distil_bert=False):
             return False
 
         if not self.check_embedding(word_embedding_gather, None, position_embedding_gather):
             return False
 
+        # If the add_before_layernorm node is an Add node, then the add_output output is the first index
+        # output of this node.
+
+        # If the add_before_layernorm node is SkipLayerNormalization node, then the add_output output
+        # is the (optional) fourth index output of this node.
+        add_output = None
         optional_embedding_sum_output = False
-        if self.is_embedding_sum_needed(add_before_layernorm):
+        if (add_before_layernorm.op_type == "Add" and self.is_embedding_sum_needed(add_before_layernorm)) or (
+            add_before_layernorm.op_type == "SkipLayerNormalization" and len(add_before_layernorm.output) >= 4
+        ):
             optional_embedding_sum_output = True
+            add_output = (
+                add_before_layernorm.output[0]
+                if add_before_layernorm.op_type == "Add"
+                else add_before_layernorm.output[3]
+            )
 
         # make the fused node
-        embed_node = self.create_fused_node(input_ids, layernorm, word_embedding_gather, position_embedding_gather,
-                                           None, position_ids, optional_embedding_sum_output)
+        embed_node = self.create_fused_node(
+            input_ids,
+            layernorm,
+            word_embedding_gather,
+            position_embedding_gather,
+            optional_segment_gather,
+            position_ids,
+            optional_embedding_sum_output,
+        )
 
         # direct the output to another add too
         self.model.replace_input_of_all_nodes(layernorm.output[0], embed_node.output[0])
         if optional_embedding_sum_output:
             self.model.replace_input_of_all_nodes(add_output, embed_node.output[2])
 
         return True
@@ -525,44 +636,45 @@
 
         if not self.match_position_embedding(position_embedding_gather, input_ids, output_name_to_node):
             return False
 
         if not self.check_embedding(word_embedding_gather, None, position_embedding_gather):
             return False
 
-        embed_node = self.create_fused_node(input_ids, layernorm, word_embedding_gather, position_embedding_gather,
-                                            None)
+        embed_node = self.create_fused_node(
+            input_ids, layernorm, word_embedding_gather, position_embedding_gather, None
+        )
         self.finish_fusion(layernorm, embed_node)
         return True
 
     def fuse_bert(self, layernorm, add_before_layernorm, input_name_to_nodes, output_name_to_node):
         """Fuse embedding layer for Bert
         Args:
             layernorm (NodeProto): node of LayerNormalization or SkipLayerNormalization
             add_before_layernorm (NodeProto): the Add node before LayerNormalization, or the SkipLayerNormalization itself
             input_name_to_nodes (Dict[str, List[NodeProto]]): map from input name to nodes
             output_name_to_node (Dict[str, List[NodeProto]]): map from output name to nodes
         """
 
-        add_2_gather = self.model.match_parent_path(add_before_layernorm, ['Add'], [0])
+        add_2_gather = self.model.match_parent_path(add_before_layernorm, ["Add"], [0])
         if add_2_gather is None:
             return False
 
         two_gather = self.match_two_gather(add_2_gather[0])
         if two_gather is None:
             return False
 
         word_embedding_gather, segment_embedding_gather = two_gather
 
         input_ids = word_embedding_gather.input[1]
 
         if not self.check_attention_subgraph(layernorm, input_name_to_nodes, is_distil_bert=False):
             return False
 
-        position_embedding_path = self.model.match_parent_path(add_before_layernorm, ['Gather'], [1])
+        position_embedding_path = self.model.match_parent_path(add_before_layernorm, ["Gather"], [1])
         if position_embedding_path is None:
             return False
 
         position_embedding_gather = position_embedding_path[0]
         if not self.match_position_embedding(position_embedding_gather, input_ids, output_name_to_node):
             if not self.match_position_embedding(segment_embedding_gather, input_ids, output_name_to_node):
                 return False
@@ -570,51 +682,121 @@
             temp = segment_embedding_gather
             segment_embedding_gather = position_embedding_gather
             position_embedding_gather = temp
 
         if not self.check_embedding(word_embedding_gather, segment_embedding_gather, position_embedding_gather):
             return False
 
-        embed_node = self.create_fused_node(input_ids, layernorm, word_embedding_gather, position_embedding_gather,
-                                            segment_embedding_gather)
+        embed_node = self.create_fused_node(
+            input_ids,
+            layernorm,
+            word_embedding_gather,
+            position_embedding_gather,
+            segment_embedding_gather,
+        )
         self.finish_fusion(layernorm, embed_node)
         return True
 
     def fuse(self, node, input_name_to_nodes, output_name_to_node):
+        first_add_path = self.model.match_parent_path(node, ["Add"], [0])
         if node.op_type == "LayerNormalization":
-            first_add_path = self.model.match_parent_path(node, ['Add'], [0])
             if first_add_path is None:
                 return
             add_before_layernorm = first_add_path[0]
+            optional_segment_gather = None
         else:  # SkipLayerNormalization
-            add_before_layernorm = node  # Add is fused into SkipLayerNormalization
+            gather_0_path = self.model.match_parent_path(node, ["Gather"], [0])
+            gather_1_path = self.model.match_parent_path(node, ["Gather"], [1])
+            if gather_0_path is None and gather_1_path is not None:
+                add_before_layernorm = first_add_path[0]
+                optional_segment_gather = gather_1_path[0]
+            elif gather_0_path is not None and gather_1_path is None:
+                add_before_layernorm = first_add_path[0]
+                optional_segment_gather = gather_0_path[0]
+            else:
+                add_before_layernorm = node  # Add is fused into SkipLayerNormalization
+                optional_segment_gather = None
 
-        if self.fuse_gpt2(node, add_before_layernorm, input_name_to_nodes, output_name_to_node):
+        if self.fuse_gpt2(
+            node, add_before_layernorm, input_name_to_nodes, output_name_to_node, optional_segment_gather
+        ):
             return
 
         if self.fuse_distilbert(node, add_before_layernorm, input_name_to_nodes, output_name_to_node):
             return
 
         if self.fuse_bert(node, add_before_layernorm, input_name_to_nodes, output_name_to_node):
             return
 
 
 class FusionEmbedLayerNormalization(FusionEmbedLayerNoMask):
-    def __init__(self, model: OnnxModel):
+    def __init__(self, model: OnnxModel, use_mask_index=False):
         super().__init__(model, "with mask")
+        self.use_mask_index = use_mask_index
+
+    def replace_mask(self, mask_int32, attention_nodes):
+        # Inputs of EmbedLayerNorm: input_ids, segment_ids (optional), word_embedding, position_embedding,
+        #           segment_embedding (optional), gamma, beta, mask (optional), position_ids (optional)
+        embed_node = self.embed_node
+        if len(embed_node.input) == 7:
+            embed_node.input.append(mask_int32)
+            logger.debug("append mask to %s", embed_node.name)
+        elif len(embed_node.input) > 7 and not embed_node.input[7]:
+            embed_node.input[7] = mask_int32
+            logger.debug("replace mask in %s", embed_node.name)
+        else:
+            logger.debug("skip mask in %s", embed_node.name)
+            return
+
+        for attention_node in attention_nodes:
+            logger.debug("update mask_index in %s", attention_node.name)
+            if attention_node.op_type == "Attention":
+                attention_node.input[3] = embed_node.output[1]
+            elif attention_node.op_type == "MultiHeadAttention":
+                attention_node.input[4] = embed_node.output[1]
 
     def fuse(self, node, input_name_to_nodes, output_name_to_node):
         # Reset attention and embed_node so that we know fusion is successful when they are not None.
         self.attention = None
+        self.cross_attention = None
         self.embed_node = None
         super().fuse(node, input_name_to_nodes, output_name_to_node)
 
-        if self.attention and self.embed_node:
-            mask_index = self.attention.input[3]
-            if mask_index in output_name_to_node:
-                node = output_name_to_node[mask_index]
-                if node.op_type == "ReduceSum":
-                    embed_node = self.embed_node
-                    mask_input_name = node.input[0]
-                    self.nodes_to_remove.extend([node])
-                    embed_node.input.append(mask_input_name)
-                    embed_node.output[1] = mask_index
+        if self.embed_node is None:
+            return
+
+        if not self.use_mask_index:
+            logger.debug("--use_mask_index is not set: EmbedLayerNormalization will not have mask")
+            self.increase_counter("EmbedLayerNormalization(no mask)")
+            return
+
+        if self.attention is None and self.cross_attention is None:
+            logger.debug("EmbedLayerNormalization will not have mask since attention node is not found")
+            self.increase_counter("EmbedLayerNormalization(no mask)")
+            return
+
+        if self.attention:
+            mask_int32 = self.attention.input[3]
+        else:
+            mask_int32 = self.cross_attention.input[4]
+
+        children_nodes = input_name_to_nodes[mask_int32]
+        if self.model.find_graph_input(mask_int32):
+            attention_nodes = [node for node in children_nodes if node.op_type in ["Attention", "MultiHeadAttention"]]
+            self.replace_mask(mask_int32, attention_nodes)
+            self.increase_counter("EmbedLayerNormalization(with mask)")
+            return
+
+        if mask_int32 not in output_name_to_node:
+            logger.debug("EmbedLayerNormalization will not have mask since %s is not a node output", mask_int32)
+            self.increase_counter("EmbedLayerNormalization(no mask)")
+            return
+
+        node = output_name_to_node[mask_int32]
+        if node.op_type in ["ReduceSum", "Cast"]:
+            attention_nodes = [node for node in children_nodes if node.op_type in ["Attention", "MultiHeadAttention"]]
+            if node.op_type == "ReduceSum":
+                mask_int32 = node.input[0]
+                if len(children_nodes) == len(attention_nodes):
+                    self.nodes_to_remove.append(node)
+            self.replace_mask(mask_int32, attention_nodes)
+            self.increase_counter("EmbedLayerNormalization(with mask)")
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_fastgelu.py` & `onnxruntime/transformers/fusion_fastgelu.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,16 +1,17 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
-from typing import Dict, Optional
+# --------------------------------------------------------------------------
 from logging import getLogger
+from typing import Dict, Optional
+
+from fusion_base import Fusion
 from onnx import helper
 from onnx_model import OnnxModel
-from fusion_base import Fusion
 
 logger = getLogger(__name__)
 
 
 class FusionFastGelu(Fusion):
     def __init__(self, model: OnnxModel):
         super().__init__(model, "FastGelu", "Tanh")
@@ -36,87 +37,102 @@
               |                                                              |
               +------> Mul(B=0.5)--------------------------------------------+
         Note that constant input for Add and Mul could be first or second input: like either A=0.5 or B=0.5 is fine.
         """
         if tanh_node.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[tanh_node.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Add':
+        if len(children) != 1 or children[0].op_type != "Add":
             return
         add_after_tanh = children[0]
 
         if not self.model.has_constant_input(add_after_tanh, 1.0):
             return
 
         if add_after_tanh.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[add_after_tanh.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Mul':
+        if len(children) != 1 or children[0].op_type != "Mul":
             return
         mul_after_tanh = children[0]
 
-        mul_half = self.model.match_parent(mul_after_tanh, 'Mul', None, output_name_to_node)
+        mul_half = self.model.match_parent(mul_after_tanh, "Mul", None, output_name_to_node)
         if mul_half is None:
             return
 
         i = self.model.find_constant_input(mul_half, 0.5)
         if i < 0:
             return
 
         root_input = mul_half.input[0 if i == 1 else 1]
 
-        #root_node could be None when root_input is graph input
+        # root_node could be None when root_input is graph input
         root_node = self.model.get_parent(mul_half, 0 if i == 1 else 1, output_name_to_node)
 
-        mul_before_tanh = self.model.match_parent(tanh_node, 'Mul', 0, output_name_to_node)
+        mul_before_tanh = self.model.match_parent(tanh_node, "Mul", 0, output_name_to_node)
         if mul_before_tanh is None:
             return
 
         i = self.model.find_constant_input(mul_before_tanh, 0.7978, delta=0.0001)
         if i < 0:
             return
 
-        add_before_tanh = self.model.match_parent(mul_before_tanh, 'Add', 0 if i == 1 else 1, output_name_to_node)
+        add_before_tanh = self.model.match_parent(mul_before_tanh, "Add", 0 if i == 1 else 1, output_name_to_node)
         if add_before_tanh is None:
             return
 
-        mul_after_pow = self.model.match_parent(add_before_tanh,
-                                                'Mul',
-                                                None,
-                                                output_name_to_node,
-                                                exclude=[root_node] if root_node else [])
+        mul_after_pow = self.model.match_parent(
+            add_before_tanh,
+            "Mul",
+            None,
+            output_name_to_node,
+            exclude=[root_node] if root_node else [],
+        )
         if mul_after_pow is None:
             return
 
         i = self.model.find_constant_input(mul_after_pow, 0.0447, delta=0.0001)
         if i < 0:
             return
 
-        pow = self.model.match_parent(mul_after_pow, 'Pow', 0 if i == 1 else 1, output_name_to_node)
+        pow = self.model.match_parent(mul_after_pow, "Pow", 0 if i == 1 else 1, output_name_to_node)
         if pow is None:
             return
 
         if not self.model.has_constant_input(pow, 3.0):
             return
 
         if pow.input[0] != root_input:
             return
 
         subgraph_nodes = [
-            mul_after_tanh, mul_half, add_after_tanh, tanh_node, mul_before_tanh, add_before_tanh, mul_after_pow, pow
+            mul_after_tanh,
+            mul_half,
+            add_after_tanh,
+            tanh_node,
+            mul_before_tanh,
+            add_before_tanh,
+            mul_after_pow,
+            pow,
         ]
-        if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, [mul_after_tanh.output[0]], input_name_to_nodes,
-                                                output_name_to_node):
+        if not self.model.is_safe_to_fuse_nodes(
+            subgraph_nodes,
+            [mul_after_tanh.output[0]],
+            input_name_to_nodes,
+            output_name_to_node,
+        ):
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
-        fused_node = helper.make_node('FastGelu',
-                                      inputs=[root_input],
-                                      outputs=mul_after_tanh.output,
-                                      name=self.model.create_node_name('FastGelu'))
+        fused_node = helper.make_node(
+            "FastGelu",
+            inputs=[root_input],
+            outputs=mul_after_tanh.output,
+            name=self.model.create_node_name("FastGelu"),
+        )
         fused_node.domain = "com.microsoft"
         self.nodes_to_add.append(fused_node)
         self.node_name_to_graph_name[fused_node.name] = self.this_graph_name
         return True
 
     def fuse_2(self, tanh_node, input_name_to_nodes: Dict, output_name_to_node: Dict) -> Optional[bool]:
         """
@@ -130,187 +146,215 @@
               |                                                                           |
               +---------------------------------------------------------------------------+
         Note that constant input for Add and Mul could be first or second input: like either A=0.5 or B=0.5 is fine.
         """
         if tanh_node.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[tanh_node.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Add':
+        if len(children) != 1 or children[0].op_type != "Add":
             return
         add_after_tanh = children[0]
 
         if not self.model.has_constant_input(add_after_tanh, 1.0):
             return
 
         if add_after_tanh.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[add_after_tanh.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Mul':
+        if len(children) != 1 or children[0].op_type != "Mul":
             return
         mul_half = children[0]
 
         i = self.model.find_constant_input(mul_half, 0.5)
         if i < 0:
             return
 
         if mul_half.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[mul_half.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Mul':
+        if len(children) != 1 or children[0].op_type != "Mul":
             return
         mul_after_mul_half = children[0]
 
-        root_node = self.model.get_parent(mul_after_mul_half,
-                                          0 if mul_after_mul_half.input[1] == mul_half.output[0] else 1,
-                                          output_name_to_node)
+        root_node = self.model.get_parent(
+            mul_after_mul_half,
+            0 if mul_after_mul_half.input[1] == mul_half.output[0] else 1,
+            output_name_to_node,
+        )
         if root_node is None:
             return
 
-        mul_before_tanh = self.model.match_parent(tanh_node, 'Mul', 0, output_name_to_node)
+        mul_before_tanh = self.model.match_parent(tanh_node, "Mul", 0, output_name_to_node)
         if mul_before_tanh is None:
             return
 
         i = self.model.find_constant_input(mul_before_tanh, 0.7978, delta=0.0001)
         if i < 0:
             return
 
-        add_before_tanh = self.model.match_parent(mul_before_tanh, 'Add', 0 if i == 1 else 1, output_name_to_node)
+        add_before_tanh = self.model.match_parent(mul_before_tanh, "Add", 0 if i == 1 else 1, output_name_to_node)
         if add_before_tanh is None:
             return
 
-        mul_after_pow = self.model.match_parent(add_before_tanh, 'Mul', None, output_name_to_node, exclude=[root_node])
+        mul_after_pow = self.model.match_parent(add_before_tanh, "Mul", None, output_name_to_node, exclude=[root_node])
         if mul_after_pow is None:
             return
 
         i = self.model.find_constant_input(mul_after_pow, 0.0447, delta=0.0001)
         if i < 0:
             return
 
-        pow = self.model.match_parent(mul_after_pow, 'Pow', 0 if i == 1 else 1, output_name_to_node)
+        pow = self.model.match_parent(mul_after_pow, "Pow", 0 if i == 1 else 1, output_name_to_node)
         if pow is None:
             return
 
         if not self.model.has_constant_input(pow, 3.0):
             return
 
         if pow.input[0] != root_node.output[0]:
             return
 
         subgraph_nodes = [
-            mul_after_mul_half, mul_half, add_after_tanh, tanh_node, mul_before_tanh, add_before_tanh, mul_after_pow,
-            pow
+            mul_after_mul_half,
+            mul_half,
+            add_after_tanh,
+            tanh_node,
+            mul_before_tanh,
+            add_before_tanh,
+            mul_after_pow,
+            pow,
         ]
-        if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, [mul_after_mul_half.output[0]], input_name_to_nodes,
-                                                output_name_to_node):
+        if not self.model.is_safe_to_fuse_nodes(
+            subgraph_nodes,
+            [mul_after_mul_half.output[0]],
+            input_name_to_nodes,
+            output_name_to_node,
+        ):
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
-        fused_node = helper.make_node('FastGelu',
-                                      inputs=[root_node.output[0]],
-                                      outputs=mul_after_mul_half.output,
-                                      name=self.model.create_node_name('FastGelu'))
+        fused_node = helper.make_node(
+            "FastGelu",
+            inputs=[root_node.output[0]],
+            outputs=mul_after_mul_half.output,
+            name=self.model.create_node_name("FastGelu"),
+        )
         fused_node.domain = "com.microsoft"
         self.nodes_to_add.append(fused_node)
         self.node_name_to_graph_name[fused_node.name] = self.this_graph_name
         return True
 
     def fuse_3(self, tanh_node, input_name_to_nodes: Dict, output_name_to_node: Dict) -> Optional[bool]:
         """
-            OpenAI's gelu implementation, also used in Megatron:
-               Gelu(x) = x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1.0 + 0.044715 * x * x)))
+        OpenAI's gelu implementation, also used in Megatron:
+           Gelu(x) = x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1.0 + 0.044715 * x * x)))
 
-            Fuse subgraph into a FastGelu node:
-                +------------ Mul (B=0.79788456) -------------------+
-                |                                                   |
-                +-------------------------------+                   |
-                |                               |                   |
-                |                               v                   v
-              [root] --> Mul (B=0.044715) --> Mul --> Add(B=1) --> Mul --> Tanh --> Add(B=1) --> Mul-->
-                |                                                                                 ^
-                |                                                                                 |
-                +-----------> Mul (B=0.5) --------------------------------------------------------+
-            """
+        Fuse subgraph into a FastGelu node:
+            +------------ Mul (B=0.79788456) -------------------+
+            |                                                   |
+            +-------------------------------+                   |
+            |                               |                   |
+            |                               v                   v
+          [root] --> Mul (B=0.044715) --> Mul --> Add(B=1) --> Mul --> Tanh --> Add(B=1) --> Mul-->
+            |                                                                                 ^
+            |                                                                                 |
+            +-----------> Mul (B=0.5) --------------------------------------------------------+
+        """
         if tanh_node.output[0] not in input_name_to_nodes:
             return
 
         children = input_name_to_nodes[tanh_node.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Add':
+        if len(children) != 1 or children[0].op_type != "Add":
             return
         add_after_tanh = children[0]
 
         if not self.model.has_constant_input(add_after_tanh, 1.0):
             return
 
         if add_after_tanh.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[add_after_tanh.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Mul':
+        if len(children) != 1 or children[0].op_type != "Mul":
             return
         mul_last = children[0]
 
-        mul_half = self.model.match_parent(mul_last, 'Mul', None, output_name_to_node)
+        mul_half = self.model.match_parent(mul_last, "Mul", None, output_name_to_node)
         if mul_half is None:
             return
 
         i = self.model.find_constant_input(mul_half, 0.5)
         if i < 0:
             return
 
         root_input = mul_half.input[0 if i == 1 else 1]
 
-        mul_before_tanh = self.model.match_parent(tanh_node, 'Mul', 0, output_name_to_node)
+        mul_before_tanh = self.model.match_parent(tanh_node, "Mul", 0, output_name_to_node)
         if mul_before_tanh is None:
             return
 
-        add_1 = self.model.match_parent(mul_before_tanh, 'Add', None, output_name_to_node)
+        add_1 = self.model.match_parent(mul_before_tanh, "Add", None, output_name_to_node)
         if add_1 is None:
             return
         j = self.model.find_constant_input(add_1, 1.0)
         if j < 0:
             return
 
-        mul_7978 = self.model.match_parent(mul_before_tanh, 'Mul', None, output_name_to_node)
+        mul_7978 = self.model.match_parent(mul_before_tanh, "Mul", None, output_name_to_node)
         if mul_7978 is None:
             return
         k = self.model.find_constant_input(mul_7978, 0.7978, delta=0.0001)
         if k < 0:
             return
         if mul_7978.input[0 if k == 1 else 1] != root_input:
             return
 
-        mul_before_add_1 = self.model.match_parent(add_1, 'Mul', 0 if j == 1 else 1, output_name_to_node)
+        mul_before_add_1 = self.model.match_parent(add_1, "Mul", 0 if j == 1 else 1, output_name_to_node)
         if mul_before_add_1 is None:
             return
 
         if mul_before_add_1.input[0] == root_input:
             another = 1
         elif mul_before_add_1.input[1] == root_input:
             another = 0
         else:
             return
 
-        mul_0447 = self.model.match_parent(mul_before_add_1, 'Mul', another, output_name_to_node)
+        mul_0447 = self.model.match_parent(mul_before_add_1, "Mul", another, output_name_to_node)
         if mul_0447 is None:
             return
         m = self.model.find_constant_input(mul_0447, 0.0447, delta=0.0001)
         if m < 0:
             return
 
         if mul_0447.input[0 if m == 1 else 1] != root_input:
             return
 
         subgraph_nodes = [
-            mul_0447, mul_before_add_1, add_1, mul_before_tanh, tanh_node, add_after_tanh, mul_7978, mul_half, mul_last
+            mul_0447,
+            mul_before_add_1,
+            add_1,
+            mul_before_tanh,
+            tanh_node,
+            add_after_tanh,
+            mul_7978,
+            mul_half,
+            mul_last,
         ]
-        if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, [mul_last.output[0]], input_name_to_nodes,
-                                                output_name_to_node):
+        if not self.model.is_safe_to_fuse_nodes(
+            subgraph_nodes,
+            [mul_last.output[0]],
+            input_name_to_nodes,
+            output_name_to_node,
+        ):
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
-        fused_node = helper.make_node('FastGelu',
-                                      inputs=[root_input],
-                                      outputs=mul_last.output,
-                                      name=self.model.create_node_name('FastGelu'))
+        fused_node = helper.make_node(
+            "FastGelu",
+            inputs=[root_input],
+            outputs=mul_last.output,
+            name=self.model.create_node_name("FastGelu"),
+        )
         fused_node.domain = "com.microsoft"
         self.nodes_to_add.append(fused_node)
         self.node_name_to_graph_name[fused_node.name] = self.this_graph_name
         return True
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gelu.py` & `onnxruntime/transformers/fusion_gelu.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,17 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
-from typing import Dict, Optional
+# --------------------------------------------------------------------------
 from logging import getLogger
+from typing import Dict, Optional
+
+from fusion_base import Fusion
 from onnx import helper
 from onnx_model import OnnxModel
-from fusion_base import Fusion
 
 logger = getLogger(__name__)
 
 
 class FusionGelu(Fusion):
     def __init__(self, model: OnnxModel):
         super().__init__(model, "Gelu", "Erf")
@@ -41,66 +42,67 @@
                               (B=1.4142...)       (1)            (0.5)
 
         Note that constant input for Add and Mul could be first or second input: like either A=0.5 or B=0.5 is fine.
         """
         if erf_node.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[erf_node.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Add':
+        if len(children) != 1 or children[0].op_type != "Add":
             return
         add_after_erf = children[0]
 
         if not self.model.has_constant_input(add_after_erf, 1):
             return
 
         if add_after_erf.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[add_after_erf.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Mul':
+        if len(children) != 1 or children[0].op_type != "Mul":
             return
         mul_after_erf = children[0]
 
-        div = self.model.match_parent(erf_node, 'Div', 0, output_name_to_node)
+        div = self.model.match_parent(erf_node, "Div", 0, output_name_to_node)
         if div is None:
             return
 
         if self.model.find_constant_input(div, 1.4142, delta=0.001) != 1:
             return
 
         subgraph_input = div.input[0]
 
         another = 1 if mul_after_erf.input[0] == add_after_erf.output[0] else 0
         if subgraph_input == mul_after_erf.input[another]:  # pattern 2
             children = input_name_to_nodes[mul_after_erf.output[0]]
-            if len(children) != 1 or children[0].op_type != 'Mul':
+            if len(children) != 1 or children[0].op_type != "Mul":
                 return
             mul_half = children[0]
             if not self.model.has_constant_input(mul_half, 0.5):
                 return
             subgraph_output = mul_half.output[0]
         else:  # pattern 1
-            mul_half = self.model.match_parent(mul_after_erf, 'Mul', another, output_name_to_node)
+            mul_half = self.model.match_parent(mul_after_erf, "Mul", another, output_name_to_node)
             if mul_half is None:
                 return
 
             if not self.model.has_constant_input(mul_half, 0.5):
                 return
 
             if subgraph_input not in mul_half.input:
                 return
 
             subgraph_output = mul_after_erf.output[0]
 
         subgraph_nodes = [div, erf_node, add_after_erf, mul_after_erf, mul_half]
-        if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, [subgraph_output], input_name_to_nodes,
-                                                output_name_to_node):
+        if not self.model.is_safe_to_fuse_nodes(
+            subgraph_nodes, [subgraph_output], input_name_to_nodes, output_name_to_node
+        ):
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
-        fused_node = helper.make_node('Gelu', inputs=[subgraph_input], outputs=[subgraph_output])
+        fused_node = helper.make_node("Gelu", inputs=[subgraph_input], outputs=[subgraph_output])
         fused_node.domain = "com.microsoft"
         self.nodes_to_add.append(fused_node)
         self.node_name_to_graph_name[fused_node.name] = self.this_graph_name
         return True
 
     def fuse_2(self, erf_node, input_name_to_nodes: Dict, output_name_to_node: Dict) -> Optional[bool]:
         """
@@ -113,45 +115,45 @@
                               (B=1.4142...)       (A=1)   (A=0.5)
 
         Note that constant input for Add and Mul could be first or second input: like either A=0.5 or B=0.5 is fine.
         """
         if erf_node.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[erf_node.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Add':
+        if len(children) != 1 or children[0].op_type != "Add":
             return
         add_after_erf = children[0]
 
         if not self.model.has_constant_input(add_after_erf, 1):
             return
 
         if add_after_erf.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[add_after_erf.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Mul':
+        if len(children) != 1 or children[0].op_type != "Mul":
             return
         mul_after_erf = children[0]
 
         if not self.model.has_constant_input(mul_after_erf, 0.5):
             return
 
         if mul_after_erf.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[mul_after_erf.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Mul':
+        if len(children) != 1 or children[0].op_type != "Mul":
             return
         mul = children[0]
 
-        div = self.model.match_parent(erf_node, 'Div', 0, output_name_to_node)
+        div = self.model.match_parent(erf_node, "Div", 0, output_name_to_node)
         if div is None:
             return
 
         sqrt_node = None
         if self.model.find_constant_input(div, 1.4142, delta=0.001) != 1:
-            sqrt_node = self.model.match_parent(div, 'Sqrt', 1, output_name_to_node)
+            sqrt_node = self.model.match_parent(div, "Sqrt", 1, output_name_to_node)
             if sqrt_node is None:
                 return
             if not self.model.has_constant_input(sqrt_node, 2.0):
                 return
 
         root_node = self.model.get_parent(div, 0, output_name_to_node)
         if root_node is None:
@@ -160,20 +162,21 @@
         if root_node.output[0] not in mul.input:
             return
 
         subgraph_nodes = [div, erf_node, add_after_erf, mul_after_erf, mul]
         if sqrt_node:
             subgraph_nodes.append(sqrt_node)
 
-        if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, [mul.output[0]], input_name_to_nodes,
-                                                output_name_to_node):
+        if not self.model.is_safe_to_fuse_nodes(
+            subgraph_nodes, [mul.output[0]], input_name_to_nodes, output_name_to_node
+        ):
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
-        fused_node = helper.make_node('Gelu', inputs=[root_node.output[0]], outputs=[mul.output[0]])
+        fused_node = helper.make_node("Gelu", inputs=[root_node.output[0]], outputs=[mul.output[0]])
         fused_node.domain = "com.microsoft"
         self.nodes_to_add.append(fused_node)
         self.node_name_to_graph_name[fused_node.name] = self.this_graph_name
         return True
 
     def fuse_3(self, erf_node, input_name_to_nodes: Dict, output_name_to_node: Dict) -> Optional[bool]:
         """
@@ -187,57 +190,61 @@
 
         Note that constant input for Add and Mul could be first or second input: like either A=0.5 or B=0.5 is fine.
         """
 
         if erf_node.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[erf_node.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Add':
+        if len(children) != 1 or children[0].op_type != "Add":
             return
         add_after_erf = children[0]
 
         if not self.model.has_constant_input(add_after_erf, 1):
             return
 
         if add_after_erf.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[add_after_erf.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Mul':
+        if len(children) != 1 or children[0].op_type != "Mul":
             return
         mul_half = children[0]
 
         if not self.model.has_constant_input(mul_half, 0.5):
             return
 
-        first_mul = self.model.match_parent(erf_node, 'Mul', 0, output_name_to_node)
+        first_mul = self.model.match_parent(erf_node, "Mul", 0, output_name_to_node)
         if first_mul is None:
             return
 
         i = self.model.find_constant_input(first_mul, 0.7071067690849304, delta=0.001)
         if i < 0:
             return
 
         root_node = self.model.get_parent(first_mul, 0 if i == 1 else 1, output_name_to_node)
         if root_node is None:
             return
 
         if mul_half.output[0] not in input_name_to_nodes:
             return
         children = input_name_to_nodes[mul_half.output[0]]
-        if len(children) != 1 or children[0].op_type != 'Mul':
+        if len(children) != 1 or children[0].op_type != "Mul":
             return
         last_mul = children[0]
 
         if not (last_mul.input[0] == root_node.output[0] or last_mul.input[1] == root_node.output[0]):
             return
 
         subgraph_nodes = [first_mul, erf_node, add_after_erf, mul_half, last_mul]
-        if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, [last_mul.output[0]], input_name_to_nodes,
-                                                output_name_to_node):
+        if not self.model.is_safe_to_fuse_nodes(
+            subgraph_nodes,
+            [last_mul.output[0]],
+            input_name_to_nodes,
+            output_name_to_node,
+        ):
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
-        fused_node = helper.make_node('Gelu', inputs=[root_node.output[0]], outputs=[last_mul.output[0]])
+        fused_node = helper.make_node("Gelu", inputs=[root_node.output[0]], outputs=[last_mul.output[0]])
         fused_node.domain = "com.microsoft"
         self.nodes_to_add.append(fused_node)
         self.node_name_to_graph_name[fused_node.name] = self.this_graph_name
         return True
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gelu_approximation.py` & `onnxruntime/transformers/fusion_gelu_approximation.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,24 +1,27 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
-from logging import getLogger
+from logging import getLogger  # noqa: F401
+
+from fusion_base import Fusion
 from onnx import helper
 from onnx_model import OnnxModel
-from fusion_base import Fusion
 
 
 class FusionGeluApproximation(Fusion):
     def __init__(self, model: OnnxModel):
-        super().__init__(model, 'FastGelu', ['Gelu', 'BiasGelu'], 'GeluApproximation')
+        super().__init__(model, "FastGelu", ["Gelu", "BiasGelu"], "GeluApproximation")
 
     def fuse(self, node, input_name_to_nodes, output_name_to_node):
-        new_node = helper.make_node("FastGelu",
-                                    inputs=node.input,
-                                    outputs=node.output,
-                                    name=self.model.create_node_name("FastGelu", node.op_type + "_Approximation"))
+        new_node = helper.make_node(
+            "FastGelu",
+            inputs=node.input,
+            outputs=node.output,
+            name=self.model.create_node_name("FastGelu", node.op_type + "_Approximation"),
+        )
         new_node.domain = "com.microsoft"
         self.nodes_to_remove.append(node)
         self.nodes_to_add.append(new_node)
         self.node_name_to_graph_name[new_node.name] = self.this_graph_name
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gpt_attention.py` & `onnxruntime/transformers/fusion_gpt_attention.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,29 +1,31 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
-import numpy as np
+# --------------------------------------------------------------------------
 from logging import getLogger
-from onnx import helper, numpy_helper, TensorProto
-from onnx_model import OnnxModel
+
+import numpy as np
 from fusion_base import Fusion
 from fusion_utils import FusionUtils
+from onnx import TensorProto, helper, numpy_helper  # noqa: F401
+from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class FusionGptAttentionPastBase(Fusion):
-    """Base class for GPT Attention Fusion with past state
-    """
+    """Base class for GPT Attention Fusion with past state"""
+
     def __init__(self, model: OnnxModel, num_heads: int):
-        super().__init__(model, "Attention", "LayerNormalization", "with past")
+        super().__init__(model, "Attention", ["LayerNormalization", "SkipLayerNormalization"], "with past")
         self.num_heads = num_heads
         self.utils = FusionUtils(model)
         self.casted_attention_mask = {}  # map from name of attention mask to the name that casted to int32
+        self.mask_filter_value = None
 
     def match_past_pattern_1(self, concat_k, concat_v, output_name_to_node):
         # Pattern 1:
         #                      {past}
         #                    /        \
         #                   /          \
         #    Gather(axes=0, indices=0)  Gather(indices=1)
@@ -37,34 +39,34 @@
         #  Unsqueeze        Unsqueeze
         #        \        /
         #         \      /
         #           Concat
         #             |
         #         {present}
         gather = self.model.get_parent(concat_v, 0, output_name_to_node)
-        if gather.op_type != 'Gather':
+        if gather is None or gather.op_type != "Gather":
             logger.debug("match_past_pattern_1: expect Gather for past")
             return None
 
-        if not self.model.find_constant_input(gather, 1) == 1:
+        if self.model.find_constant_input(gather, 1) != 1:
             logger.debug("match_past_pattern_1: expect indices=1 for Gather of past")
             return None
         past = gather.input[0]
 
         parent = self.model.get_parent(concat_k, 0, output_name_to_node)
-        if parent.op_type == 'Gather':
+        if parent and parent.op_type == "Gather":
             gather_past_k = parent
         else:
-            past_k_nodes = self.model.match_parent_path(concat_k, ['Transpose', 'Gather'], [0, 0])
+            past_k_nodes = self.model.match_parent_path(concat_k, ["Transpose", "Gather"], [0, 0])
             if past_k_nodes is None:
                 logger.debug("match_past_pattern_1: failed match Transpose and Gather")
                 return None
             gather_past_k = past_k_nodes[-1]
 
-        if not self.model.find_constant_input(gather_past_k, 0) == 1:
+        if self.model.find_constant_input(gather_past_k, 0) != 1:
             logger.debug("match_past_pattern_1: expect indices=0 for Gather k of past")
             return None
         past_k = gather_past_k.input[0]
         if past != past_k:
             logger.debug("match_past_pattern_1: expect past to be same")
             return None
 
@@ -89,70 +91,68 @@
         #          Unsqueeze    Unsqueeze
         #                \       /
         #                 Concat
         #                   |
         #               {present}
         #
         squeeze = self.model.get_parent(concat_v, 0, output_name_to_node)
-        if squeeze.op_type != 'Squeeze':
+        if squeeze is None or squeeze.op_type != "Squeeze":
             logger.debug("match_past_pattern_2: expect Squeeze as parent of concat_v")
             return None
 
         split = self.model.get_parent(squeeze, 0, output_name_to_node)
-        if split.op_type != "Split":
+        if split is None or split.op_type != "Split":
             logger.debug("match_past_pattern_2: expect Split for past path")
             return None
 
         opset_version = self.model.get_opset_version()
         if opset_version < 13:
-            if not FusionUtils.check_node_attribute(squeeze, 'axes', [0]):
+            if not FusionUtils.check_node_attribute(squeeze, "axes", [0]):
                 logger.debug("match_past_pattern_2: axes != [0] for Squeeze in past path")
                 return None
 
-            if not FusionUtils.check_node_attribute(split, 'split', [1, 1]):
+            if not FusionUtils.check_node_attribute(split, "split", [1, 1]):
                 logger.debug("match_past_pattern_2: split != [1, 1] for Split in past path")
                 return None
         else:
             if not self.utils.check_node_input_value(squeeze, 1, [0]):
                 logger.debug("match_past_pattern_2: axes != [0] for Squeeze in past path")
                 return None
 
             if not self.utils.check_node_input_value(split, 1, [1, 1]):
                 logger.debug("match_past_pattern_2: split != [1, 1] for Split in past path")
                 return None
 
-        if not FusionUtils.check_node_attribute(split, 'axis', 0, default_value=0):
+        if not FusionUtils.check_node_attribute(split, "axis", 0, default_value=0):
             logger.debug("match_past_pattern_2: attribute axis of Split are not expected in past path")
             return None
         past = split.input[0]
 
-        past_k_nodes = self.model.match_parent_path(concat_k, ['Squeeze', 'Split'], [0, 0])
+        past_k_nodes = self.model.match_parent_path(concat_k, ["Squeeze", "Split"], [0, 0])
         if past_k_nodes is None:
             logger.debug("match_past_pattern_2: failed to match past_k_nodes path")
             return None
         past_k = past_k_nodes[-1].input[0]
 
         if past != past_k:
             logger.info("match_past_pattern_2: expect past to be same")
             return None
 
         return past
 
     def match_present(self, concat_v, input_name_to_nodes):
-        unsqueeze_present_v = self.model.find_first_child_by_type(concat_v,
-                                                                  'Unsqueeze',
-                                                                  input_name_to_nodes,
-                                                                  recursive=False)
+        unsqueeze_present_v = self.model.find_first_child_by_type(
+            concat_v, "Unsqueeze", input_name_to_nodes, recursive=False
+        )
         if not unsqueeze_present_v:
             logger.info("expect unsqueeze for present")
             return None
-        concat_present = self.model.find_first_child_by_type(unsqueeze_present_v,
-                                                             'Concat',
-                                                             input_name_to_nodes,
-                                                             recursive=False)
+        concat_present = self.model.find_first_child_by_type(
+            unsqueeze_present_v, "Concat", input_name_to_nodes, recursive=False
+        )
         if not concat_present:
             logger.info("expect concat for present")
             return None
 
         present = concat_present.output[0]
         return present
 
@@ -168,140 +168,280 @@
         return attention_mask_input_name
 
 
 class FusionGptAttention(FusionGptAttentionPastBase):
     """
     Fuse GPT-2 Attention with past state subgraph into one Attention node.
     """
+
     def __init__(self, model: OnnxModel, num_heads: int):
         super().__init__(model, num_heads)
 
-    def create_attention_node(self, fc_weight, fc_bias, gemm_qkv, past, present, input, output, mask,
-                              is_unidirectional):
-        attention_node_name = self.model.create_node_name('GptAttention')
-        attention_node = helper.make_node('Attention',
-                                          inputs=[input, fc_weight, fc_bias, mask, past],
-                                          outputs=[attention_node_name + "_output", present],
-                                          name=attention_node_name)
+    def create_attention_node(
+        self,
+        fc_weight,
+        fc_bias,
+        gemm_qkv,
+        past,
+        present,
+        input,
+        output,
+        mask,
+        is_unidirectional,
+    ):
+        attention_node_name = self.model.create_node_name("GptAttention")
+        attention_node = helper.make_node(
+            "Attention",
+            inputs=[input, fc_weight, fc_bias, mask, past],
+            outputs=[attention_node_name + "_output", present],
+            name=attention_node_name,
+        )
         attention_node.domain = "com.microsoft"
-        attention_node.attribute.extend([
-            helper.make_attribute("num_heads", self.num_heads),
-            helper.make_attribute("unidirectional", 1 if is_unidirectional else 0)
-        ])
-
-        matmul_node = helper.make_node('MatMul',
-                                       inputs=[attention_node_name + "_output", gemm_qkv.input[1]],
-                                       outputs=[attention_node_name + "_matmul_output"],
-                                       name=attention_node_name + "_matmul")
-
-        add_node = helper.make_node('Add',
-                                    inputs=[attention_node_name + "_matmul_output", gemm_qkv.input[2]],
-                                    outputs=[output],
-                                    name=attention_node_name + "_add")
+        attention_node.attribute.extend(
+            [
+                helper.make_attribute("num_heads", self.num_heads),
+                helper.make_attribute("unidirectional", 1 if is_unidirectional else 0),
+            ]
+        )
+
+        if self.mask_filter_value is not None:
+            attention_node.attribute.extend([helper.make_attribute("mask_filter_value", float(self.mask_filter_value))])
+
+        matmul_node = helper.make_node(
+            "MatMul",
+            inputs=[attention_node_name + "_output", gemm_qkv.input[1]],
+            outputs=[attention_node_name + "_matmul_output"],
+            name=attention_node_name + "_matmul",
+        )
+
+        add_node = helper.make_node(
+            "Add",
+            inputs=[attention_node_name + "_matmul_output", gemm_qkv.input[2]],
+            outputs=[output],
+            name=attention_node_name + "_add",
+        )
         self.nodes_to_add.extend([attention_node, matmul_node, add_node])
         self.node_name_to_graph_name[attention_node.name] = self.this_graph_name
         self.node_name_to_graph_name[matmul_node.name] = self.this_graph_name
         self.node_name_to_graph_name[add_node.name] = self.this_graph_name
 
     def fuse(self, normalize_node, input_name_to_nodes, output_name_to_node):
         past = None
         present = None
         return_indice = []
-        qkv_nodes = self.model.match_parent_path(
-            normalize_node,
-            ['Add', 'Reshape', 'Gemm', 'Reshape', 'Reshape', 'Transpose', 'MatMul'],
-            [0,      None,      0,     0,          0,         0,           0],
-            output_name_to_node=output_name_to_node,
-            return_indice=return_indice
-            ) # yapf: disable
+
+        is_normalize_node_skiplayernorm = normalize_node.op_type == "SkipLayerNormalization"
+        qkv_nodes = None
+
+        if not is_normalize_node_skiplayernorm:
+            qkv_nodes = self.model.match_parent_path(
+                normalize_node,
+                ["Add", "Reshape", "Gemm", "Reshape", "Reshape", "Transpose", "MatMul"],
+                [0, None, 0, 0, 0, 0, 0],
+                output_name_to_node=output_name_to_node,
+                return_indice=return_indice,
+            )  # yapf: disable
+        else:
+            qkv_nodes = self.model.match_parent_path(
+                normalize_node,
+                ["Reshape", "Gemm", "Reshape", "Reshape", "Transpose", "MatMul"],
+                [None, 0, 0, 0, 0, 0],
+                output_name_to_node=output_name_to_node,
+                return_indice=return_indice,
+            )  # yapf: disable
+
         if qkv_nodes is None:
             return
-        (add_qkv, reshape_qkv, gemm_qkv, reshape_1, reshape_2, transpose_qkv, matmul_qkv) = qkv_nodes
 
-        another_input = add_qkv.input[1 - return_indice[0]]
+        another_input = None
+        if not is_normalize_node_skiplayernorm:
+            (
+                add_qkv,
+                reshape_qkv,
+                gemm_qkv,
+                reshape_1,
+                reshape_2,
+                transpose_qkv,
+                matmul_qkv,
+            ) = qkv_nodes
 
-        v_nodes = self.model.match_parent_path(matmul_qkv, ['Concat', 'Transpose', 'Reshape', 'Split'], [1, 1, 0, 0])
+            another_input = add_qkv.input[1 - return_indice[0]]
+        else:
+            (
+                reshape_qkv,
+                gemm_qkv,
+                reshape_1,
+                reshape_2,
+                transpose_qkv,
+                matmul_qkv,
+            ) = qkv_nodes
+
+        v_nodes = self.model.match_parent_path(matmul_qkv, ["Concat", "Transpose", "Reshape", "Split"], [1, 1, 0, 0])
         if v_nodes is None:
             logger.debug("fuse_attention: failed to match v path")
             return
         (concat_v, transpose_v, reshape_v, split_fc) = v_nodes
 
-        fc_nodes = self.model.match_parent_path(split_fc, ['Reshape', 'Gemm', 'Reshape', 'LayerNormalization'],
-                                                [0, 0, 0, 0], output_name_to_node)
+        # Try match pattern using Gemm + LayerNormalization
+        fc_nodes = self.model.match_parent_path(
+            split_fc,
+            ["Reshape", "Gemm", "Reshape", "LayerNormalization"],
+            [0, 0, 0, 0],
+            output_name_to_node,
+        )
+
+        # Try match pattern using Gemm + SkipLayerNormalization
         if fc_nodes is None:
-            fc_nodes = self.model.match_parent_path(split_fc, ['Add', 'MatMul', 'LayerNormalization'], [0, None, 0],
-                                                    output_name_to_node)
+            fc_nodes = self.model.match_parent_path(
+                split_fc,
+                ["Reshape", "Gemm", "Reshape", "SkipLayerNormalization"],
+                [0, 0, 0, 0],
+                output_name_to_node,
+            )
+
+        # Try match pattern using MatMul
+        if fc_nodes is None:
+            # LayerNormalization
+            fc_nodes = self.model.match_parent_path(
+                split_fc,
+                ["Add", "MatMul", "LayerNormalization"],
+                [0, None, 0],
+                output_name_to_node,
+            )
+
+            # SkipLayerNormalization
+            if fc_nodes is None:
+                fc_nodes = self.model.match_parent_path(
+                    split_fc,
+                    ["Add", "MatMul", "SkipLayerNormalization"],
+                    [0, None, 0],
+                    output_name_to_node,
+                )
+
             if fc_nodes is None:
                 logger.debug("fuse_attention: failed to match fc path")
                 return
+
             fc_weight = fc_nodes[1].input[1]
             i, _ = self.model.get_constant_input(fc_nodes[0])
             fc_bias = fc_nodes[0].input[i]
         else:
             fc_weight = fc_nodes[1].input[1]
             fc_bias = fc_nodes[1].input[2]
 
         layernorm_before_attention = fc_nodes[-1]
 
-        if not another_input in layernorm_before_attention.input:
-            logger.debug("Add and LayerNormalization shall have one same input")
+        # `another_input` will be non-None only if
+        # (1) SkipLayerNorm fusion wasn't turned ON
+        # (2) SkipLayerNorm fusion was turned ON but upstream layer's LayerNorm + Add was not
+        # fused into a SkipLayerNorm. This can happen if the shapes to the Add node are different.
+        # So, keep the following check if SkipLayerNorm fusion is turned ON or OFF.
+        if another_input is not None and another_input not in layernorm_before_attention.input:
+            logger.debug("Upstream Add and (Skip)LayerNormalization shall have one same input")
             return
 
         is_unidirectional = True
         slice_mask = None
         input_mask_nodes = None
         concat_k_to_match = None
-        qk_nodes = self.model.match_parent_path(matmul_qkv, ['Softmax', 'Sub', 'Mul', 'Div', 'MatMul'], [0, 0, 0, 0, 0])
+        qk_nodes = self.model.match_parent_path(matmul_qkv, ["Softmax", "Sub", "Mul", "Div", "MatMul"], [0, 0, 0, 0, 0])
         if qk_nodes is not None:
             (softmax_qk, sub_qk, mul_qk, div_qk, matmul_qk) = qk_nodes
             mask_nodes = self.model.match_parent_path(
                 sub_qk,
-                ['Mul', 'Sub', 'Slice', 'Slice', 'Unsqueeze', 'Sub', 'Squeeze', 'Slice', 'Shape', 'Div'],
-                [1,      0,     1,       0,       1,           0,     0,         0,       0,       0])  # yapf: disable
+                [
+                    "Mul",
+                    "Sub",
+                    "Slice",
+                    "Slice",
+                    "Unsqueeze",
+                    "Sub",
+                    "Squeeze",
+                    "Slice",
+                    "Shape",
+                    "Div",
+                ],
+                [1, 0, 1, 0, 1, 0, 0, 0, 0, 0],
+            )  # yapf: disable
             if mask_nodes is None:
                 logger.debug("fuse_attention: failed to match unidirectional mask path")
                 return
             div_mask = mask_nodes[-1]
             slice_mask = mask_nodes[3]
 
             if div_qk != div_mask:
                 logger.debug("fuse_attention: skip since div_qk != div_mask")
                 return
+
+            if len(mask_nodes) > 1 and mask_nodes[0].op_type == "Mul":
+                _, mul_val = self.model.get_constant_input(mask_nodes[0])
+                if mul_val != -10000:
+                    self.mask_filter_value = -mul_val
+
         else:
             # New pattern for gpt2 from PyTorch 1.5.0 and Transformers 2.9.0.
             i, qk_nodes, _ = self.model.match_parent_paths(
-                matmul_qkv, [(['Softmax', 'Where', 'Div', 'MatMul'], [0, 0, 1, 0]),
-                             (['Softmax', 'Add', 'Where', 'Div', 'MatMul'], [0, 0, None, 1, 0])], output_name_to_node)
+                matmul_qkv,
+                [
+                    (["Softmax", "Where", "Div", "MatMul"], [0, 0, 1, 0]),
+                    (["Softmax", "Add", "Where", "Div", "MatMul"], [0, 0, None, 1, 0]),
+                ],
+                output_name_to_node,
+            )
             if qk_nodes is None:
                 logger.debug("fuse_attention: failed to match qk nodes")
                 return
 
             where_qk = qk_nodes[-3]
             div_qk = qk_nodes[-2]
             matmul_qk = qk_nodes[-1]
 
             if i == 1:
                 add_qk = qk_nodes[1]
                 _, input_mask_nodes, _ = self.model.match_parent_paths(
                     add_qk,
                     [
-                        (['Mul', 'Sub', 'Cast', 'Unsqueeze', 'Unsqueeze', 'Reshape'], [None, 0, 1, 0, 0, 0]),
-                        (['Mul', 'Sub', 'Unsqueeze', 'Unsqueeze', 'Reshape'], [None, 0, 1, 0, 0]),
-                        (['Mul', 'Sub', 'Unsqueeze', 'Unsqueeze'], [None, 0, 1, 0]),  # useless cast and reshape are removed.
+                        (
+                            ["Mul", "Sub", "Cast", "Unsqueeze", "Unsqueeze", "Reshape"],
+                            [None, 0, 1, 0, 0, 0],
+                        ),
+                        (
+                            ["Mul", "Sub", "Unsqueeze", "Unsqueeze", "Reshape"],
+                            [None, 0, 1, 0, 0],
+                        ),
+                        (
+                            ["Mul", "Sub", "Unsqueeze", "Unsqueeze"],
+                            [None, 0, 1, 0],
+                        ),  # useless cast and reshape are removed.
                     ],
-                    output_name_to_node)  # yapf: disable
+                    output_name_to_node,
+                )  # yapf: disable
                 if input_mask_nodes is None:
                     logger.debug("fuse_attention: failed to match input attention mask path")
                     return
+                if len(input_mask_nodes) > 1 and input_mask_nodes[0].op_type == "Mul":
+                    _, mul_val = self.model.get_constant_input(input_mask_nodes[0])
+                    if mul_val != -10000:
+                        self.mask_filter_value = mul_val
 
             mask_nodes = self.model.match_parent_path(
                 where_qk,
-                ['Cast', 'Slice', 'Slice', 'Unsqueeze', 'Sub', 'Squeeze', 'Slice', 'Shape'],
-                [ 0,     0,       0,       1,           0,     0,         0,       0],
-                output_name_to_node)  # yapf: disable
+                [
+                    "Cast",
+                    "Slice",
+                    "Slice",
+                    "Unsqueeze",
+                    "Sub",
+                    "Squeeze",
+                    "Slice",
+                    "Shape",
+                ],
+                [0, 0, 0, 1, 0, 0, 0, 0],
+                output_name_to_node,
+            )  # yapf: disable
             if mask_nodes is None:
                 # TODO: match mask path for GPT2LMHeadModel_BeamSearchStep.
                 logger.debug("fuse_attention: failed to match mask path")
                 return
 
             slice_mask = mask_nodes[2]
 
@@ -314,38 +454,42 @@
             elif div_or_concat.op_type == "Concat":
                 concat_k_to_match = div_or_concat
             else:
                 logger.debug("fuse_attention: failed to match mask path")
 
         # Validate that the mask data is either lower triangular (unidirectional) or all ones
         mask_data = numpy_helper.to_array(self.model.get_initializer(slice_mask.input[0]))
-        if not (len(mask_data.shape) == 4 and mask_data.shape[:2] == (1, 1)
-                and mask_data.shape[2] == mask_data.shape[3]):
+        if not (
+            len(mask_data.shape) == 4 and mask_data.shape[:2] == (1, 1) and mask_data.shape[2] == mask_data.shape[3]
+        ):
             logger.debug("fuse_attention: skip since mask shape is not 1x1xWxW")
             return
         if np.allclose(mask_data, np.ones_like(mask_data)):
             is_unidirectional = False
         elif not np.allclose(mask_data, np.tril(np.ones_like(mask_data))):
             logger.debug("fuse_attention: skip since mask is neither lower triangular nor ones")
             return
 
-        q_nodes = self.model.match_parent_path(matmul_qk, ['Transpose', 'Reshape', 'Split'], [0, 0, 0])
+        q_nodes = self.model.match_parent_path(matmul_qk, ["Transpose", "Reshape", "Split"], [0, 0, 0])
         if q_nodes is None:
             logger.debug("fuse_attention: failed to match q path")
             return
         (transpose_q, reshape_q, split_q) = q_nodes
         if split_fc != split_q:
             logger.debug("fuse_attention: skip since split_fc != split_q")
             return
 
-        k_nodes = self.model.match_parent_path(matmul_qk, ['Concat', 'Transpose', 'Reshape', 'Split'], [1, 1, 0, 0])
+        k_nodes = self.model.match_parent_path(matmul_qk, ["Concat", "Transpose", "Reshape", "Split"], [1, 1, 0, 0])
         if k_nodes is None:
             # This pattern is from pytorch 1.7.1 and transformers 4.6.1
-            k_nodes = self.model.match_parent_path(matmul_qk, ['Transpose', 'Concat', 'Transpose', 'Reshape', 'Split'],
-                                                   [1, 0, 1, 0, 0])
+            k_nodes = self.model.match_parent_path(
+                matmul_qk,
+                ["Transpose", "Concat", "Transpose", "Reshape", "Split"],
+                [1, 0, 1, 0, 0],
+            )
             if k_nodes is None:
                 logger.debug("fuse_attention: failed to match k path")
                 return
             else:
                 (_, concat_k, transpose_k, reshape_k, split_k) = k_nodes
         else:
             (concat_k, transpose_k, reshape_k, split_k) = k_nodes
@@ -353,22 +497,23 @@
             logger.debug("fuse_attention: skip since split_fc != split_k")
             return
 
         if concat_k_to_match and concat_k != concat_k_to_match:
             logger.debug("fuse_attention: skip since concat_k != concat_k_to_match")
             return
 
-        attention_mask_input_name = ''
+        attention_mask_input_name = ""
         if input_mask_nodes is not None:
             input_name = input_mask_nodes[-1].input[0]
             attention_mask_input_name = self.cast_attention_mask(input_name)
 
         # Match past and present paths
-        past = self.match_past_pattern_1(concat_k, concat_v, output_name_to_node) or \
-               self.match_past_pattern_2(concat_k, concat_v, output_name_to_node)
+        past = self.match_past_pattern_1(concat_k, concat_v, output_name_to_node) or self.match_past_pattern_2(
+            concat_k, concat_v, output_name_to_node
+        )
         if past is None:
             logger.info("fuse_attention: failed to match past path")
             return
         if not self.model.find_graph_input(past):
             logger.debug("past is not graph input.")
             # For GPT2LMHeadModel_BeamSearchStep, there is an extra Gather node to select beam index so it is not graph input.
 
@@ -376,13 +521,22 @@
         if present is None:
             logger.info("fuse_attention: failed to match present path")
             return
         if not self.model.find_graph_output(present):
             logger.info("expect present to be graph output")
             return
 
-        self.create_attention_node(fc_weight, fc_bias, gemm_qkv, past, present, layernorm_before_attention.output[0],
-                                   reshape_qkv.output[0], attention_mask_input_name, is_unidirectional)
+        self.create_attention_node(
+            fc_weight,
+            fc_bias,
+            gemm_qkv,
+            past,
+            present,
+            layernorm_before_attention.output[0],
+            reshape_qkv.output[0],
+            attention_mask_input_name,
+            is_unidirectional,
+        )
 
         # we rely on prune_graph() to clean old subgraph nodes:
         # qk_nodes + q_nodes + k_nodes + v_nodes + mask_nodes + [reshape_qkv, transpose_qkv, matmul_qkv]
         self.prune_graph = True
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_gpt_attention_megatron.py` & `onnxruntime/transformers/fusion_gpt_attention_no_past.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,228 +1,262 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
-import numpy as np
+# --------------------------------------------------------------------------
 from logging import getLogger
-from onnx import helper, numpy_helper, TensorProto
-from onnx_model import OnnxModel
+
+import numpy as np  # noqa: F401
 from fusion_base import Fusion
-from fusion_utils import FusionUtils
-from fusion_gpt_attention import FusionGptAttentionPastBase
+from fusion_utils import FusionUtils  # noqa: F401
+from onnx import TensorProto, helper, numpy_helper  # noqa: F401
+from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
-def is_close(value, expected_value):
-    return abs(value - expected_value) <= 1e-6
-
-
-class FusionGptAttentionMegatron(FusionGptAttentionPastBase):
+class FusionGptAttentionNoPast(Fusion):
     """
-    Fuse GPT-2 Attention with past state subgraph from Megatron into one Attention node.
+    Fuse GPT-2 Attention without past state into one Attention node.
+    This does not support attention_mask graph input right now.
     """
+
     def __init__(self, model: OnnxModel, num_heads: int):
-        super().__init__(model, num_heads)
+        super().__init__(model, "Attention", ["LayerNormalization", "SkipLayerNormalization"], "without past")
+        # TODO: detect num_heads from graph like FusionAttention
+        self.num_heads = num_heads
+        self.mask_filter_value = None
 
-    def fuse_attention_node(self, matmul_before_split, add_before_split, past, present, input, reshape_qkv, mask):
-        attention_node_name = self.model.create_node_name('GptAttention')
-        int32_mask = self.cast_attention_mask(mask)
-        output = reshape_qkv.output[0]
-        i = 1 if (add_before_split.input[0] == matmul_before_split.output[0]) else 0
+    def create_attention_node(self, gemm, gemm_qkv, input, output):
+        attention_node_name = self.model.create_node_name("Attention")
         attention_node = helper.make_node(
-            'Attention',
-            inputs=[input, matmul_before_split.input[1], add_before_split.input[i], int32_mask, past],
-            outputs=[output, present],
-            name=attention_node_name)
+            "Attention",
+            inputs=[input, gemm.input[1], gemm.input[2]],
+            outputs=[attention_node_name + "_output"],
+            name=attention_node_name,
+        )
         attention_node.domain = "com.microsoft"
-        attention_node.attribute.extend([
-            helper.make_attribute("num_heads", self.num_heads),
-            helper.make_attribute("unidirectional", 0)  # unidirectional shall not be ON for 4D attention mask
-        ])
-
-        nodes_to_add = [attention_node]
-        self.nodes_to_add.extend(nodes_to_add)
-
-        for node in nodes_to_add:
-            self.node_name_to_graph_name[node.name] = self.this_graph_name
-
-        self.nodes_to_remove.append(reshape_qkv)
-
-        # we rely on prune_graph() to clean old subgraph nodes
-        self.prune_graph = True
-
-    def match_mask(self, sub_qk, mul_qk, matmul_qk, layernorm_before_attention):
-        mask_nodes = self.model.match_parent_path(
-            sub_qk,
-            ['Mul', 'Sub', 'Slice', 'Slice'],
-            [1,      0,       1,       0])  # yapf: disable
-        if mask_nodes is None:
-            logger.debug("fuse_attention: failed to match unidirectional mask path")
-            return None
-        (mul_mask, sub_mask, last_slice_mask, slice_mask) = mask_nodes
-
-        if mul_qk.input[1] != last_slice_mask.output[0]:
-            logger.debug("fuse_attention failed: mul_qk.input[1] != last_slice_mask.output[0]")
-            return None
-
-        if not self.utils.check_node_input_value(mul_mask, 1, 10000.0):
-            logger.debug("fuse_attention failed: mul_mask input 1 is not constant 10000.0")
-            return None
-
-        if not self.utils.check_node_input_value(sub_mask, 0, 1.0):
-            logger.debug("fuse_attention failed: sub_mask input 0 is not constant 1.0")
-            return None
-
-        if not self.model.find_graph_input(slice_mask.input[0]):
-            logger.info("expect slick_mask input 0 to be graph input")
-            return None
-
-        if not self.utils.check_node_input_value(last_slice_mask, 1, [0]):
-            logger.debug("fuse_attention failed: last_slice_mask input 1 (starts) is not constant [0]")
-            return None
-
-        if not self.utils.check_node_input_value(last_slice_mask, 3, [3]):
-            logger.debug("fuse_attention failed: last_slice_mask input 3 (axes) is not constant [3]")
-            return False
-
-        if not self.utils.check_node_input_value(last_slice_mask, 4, [1]):
-            logger.debug("fuse_attention failed: last_slice_mask input 4 (steps) is not constant [1]")
-            return False
-
-        if not self.utils.check_node_input_value(slice_mask, 3, [2]):
-            logger.debug("fuse_attention failed: slice_mask input 3 (axes) is not constant [2]")
-            return None
-
-        if not self.utils.check_node_input_value(slice_mask, 4, [1]):
-            logger.debug("fuse_attention failed: slice_mask input 4 (steps) is not constant [1]")
-            return None
-
-        last_slice_path = self.model.match_parent_path(last_slice_mask, ['Unsqueeze', 'Gather', 'Shape', 'MatMul'],
-                                                       [2, 0, 0, 0])
-        if last_slice_path is None or last_slice_path[-1] != matmul_qk:
-            logger.debug("fuse_attention: failed to match last slice path")
-            return None
-
-        first_slice_path = self.model.match_parent_path(slice_mask, ['Unsqueeze', 'Gather', 'Shape', 'MatMul'],
-                                                        [2, 0, 0, 0])
-        if first_slice_path is None or first_slice_path[-1] != matmul_qk:
-            logger.debug("fuse_attention: failed to match first slice path")
-            return None
-
-        first_slice_sub = self.model.match_parent_path(slice_mask, ['Unsqueeze', 'Sub', 'Gather', 'Shape', 'MatMul'],
-                                                       [1, 0, 0, 0, 0])
-        if first_slice_sub is None or first_slice_sub[-1] != matmul_qk:
-            logger.debug("fuse_attention: failed to match last slice sub path")
-            return None
-
-        first_slice_sub_1 = self.model.match_parent_path(slice_mask,
-                                                         ['Unsqueeze', 'Sub', 'Gather', 'Shape', 'LayerNormalization'],
-                                                         [1, 0, 1, 0, 0])
-        if first_slice_sub_1 is None or first_slice_sub_1[-1] != layernorm_before_attention:
-            logger.debug("fuse_attention: failed to match last slice sub path 1")
-            return None
-
-        return slice_mask.input[0]
+        attention_node.attribute.extend(
+            [
+                helper.make_attribute("num_heads", self.num_heads),
+                helper.make_attribute("unidirectional", 1),
+            ]
+        )
+        if self.mask_filter_value is not None:
+            attention_node.attribute.extend([helper.make_attribute("mask_filter_value", float(self.mask_filter_value))])
+
+        matmul_node = helper.make_node(
+            "MatMul",
+            inputs=[attention_node_name + "_output", gemm_qkv.input[1]],
+            outputs=[attention_node_name + "_matmul_output"],
+            name=attention_node_name + "_matmul",
+        )
+
+        add_node = helper.make_node(
+            "Add",
+            inputs=[attention_node_name + "_matmul_output", gemm_qkv.input[2]],
+            outputs=[output],
+            name=attention_node_name + "_add",
+        )
+
+        self.nodes_to_add.extend([attention_node, matmul_node, add_node])
+        self.node_name_to_graph_name[attention_node.name] = self.this_graph_name
+        self.node_name_to_graph_name[matmul_node.name] = self.this_graph_name
+        self.node_name_to_graph_name[add_node.name] = self.this_graph_name
 
     def fuse(self, normalize_node, input_name_to_nodes, output_name_to_node):
-        past = None
-        present = None
+        # (TODO) hasesh/tlwu: Investigate what fixes the following logic needs in order
+        # to fuse the Attention sub-graph. With some changes to other fusions, this stopped
+        # working.
+        return_indice = []
+
+        is_normalize_node_skiplayernorm = normalize_node.op_type == "SkipLayerNormalization"
+        qkv_nodes = None
+
+        if not is_normalize_node_skiplayernorm:
+            qkv_nodes = self.model.match_parent_path(
+                normalize_node,
+                ["Add", "Reshape", "Gemm", "Reshape", "Reshape", "Transpose", "MatMul"],
+                [0, None, 0, 0, 0, 0, 0],
+                output_name_to_node=output_name_to_node,
+                return_indice=return_indice,
+            )  # yapf: disable
+        else:
+            qkv_nodes = self.model.match_parent_path(
+                normalize_node,
+                ["Reshape", "Gemm", "Reshape", "Reshape", "Transpose", "MatMul"],
+                [None, 0, 0, 0, 0, 0],
+                output_name_to_node=output_name_to_node,
+                return_indice=return_indice,
+            )  # yapf: disable
 
-        qkv_nodes = self.model.match_parent_path(
-            normalize_node,
-            ['Add', 'Add', 'MatMul', 'Reshape', 'Transpose', 'MatMul'],
-            [  0,      1,     None,          0,         0,           0],
-            output_name_to_node=output_name_to_node,
-            ) # yapf: disable
         if qkv_nodes is None:
             return
-        (add_skip, add_after_attention, matmul_after_attention, reshape_qkv, transpose_qkv, matmul_qkv) = qkv_nodes
 
-        skip_input = add_skip.input[0]
+        another_input = None
+        if not is_normalize_node_skiplayernorm:
+            (
+                add_qkv,
+                reshape_qkv,
+                gemm_qkv,
+                reshape_1,
+                reshape_2,
+                transpose_qkv,
+                matmul_qkv,
+            ) = qkv_nodes
+
+            another_input = add_qkv.input[1 - return_indice[0]]
+        else:
+            (
+                reshape_qkv,
+                gemm_qkv,
+                reshape_1,
+                reshape_2,
+                transpose_qkv,
+                matmul_qkv,
+            ) = qkv_nodes
 
         v_nodes = self.model.match_parent_path(
             matmul_qkv,
-            ['Concat', 'Transpose', 'Reshape', 'Split', 'Add', 'MatMul', 'LayerNormalization'],
-            [1,        1,            0,         0,       0,         None,      0]) # yapf: disable
+            ["Transpose", "Reshape", "Split", "Reshape", "Gemm", "Reshape"],
+            [1, 0, 0, 0, 0, 0],
+        )  # yapf: disable
         if v_nodes is None:
             logger.debug("fuse_attention: failed to match v path")
             return
-        (concat_v, transpose_v, reshape_v, split_v, add_before_split, matmul_before_split,
-         layernorm_before_attention) = v_nodes
-        if skip_input != layernorm_before_attention.input[0]:
-            logger.debug("fuse_attention: skip_input != layernorm_before_attention.input[0]")
-            return
-
-        qk_nodes = self.model.match_parent_path(matmul_qkv, ['Softmax', 'Sub', 'Mul', 'MatMul'], [0, 0, 0, 0])
-        if qk_nodes is None:
-            logger.debug("fuse_attention: failed to match qk path")
-            return None
-        (softmax_qk, sub_qk, mul_qk, matmul_qk) = qk_nodes
-        if self.model.get_node_attribute(softmax_qk, "axis") != 3:
-            logger.debug("fuse_attention failed: softmax_qk axis != 3")
-            return None
+        (
+            transpose_v,
+            reshape_v,
+            split_v,
+            reshape_after_gemm,
+            gemm,
+            reshape_before_gemm,
+        ) = v_nodes
+
+        layernorm_before_attention = self.model.get_parent(reshape_before_gemm, 0, output_name_to_node)
+        if layernorm_before_attention is None or (
+            layernorm_before_attention.op_type != "LayerNormalization"
+            and layernorm_before_attention.op_type != "SkipLayerNormalization"
+        ):
+            if layernorm_before_attention.op_type != "Add":
+                logger.debug(f"failed to get (skip)layernorm before gemm. Got {layernorm_before_attention.op_type}")
+                return
+
+        # `another_input` will be non-None only if
+        # (1) SkipLayerNorm fusion wasn't turned ON
+        # (2) SkipLayerNorm fusion was turned ON but upstream layer's LayerNorm + Add was not
+        # fused into a SkipLayerNorm. This can happen if the shapes to the Add node are different.
+        # So, keep the following check if SkipLayerNorm fusion is turned ON or OFF.
+        if another_input is not None:
+            if another_input not in layernorm_before_attention.input:
+                # match openai-gpt
+                if another_input not in layernorm_before_attention.output:
+                    logger.debug("Add and (Skip)LayerNormalization shall have one same input")
+                    return
+
+        qk_nodes = self.model.match_parent_path(matmul_qkv, ["Softmax", "Sub", "Mul", "Div", "MatMul"], [0, 0, 0, 0, 0])
+        if qk_nodes is not None:
+            (softmax_qk, sub_qk, mul_qk, div_qk, matmul_qk) = qk_nodes
+            mask_nodes = self.model.match_parent_path(
+                sub_qk,
+                [
+                    "Mul",
+                    "Sub",
+                    "Slice",
+                    "Slice",
+                    "Unsqueeze",
+                    "Sub",
+                    "Squeeze",
+                    "Slice",
+                    "Shape",
+                    "Div",
+                ],
+                [1, 0, 1, 0, 1, 0, 0, 0, 0, 0],
+            )  # yapf: disable
+            if mask_nodes is None:
+                logger.debug("fuse_attention: failed to match mask path")
+                return
+            div_mask = mask_nodes[-1]
+
+            if div_qk != div_mask:
+                logger.debug("fuse_attention: skip since div_qk != div_mask")
+                return
+            if len(mask_nodes) > 1 and mask_nodes[0].op_type == "Mul":
+                _, mul_val = self.model.get_constant_input(mask_nodes[0])
+                if mul_val != -10000:
+                    self.mask_filter_value = mul_val
+
+        else:
+            # New pattern for gpt2 from PyTorch 1.5.0 and Transformers 2.9.0.
+            qk_nodes = self.model.match_parent_path(matmul_qkv, ["Softmax", "Where", "Div", "MatMul"], [0, 0, 1, 0])
+            if qk_nodes is not None:
+                (softmax_qk, where_qk, div_qk, matmul_qk) = qk_nodes
+                mask_nodes = self.model.match_parent_path(
+                    where_qk,
+                    [
+                        "Cast",
+                        "Slice",
+                        "Slice",
+                        "Unsqueeze",
+                        "Sub",
+                        "Squeeze",
+                        "Slice",
+                        "Shape",
+                        "Div",
+                    ],
+                    [0, 0, 0, 1, 0, 0, 0, 0, 0],
+                )  # yapf: disable
+                if mask_nodes is None:
+                    logger.debug("fuse_attention: failed to match mask path")
+                    return
+                div_mask = mask_nodes[-1]
+
+                if div_qk != div_mask:
+                    logger.debug("fuse_attention: skip since div_qk != div_mask")
+                    return
+            else:
+                # match openai-gpt
+                qk_nodes = self.model.match_parent_path(
+                    matmul_qkv,
+                    ["Softmax", "Add", "Mul", "Div", "MatMul"],
+                    [0, 0, 0, 0, 0],
+                )
+                if qk_nodes is None:
+                    logger.debug("fuse_attention: failed to match qk path")
+                    return
+                (softmax_qk, add_qk, mul_qk, div_qk, matmul_qk) = qk_nodes
+                mask_nodes = self.model.match_parent_path(
+                    mul_qk,
+                    ["Slice", "Slice", "Unsqueeze", "Squeeze", "Slice", "Shape", "Div"],
+                    [1, 0, 2, 0, 0, 0, 0],
+                )  # yapf: disable
+                if mask_nodes is None:
+                    logger.debug("fuse_attention: failed to match mask path")
+                    return
+                div_mask = mask_nodes[-1]
+
+                if div_qk != div_mask:
+                    logger.debug("fuse_attention: skip since div_qk != div_mask")
+                    return
 
-        attention_mask = self.match_mask(sub_qk, mul_qk, matmul_qk, layernorm_before_attention)
-
-        q_nodes = self.model.match_parent_path(matmul_qk, ['Div', 'Transpose', 'Reshape', 'Split'], [0, 0, 0, 0])
+        q_nodes = self.model.match_parent_path(matmul_qk, ["Transpose", "Reshape", "Split"], [0, 0, 0])
         if q_nodes is None:
             logger.debug("fuse_attention: failed to match q path")
             return
-        (div_q, transpose_q, reshape_q, split_q) = q_nodes
+        (transpose_q, reshape_q, split_q) = q_nodes
         if split_v != split_q:
             logger.debug("fuse_attention: skip since split_v != split_q")
             return
 
-        k_nodes = self.model.match_parent_path(matmul_qk,
-                                               ['Div', 'Transpose', 'Concat', 'Transpose', 'Reshape', 'Split'],
-                                               [1, 0, 0, 1, 0, 0])
+        k_nodes = self.model.match_parent_path(matmul_qk, ["Transpose", "Reshape", "Split"], [1, 0, 0])
         if k_nodes is None:
             logger.debug("fuse_attention: failed to match k path")
             return
-        (div_k, _, concat_k, transpose_k, reshape_k, split_k) = k_nodes
+        (transpose_k, reshape_k, split_k) = k_nodes
         if split_v != split_k:
             logger.debug("fuse_attention: skip since split_v != split_k")
             return
 
-        i, value = self.model.get_constant_input(reshape_k)
-        if not (isinstance(value, np.ndarray) and list(value.shape) == [4] and value[0] == 0 and value[1] == 0
-                and value[2] > 0 and value[3] > 0):
-            logger.debug("fuse_attention: reshape constant input is not [0, 0, N, H]")
-            return
-
-        num_heads = value[2]
-        if num_heads != self.num_heads:
-            logger.info(f"Detected num_heads={num_heads}. Ignore user specified value {self.num_heads}")
-            self.num_heads = num_heads
-
-        hidden_size_per_head = value[3]
-        i, value = self.model.get_constant_input(div_k)
-        expected_value = float(np.sqrt(np.sqrt(hidden_size_per_head)))
-        if not is_close(value, expected_value):
-            logger.debug(f"fuse_attention: div_k value={value} expected={expected_value}")
-            return
-
-        i, value = self.model.get_constant_input(div_q)
-        if not is_close(value, expected_value):
-            logger.debug(f"fuse_attention: div_q value={value} expected={expected_value}")
-            return
+        self.create_attention_node(gemm, gemm_qkv, layernorm_before_attention.output[0], reshape_qkv.output[0])
 
-        # Match past and present paths
-        past = self.match_past_pattern_2(concat_k, concat_v, output_name_to_node)
-        if past is None:
-            logger.debug("fuse_attention: match past failed")
-            return
-        if not self.model.find_graph_input(past):
-            logger.debug("fuse_attention: past is not graph input.")
-            # For GPT2LMHeadModel_BeamSearchStep, there is an extra Gather node to select beam index so it is not graph input.
-
-        present = self.match_present(concat_v, input_name_to_nodes)
-        if present is None:
-            logger.debug("fuse_attention: match present failed")
-            return
-        if not self.model.find_graph_output(present):
-            logger.info("fuse_attention: expect present to be graph output")
-            return
-
-        self.fuse_attention_node(matmul_before_split, add_before_split, past, present,
-                                 layernorm_before_attention.output[0], reshape_qkv, attention_mask)
+        # we rely on prune_graph() to clean old subgraph nodes:
+        # qk_nodes + q_nodes + k_nodes + v_nodes + mask_nodes + [reshape_qkv, transpose_qkv, matmul_qkv]
+        self.prune_graph = True
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_layernorm.py` & `onnxruntime/transformers/fusion_layernorm.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,16 +1,17 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
-from typing import Dict
+# --------------------------------------------------------------------------
 from logging import getLogger
+from typing import Dict
+
+from fusion_base import Fusion
 from onnx import helper
 from onnx_model import OnnxModel
-from fusion_base import Fusion
 
 logger = getLogger(__name__)
 
 
 class FusionLayerNormalization(Fusion):
     def __init__(self, model: OnnxModel):
         super().__init__(model, "LayerNormalization", "ReduceMean")
@@ -39,186 +40,257 @@
         """
         children = self.model.get_children(node, input_name_to_nodes)
         if len(children) == 0 or len(children) > 2:
             return
 
         root_input = node.input[0]
 
-        if children[0].op_type != 'Sub' or children[0].input[0] != root_input:
+        if children[0].op_type != "Sub" or children[0].input[0] != root_input:
             return
 
         if len(children) == 2:
-            if children[1].op_type != 'Sub' or children[1].input[0] != root_input:
+            if children[1].op_type != "Sub" or children[1].input[0] != root_input:
                 return
 
         div_node = None
         for child in children:
-            div_node = self.model.find_first_child_by_type(child, 'Div', input_name_to_nodes, recursive=False)
+            div_node = self.model.find_first_child_by_type(child, "Div", input_name_to_nodes, recursive=False)
             if div_node is not None:
                 break
         if div_node is None:
             return
 
         path_id, parent_nodes, _ = self.model.match_parent_paths(
-            div_node, [(['Sqrt', 'Add', 'ReduceMean', 'Pow', 'Sub'], [1, 0, 0, 0, 0]),
-                       (['Sqrt', 'Add', 'ReduceMean', 'Pow', 'Cast', 'Sub'], [1, 0, 0, 0, 0, 0])], output_name_to_node)
+            div_node,
+            [
+                (["Sqrt", "Add", "ReduceMean", "Pow", "Sub"], [1, 0, 0, 0, 0]),
+                (
+                    ["Sqrt", "Add", "ReduceMean", "Pow", "Cast", "Sub"],
+                    [1, 0, 0, 0, 0, 0],
+                ),
+            ],
+            output_name_to_node,
+        )
         if path_id < 0:
             return
 
         sub_node = parent_nodes[-1]
         if sub_node not in children:
             return
 
         second_add_node = parent_nodes[1]
         i, add_weight = self.model.get_constant_input(second_add_node)
-        if add_weight is None or add_weight <= 0 or add_weight > 1.0E-4:
+        if add_weight is None or add_weight <= 0 or add_weight > 1.0e-4:
             logger.warning(f"epsilon value is not expeced: {add_weight}")
             return
 
         pow_node = parent_nodes[3]
-        if not self.model.find_constant_input(pow_node, 2.0) == 1:
+        if self.model.find_constant_input(pow_node, 2.0) != 1:
             return
 
         mul_node = input_name_to_nodes[div_node.output[0]][0]
-        if mul_node.op_type != 'Mul':
+        if mul_node.op_type != "Mul":
             return
 
         last_add_node = input_name_to_nodes[mul_node.output[0]][0]
-        if last_add_node.op_type != 'Add':
+        if last_add_node.op_type != "Add":
             return
 
         subgraph_nodes = [node]
         subgraph_nodes.extend(children)
         subgraph_nodes.extend(parent_nodes[:-1])
 
         subgraph_nodes.extend([last_add_node, mul_node, div_node])
-        if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, last_add_node.output, input_name_to_nodes,
-                                                output_name_to_node):
-            logger.debug(f"It is not safe to fuse LayerNormalization node. Skip")
+        if not self.model.is_safe_to_fuse_nodes(
+            subgraph_nodes,
+            last_add_node.output,
+            input_name_to_nodes,
+            output_name_to_node,
+        ):
+            logger.debug("It is not safe to fuse LayerNormalization node. Skip")
             return
 
         weight_input = mul_node.input[1 - self.model.input_index(div_node.output[0], mul_node)]
         if not self.model.is_constant_with_specified_dimension(weight_input, 1, "layernorm weight"):
             return
 
         bias_input = last_add_node.input[1 - self.model.input_index(mul_node.output[0], last_add_node)]
         if not self.model.is_constant_with_specified_dimension(bias_input, 1, "layernorm bias"):
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
 
-        normalize_node = helper.make_node('LayerNormalization',
-                                          inputs=[node.input[0], weight_input, bias_input],
-                                          outputs=[last_add_node.output[0]],
-                                          name=self.model.create_node_name("LayerNormalization",
-                                                                           name_prefix="LayerNorm"))
+        normalize_node = helper.make_node(
+            "LayerNormalization",
+            inputs=[node.input[0], weight_input, bias_input],
+            outputs=[last_add_node.output[0]],
+            name=self.model.create_node_name("LayerNormalization", name_prefix="LayerNorm"),
+        )
         normalize_node.attribute.extend([helper.make_attribute("epsilon", float(add_weight))])
         self.nodes_to_add.append(normalize_node)
         self.node_name_to_graph_name[normalize_node.name] = self.this_graph_name
 
 
 class FusionLayerNormalizationTF(Fusion):
     def __init__(self, model: OnnxModel):
         super().__init__(model, "LayerNormalization", "Add", "TF")
 
     def fuse(self, node, input_name_to_nodes: Dict, output_name_to_node: Dict):
         """
-        Layer Norm from Tensorflow model(using keras2onnx or tf2onnx):
-         +------------------------------------+
-         |                                    |
-         |                                    |
-       (Cast_1)                               |
-         |                                    |
-         |                                    v                                           (B)                             (B)             (A)
-        Add --> (Cast_1) --> ReduceMean -->  Sub  --> Mul --> ReduceMean --> (Cast_3) --> Add --> Sqrt --> Reciprocol --> Mul --> Mul --> Sub --> Add
-         |                       |                                                                                         |       ^              ^
-         |                       |                                                                                         |       |              |
-         |                       +--------------------------------------------------(Cast_2)-------------------------------|-------+              |
-         |                                                                                                                 v                      |
-         +---------------------------------------------------------------------------------------------------------------> Mul--------------------+
+         Layer Norm from Tensorflow model(using keras2onnx or tf2onnx):
+          +------------------------------------+
+          |                                    |
+          |                                    |
+        (Cast_1)                               |
+          |                                    |
+          |                                    v                                           (B)                             (B)             (A)
+         Add --> (Cast_1) --> ReduceMean -->  Sub  --> Mul --> ReduceMean --> (Cast_3) --> Add --> Sqrt --> Reciprocol --> Mul --> Mul --> Sub --> Add
+          |                       |                                                                                         |       ^              ^
+          |                       |                                                                                         |       |              |
+          |                       +--------------------------------------------------(Cast_2)-------------------------------|-------+              |
+          |                                                                                                                 v                      |
+          +---------------------------------------------------------------------------------------------------------------> Mul--------------------+
         """
         return_indice = []
         _, parent_nodes, return_indice = self.model.match_parent_paths(
             node,
-            [(['Sub', 'Mul', 'Mul', 'Reciprocal', 'Sqrt', 'Add', 'ReduceMean', 'Mul', 'Sub', 'ReduceMean'],
-            [   1,     1,   None,            0,      0,     0,         None,     0,    0,          None]),
-            (['Sub', 'Mul', 'Mul', 'Reciprocal', 'Sqrt', 'Add', 'Cast', 'ReduceMean', 'Mul', 'Sub', 'ReduceMean'],
-            [   1,     1,   None,            0,      0,     0,     0,      None,        0,    0,          None])],
-            output_name_to_node) # yapf: disable
+            [
+                (
+                    [
+                        "Sub",
+                        "Mul",
+                        "Mul",
+                        "Reciprocal",
+                        "Sqrt",
+                        "Add",
+                        "ReduceMean",
+                        "Mul",
+                        "Sub",
+                        "ReduceMean",
+                    ],
+                    [1, 1, None, 0, 0, 0, None, 0, 0, None],
+                ),
+                (
+                    [
+                        "Sub",
+                        "Mul",
+                        "Mul",
+                        "Reciprocal",
+                        "Sqrt",
+                        "Add",
+                        "Cast",
+                        "ReduceMean",
+                        "Mul",
+                        "Sub",
+                        "ReduceMean",
+                    ],
+                    [1, 1, None, 0, 0, 0, 0, None, 0, 0, None],
+                ),
+            ],
+            output_name_to_node,
+        )  # yapf: disable
 
         if parent_nodes is None:
             return
 
         assert len(return_indice) == 3
         if not (return_indice[0] in [0, 1] and return_indice[1] in [0, 1] and return_indice[2] in [0, 1]):
             logger.debug("return indice is exepected in [0, 1], but got {return_indice}")
             return
 
-        sub_node_0, mul_node_0, mul_node_1, reciprocol_node, sqrt_node, add_node_0 = parent_nodes[:6]
+        (
+            sub_node_0,
+            mul_node_0,
+            mul_node_1,
+            reciprocol_node,
+            sqrt_node,
+            add_node_0,
+        ) = parent_nodes[:6]
         reduce_mean_node_0, mul_node_2, sub_node_1, reduce_mean_node_1 = parent_nodes[-4:]
 
         cast_node_3 = None
         if len(parent_nodes) == 11:
             cast_node_3 = parent_nodes[6]
-            assert (cast_node_3.op_type == 'Cast')
+            assert cast_node_3.op_type == "Cast"
 
-        mul_node_3 = self.model.match_parent(node, 'Mul', 0, output_name_to_node)
+        mul_node_3 = self.model.match_parent(node, "Mul", 0, output_name_to_node)
         if mul_node_3 is None:
             logger.debug("mul_node_3 not found")
             return
 
         node_before_reduce = self.model.get_parent(reduce_mean_node_1, 0, output_name_to_node)
-        root_node = node_before_reduce if cast_node_3 is None else self.model.get_parent(
-            node_before_reduce, 0, output_name_to_node)
+        root_node = (
+            node_before_reduce
+            if cast_node_3 is None
+            else self.model.get_parent(node_before_reduce, 0, output_name_to_node)
+        )
         if root_node is None:
             logger.debug("root node is none")
             return
 
         i, epsilon = self.model.get_constant_input(add_node_0)
-        if epsilon is None or epsilon <= 0 or (epsilon > 1.0E-5 and cast_node_3 is None):
+        if epsilon is None or epsilon <= 0 or (epsilon > 1.0e-5 and cast_node_3 is None):
             logger.debug("epsilon is not matched")
             return
 
-        if cast_node_3 is None and (reduce_mean_node_1.input[0] not in mul_node_3.input
-                                    or reduce_mean_node_1.input[0] not in sub_node_1.input):
+        if cast_node_3 is None and (
+            reduce_mean_node_1.input[0] not in mul_node_3.input or reduce_mean_node_1.input[0] not in sub_node_1.input
+        ):
             logger.debug("reduce_mean_node_1 and mul_node_3 shall link from root node")
             return
 
-        if cast_node_3 is not None and (node_before_reduce.input[0] not in mul_node_3.input
-                                        or reduce_mean_node_1.input[0] not in sub_node_1.input):
+        if cast_node_3 is not None and (
+            node_before_reduce.input[0] not in mul_node_3.input or reduce_mean_node_1.input[0] not in sub_node_1.input
+        ):
             logger.debug("reduce_mean_node_1 and mul_node_3 shall link from root node")
             return
 
         if mul_node_2.input[0] != mul_node_2.input[1]:
             logger.debug("mul_node_2 shall have two same inputs")
             return
 
         subgraph_nodes = [
-            node, sub_node_0, mul_node_0, mul_node_1, reciprocol_node, sqrt_node, add_node_0, reduce_mean_node_0,
-            mul_node_2, sub_node_1, reduce_mean_node_1, mul_node_3
+            node,
+            sub_node_0,
+            mul_node_0,
+            mul_node_1,
+            reciprocol_node,
+            sqrt_node,
+            add_node_0,
+            reduce_mean_node_0,
+            mul_node_2,
+            sub_node_1,
+            reduce_mean_node_1,
+            mul_node_3,
         ]
 
         if cast_node_3 is not None:
-            cast_node_2 = self.model.match_parent(mul_node_0, 'Cast', 0, output_name_to_node)
+            cast_node_2 = self.model.match_parent(mul_node_0, "Cast", 0, output_name_to_node)
             if cast_node_2 is None:
                 logger.debug("cast_node_2 not found")
                 return
             subgraph_nodes.extend([node_before_reduce, cast_node_2, cast_node_3])
 
-        if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, node.output, self.model.input_name_to_nodes(),
-                                                self.model.output_name_to_node()):
+        if not self.model.is_safe_to_fuse_nodes(
+            subgraph_nodes,
+            node.output,
+            self.model.input_name_to_nodes(),
+            self.model.output_name_to_node(),
+        ):
             logger.debug("not safe to fuse layer normalization")
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
 
         weight_input = mul_node_1.input[1]
         bias_input = sub_node_0.input[0]
 
-        #TODO: add epsilon attribute
-        fused_node = helper.make_node('LayerNormalization',
-                                      inputs=[mul_node_3.input[0], weight_input, bias_input],
-                                      outputs=[node.output[0]],
-                                      name=self.model.create_node_name("LayerNormalization", name_prefix="LayerNorm"))
+        # TODO: add epsilon attribute
+        fused_node = helper.make_node(
+            "LayerNormalization",
+            inputs=[mul_node_3.input[0], weight_input, bias_input],
+            outputs=[node.output[0]],
+            name=self.model.create_node_name("LayerNormalization", name_prefix="LayerNorm"),
+        )
         fused_node.attribute.extend([helper.make_attribute("epsilon", float(epsilon))])
         self.nodes_to_add.append(fused_node)
         self.node_name_to_graph_name[fused_node.name] = self.this_graph_name
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_reshape.py` & `onnxruntime/transformers/fusion_reshape.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,59 +1,73 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
-from fusion_base import Fusion
 from logging import getLogger
+
 import numpy as np
-from onnx import helper, numpy_helper, TensorProto
+from fusion_base import Fusion
+from onnx import TensorProto, helper, numpy_helper
 from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class FusionReshape(Fusion):
     def __init__(self, model: OnnxModel):
         super().__init__(model, "Reshape", "Reshape")
+        self.prune_graph: bool = False
 
     def replace_reshape_node(self, shape, reshape_node, concat_node):
         shape_value = np.asarray(shape, dtype=np.int64)
-        constant_shape_name = self.model.create_node_name('Constant', 'constant_shape')
-        new_node = helper.make_node('Constant',
-                                    inputs=[],
-                                    outputs=[constant_shape_name],
-                                    value=helper.make_tensor(name='const_tensor',
-                                                             data_type=TensorProto.INT64,
-                                                             dims=shape_value.shape,
-                                                             vals=bytes(shape_value),
-                                                             raw=True))
+        constant_shape_name = self.model.create_node_name("Constant", "constant_shape")
+        new_node = helper.make_node(
+            "Constant",
+            inputs=[],
+            outputs=[constant_shape_name],
+            value=helper.make_tensor(
+                name="const_tensor",
+                data_type=TensorProto.INT64,
+                dims=shape_value.shape,
+                vals=bytes(shape_value),
+                raw=True,
+            ),
+        )
         reshape_node.input[1] = constant_shape_name
-        reshape_node.name = self.model.create_node_name('Reshape', 'Reshape_Fuse')
+        reshape_node.name = self.model.create_node_name("Reshape", "Reshape_Fuse")
         self.nodes_to_remove.extend([concat_node])
         self.nodes_to_add.append(new_node)
         self.node_name_to_graph_name[new_node.name] = self.this_graph_name
 
     def fuse(self, reshape_node, input_name_to_nodes, output_name_to_node):
         if reshape_node.input[1] not in output_name_to_node:
             return
 
         concat_node = output_name_to_node[reshape_node.input[1]]
-        if concat_node.op_type != 'Concat' or len(concat_node.input) < 3 or len(concat_node.input) > 4:
+        if concat_node.op_type != "Concat" or len(concat_node.input) < 3 or len(concat_node.input) > 4:
             return
 
-        path0 = self.model.match_parent_path(concat_node, ['Unsqueeze', 'Gather', 'Shape'], [0, 0, 0],
-                                             output_name_to_node)
+        path0 = self.model.match_parent_path(
+            concat_node,
+            ["Unsqueeze", "Gather", "Shape"],
+            [0, 0, 0],
+            output_name_to_node,
+        )
         if path0 is None:
             return
 
         (unsqueeze_0, gather_0, shape_0) = path0
 
-        path1 = self.model.match_parent_path(concat_node, ['Unsqueeze', 'Gather', 'Shape'], [1, 0, 0],
-                                             output_name_to_node)
+        path1 = self.model.match_parent_path(
+            concat_node,
+            ["Unsqueeze", "Gather", "Shape"],
+            [1, 0, 0],
+            output_name_to_node,
+        )
         if path1 is None:
             return
         (unsqueeze_1, gather_1, shape_1) = path1
 
         shape = []
         gather_value = self.model.get_constant_value(gather_0.input[1])
         if gather_value == 0:
@@ -66,77 +80,95 @@
         if len(shape) != 2:
             return
 
         path2 = []
         path3 = []
         shape_nodes = [shape_0, shape_1]
         if len(concat_node.input) == 3 and self.model.get_initializer(concat_node.input[2]) is None:
-            path2 = self.model.match_parent_path(concat_node, ['Unsqueeze', 'Mul', 'Gather', 'Shape'], [2, 0, 0, 0],
-                                                 output_name_to_node)
+            path2 = self.model.match_parent_path(
+                concat_node,
+                ["Unsqueeze", "Mul", "Gather", "Shape"],
+                [2, 0, 0, 0],
+                output_name_to_node,
+            )
             if path2 is None:
                 path2 = self.model.match_parent_path(
-                    concat_node, ['Unsqueeze', 'Mul', 'Squeeze', 'Slice', 'Shape'], [2, 0, 0, 0, 0],
-                    output_name_to_node)  # GPT2 exported by PyTorch 1.4 with opset_version=11
+                    concat_node,
+                    ["Unsqueeze", "Mul", "Squeeze", "Slice", "Shape"],
+                    [2, 0, 0, 0, 0],
+                    output_name_to_node,
+                )  # GPT2 exported by PyTorch 1.4 with opset_version=11
                 if path2 is None:
                     return
 
-            path3 = self.model.match_parent_path(concat_node, ['Unsqueeze', 'Mul', 'Gather', 'Shape'], [2, 0, 1, 0],
-                                                 output_name_to_node)
+            path3 = self.model.match_parent_path(
+                concat_node,
+                ["Unsqueeze", "Mul", "Gather", "Shape"],
+                [2, 0, 1, 0],
+                output_name_to_node,
+            )
             if path3 is None:
                 path3 = self.model.match_parent_path(
-                    concat_node, ['Unsqueeze', 'Mul', 'Squeeze', 'Slice', 'Shape'], [2, 0, 1, 0, 0],
-                    output_name_to_node)  # GPT2 exported by PyTorch 1.4 with opset_version=11
+                    concat_node,
+                    ["Unsqueeze", "Mul", "Squeeze", "Slice", "Shape"],
+                    [2, 0, 1, 0, 0],
+                    output_name_to_node,
+                )  # GPT2 exported by PyTorch 1.4 with opset_version=11
                 if path3 is None:
                     return
 
             shape_nodes.extend([path2[-1], path3[-1]])
             shape.append(-1)
-        elif (len(concat_node.input) > 2):
-            concat_2 = self.model.get_initializer(concat_node.input[2])
-            if concat_2 is None:
+        elif len(concat_node.input) > 2:
+            concat_value = self.model.get_constant_value(concat_node.input[2])
+            if concat_value is None:
                 return
-            concat_value = numpy_helper.to_array(concat_2)
-            if isinstance(concat_value, list):
-                shape.extend(concat_value)
+            if isinstance(concat_value, np.ndarray):
+                shape.extend(concat_value.tolist())
             else:
                 shape.append(concat_value)
 
-        if len(concat_node.input) == 4 and self.model.get_initializer(concat_node.input[3]) is None:
+        if len(concat_node.input) == 4 and self.model.get_constant_value(concat_node.input[3]) is None:
             if -1 in shape:
                 return
 
-            path2 = self.model.match_parent_path(concat_node, ['Unsqueeze', 'Div', 'Gather', 'Shape'], [3, 0, 0, 0],
-                                                 output_name_to_node)
+            path2 = self.model.match_parent_path(
+                concat_node,
+                ["Unsqueeze", "Div", "Gather", "Shape"],
+                [3, 0, 0, 0],
+                output_name_to_node,
+            )
             if path2 is None:
                 path2 = self.model.match_parent_path(
-                    concat_node, ['Unsqueeze', 'Div', 'Squeeze', 'Slice', 'Shape'], [3, 0, 0, 0, 0],
-                    output_name_to_node)  # GPT2 exported by PyTorch 1.4 with opset_version=11
+                    concat_node,
+                    ["Unsqueeze", "Div", "Squeeze", "Slice", "Shape"],
+                    [3, 0, 0, 0, 0],
+                    output_name_to_node,
+                )  # GPT2 exported by PyTorch 1.4 with opset_version=11
                 if path2 is None:
                     return
             shape_nodes.extend([path2[-1]])
             shape.append(-1)
-        elif (len(concat_node.input) > 3):
+        elif len(concat_node.input) > 3:
             concat_3 = self.model.get_initializer(concat_node.input[3])
             if concat_3 is None:
                 return
 
             concat_value = numpy_helper.to_array(concat_3)
-            if isinstance(concat_value, list):
-                shape.extend(concat_value)
+            if isinstance(concat_value, np.ndarray):
+                shape.extend(concat_value.tolist())
             else:
                 shape.append(concat_value)
 
         root_input = reshape_node.input[0]
         same_shape_input = True
         for shape_node in shape_nodes:
             if shape_node.input[0] != root_input:
                 same_shape_input = False
 
         if not same_shape_input:
             return
 
         self.replace_reshape_node(shape, reshape_node, concat_node)
 
-        self.nodes_to_remove.extend(path0)
-        self.nodes_to_remove.extend(path1)
-        self.nodes_to_remove.extend(path2)
-        self.nodes_to_remove.extend(path3)
+        # TODO(tlwu): Subgraph blocks pruning un-used nodes. Add code to remove un-used nodes safely.
+        self.prune_graph = True
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_shape.py` & `onnxruntime/transformers/fusion_shape.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,31 +1,33 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
-from fusion_base import Fusion
 from logging import getLogger
-from onnx import TensorProto, NodeProto
-from onnx_model import OnnxModel
+from typing import Dict, List, Union
+
+from fusion_base import Fusion
 from fusion_utils import FusionUtils
-from typing import Union, Dict, List
+from numpy import ndarray
+from onnx import NodeProto, TensorProto
+from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class FusionShape(Fusion):
     def __init__(self, model: OnnxModel):
         super().__init__(model, "Shape", "Concat")
         self.utils = FusionUtils(model)
         self.shape_infer = None
         self.shape_infer_done = False
 
     def get_dimensions_from_tensor_proto(self, tensor_proto: TensorProto) -> Union[int, None]:
-        if tensor_proto.type.tensor_type.HasField('shape'):
+        if tensor_proto.type.tensor_type.HasField("shape"):
             return len(tensor_proto.type.tensor_type.shape.dim)
         else:
             return None
 
     def get_dimensions(self, input_name: str) -> Union[int, None]:
         graph_input = self.model.find_graph_input(input_name)
         if graph_input:
@@ -36,65 +38,73 @@
             self.shape_infer_done = True
 
         if self.shape_infer is not None:
             return self.get_dimensions_from_tensor_proto(self.shape_infer.known_vi_[input_name])
 
         return None
 
-    def fuse(self, concat_node: NodeProto, input_name_to_nodes: Dict[str, List[NodeProto]],
-             output_name_to_node: Dict[str, NodeProto]):
+    def fuse(
+        self,
+        concat_node: NodeProto,
+        input_name_to_nodes: Dict[str, List[NodeProto]],
+        output_name_to_node: Dict[str, NodeProto],
+    ):
         """
         Smplify subgraph like
 
                    (2d_input)
                     /       \
                 Shape       shape
                 /             \
             Gather(indices=0)  Gather(indices=1)
                 |                |
             Unsqueeze(axes=0)   Unsqueeze(axes=0)
-                   \          /
-                      Concat 
+                   \\          /
+                      Concat
                         |
 
         into  (2d_input) --> Shape -->
         """
         opset_version = self.model.get_opset_version()
 
         inputs = len(concat_node.input)
         root = None
         shape_output = None
         for i in range(inputs):
-            path = self.model.match_parent_path(concat_node, ['Unsqueeze', 'Gather', 'Shape'], [i, 0, 0],
-                                                output_name_to_node)
+            path = self.model.match_parent_path(
+                concat_node,
+                ["Unsqueeze", "Gather", "Shape"],
+                [i, 0, 0],
+                output_name_to_node,
+            )
             if path is None:
                 return
 
             unsqueeze, gather, shape = path
             if i == 0:
                 shape_output = shape.output[0]
             if root is None:
                 root = shape.input[0]
                 if self.get_dimensions(root) != inputs:
                     return
             elif shape.input[0] != root:
                 return
 
-            if not FusionUtils.check_node_attribute(unsqueeze, 'axis', 0, default_value=0):
+            if not FusionUtils.check_node_attribute(unsqueeze, "axis", 0, default_value=0):
                 return
 
             if opset_version < 13:
-                if not FusionUtils.check_node_attribute(unsqueeze, 'axes', [0]):
+                if not FusionUtils.check_node_attribute(unsqueeze, "axes", [0]):
                     return
             else:
                 if not self.utils.check_node_input_value(unsqueeze, 1, [0]):
                     return
 
             value = self.model.get_constant_value(gather.input[1])
-            from numpy import ndarray, array_equal
+
             if not (isinstance(value, ndarray) and value.size == 1 and value.item() == i):
                 return
 
         if self.model.find_graph_output(concat_node.output[0]) is None:
             self.model.replace_input_of_all_nodes(concat_node.output[0], shape_output)
-            self.fused_count += 1
+            self.increase_counter("Reshape")
             self.prune_graph = True
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/fusion_skiplayernorm.py` & `onnxruntime/transformers/fusion_skiplayernorm.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,145 +1,199 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 from logging import getLogger
-from onnx import helper
-from onnx_model import OnnxModel
+
 from fusion_base import Fusion
 from fusion_utils import NumpyHelper
+from onnx import helper
+from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class FusionSkipLayerNormalization(Fusion):
     """
     Fuse Add + LayerNormalization into one node: SkipLayerNormalization
     Note: This fusion does not check the input shape of Add and LayerNormalization.
     """
-    def __init__(self, model: OnnxModel):
-        super().__init__(model, "SkipLayerNormalization", "LayerNormalization")
+
+    def __init__(
+        self,
+        model: OnnxModel,
+        fused_op_type: str = "SkipLayerNormalization",
+        search_op_types: str = "LayerNormalization",
+    ):
+        super().__init__(model, fused_op_type, search_op_types)
         # Update shape inference is needed since other fusions might add new edge which does not have shape info yet.
         self.shape_infer_helper = self.model.infer_runtime_shape({"batch_size": 4, "seq_len": 7}, update=True)
 
+        if self.shape_infer_helper is None:
+            # TODO(tianleiwu): support subgraph in shape inference or add broadcasting in SkipLayerNormalization op.
+            logger.warning("symbolic shape inference disabled or failed.")
+
     def fuse(self, node, input_name_to_nodes, output_name_to_node):
         add = self.model.get_parent(node, 0, output_name_to_node)
 
         # In some models there is input_ids->gather->add->LayerNorm and one of input of the
         # add node is initializer with fixed shape which should not be fused into SkipLayerNorm
         if add is None:
             return
 
         for add_input in add.input:
-            if self.model.get_initializer(add_input) != None:
+            if self.model.get_initializer(add_input) is not None:
                 return
 
         # The number of input node of add should be 2
         if len(self.model.get_parents(add)) != 2:
             return
 
+        # Root Mean Square Layer Normalization
+        simplified = node.op_type == "SimplifiedLayerNormalization"
+
         if self.shape_infer_helper is not None:
             if not self.shape_infer_helper.compare_shape(add.input[0], add.input[1]):
                 logger.debug(
-                    f"skip skiplayernorm fusion since shape of inputs ({add.input[0]}, {add.input[1]}) are not same")
+                    "skip SkipLayerNormalization fusion since shape of inputs (%s, %s) are not same",
+                    add.input[0],
+                    add.input[1],
+                )
                 return
         else:
-            # shape_infer_helper can not handle subgraphs. Current work around is to disable skiplayernorm fusion
-            # longterm todo: support subgraph in symbolic_shape_infer or support add broadcasting in skiplayernorm op
-            logger.warning(
-                "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model"
-            )
+            logger.debug("skip SkipLayerNormalization fusion since symbolic shape inference failed")
+            return
 
-        gather_path = self.model.match_parent_path(add, ['Gather'], [None])
+        gather_path = self.model.match_parent_path(add, ["Gather"], [None])
         if gather_path is not None and self.model.find_graph_input(gather_path[0].input[1]) is None:
-            if self.model.match_parent_path(gather_path[0], ['ConstantOfShape'], [1]) is None:
+            if self.model.match_parent_path(gather_path[0], ["ConstantOfShape"], [1]) is None:
                 return
 
-        if add is not None and add.op_type == 'Add' and self.model.is_safe_to_fuse_nodes(
-            [add, node], node.output, input_name_to_nodes, output_name_to_node):
+        residual_add_has_multiple_consumers = False
+        add_children = self.model.get_children(add, input_name_to_nodes)
+
+        # This means that the residual Add before the LayerNormalization produces an output
+        # that is consumed by some other nodes other than the LayerNormalization itself
+        # We can still go ahead with the SkipLayerNormalization fusion but we need to
+        # preserve the output of Add and that needs to be produced by SkipLayerNormalization.
+        if len(add_children) != 1:
+            residual_add_has_multiple_consumers = True
+
+        outputs_to_keep = node.output
+
+        if residual_add_has_multiple_consumers:
+            outputs_to_keep.extend([add.output[0]])
+
+        outputs = [node.output[0]]
+
+        # Skip the other optional outputs of SkipLayerNormalization before adding the Add's output
+        if residual_add_has_multiple_consumers:
+            outputs.extend(["", "", add.output[0]])
+
+        if (
+            add is not None
+            and add.op_type == "Add"
+            and self.model.is_safe_to_fuse_nodes([add, node], outputs_to_keep, input_name_to_nodes, output_name_to_node)
+        ):
             self.nodes_to_remove.extend([add, node])
 
-            inputs = [add.input[0], add.input[1], node.input[1], node.input[2]]
-            normalize_node = helper.make_node("SkipLayerNormalization",
-                                              inputs=inputs,
-                                              outputs=[node.output[0]],
-                                              name=self.model.create_node_name("SkipLayerNormalization",
-                                                                               name_prefix="SkipLayerNorm"))
+            inputs = (
+                [add.input[0], add.input[1], node.input[1], node.input[2]]
+                if not simplified
+                else [add.input[0], add.input[1], node.input[1]]
+            )
+            normalize_node = helper.make_node(
+                self.fused_op_type,
+                inputs=inputs,
+                outputs=outputs,
+                name=self.model.create_node_name(self.fused_op_type, name_prefix="SkipLayerNorm"),
+            )
             normalize_node.domain = "com.microsoft"
 
             # Pass attribute "epsilon" from layernorm node to SkipLayerNormalization
             for att in node.attribute:
-                if att.name == 'epsilon':
+                if att.name == "epsilon":
                     normalize_node.attribute.extend([att])
 
             # Set default epsilon if no epsilon exists from layernorm
             if len(normalize_node.attribute) == 0:
-                normalize_node.attribute.extend([helper.make_attribute("epsilon", 1.0E-12)])
+                normalize_node.attribute.extend([helper.make_attribute("epsilon", 1.0e-12)])
 
             self.nodes_to_add.append(normalize_node)
             self.node_name_to_graph_name[normalize_node.name] = self.this_graph_name
 
 
 class FusionBiasSkipLayerNormalization(Fusion):
     def __init__(self, model: OnnxModel):
         super().__init__(model, "SkipLayerNormalization", "SkipLayerNormalization", "add bias")
 
     def fuse(self, node, input_name_to_nodes, output_name_to_node):
         if len(node.input) != 4:
             return
 
         return_indice = []
-        nodes = self.model.match_parent_path(node, ['Add', 'MatMul'], [None, None], None, return_indice)
+        nodes = self.model.match_parent_path(node, ["Add", "MatMul"], [None, None], None, return_indice)
         if nodes is None:
-            return
-        assert len(return_indice) == 2
+            # In case of fp16, we could have a Cast between the MatMul and the bias Add
+            nodes = self.model.match_parent_path(
+                node, ["Add", "Cast", "MatMul"], [None, None, None], None, return_indice
+            )
+            if nodes is None:
+                return
+
+        assert len(return_indice) == 2 or len(return_indice) == 3
         add_input_index = return_indice[0]
         if add_input_index >= 2:
             return
 
         (add, matmul) = nodes
 
         # bias should be one dimension
         bias_index = -1
+        bias_weight = None
         for i, input in enumerate(add.input):
             initializer = self.model.get_initializer(input)
             if initializer is None:
                 continue
             bias_index = i
             bias_weight = NumpyHelper.to_array(initializer)
             break
         if bias_weight is None:
-            logger.debug(f"Bias weight not found")
+            logger.debug("Bias weight not found")
             return
         if len(bias_weight.shape) != 1:
-            logger.debug(f"Bias weight is not 1D")
+            logger.debug("Bias weight is not 1D")
             return
 
         subgraph_nodes = [node, add]
-        if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, [node.output[0]], input_name_to_nodes,
-                                                output_name_to_node):
-            logger.debug(f"Skip fusing SkipLayerNormalization with Bias since it is not safe")
+        if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, node.output, input_name_to_nodes, output_name_to_node):
+            logger.debug("Skip fusing SkipLayerNormalization with Bias since it is not safe")
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
         inputs = [
-            node.input[1 - add_input_index], matmul.output[0], node.input[2], node.input[3], add.input[bias_index]
+            node.input[1 - add_input_index],
+            matmul.output[0],
+            node.input[2],
+            node.input[3],
+            add.input[bias_index],
         ]
-        new_node = helper.make_node("SkipLayerNormalization",
-                                    inputs=inputs,
-                                    outputs=node.output,
-                                    name=self.model.create_node_name("SkipLayerNormalization",
-                                                                     "SkipLayerNorm_AddBias_"))
+        new_node = helper.make_node(
+            "SkipLayerNormalization",
+            inputs=inputs,
+            outputs=node.output,
+            name=self.model.create_node_name("SkipLayerNormalization", "SkipLayerNorm_AddBias_"),
+        )
         new_node.domain = "com.microsoft"
 
         # Pass attribute "epsilon" from skiplayernorm node to skiplayernorm(add bias)
         for att in node.attribute:
-            if att.name == 'epsilon':
+            if att.name == "epsilon":
                 new_node.attribute.extend([att])
 
         # Set default epsilon if no epsilon exists from skiplayernorm
         if len(new_node.attribute) == 0:
-            new_node.attribute.extend([helper.make_attribute("epsilon", 1.0E-12)])
+            new_node.attribute.extend([helper.make_attribute("epsilon", 1.0e-12)])
 
         self.nodes_to_add.append(new_node)
         self.node_name_to_graph_name[new_node.name] = self.this_graph_name
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_beamsearch_helper.py` & `onnxruntime/transformers/models/gpt2/gpt2_helper.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,677 +1,665 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 # This script helps onnx conversion and validation for GPT2 model with past state.
-import os
 import logging
-import torch
-import onnx
+import os
+import pickle
 import random
-import numpy
+import shutil
+import sys
+import tempfile
 import time
-import re
 from pathlib import Path
-from typing import List, Dict, Tuple, Union
-from transformers import GPT2LMHeadModel, GPT2Config
-from benchmark_helper import Precision
-from gpt2_helper import Gpt2Helper, Gpt2Inputs, GPT2ModelNoPastState, MyGPT2Model, MyGPT2LMHeadModel, MyGPT2LMHeadModel_NoPadding
-from torch_onnx_export_helper import torch_onnx_export
+from typing import Dict, List, Tuple, Union
+
+import numpy
+import onnx
+import torch
+from transformers import GPT2Config, GPT2LMHeadModel, GPT2Model, TFGPT2Model
+
+sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
+
+from benchmark_helper import Precision  # noqa: E402
+from float16 import float_to_float16_max_diff  # noqa: E402
+from io_binding_helper import IOBindingHelper  # noqa: E402
+from onnx_model import OnnxModel  # noqa: E402
+from torch_onnx_export_helper import torch_onnx_export  # noqa: E402
 
 logger = logging.getLogger(__name__)
 
-BIG_NEG = -1e4
+PRETRAINED_GPT2_MODELS = ["distilgpt2", "gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl"]
 
+DEFAULT_TOLERANCE = {
+    Precision.FLOAT32: 0.0005,
+    Precision.FLOAT16: 0.2,
+    Precision.INT8: 3.0,
+}
 
-class Gpt2HelperFactory:
-    @staticmethod
-    def create_helper(helper_type="default"):
-        helpers = {
-            "default": Gpt2Helper,
-            "beam_search_step": Gpt2BeamSearchHelper,
-            "configurable_one_step_search": Gpt2BeamSearchHelper,
-        }
-        w = helpers[helper_type]
-        return w
 
+class GPT2ModelNoPastState(GPT2Model):
+    """Here we wrap a class to disable past state output."""
 
-class GPT2LMHeadModel_BeamSearchStep(GPT2LMHeadModel):
-    """Here we wrap a class for Onnx model conversion for GPT2LMHeadModel with past state and one
-    step beam search."""
-    def __init__(self, config, batch_size, beam_size):
+    def __init__(self, config):
         super().__init__(config)
-        self.config.batch_size = batch_size
-        self.config.beam_size = beam_size
 
-    def forward(
-        self,
-        input_ids,
-        position_ids,
-        attention_mask,
-        beam_select_idx,
-        input_log_probs,
-        input_unfinished_sents,
-        prev_step_results,
-        prev_step_scores,
-        *past,
-    ):
-        input_ids = input_ids.view(self.config.batch_size, -1, input_ids.size(-1))
-        past = [past[i].index_select(1, beam_select_idx[0]) for i in range(len(past))]
-        result = super().forward(
-            input_ids.view(-1, input_ids.size(-1)),
-            position_ids=position_ids,
-            attention_mask=attention_mask,
-            past_key_values=past,
-            return_dict=False,
-        )
-        logits_flat, present_flat = MyGPT2Model.post_process(result, self.config.n_layer)
-        next_token_logits = logits_flat[:, -1].view(self.config.batch_size, -1, logits_flat.size(-1))
-        next_token_log_probs = torch.log_softmax(next_token_logits, dim=-1)
-        next_token_log_probs, next_token_ids = torch.topk(next_token_log_probs,
-                                                          self.config.beam_size,
-                                                          dim=-1,
-                                                          largest=True,
-                                                          sorted=True)
-
-        # finished sentences is always with EOS, and all but the first one has -inf, so that they will be automatically dropped in the round of beam search.
-        finished_sents = ~input_unfinished_sents
-        next_token_log_probs.masked_fill_(finished_sents.unsqueeze(-1), -numpy.inf)
-        next_token_log_probs[..., 0].masked_fill_(finished_sents, 0)
-        next_token_ids.masked_fill_(finished_sents.unsqueeze(-1), self.config.eos_token_id)
-        output_log_probs = input_log_probs.unsqueeze(-1) + next_token_log_probs
-
-        # select N sequences from beams of each input, sorted by sequence probability
-        output_log_probs = output_log_probs.view(self.config.batch_size, -1)  # shape=(batch, beam_size^2)
-        output_log_probs, selected_index_flat = output_log_probs.topk(self.config.beam_size,
-                                                                      dim=-1,
-                                                                      largest=True,
-                                                                      sorted=True)  # output shape=(batch, beam_size)
-
-        # select the correspondent sentences/next tokens
-        selected_input_seq = selected_index_flat // self.config.beam_size
-        next_token_ids = next_token_ids.view(self.config.batch_size, -1).gather(-1, selected_index_flat)
-
-        prev_step_results = prev_step_results.view(self.config.batch_size, -1, prev_step_results.size(-1))
-        prev_step_results = prev_step_results.gather(
-            1,
-            selected_input_seq.unsqueeze(-1).repeat(1, 1, prev_step_results.size(-1)))
-
-        output_unfinished_sents = input_unfinished_sents.gather(1, selected_input_seq)
-        output_unfinished_sents = (output_unfinished_sents & next_token_ids.ne(self.config.eos_token_id))
-
-        # get the next full input_ids
-        current_step_results = torch.cat([prev_step_results, next_token_ids.unsqueeze(-1)], dim=-1).contiguous()
-
-        prev_step_scores = prev_step_scores.view(self.config.batch_size, -1, prev_step_scores.size(-1))
-        prev_step_scores = prev_step_scores.gather(
-            1,
-            selected_input_seq.unsqueeze(-1).repeat(1, 1, prev_step_scores.size(-1)))
-        current_step_scores = torch.cat([prev_step_scores, output_log_probs.unsqueeze(-1)], dim=-1).contiguous()
+    def forward(self, input_ids):
+        return super().forward(input_ids, use_cache=False, return_dict=False)
 
-        return (
-            next_token_ids,
-            present_flat,
-            selected_input_seq,
-            output_log_probs,
-            output_unfinished_sents,
-            current_step_results.view(self.config.batch_size * self.config.beam_size, -1),
-            current_step_scores.view(self.config.batch_size * self.config.beam_size, -1),
-        )
 
+class TFGPT2ModelNoPastState(TFGPT2Model):
+    """Here we wrap a class to disable past state output."""
 
-class GPT2LMHeadModel_ConfigurableOneStepSearch(GPT2LMHeadModel):
-    """Here we wrap a class for Onnx model conversion for GPT2LMHeadModel with past state and one
-    step beam search with configuration support."""
-    def __init__(self,
-                 config,
-                 batch_size,
-                 beam_size,
-                 ignore_eos=False,
-                 temperature=1.0,
-                 repetition_penalty=1.0,
-                 excluded_token_ids=None,
-                 length_penalty=1.0,
-                 do_sample=False,
-                 do_sample_top_p=1,
-                 do_sample_top_k=0):
+    def __init__(self, config):
+        config.use_cache = False
         super().__init__(config)
-        self.config.batch_size = batch_size
-        self.config.beam_size = beam_size
-        self.config.ignore_eos = ignore_eos
-        self.config.temperature = temperature
-        self.config.repetition_penalty = repetition_penalty
-        self.config.excluded_token_ids = excluded_token_ids
-        self.config.length_penalty = length_penalty
-        self.config.do_sample = do_sample
-        self.config.do_sample_top_p = do_sample_top_p
-        self.config.do_sample_top_k = do_sample_top_k
-
-    @staticmethod
-    def collapse_first_two_dims(tensor):
-        return tensor.view(-1, *tensor.size()[2:])
-
-    @staticmethod
-    def top_k_top_p_filtering(log_probs, top_p=1.0, top_k=0):
-        '''Set tail event (out of top_p) to a big negative number'''
-        sorted_log_probs, sorted_indices = torch.sort(log_probs, descending=True)
-        cumulative_probs = torch.cumsum(sorted_log_probs.exp(), dim=-1)
-        sorted_indices_to_remove = cumulative_probs >= top_p
-        sorted_indices_to_remove = torch.cat(
-            [torch.zeros_like(sorted_indices_to_remove[..., :1]), sorted_indices_to_remove[..., :-1]], dim=-1)
-        if top_k > 0:
-            sorted_indices_to_remove = torch.cat(
-                [sorted_indices_to_remove[..., :top_k],
-                 torch.ones_like(sorted_indices_to_remove[..., top_k:])], dim=-1)
-        sorted_log_probs.masked_fill_(sorted_indices_to_remove, BIG_NEG)
-        return log_probs.scatter(-1, sorted_indices, sorted_log_probs)
 
-    def forward(
-        self,
-        input_ids,
-        beam_select_idx,
-        input_log_probs,
-        input_unfinished_sents,
-        prev_step_scores,
-        *past,
-    ):
-        input_ids = input_ids.view(self.config.batch_size, -1, input_ids.size(-1))
-        input_num_seq_per_sample = input_ids.size(1)
+    def forward(self, input_ids):
+        return super().call(input_ids, use_cache=False)
 
-        input_ids_unfinished_flat = self.collapse_first_two_dims(input_ids).index_select(
-            0,
-            input_unfinished_sents.view(-1).nonzero(as_tuple=False).view(-1))
 
-        if self.config.ignore_eos:
-            attention_mask = (input_ids_unfinished_flat != self.config.eos_token_id).float()
-        else:
-            attention_mask = torch.ones(input_ids_unfinished_flat.shape).float().to(input_ids_unfinished_flat.device)
-        position_ids = (attention_mask.cumsum(-1) - 1).clamp(min=0).long()
+class MyGPT2Model(GPT2Model):
+    """Here we wrap a class for Onnx model conversion for GPT2Model with past state."""
 
-        if past:
-            last_seq_len = past[0].size(-2)
-            input_ids_unfinished_flat = input_ids_unfinished_flat[:, last_seq_len:]
-            position_ids = position_ids[:, last_seq_len:]
+    def __init__(self, config):
+        super().__init__(config)
 
-            unfinished_index_relative_to_last_unfinished = beam_select_idx.view(-1)[input_unfinished_sents.view(
-                -1).nonzero(as_tuple=False).view(-1)]
+    @staticmethod
+    def post_process(result, num_layer):
+        if isinstance(result[1][0], (tuple, list)):
+            assert len(result[1]) == num_layer and len(result[1][0]) == 2
+            # assert len(result[1][0][0].shape) == 4 and result[1][0][0].shape == result[1][0][1].shape
+            present = []
+            for i in range(num_layer):
+                # Since transformers v4.*, past key and values are separated outputs.
+                # Here we concate them into one tensor to be compatible with Attention operator.
+                present.append(
+                    torch.cat(
+                        (result[1][i][0].unsqueeze(0), result[1][i][1].unsqueeze(0)),
+                        dim=0,
+                    )
+                )
+            return (result[0], tuple(present))
 
-            past = tuple([p.index_select(1, unfinished_index_relative_to_last_unfinished) for p in past])
+        return result
 
+    def forward(self, input_ids, position_ids, attention_mask, *past):
         result = super().forward(
-            input_ids_unfinished_flat.view(-1, input_ids_unfinished_flat.size(-1)),
+            input_ids,
             position_ids=position_ids,
             attention_mask=attention_mask,
             past_key_values=past,
             return_dict=False,
         )
-        logits_flat, present_flat = MyGPT2Model.post_process(result, self.config.n_layer)
+        return MyGPT2Model.post_process(result, self.config.n_layer)
 
-        # insert finished sequence back to form a square shape of (batch_size, beam_size)
-        next_token_logits = logits_flat.new_zeros(input_ids.size()[:2] + (logits_flat.size(-1), ))
-        next_token_logits.index_fill_(2, torch.LongTensor([self.config.eos_token_id]).to(input_ids.device), -BIG_NEG)
-
-        next_token_logits.masked_scatter_(
-            input_unfinished_sents.unsqueeze(-1).expand_as(next_token_logits), logits_flat[:, -1])
-
-        # repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)
-        if self.config.repetition_penalty != 1.0:
-            _pen = next_token_logits.gather(2, input_ids)
-            _pen = torch.where(_pen > 0, _pen / self.config.repetition_penalty, _pen * self.config.repetition_penalty)
-            next_token_logits.scatter_(2, input_ids, _pen)
-
-        # similar way to encourage short sentence
-        if self.config.length_penalty != 1.0:
-            _pen = next_token_logits[..., self.config.eos_token_id]
-            # if eos > 0, increase it, else, decrease it.
-            _pen = torch.where(_pen > 0, _pen * self.config.length_penalty, _pen / self.config.length_penalty)
-            next_token_logits[..., self.config.eos_token_id] = _pen
-
-        if self.config.temperature != 1.0:
-            next_token_logits = next_token_logits / self.config.temperature
-
-        # exclude excluded_token_ids
-        if self.config.excluded_token_ids is not None:
-            next_token_logits.index_fill_(2, self.config.excluded_token_ids.to(next_token_logits.device),
-                                          BIG_NEG)  # batch x beams/sequences x vocab_size
-
-        next_token_log_probs = torch.log_softmax(next_token_logits, dim=-1)
-
-        if self.config.do_sample:
-            vocab_size = next_token_log_probs.size(-1)
-            _next_token_log_probs = self.top_k_top_p_filtering(next_token_log_probs.view(-1, vocab_size),
-                                                               top_k=self.config.do_sample_top_k,
-                                                               top_p=self.config.do_sample_top_p)
-            next_token_ids = torch.multinomial(_next_token_log_probs.exp(),
-                                               num_samples=self.config.beam_size,
-                                               replacement=False)
-            next_token_ids = next_token_ids.view(self.config.batch_size, input_num_seq_per_sample, -1)
-            next_token_log_probs = next_token_log_probs.gather(-1, next_token_ids)
-        else:
-            next_token_log_probs, next_token_ids = torch.topk(next_token_log_probs,
-                                                              self.config.beam_size,
-                                                              dim=-1,
-                                                              largest=True,
-                                                              sorted=True)
-
-        output_log_probs = input_log_probs.unsqueeze(-1) + next_token_log_probs
-
-        # select N sequences from beams of each input, sorted by sequence probability
-        output_log_probs = output_log_probs.view(self.config.batch_size, -1)  # shape=(batch, beam_size^2)
-        output_log_probs, selected_index_flat = output_log_probs.topk(self.config.beam_size,
-                                                                      dim=-1,
-                                                                      largest=True,
-                                                                      sorted=True)  # output shape=(batch, beam_size)
-
-        # select the correspondent sentences/next tokens
-        selected_input_seq = selected_index_flat // self.config.beam_size
-        next_token_ids = next_token_ids.view(self.config.batch_size, -1).gather(-1, selected_index_flat)
-
-        prev_step_results = input_ids.view(self.config.batch_size, -1, input_ids.size(-1)).contiguous()
-        prev_step_results = prev_step_results.gather(
-            1,
-            selected_input_seq.unsqueeze(-1).expand(selected_input_seq.shape + (prev_step_results.size(-1), )))
-
-        output_unfinished_sents = input_unfinished_sents.gather(1, selected_input_seq)
-        output_unfinished_sents = (output_unfinished_sents & next_token_ids.ne(self.config.eos_token_id))
-
-        current_step_results = torch.cat([prev_step_results, next_token_ids.unsqueeze(-1)], dim=-1).contiguous()
-
-        prev_step_scores = prev_step_scores.view(self.config.batch_size, -1, prev_step_scores.size(-1))
-        prev_step_scores = prev_step_scores.gather(
-            1,
-            selected_input_seq.unsqueeze(-1).expand(selected_input_seq.shape + (prev_step_scores.size(-1), )))
-        current_step_scores = torch.cat([prev_step_scores, output_log_probs.unsqueeze(-1)], dim=-1).contiguous()
-
-        # For next past state
-        index_relative_to_last_unfinished = (input_unfinished_sents.view(-1).float().cumsum(-1) - 1).clamp(
-            min=0).long().reshape_as(input_unfinished_sents).gather(1, selected_input_seq)
 
-        return (
-            current_step_results.view(self.config.batch_size * self.config.beam_size, -1),
-            present_flat,
-            index_relative_to_last_unfinished,
-            output_log_probs,
-            output_unfinished_sents,
-            current_step_scores.view(self.config.batch_size * self.config.beam_size, -1),
+class MyGPT2LMHeadModel(GPT2LMHeadModel):
+    """Here we wrap a class for Onnx model conversion for GPT2LMHeadModel with past state."""
+
+    def __init__(self, config):
+        super().__init__(config)
+
+    def forward(self, input_ids, position_ids, attention_mask, *past):
+        result = super().forward(
+            input_ids,
+            position_ids=position_ids,
+            attention_mask=attention_mask,
+            past_key_values=past,
+            return_dict=False,
         )
 
+        return MyGPT2Model.post_process(result, self.config.n_layer)
+
+
+class MyGPT2LMHeadModel_NoPadding(GPT2LMHeadModel):  # noqa: N801
+    """Here we wrap a class for Onnx model conversion for GPT2LMHeadModel with past state and no padding.
+    When you always use batch_size=1 in inference, there is no padding in inputs. In such case, position_ids
+    and attention_mask need no be in inputs.
+    """
+
+    def __init__(self, config):
+        super().__init__(config)
+
+    def forward(self, input_ids, *past):
+        result = super().forward(input_ids, past_key_values=past, return_dict=False)
+
+        return MyGPT2Model.post_process(result, self.config.n_layer)
+
 
 # Maps model class name to a tuple of model class, name of first output and use padding or not
 MODEL_CLASSES = {
-    'GPT2LMHeadModel': (MyGPT2LMHeadModel, 'logits', True),
-    'GPT2LMHeadModel_NoPadding': (MyGPT2LMHeadModel_NoPadding, 'logits', False),
-    'GPT2Model': (MyGPT2Model, 'last_state', True),
-    "GPT2LMHeadModel_BeamSearchStep":
-    (GPT2LMHeadModel_BeamSearchStep, "last_state", True),  # defined in gpt2_beamsearch_helper.py
-    "GPT2LMHeadModel_ConfigurableOneStepSearch":
-    (GPT2LMHeadModel_ConfigurableOneStepSearch, "last_state", False),  # defined in gpt2_beamsearch_helper.py
+    "GPT2LMHeadModel": (MyGPT2LMHeadModel, "logits", True),
+    "GPT2LMHeadModel_NoPadding": (MyGPT2LMHeadModel_NoPadding, "logits", False),
+    "GPT2Model": (MyGPT2Model, "last_state", True),
 }
 
 
-class Gpt2BeamSearchInputs(Gpt2Inputs):
-    def __init__(
-        self,
-        input_ids,
-        past,
-        position_ids,
-        attention_mask,
-        beam_select_idx=None,
-        input_log_probs=None,
-        input_unfinished_sents=None,
-        prev_step_results=None,
-        prev_step_scores=None,
-    ):
-        super().__init__(input_ids, position_ids, attention_mask, past=past)
-        self.prev_step_results: torch.LongTensor = prev_step_results
-        self.prev_step_scores: Union[torch.FloatTensor, torch.HalfTensor, torch.cuda.FloatTensor] = prev_step_scores
-        if beam_select_idx is None:
-            self.beam_select_idx: torch.LongTensor = torch.zeros([1, len(input_ids)]).long()
-        else:
-            self.beam_select_idx: torch.LongTensor = beam_select_idx
-        self.input_log_probs: Union[torch.FloatTensor, torch.HalfTensor, torch.cuda.FloatTensor] = input_log_probs
-        self.input_unfinished_sents: torch.ByteTensor = input_unfinished_sents
+class Gpt2Inputs:
+    def __init__(self, input_ids, position_ids, attention_mask, past):
+        self.input_ids: torch.LongTensor = input_ids
+        self.position_ids: torch.LongTensor = position_ids
+        self.attention_mask: Union[torch.LongTensor, torch.FloatTensor, torch.HalfTensor] = attention_mask
+        self.past: Union[List[torch.FloatTensor], List[torch.HalfTensor]] = past
 
     def to_list(self) -> List:
-        input_list = [
-            v for v in [
-                self.input_ids, self.position_ids, self.attention_mask, self.beam_select_idx, self.input_log_probs,
-                self.input_unfinished_sents, self.prev_step_results, self.prev_step_scores
-            ] if v is not None
-        ]
+        input_list = [v for v in [self.input_ids, self.position_ids, self.attention_mask] if v is not None]
         if self.past:
             input_list.extend(self.past)
+
         return input_list
 
+    def to_tuple(self) -> Tuple:
+        return tuple(v for v in [self.input_ids, self.position_ids, self.attention_mask, self.past] if v is not None)
+
     def to_fp32(self):
+        # For attention mask, only convert fp16 to fp32, and keep the original type if it is integer.
+        attention_mask = None
+        if self.attention_mask is not None:
+            attention_mask = (
+                self.attention_mask.to(dtype=torch.float32)
+                if (self.attention_mask.dtype == torch.float16)
+                else self.attention_mask
+            )
+
         past = [p.to(dtype=torch.float32) for p in self.past]
-        attention_mask = self.attention_mask.to(
-            dtype=torch.float32) if self.attention_mask is not None else self.attention_mask
-        return Gpt2BeamSearchInputs(
-            self.input_ids,
-            past,
-            self.position_ids,
-            attention_mask,
-            self.beam_select_idx,
-            self.input_log_probs.to(dtype=torch.float32),
-            self.input_unfinished_sents,
-            self.prev_step_results,
-            self.prev_step_scores.to(dtype=torch.float32),
-        )
+        return Gpt2Inputs(self.input_ids, self.position_ids, attention_mask, past)
 
 
-class Gpt2BeamSearchHelper(Gpt2Helper):
+class Gpt2Helper:
     """A helper class for Gpt2 model conversion, inference and verification."""
+
     @staticmethod
-    def get_dummy_inputs(batch_size: int,
-                         past_sequence_length: int,
-                         sequence_length: int,
-                         num_attention_heads: int,
-                         hidden_size: int,
-                         num_layer: int,
-                         vocab_size: int,
-                         device: torch.device,
-                         float16: bool = False,
-                         has_position_ids: bool = True,
-                         has_attention_mask: bool = True) -> Gpt2BeamSearchInputs:
+    def get_dummy_inputs(
+        batch_size: int,
+        past_sequence_length: int,
+        sequence_length: int,
+        num_attention_heads: int,
+        hidden_size: int,
+        num_layer: int,
+        vocab_size: int,
+        device: torch.device,
+        float16: bool = False,
+        has_position_ids: bool = True,
+        has_attention_mask: bool = True,
+        input_ids_dtype: torch.dtype = torch.int32,
+        position_ids_dtype: torch.dtype = torch.int32,
+        attention_mask_dtype: torch.dtype = torch.int32,
+        left_side_padding: bool = True,
+    ) -> Gpt2Inputs:
         """Create random inputs for GPT2 model.
         Returns torch tensors of input_ids, position_ids, attention_mask and a list of past state tensors.
         """
-        gpt2_dummy_inputs = Gpt2Helper.get_dummy_inputs(batch_size, past_sequence_length, sequence_length,
-                                                        num_attention_heads, hidden_size, num_layer, vocab_size, device,
-                                                        float16, has_position_ids, has_attention_mask)
         float_type = torch.float16 if float16 else torch.float32
+        past_shape = [
+            2,
+            batch_size,
+            num_attention_heads,
+            past_sequence_length,
+            int(hidden_size / num_attention_heads),
+        ]
 
-        beam_select_idx = torch.zeros([1, batch_size], device=device).long()
-        input_log_probs = torch.zeros([batch_size, 1], dtype=float_type, device=device)
-        input_unfinished_sents = torch.ones([batch_size, 1], dtype=torch.bool, device=device)
-        if has_position_ids:
-            prev_step_results = torch.randint(
-                low=0,
-                high=vocab_size - 1,
-                size=(batch_size, sequence_length),
-                dtype=torch.int64,
+        past = [(torch.rand(past_shape, dtype=float_type, device=device) * 2.0 - 1.0) for _ in range(num_layer)]
+        input_ids = torch.randint(
+            low=0,
+            high=vocab_size - 1,
+            size=(batch_size, sequence_length),
+            dtype=input_ids_dtype,
+            device=device,
+        )
+
+        attention_mask = None
+        if has_attention_mask:
+            total_sequence_length = past_sequence_length + sequence_length
+            attention_mask = torch.ones(
+                [batch_size, total_sequence_length],
+                dtype=attention_mask_dtype,
                 device=device,
             )
-        else:
-            prev_step_results = None
 
-        prev_step_scores = torch.zeros([batch_size, 1], dtype=float_type, device=device)
+            if total_sequence_length >= 2:
+                for i in range(batch_size):
+                    padding_length = random.randint(0, total_sequence_length - 1)
+                    if left_side_padding:
+                        attention_mask[i, :padding_length] = 0
+                    else:  # right side padding
+                        attention_mask[i, total_sequence_length - padding_length :] = 0
 
-        return Gpt2BeamSearchInputs(
-            gpt2_dummy_inputs.input_ids,
-            gpt2_dummy_inputs.past,
-            gpt2_dummy_inputs.position_ids,
-            gpt2_dummy_inputs.attention_mask,
-            beam_select_idx,
-            input_log_probs,
-            input_unfinished_sents,
-            prev_step_results,
-            prev_step_scores,
-        )
+        # Deduce position_ids from attention mask
+        position_ids = None
+        if has_position_ids:
+            position_ids = attention_mask.long().cumsum(-1) - 1
+            position_ids.masked_fill_(position_ids < 0, 0)
+            position_ids = position_ids[:, past_sequence_length:].to(position_ids_dtype)
+
+        return Gpt2Inputs(input_ids, position_ids, attention_mask, past)
 
     @staticmethod
-    def get_output_shapes(batch_size: int,
-                          context_len: int,
-                          past_sequence_length: int,
-                          sequence_length: int,
-                          beam_size: int,
-                          step: int,
-                          config: GPT2Config,
-                          model_class: str = "GPT2LMHeadModel",
-                          num_seq: int = 0) -> Dict[str, List[int]]:
+    def get_output_shapes(
+        batch_size: int,
+        past_sequence_length: int,
+        sequence_length: int,
+        config: GPT2Config,
+        model_class: str = "GPT2LMHeadModel",
+    ) -> Dict[str, List[int]]:
         """Returns a dictionary with output name as key, and shape as value."""
         num_attention_heads = config.num_attention_heads
         hidden_size = config.hidden_size
         num_layer = config.num_hidden_layers
         vocab_size = config.vocab_size
 
         output_name = MODEL_CLASSES[model_class][1]
 
-        if model_class == "GPT2LMHeadModel_BeamSearchStep":
-            last_state_shape = [batch_size, beam_size]
-        else:
-            last_state_shape = [batch_size * beam_size, past_sequence_length + sequence_length + 1]
-
-        if step == 0:
-            present_state_shape = [
-                2,
-                batch_size,
-                num_attention_heads,
-                past_sequence_length + sequence_length,
-                int(hidden_size / num_attention_heads),
-            ]
-        else:
-            if num_seq == 0:
-                num_seq = beam_size
-
-            present_state_shape = [
-                2,
-                batch_size * num_seq,
-                num_attention_heads,
-                past_sequence_length + sequence_length,
-                int(hidden_size / num_attention_heads),
-            ]
+        last_state_shape = [
+            batch_size,
+            sequence_length,
+            vocab_size if output_name == "logits" else hidden_size,
+        ]
+        present_state_shape = [
+            2,
+            batch_size,
+            num_attention_heads,
+            past_sequence_length + sequence_length,
+            int(hidden_size / num_attention_heads),
+        ]
 
         output_shapes = {output_name: last_state_shape}
         for i in range(num_layer):
             output_shapes["present_" + str(i)] = present_state_shape
 
-        output_shapes["output_selected_indices"] = [1, batch_size * beam_size]
-        output_shapes["output_log_probs"] = [batch_size, beam_size]
-        output_shapes["output_unfinished_sents"] = [batch_size, beam_size]
-        if model_class == "GPT2LMHeadModel_BeamSearchStep":
-            output_shapes["current_step_results"] = [batch_size * beam_size, past_sequence_length + sequence_length + 1]
-        output_shapes["current_step_scores"] = [
-            batch_size * beam_size, past_sequence_length + sequence_length - context_len + 2
-        ]
         return output_shapes
 
     @staticmethod
+    def auto_increase_buffer_size(output_buffers, output_shapes):
+        for key in output_shapes:
+            assert key in output_buffers
+            buffer = output_buffers[key]
+            if numpy.prod(output_shapes[key]) > buffer.nelement():
+                output_buffers[key] = torch.empty(
+                    numpy.prod(output_shapes[key]),
+                    dtype=buffer.dtype,
+                    device=buffer.device,
+                )
+
+    @staticmethod
     def get_output_buffers(output_shapes, device, is_float16=False):
         """Returns a dictionary of output name as key, and 1D tensor as value. The tensor has enough space for given shape."""
         data_type = torch.float16 if is_float16 else torch.float32
 
         output_buffers = {}
         for name, shape in output_shapes.items():
-            if (name == "output_selected_indices" or name == "current_step_results" or name == "last_state"):
-                output_buffers[name] = torch.empty(numpy.prod(shape), dtype=torch.long, device=device)
-            elif name == "output_unfinished_sents":
-                output_buffers[name] = torch.empty(numpy.prod(shape), dtype=torch.bool, device=device)
-            else:
-                output_buffers[name] = torch.empty(numpy.prod(shape), dtype=data_type, device=device)
+            output_buffers[name] = torch.empty(numpy.prod(shape), dtype=data_type, device=device)
         return output_buffers
 
     @staticmethod
-    def compare_outputs(torch_outputs,
-                        ort_outputs,
-                        model_class="GPT2LMHeadModel_BeamSearchStep",
-                        rtol=1e-03,
-                        atol=1e-03):
-        """Returns True if torch and ORT outputs are close for given thresholds, and False otherwise."""
-        if model_class == "GPT2LMHeadModel_BeamSearchStep":
-            results_id = -4
-            num_layers = len(ort_outputs) - 6
+    def diff_outputs(torch_outputs, ort_outputs, relative=False):
+        """Returns the maximum difference between PyTorch and OnnxRuntime outputs."""
+        expected_outputs = torch_outputs[0].cpu().numpy()
+        diff = numpy.abs(expected_outputs - ort_outputs[0])
+        if relative:
+            return numpy.amax(diff / (numpy.abs(expected_outputs) + 1e-6))
         else:
-            results_id = 0
-            num_layers = len(ort_outputs) - 5
+            return numpy.amax(diff)
 
-        is_close = numpy.allclose(ort_outputs[results_id],
-                                  torch_outputs[results_id].cpu().numpy(),
-                                  rtol=rtol,
-                                  atol=atol)
+    @staticmethod
+    def compare_outputs(torch_outputs, ort_outputs, rtol=1e-03, atol=1e-03, **kwargs):
+        """Returns True if torch and ORT outputs are close for given thresholds, and False otherwise.
+        Note: need kwargs since Gpt2BeamSearchHelper.compare_outputs has an extra parameter model_class
+        """
+        is_close = numpy.allclose(ort_outputs[0], torch_outputs[0].cpu().numpy(), rtol=rtol, atol=atol)
         logger.debug(f"PyTorch and OnnxRuntime output 0 (last_state) are close: {is_close}")
 
         is_all_close = is_close
+        num_layers = len(ort_outputs) - 1
+
         for layer in range(num_layers):
             is_close = numpy.allclose(
                 ort_outputs[1 + layer],
                 torch_outputs[1][layer].cpu().numpy(),
                 rtol=rtol,
                 atol=atol,
             )
             logger.debug(f"PyTorch and OnnxRuntime layer {layer} state (present_{layer}) are close:{is_close}")
             is_all_close = is_all_close and is_close
 
         if not is_all_close:
-            max_abs_diff = Gpt2BeamSearchHelper.diff_outputs(torch_outputs, ort_outputs)
+            max_abs_diff = Gpt2Helper.diff_outputs(torch_outputs, ort_outputs)
             logger.info(f"PyTorch and OnnxRuntime results are not all close: max_abs_diff={max_abs_diff:.5f}")
 
         return is_all_close
 
     @staticmethod
-    def export_onnx(model,
-                    device,
-                    onnx_model_path: str,
-                    verbose: bool = False,
-                    use_external_data_format: bool = False,
-                    has_position_ids: bool = True,
-                    has_attention_mask: bool = True):
+    def compare_outputs_v2(torch_outputs, ort_outputs, atol=1e-06):
+        """Compare outputs from PyTorch and OnnxRuntime
+
+        Args:
+            torch_outputs (Tuple[Torch.Tensor]): PyTorch model output
+            ort_outputs (List[numpy.ndarray]): OnnxRuntime output
+            atol (float, optional): Absolute tollerance. Defaults to 1e-06.
+
+        Returns:
+            is_all_close(bool): whether all elements are close.
+            max_abs_diff(float): maximum absolute difference.
+            messages(str): a list of debug message for each output
+        """
+        is_all_close = True
+        is_top1_matched = False
+        max_diffs = []
+        messages = []
+        for i in range(len(ort_outputs)):
+            ort_output = ort_outputs[i]
+            torch_output = (torch_outputs[0] if i == 0 else torch_outputs[1][i - 1]).cpu().numpy()
+            is_close = numpy.allclose(ort_output, torch_output, atol=atol, rtol=0)
+            max_diffs.append(numpy.amax(numpy.abs(torch_output - ort_output)))
+            is_all_close = is_all_close and is_close
+
+            if numpy.isnan(torch_output).any():
+                logger.debug(f"PyTorch output {i} has nan")
+            if numpy.isinf(torch_output).any():
+                logger.debug(f"PyTorch output {i} has inf")
+            if numpy.isnan(ort_output).any():
+                logger.debug(f"ORT output {i} has nan")
+            if numpy.isinf(ort_output).any():
+                logger.debug(f"ORT output {i} has inf")
+
+            diff = numpy.fabs(ort_output - torch_output)
+            idx = numpy.unravel_index(diff.argmax(), diff.shape)
+            messages.append(
+                f"diff={diff[idx]:.9f} index={idx} ort={ort_output[idx]:.9f} torch={float(torch_output[idx]):.9f}"
+            )
+
+            if i == 0:  # logits
+                ort_max_index = numpy.unravel_index(numpy.argmax(ort_output, axis=None), ort_output.shape)
+                torch_max_index = numpy.unravel_index(numpy.argmax(torch_output, axis=None), torch_output.shape)
+                is_top1_matched = numpy.array_equal(ort_max_index, torch_max_index)
+
+        max_diff_output_index = max_diffs.index(max(max_diffs))
+        return (
+            is_all_close,
+            max(max_diffs),
+            max_diff_output_index,
+            messages,
+            is_top1_matched,
+        )
+
+    @staticmethod
+    def export_onnx(
+        model,
+        device,
+        onnx_model_path: str,
+        verbose: bool = False,
+        use_external_data_format: bool = False,
+        has_position_ids: bool = True,
+        has_attention_mask: bool = True,
+        input_ids_dtype: torch.dtype = torch.int32,
+        position_ids_dtype: torch.dtype = torch.int32,
+        attention_mask_dtype: torch.dtype = torch.int32,
+    ):
         """Export GPT-2 model with past state to ONNX model."""
         config: GPT2Config = model.config
         num_layer = config.n_layer
-        dummy_inputs = Gpt2BeamSearchHelper.get_dummy_inputs(batch_size=1,
-                                                             past_sequence_length=1,
-                                                             sequence_length=2,
-                                                             num_attention_heads=config.num_attention_heads,
-                                                             hidden_size=config.hidden_size,
-                                                             num_layer=num_layer,
-                                                             vocab_size=config.vocab_size,
-                                                             device=device,
-                                                             float16=False,
-                                                             has_position_ids=has_position_ids,
-                                                             has_attention_mask=has_attention_mask)
+        dummy_inputs = Gpt2Helper.get_dummy_inputs(
+            batch_size=1,
+            past_sequence_length=1,
+            sequence_length=1,
+            num_attention_heads=config.num_attention_heads,
+            hidden_size=config.hidden_size,
+            num_layer=num_layer,
+            vocab_size=config.vocab_size,
+            device=device,
+            float16=False,
+            has_position_ids=has_position_ids,
+            has_attention_mask=has_attention_mask,
+            input_ids_dtype=input_ids_dtype,
+            position_ids_dtype=position_ids_dtype,
+            attention_mask_dtype=attention_mask_dtype,
+        )
         input_list = dummy_inputs.to_list()
 
         with torch.no_grad():
-            # outputs = model(input_ids, position_id, attention_mask, beam_select_idx, past)
             outputs = model(*input_list)
 
         past_names = [f"past_{i}" for i in range(num_layer)]
         present_names = [f"present_{i}" for i in range(num_layer)]
 
-        output_names = ["last_state"] + present_names
-
-        if has_position_ids:
-            output_names += [
-                "output_selected_indices",
-                "output_log_probs",
-                "output_unfinished_sents",
-                "current_step_results",
-                "current_step_scores",
-            ]
-        else:
-            output_names += [
-                "output_selected_indices",
-                "output_log_probs",
-                "output_unfinished_sents",
-                "current_step_scores",
-            ]
+        # GPT2Model outputs last_state; GPT2LMHeadModel outputs logits (prediction_scores)
+        assert outputs[0].shape[2] == config.vocab_size or outputs[0].shape[2] == config.hidden_size
+        output_names = ["logits" if outputs[0].shape[2] == config.vocab_size else "last_state", *present_names]
 
         # Shape of input tensors:
         #    input_ids: (batch_size, seq_len)
         #    past_{i}:  (2, batch_size, num_heads, past_seq_len, hidden_size/num_heads)
         #    attention_mask: (batch_size, past_seq_len + seq_len)
         # Shape of output tensors:
         #    last_state: (batch_size, seq_len, hidden_size)
         #      or logits: (batch_size, seq_len, vocab_size)
         #    present_{i}:  (2, batch_size, num_heads, past_seq_len + seq_len, hidden_size/num_heads)
         dynamic_axes = {
-            "input_ids": {
-                0: "batch_size",
-                1: "seq_len"
-            },
-            output_names[0]: {
-                0: "batch_size",
-                1: "seq_len"
-            },
+            "input_ids": {0: "batch_size", 1: "seq_len"},
+            output_names[0]: {0: "batch_size", 1: "seq_len"},
         }
         for name in past_names:
             dynamic_axes[name] = {1: "batch_size", 3: "past_seq_len"}
         for name in present_names:
-            dynamic_axes[name] = {1: "batch_size", 3: "cur_seq_len"}
+            dynamic_axes[name] = {1: "batch_size", 3: "total_seq_len"}
 
         input_names = ["input_ids"]
         if has_position_ids:
             dynamic_axes["position_ids"] = {0: "batch_size", 1: "seq_len"}
             input_names.append("position_ids")
         if has_attention_mask:
             dynamic_axes["attention_mask"] = {0: "batch_size", 1: "total_seq_len"}
             input_names.append("attention_mask")
-        dynamic_axes["beam_select_idx"] = {1: "batch_size"}
-        input_names.append("beam_select_idx")
-        dynamic_axes["input_log_probs"] = {0: "batch_size", 1: "beam_size"}
-        input_names.append("input_log_probs")
-        dynamic_axes["input_unfinished_sents"] = {0: "batch_size", 1: "beam_size"}
-        input_names.append("input_unfinished_sents")
-        if has_position_ids:
-            dynamic_axes["prev_step_results"] = {0: "batch_size", 1: "total_seq_len"}
-            input_names.append("prev_step_results")
-        dynamic_axes["prev_step_scores"] = {0: "batch_size", 1: "total_seq_len"}
-        input_names.append("prev_step_scores")
         input_names.extend(past_names)
 
-        # add dynamic output axes
-        present_axes = {1: 'batch_size', 3: 'cur_seq_len'}
-        dynamic_axes["last_state"] = {0: 'batch_size', 1: 'beam_size'}
-        for i in range(num_layer):
-            dynamic_axes["present_" + str(i)] = present_axes
-
-        dynamic_axes["output_selected_indices"] = {0: "batch_size", 1: "'beam_size_or_1'"}
-        dynamic_axes["output_log_probs"] = {0: "batch_size", 1: "'beam_size'"}
-        dynamic_axes["output_unfinished_sents"] = {0: "batch_size", 1: "'beam_size'"}
-        dynamic_axes["current_step_results"] = {0: "beam_size_or_1", 1: "total_seq_len"}
-        dynamic_axes["current_step_scores"] = {0: "beam_size_or_1", 1: "total_seq_len"}
+        assert len(outputs) == 2 and len(outputs[1]) == num_layer
 
         logger.info(
             f"Shapes: input_ids={dummy_inputs.input_ids.shape} past={dummy_inputs.past[0].shape} output={outputs[0].shape} present={outputs[1][0].shape}"
         )
 
         Path(onnx_model_path).parent.mkdir(parents=True, exist_ok=True)
 
-        torch_onnx_export(
-            model,
-            args=tuple(input_list),
-            f=onnx_model_path,
-            input_names=input_names,
-            output_names=output_names,
-            example_outputs=outputs,
-            dynamic_axes=dynamic_axes,
-            opset_version=12,
-            do_constant_folding=True,
-            use_external_data_format=use_external_data_format,
-            verbose=verbose,
+        if use_external_data_format:
+            # We let PyTorch export onnx to a temp directory first, then convert external data to one file.
+            with tempfile.TemporaryDirectory() as tmp_dir_name:
+                temp_onnx_model_path = os.path.join(tmp_dir_name, "gpt2.onnx")
+                Path(temp_onnx_model_path).parent.mkdir(parents=True, exist_ok=True)
+
+                torch_onnx_export(
+                    model,
+                    args=tuple(input_list),
+                    f=temp_onnx_model_path,
+                    export_params=True,
+                    input_names=input_names,
+                    output_names=output_names,
+                    dynamic_axes=dynamic_axes,
+                    opset_version=11,
+                    do_constant_folding=True,
+                    use_external_data_format=True,
+                    verbose=verbose,
+                )
+
+                model = onnx.load_model(temp_onnx_model_path, load_external_data=True)
+                OnnxModel.save(
+                    model,
+                    onnx_model_path,
+                    save_as_external_data=True,
+                    all_tensors_to_one_file=True,
+                )
+        else:
+            torch_onnx_export(
+                model,
+                args=tuple(input_list),
+                f=onnx_model_path,
+                export_params=True,
+                input_names=input_names,
+                output_names=output_names,
+                dynamic_axes=dynamic_axes,
+                opset_version=11,
+                do_constant_folding=True,
+                use_external_data_format=False,
+                verbose=verbose,
+            )
+
+    @staticmethod
+    def optimize_onnx(
+        onnx_model_path,
+        optimized_model_path,
+        is_float16,
+        num_attention_heads,
+        hidden_size,
+        use_external_data_format=False,
+        auto_mixed_precision=False,
+        stage=0,
+        **kwargs,
+    ):
+        """Optimize ONNX model with an option to convert it to use mixed precision."""
+        from fusion_options import FusionOptions
+        from optimizer import optimize_model
+
+        optimization_options = FusionOptions("gpt2")
+
+        m = optimize_model(
+            onnx_model_path,
+            model_type="gpt2",
+            num_heads=num_attention_heads,
+            hidden_size=hidden_size,
+            opt_level=0,
+            optimization_options=optimization_options,
+            use_gpu=False,
         )
 
+        if is_float16:
+            if auto_mixed_precision:
+                Gpt2Helper.auto_mixed_precision(m)
+            else:
+                if "keep_io_types" not in kwargs:
+                    kwargs["keep_io_types"] = False
+                m.convert_float_to_float16(use_symbolic_shape_infer=True, **kwargs)
+
+        m.save_model_to_file(optimized_model_path, use_external_data_format)
+        return m
+
     @staticmethod
-    def onnxruntime_inference(ort_session, inputs: Gpt2BeamSearchInputs, total_runs: int = 0):
+    def auto_mixed_precision(
+        onnx_model: OnnxModel,
+        op_block_list: List[str] = [  # noqa: B006
+            "Add",
+            "LayerNormalization",
+            "SkipLayerNormalization",
+            "FastGelu",
+            "EmbedLayerNormalization",
+        ],
+    ):
+        """Convert GPT-2 model to mixed precision.
+           It detects whether original model has fp16 weights, and set parameters for float16 conversion automatically.
+        Args:
+            onnx_model (OnnxModel): optimized ONNX model
+            op_block_list (List[str], optional): operators to compute in fp32. Defaults to ["Add", "LayerNormalization",
+                                                 "SkipLayerNormalization", "FastGelu", "EmbedLayerNormalization"]
+        Returns:
+            parameters(dict): a dictionary of parameters used in float16 conversion
+        """
+        op_full_set = {node.op_type for node in onnx_model.nodes()}
+        fp32_op_set = set(op_block_list)
+        fp16_op_set = op_full_set.difference(fp32_op_set)
+        logger.info(f"fp32 op: {fp32_op_set} fp16 op: {fp16_op_set}")
+
+        # logits is the first output
+        logits_output_name = onnx_model.graph().output[0].name
+
+        # We use the weight in last MatMul node to detect whether the model is stored with float16 weights from training.
+        is_weight_fp16_precision = False
+        output_name_to_node = onnx_model.output_name_to_node()
+        assert logits_output_name in output_name_to_node
+        node = output_name_to_node[logits_output_name]
+        last_matmul_node = None
+        if node.op_type == "MatMul":
+            last_matmul_node = node
+            logger.info(f"Found last MatMul node for logits: {node.name}")
+            initializer = None
+            for input in node.input:
+                initializer = onnx_model.get_initializer(input)
+                if initializer is not None:
+                    break
+
+            # when the max difference of value after converting float to float16 is lower than a threshold (1e-6),
+            # we can deduce that the weights are stored in float16 precision.
+            max_diff = float_to_float16_max_diff(initializer)
+            logger.debug(f"max diff of converting weights in last MatMul node {node.name}: {max_diff}")
+            is_weight_fp16_precision = max_diff < 1e-6
+        else:
+            logger.warning(f"Failed to find MatMul node for logits. Found {node.op_type} of node {node.name}")
+
+        keep_io_types = []
+        node_block_list = []
+        if (not is_weight_fp16_precision) and (last_matmul_node is not None):
+            # When original weight is float32 precision, keep logits and last MatMul in float32 could get better precision.
+            keep_io_types = [logits_output_name]
+            node_block_list = [last_matmul_node.name]
+
+        parameters = {
+            "keep_io_types": keep_io_types,
+            "op_block_list": op_block_list,
+            "node_block_list": node_block_list,
+            "force_fp16_initializers": is_weight_fp16_precision,
+        }
+
+        logger.info(f"auto_mixed_precision parameters: {parameters}")
+        onnx_model.convert_float_to_float16(use_symbolic_shape_infer=True, **parameters)
+
+        return parameters
+
+    @staticmethod
+    def pytorch_inference(model, inputs: Gpt2Inputs, total_runs: int = 0):
+        """Run inference of PyTorch model, and returns average latency in ms when total_runs > 0 besides outputs."""
+        logger.debug("start pytorch_inference")
+
+        # Convert it to fp32 as the PyTroch model cannot deal with half input.
+        input_list = inputs.to_fp32().to_list()
+
+        with torch.no_grad():
+            outputs = model(*input_list)
+
+        if total_runs == 0:
+            return outputs
+
+        latency = []
+        with torch.no_grad():
+            for _ in range(total_runs):
+                start = time.time()
+                outputs = model(*input_list)
+                latency.append(time.time() - start)
+
+        average_latency = sum(latency) * 1000 / len(latency)
+        logger.debug("PyTorch inference time = {} ms".format(format(average_latency, ".2f")))
+
+        return outputs, average_latency
+
+    @staticmethod
+    def onnxruntime_inference(ort_session, inputs: Gpt2Inputs, total_runs: int = 0):
         """Run inference of ONNX model, and returns average latency in ms when total_runs > 0 besides outputs."""
-        logger.debug(f"start onnxruntime_inference")
+        logger.debug("start onnxruntime_inference")
 
         ort_inputs = {"input_ids": numpy.ascontiguousarray(inputs.input_ids.cpu().numpy())}
 
-        if inputs.position_ids is not None:
-            ort_inputs["position_ids"] = numpy.ascontiguousarray(inputs.position_ids.cpu().numpy())
-        if inputs.attention_mask is not None:
-            ort_inputs["attention_mask"] = numpy.ascontiguousarray(inputs.attention_mask.cpu().numpy())
-        if inputs.beam_select_idx is not None:
-            ort_inputs["beam_select_idx"] = numpy.ascontiguousarray(inputs.beam_select_idx.cpu().numpy())
-        if inputs.input_log_probs is not None:
-            ort_inputs["input_log_probs"] = numpy.ascontiguousarray(inputs.input_log_probs.cpu().numpy())
-        if inputs.input_unfinished_sents is not None:
-            ort_inputs["input_unfinished_sents"] = numpy.ascontiguousarray(inputs.input_unfinished_sents.cpu().numpy())
-        if inputs.prev_step_results is not None:
-            ort_inputs["prev_step_results"] = numpy.ascontiguousarray(inputs.prev_step_results.cpu().numpy())
-        if inputs.prev_step_scores is not None:
-            ort_inputs["prev_step_scores"] = numpy.ascontiguousarray(inputs.prev_step_scores.cpu().numpy())
         if inputs.past is not None:
             for i, past_i in enumerate(inputs.past):
                 ort_inputs[f"past_{i}"] = numpy.ascontiguousarray(past_i.cpu().numpy())
 
+        if inputs.attention_mask is not None:
+            ort_inputs["attention_mask"] = numpy.ascontiguousarray(inputs.attention_mask.cpu().numpy())
+
+        if inputs.position_ids is not None:
+            ort_inputs["position_ids"] = numpy.ascontiguousarray(inputs.position_ids.cpu().numpy())
+
         ort_outputs = ort_session.run(None, ort_inputs)
         if total_runs == 0:
             return ort_outputs
 
         latency = []
         for _ in range(total_runs):
             start = time.time()
@@ -680,293 +668,370 @@
 
         average_latency = sum(latency) * 1000 / len(latency)
         logger.debug("OnnxRuntime Inference time = {} ms".format(format(average_latency, ".2f")))
 
         return ort_outputs, average_latency
 
     @staticmethod
-    def prepare_io_binding(ort_session,
-                           input_ids,
-                           position_ids,
-                           attention_mask,
-                           past,
-                           output_buffers,
-                           output_shapes,
-                           beam_select_idx=None,
-                           input_log_probs=None,
-                           input_unfinished_sents=None,
-                           prev_step_results=None,
-                           prev_step_scores=None):
+    def prepare_io_binding(
+        ort_session,
+        input_ids,
+        position_ids,
+        attention_mask,
+        past,
+        output_buffers,
+        output_shapes,
+    ):
         """Returnas IO binding object for a session."""
+        return IOBindingHelper.prepare_io_binding(
+            ort_session,
+            input_ids,
+            position_ids,
+            attention_mask,
+            past,
+            output_buffers,
+            output_shapes,
+        )
 
-        # Bind inputs and outputs to onnxruntime session
-        io_binding = Gpt2Helper.prepare_io_binding(ort_session,
-                                                   input_ids,
-                                                   position_ids,
-                                                   attention_mask,
-                                                   past=past,
-                                                   output_buffers=output_buffers,
-                                                   output_shapes=output_shapes)
-
-        # Bind inputs
-        data_type = output_buffers[ort_session.get_outputs()[1].name].dtype
-        float_type = numpy.float16 if data_type == torch.float16 else numpy.float32
-
-        if past is not None:
-            for i, past_i in enumerate(past):
-                assert past_i.is_contiguous()
-
-                data_ptr = past_i.data_ptr()
-                if data_ptr == 0:
-                    # When past_sequence_length is 0, its data_ptr will be zero. IO Binding asserts that data_ptr shall not be zero.
-                    # Here we workaround and pass data pointer of input_ids. Actual data is not used for past so it does not matter.
-                    data_ptr = input_ids.data_ptr()
-
-                io_binding.bind_input(f'past_{i}', past_i.device.type, 0, float_type, list(past_i.size()), data_ptr)
-
-        if attention_mask is not None:
-            assert attention_mask.is_contiguous()
-            io_binding.bind_input('attention_mask', attention_mask.device.type, 0, float_type,
-                                  list(attention_mask.size()), attention_mask.data_ptr())
-
-        if beam_select_idx is not None:
-            assert beam_select_idx.is_contiguous()
-            io_binding.bind_input(
-                "beam_select_idx",
-                beam_select_idx.device.type,
-                0,
-                numpy.longlong,
-                list(beam_select_idx.size()),
-                beam_select_idx.data_ptr(),
-            )
-
-        if input_log_probs is not None:
-            assert input_log_probs.is_contiguous()
-            io_binding.bind_input(
-                "input_log_probs",
-                input_log_probs.device.type,
-                0,
-                float_type,
-                list(input_log_probs.size()),
-                input_log_probs.data_ptr(),
-            )
-
-        if input_unfinished_sents is not None:
-            assert input_unfinished_sents.is_contiguous()
-            io_binding.bind_input(
-                "input_unfinished_sents",
-                input_unfinished_sents.device.type,
-                0,
-                numpy.bool,
-                list(input_unfinished_sents.size()),
-                input_unfinished_sents.data_ptr(),
-            )
-
-        if prev_step_results is not None:
-            assert prev_step_results.is_contiguous()
-            io_binding.bind_input(
-                "prev_step_results",
-                prev_step_results.device.type,
-                0,
-                numpy.longlong,
-                list(prev_step_results.size()),
-                prev_step_results.data_ptr(),
-            )
-
-        if prev_step_scores is not None:
-            assert prev_step_scores.is_contiguous()
-            io_binding.bind_input(
-                "prev_step_scores",
-                prev_step_scores.device.type,
-                0,
-                float_type,
-                list(prev_step_scores.size()),
-                prev_step_scores.data_ptr(),
-            )
-
-        # Bind outputs
-        for output in ort_session.get_outputs():
-            output_name = output.name
-            output_buffer = output_buffers[output_name]
-            logger.debug(f"{output_name} device type={output_buffer.device.type} shape={list(output_buffer.size())}")
-            if (output_name == "output_selected_indices" or output_name == "last_state"
-                    or output_name == "current_step_results"):
-                io_binding.bind_output(
-                    output_name,
-                    output_buffer.device.type,
-                    0,
-                    numpy.longlong,
-                    output_shapes[output_name],
-                    output_buffer.data_ptr(),
-                )
-            elif output_name == "output_unfinished_sents":
-                io_binding.bind_output(
-                    output_name,
-                    output_buffer.device.type,
-                    0,
-                    numpy.bool,
-                    output_shapes[output_name],
-                    output_buffer.data_ptr(),
-                )
-            else:
-                io_binding.bind_output(
-                    output_name,
-                    output_buffer.device.type,
-                    0,
-                    float_type,
-                    output_shapes[output_name],
-                    output_buffer.data_ptr(),
-                )
-
-        return io_binding
+    @staticmethod
+    def get_outputs_from_io_binding_buffer(ort_session, output_buffers, output_shapes, return_numpy=True):
+        """Copy results to cpu. Returns a list of numpy array."""
+        return IOBindingHelper.get_outputs_from_io_binding_buffer(
+            ort_session, output_buffers, output_shapes, return_numpy
+        )
 
     @staticmethod
-    def onnxruntime_inference_with_binded_io(ort_session,
-                                             inputs: Gpt2BeamSearchInputs,
-                                             output_buffers: Dict[str, torch.Tensor],
-                                             output_shapes: Dict[str, List[int]],
-                                             total_runs: int = 0,
-                                             return_numpy: bool = True,
-                                             include_copy_output_latency: bool = False):
-        """Inference with IO binding. Returns outputs, and optional latency when total_runs > 0.
-        """
-        logger.debug(f"start onnxruntime_inference_with_binded_io")
+    def onnxruntime_inference_with_binded_io(
+        ort_session,
+        inputs: Gpt2Inputs,
+        output_buffers: Dict[str, torch.Tensor],
+        output_shapes: Dict[str, List[int]],
+        total_runs: int = 0,
+        return_numpy: bool = True,
+        include_copy_output_latency: bool = False,
+    ):
+        """Inference with IO binding. Returns outputs, and optional latency when total_runs > 0."""
+        logger.debug("start onnxruntime_inference_with_binded_io")
 
         # Bind inputs and outputs to onnxruntime session
-        io_binding = Gpt2BeamSearchHelper.prepare_io_binding(
+        io_binding = Gpt2Helper.prepare_io_binding(
             ort_session,
             inputs.input_ids,
             inputs.position_ids,
             inputs.attention_mask,
             inputs.past,
             output_buffers,
             output_shapes,
-            inputs.beam_select_idx,
-            inputs.input_log_probs,
-            inputs.input_unfinished_sents,
-            inputs.prev_step_results,
-            inputs.prev_step_scores,
         )
 
         # Run onnxruntime with io binding
         ort_session.run_with_iobinding(io_binding)
 
         # Copy results to cpu for verification
-        ort_outputs = Gpt2BeamSearchHelper.get_outputs_from_io_binding_buffer(ort_session, output_buffers,
-                                                                              output_shapes, return_numpy)
+        ort_outputs = Gpt2Helper.get_outputs_from_io_binding_buffer(
+            ort_session, output_buffers, output_shapes, return_numpy
+        )
 
         if total_runs == 0:
             return ort_outputs
 
         latency = []
         for _ in range(total_runs):
             start = time.time()
             # Run onnxruntime with io binding
             ort_session.run_with_iobinding(io_binding)
             if include_copy_output_latency:
-                _ = Gpt2BeamSearchHelper.get_outputs_from_io_binding_buffer(ort_session, output_buffers, output_shapes,
-                                                                            return_numpy)
+                _ = Gpt2Helper.get_outputs_from_io_binding_buffer(
+                    ort_session, output_buffers, output_shapes, return_numpy
+                )
             latency.append(time.time() - start)
 
         average_latency = sum(latency) * 1000 / len(latency)
         logger.debug("OnnxRuntime with IO binding inference time = {} ms".format(format(average_latency, ".2f")))
 
         return ort_outputs, average_latency
 
     @staticmethod
-    def test_parity(ort_session,
-                    model,
-                    device,
-                    is_float16=False,
-                    rtol=5e-4,
-                    atol=5e-4,
-                    total_test_cases=100,
-                    use_io_binding=True,
-                    model_class="GPT2LMHeadModel_BeamSearchStep",
-                    has_position_ids=True,
-                    has_attention_mask=True):
+    def save_outputs(i, ort_outputs, torch_outputs):
+        with open(f"ort_outputs_{i}.pickle", "wb") as f:
+            pickle.dump(ort_outputs, f)
+        logger.info(f"ORT output are saved to ort_outputs_{i}.pickle")
+
+        with open(f"torch_outputs_{i}.pickle", "wb") as f:
+            pickle.dump(torch_outputs, f)
+        logger.info(f"Torch output are saved to torch_outputs_{i}.pickle")
+
+    @staticmethod
+    def save_inputs(i, dummy_inputs, ort_outputs, torch_outputs):
+        with open(f"dummy_inputs_{i}.pickle", "wb") as f:
+            pickle.dump(dummy_inputs, f)
+        logger.info(f"inputs are saved to dummy_inputs_{i}.pickle")
+
+    @staticmethod
+    def test_parity(
+        ort_session,
+        model,
+        device,
+        is_float16=False,
+        rtol=5e-4,
+        atol=5e-4,
+        test_cases_per_run=10000,
+        total_runs=1,
+        use_io_binding=True,
+        model_class="GPT2LMHeadModel",
+        has_position_ids=True,
+        has_attention_mask=True,
+        input_ids_dtype=torch.int32,
+        position_ids_dtype=torch.int32,
+        attention_mask_dtype=torch.int32,
+        stage=0,
+        verbose=False,
+        enable_pickle_output=False,
+    ):
         """Generate random inputs and compare the results of PyTorch and Onnx Runtime."""
 
         config: GPT2Config = model.config
 
         logger.info(
-            f"Running parity test (rtol={rtol}, atol={atol}, test_cases={total_test_cases}, use_io_binding={use_io_binding} model_class={model_class} is_float16={is_float16}) ..."
+            f"Running parity test (atol={atol}, test_cases={test_cases_per_run}, runs={total_runs}, use_io_binding={use_io_binding}, model_class={model_class}, is_float16={is_float16}) ..."
         )
 
-        max_batch_size = 1
+        max_batch_size = 8
         max_past_seq_len = 4  # Do not use large number here for higher chance of hitting empty past (past_seq_len=0)
         max_seq_len = 2
-        beam_size = 4
 
         output_buffers = None
         if use_io_binding:
-            max_output_shapes = Gpt2BeamSearchHelper.get_output_shapes(
-                max_batch_size,
-                max_past_seq_len,
-                max_past_seq_len,
-                max_seq_len,
-                beam_size,
-                0,
-                config,
-                model_class,
+            max_output_shapes = Gpt2Helper.get_output_shapes(
+                max_batch_size, max_past_seq_len, max_seq_len, config, model_class
             )
-            output_buffers = Gpt2BeamSearchHelper.get_output_buffers(max_output_shapes, device, is_float16)
+            output_buffers = Gpt2Helper.get_output_buffers(max_output_shapes, device, is_float16)
 
         passed_test_cases = 0
-        for _ in range(total_test_cases):
-            past_sequence_length = random.randint(0, max_past_seq_len)
-            sequence_length = random.randint(1 + past_sequence_length, max_seq_len + past_sequence_length)
+        top1_matched_cases = 0
+
+        max_abs_diff_list = []
+        top1_matched_cases_per_run = [0] * total_runs
+        total_test_cases = test_cases_per_run * total_runs
+        for i in range(total_test_cases):
+            run_id = int(i / test_cases_per_run)
+            sequence_length = random.randint(1, max_seq_len)
+            past_sequence_length = 0 if (stage == 1) else random.randint(0, max_past_seq_len)
             batch_size = random.randint(1, max_batch_size)
 
             logger.debug(
-                f"Running parity test for batch_size={batch_size} past_sequence_length={past_sequence_length}...")
-            dummy_inputs = Gpt2BeamSearchHelper.get_dummy_inputs(batch_size, past_sequence_length, sequence_length,
-                                                                 config.num_attention_heads, config.hidden_size,
-                                                                 config.n_layer, config.vocab_size, device, is_float16,
-                                                                 has_position_ids, has_attention_mask)
-
-            outputs = Gpt2BeamSearchHelper.pytorch_inference(model, dummy_inputs)
+                f"Running parity test for batch_size={batch_size} past_sequence_length={past_sequence_length}..."
+            )
+            dummy_inputs = Gpt2Helper.get_dummy_inputs(
+                batch_size,
+                past_sequence_length,
+                sequence_length,
+                config.num_attention_heads,
+                config.hidden_size,
+                config.n_layer,
+                config.vocab_size,
+                device,
+                is_float16,
+                has_position_ids,
+                has_attention_mask,
+                input_ids_dtype=input_ids_dtype,
+                position_ids_dtype=position_ids_dtype,
+                attention_mask_dtype=attention_mask_dtype,
+                left_side_padding=True,
+            )
+            outputs = Gpt2Helper.pytorch_inference(model, dummy_inputs)
             if use_io_binding:
-                ort_outputs = Gpt2BeamSearchHelper.onnxruntime_inference(ort_session, dummy_inputs)
+                ort_outputs = Gpt2Helper.onnxruntime_inference(ort_session, dummy_inputs)
             else:
-                output_shapes = Gpt2BeamSearchHelper.get_output_shapes(
+                output_shapes = Gpt2Helper.get_output_shapes(
                     batch_size,
                     past_sequence_length,
-                    past_sequence_length,
                     sequence_length,
-                    beam_size,
-                    0,
                     config,
                     model_class,
                 )
-                ort_outputs = Gpt2BeamSearchHelper.onnxruntime_inference_with_binded_io(
-                    ort_session, dummy_inputs, output_buffers, output_shapes)
+                ort_outputs = Gpt2Helper.onnxruntime_inference_with_binded_io(
+                    ort_session, dummy_inputs, output_buffers, output_shapes
+                )
 
-            is_all_close = Gpt2BeamSearchHelper.compare_outputs(outputs,
-                                                                ort_outputs,
-                                                                model_class=model_class,
-                                                                rtol=rtol,
-                                                                atol=atol)
+            (
+                is_all_close,
+                max_abs_diff,
+                max_diff_output_index,
+                messages,
+                is_top1_matched,
+            ) = Gpt2Helper.compare_outputs_v2(outputs, ort_outputs, atol=atol)
+            if not numpy.isnan(max_abs_diff):
+                max_abs_diff_list.append(max_abs_diff)
             if is_all_close:
                 passed_test_cases += 1
-        logger.info(f"Parity Test Cases={total_test_cases}; Passed={passed_test_cases}")
+
+            if is_top1_matched:
+                top1_matched_cases += 1
+                top1_matched_cases_per_run[run_id] += 1
+
+            if verbose and not is_all_close:
+                logger.info(
+                    f"test_case={i} batch_size={batch_size} past_sequence_length={past_sequence_length} sequence_length={sequence_length} MaxDiff={max_abs_diff}"
+                )
+                for i, message in enumerate(messages):  # noqa: PLW2901
+                    logger.info(f"\t{i}: Name={ort_session.get_outputs()[i].name}, {message}")
+
+            # Collect data for debugging
+            if enable_pickle_output and (numpy.isnan(max_abs_diff) or max_abs_diff > 100 * atol):
+                Gpt2Helper.save_inputs(i, dummy_inputs)
+                Gpt2Helper.save_outputs(i, ort_outputs, outputs)
+
+        if max_abs_diff_list:
+            result = {
+                f"max_diff_percentile_{p}": f"{numpy.percentile(max_abs_diff_list, p):.5f}" for p in [50, 90, 95, 99]
+            }
+        else:
+            result = {f"max_diff_percentile_{p}": "nan" for p in [50, 90, 95, 99]}
+
+        result["top1_match_rate"] = top1_matched_cases * 1.0 / total_test_cases
+        result["top1_match_rate_per_run"] = [x * 1.0 / test_cases_per_run for x in top1_matched_cases_per_run]
+        result["diff_pass_rate"] = passed_test_cases * 1.0 / total_test_cases
+        result["nan_rate"] = (total_test_cases - len(max_abs_diff_list)) * 1.0 / total_test_cases
+
+        logger.info(
+            f"Parity Test Cases={total_test_cases}; Passed={passed_test_cases}; Nan={total_test_cases-len(max_abs_diff_list)}; Top1_Matched={top1_matched_cases}"
+        )
+
         if passed_test_cases > 0.95 * total_test_cases:
             logger.info(f"Parity is good: passed rate={int(passed_test_cases*100/total_test_cases):.0f}%")
-        return passed_test_cases == total_test_cases
+
+        return result
+
+    @staticmethod
+    def test_performance(
+        ort_session,
+        model,
+        device,
+        is_float16=False,
+        total_runs=100,
+        use_io_binding=True,
+        model_class="GPT2LMHeadModel",
+        has_position_ids=True,
+        has_attention_mask=True,
+        input_ids_dtype=torch.int32,
+        position_ids_dtype=torch.int32,
+        attention_mask_dtype=torch.int32,
+        batch_size=8,
+        sequence_length=1,
+        past_sequence_length=32,
+    ):
+        """Generate random inputs and measure average latency of Onnx Runtime."""
+
+        config: GPT2Config = model.config
+
+        output_buffers = None
+        if use_io_binding:
+            output_shapes = Gpt2Helper.get_output_shapes(
+                batch_size, past_sequence_length, sequence_length, config, model_class
+            )
+            output_buffers = Gpt2Helper.get_output_buffers(output_shapes, device, is_float16)
+
+        dummy_inputs = Gpt2Helper.get_dummy_inputs(
+            batch_size,
+            past_sequence_length,
+            sequence_length,
+            config.num_attention_heads,
+            config.hidden_size,
+            config.n_layer,
+            config.vocab_size,
+            device,
+            is_float16,
+            has_position_ids,
+            has_attention_mask,
+            input_ids_dtype=input_ids_dtype,
+            position_ids_dtype=position_ids_dtype,
+            attention_mask_dtype=attention_mask_dtype,
+        )
+
+        if use_io_binding:
+            _, latency = Gpt2Helper.onnxruntime_inference(ort_session, dummy_inputs, total_runs)
+        else:
+            _, latency = Gpt2Helper.onnxruntime_inference_with_binded_io(
+                ort_session, dummy_inputs, output_buffers, output_shapes, total_runs
+            )
+
+        return latency
 
     @staticmethod
     def torchscript(model, config, device, has_position_ids=True, has_attention_mask=True):
         """JIT trace for TorchScript."""
-        input_list = Gpt2BeamSearchHelper.get_dummy_inputs(
+        input_list = Gpt2Helper.get_dummy_inputs(
             batch_size=1,
             past_sequence_length=1,
             sequence_length=1,
             num_attention_heads=config.num_attention_heads,
             hidden_size=config.hidden_size,
             num_layer=config.n_layer,
             vocab_size=config.vocab_size,
             device=device,
             float16=False,
             has_position_ids=has_position_ids,
             has_attention_mask=has_attention_mask,
         ).to_list()
         return torch.jit.trace(model, input_list)
+
+    @staticmethod
+    def get_onnx_paths(
+        output_dir,
+        model_name_or_path,
+        model_class: str = "GPT2LMHeadModel",
+        has_past=True,
+        new_folder=False,
+        remove_existing=["raw", "fp32", "fp16", "int8"],  # noqa: B006
+    ):
+        """Build a  path name for given model based on given attributes."""
+        model_name = model_name_or_path
+        if os.path.isdir(model_name_or_path):
+            model_name = Path(model_name_or_path).parts[-1]
+        else:
+            model_name.split("/")[-1]
+
+        if model_class != "GPT2LMHeadModel":
+            model_name += "_" + model_class
+
+        if has_past:
+            model_name += "_past"
+
+        if new_folder:
+            suffix = {"raw": "", "fp32": "_fp32", "fp16": "_fp16", "int8": "_int8"}
+            # Remove the directories if existed.
+            for model_type in ["raw", "fp32", "fp16", "int8"]:
+                new_dir = os.path.join(output_dir, model_name + suffix[model_type])
+                if os.path.exists(new_dir):
+                    if model_type in remove_existing:
+                        try:
+                            shutil.rmtree(new_dir)
+                            logger.info(f"Removed the existed directory: {new_dir}")
+                        except OSError as e:
+                            logger.info(f"Failed to remove the directory {new_dir}: {e.strerror}")
+                    else:
+                        logger.info(f"Directory for {model_type} existed: {new_dir}")
+
+            # store each model to its own directory (for external data format).
+            return {
+                "raw": os.path.join(os.path.join(output_dir, model_name), model_name + ".onnx"),
+                "fp32": os.path.join(
+                    os.path.join(output_dir, model_name + "_fp32"),
+                    model_name + "_fp32.onnx",
+                ),
+                "fp16": os.path.join(
+                    os.path.join(output_dir, model_name + "_fp16"),
+                    model_name + "_fp16.onnx",
+                ),
+                "int8": os.path.join(
+                    os.path.join(output_dir, model_name + "_int8"),
+                    model_name + "_int8.onnx",
+                ),
+            }
+
+        return {
+            "raw": os.path.join(output_dir, model_name + ".onnx"),
+            "fp32": os.path.join(output_dir, model_name + "_fp32.onnx"),
+            "fp16": os.path.join(output_dir, model_name + "_fp16.onnx"),
+            "int8": os.path.join(output_dir, model_name + "_int8.onnx"),
+        }
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_beamsearch_tester.py` & `onnxruntime/transformers/models/gpt2/gpt2_tester.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,418 +1,505 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 # This script helps evaluation of GPT-2 model.
-import os
 import logging
-import torch
-import random
-import numpy
-import time
-import timeit
 import math
+import os
 import statistics
-from pathlib import Path
-from gpt2_tester import Gpt2Tester, Gpt2Metric
-from gpt2_beamsearch_helper import Gpt2BeamSearchHelper, Gpt2BeamSearchInputs
-from benchmark_helper import Precision
+import sys
+import timeit
+
+import numpy
+import torch
+from gpt2_helper import Gpt2Helper, Gpt2Inputs
+
+sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
+
+from benchmark_helper import Precision  # noqa: E402
 
 logger = logging.getLogger(__name__)
 
 
-class Gpt2TesterFactory:
-    @staticmethod
-    def create_tester(tester_type="default"):
-        testers = {
-            "default": Gpt2Tester,
-            "beam_search_step": Gpt2BeamSearchTester,
-            "configurable_one_step_search": Gpt2BeamSearchTester,
-        }
-        w = testers[tester_type]
-        return w
+class Gpt2Metric:
+    def __init__(self, treatment_name, baseline_name="Torch", top_k=20):
+        assert top_k > 1 and top_k <= 100
+        self.baseline = baseline_name
+        self.treatment = treatment_name
+        self.name: str = f"{treatment_name} vs {baseline_name}"
+        self.top_k = top_k
+        self.top_1_error: int = 0
+        self.top_k_error: int = 0
+        self.total_samples: int = 0
+        self.max_logits_diff: float = 0  # for non-empty past state
+        self.max_logits_diff_no_past: float = 0  # for empty past state
+        self.batch_top1_error: torch.FloatTensor = None  # top 1 error for current batch
+        self.batch_topk_error: torch.FloatTensor = None  # top k error for current batch
+        self.seq_len_latency = {}
+
+    def print(self):
+        if self.baseline != self.treatment:
+            print("---")
+            print(f"Metrics for {self.treatment} (baseline={self.baseline}):")
+            if self.total_samples > 0:
+                top_1_error_rate = 100.0 * self.top_1_error / self.total_samples
+                top_k_error_rate = 100.0 * self.top_k_error / self.total_samples
+                print(
+                    f"Total={self.total_samples} Top1Error={self.top_1_error} ({top_1_error_rate:.2f}%) Top{self.top_k}Error={self.top_k_error} ({top_k_error_rate:.2f}%)"
+                )
+            print("Max logits diffs:")
+            print(f"\twith past  = {self.max_logits_diff:.6f}")
+            print(f"\tempty past = {self.max_logits_diff_no_past:.6f}")
+        else:
+            print(f"Metrics for {self.treatment} (baseline):")
+
+        if self.seq_len_latency:
+            print("Past sequence length range and average latency:")
+            total = 0
+            count = 0
+            for key in sorted(self.seq_len_latency.keys()):
+                average = statistics.mean(self.seq_len_latency[key]) * 1000.0
+                if key == 0:
+                    print(f"\t{key}:         \t{average:.2f} ms")
+                else:
+                    print(f"\t[{2**key}, {2 ** (key + 1) - 1}]:\t{average:.2f} ms")
+                total += average * len(self.seq_len_latency[key])
+                count += len(self.seq_len_latency[key])
+            print(f"Average Latency: {total / count:.2f} ms")
+
+    def diff_logits(self, baseline_logits, treatment_logits, is_empty_past: bool):
+        diff = (baseline_logits - treatment_logits).abs().max()
+        if is_empty_past:
+            self.max_logits_diff_no_past = max(self.max_logits_diff_no_past, diff)
+        else:
+            self.max_logits_diff = max(self.max_logits_diff, diff)
 
+        return diff
 
-class Gpt2BeamSearchTester(Gpt2Tester):
+    def start_batch(self, batch_size: int):
+        self.total_samples += batch_size
+        self.batch_top1_error = torch.zeros((batch_size, 1), dtype=torch.bool)
+        self.batch_topk_error = torch.zeros((batch_size, 1), dtype=torch.bool)
+
+    def eval_batch(self, baseline, treatment, past_seq_len, verbose=True):
+        self._eval_topk(baseline.top_1_tokens, treatment.top_1_tokens, 1, verbose)
+        self._eval_topk(baseline.top_k_tokens, treatment.top_k_tokens, self.top_k, verbose)
+
+        max_diff = self.diff_logits(baseline.logits, treatment.logits, past_seq_len == 0)
+        if verbose:
+            print(f"Max logits diffs of {self.name}: {max_diff}")
+
+    def _eval_topk(self, baseline_topk, treatment_topk, top_k, verbose=True):
+        if not torch.all(torch.eq(baseline_topk, treatment_topk)):
+            if top_k == 1:
+                if verbose:
+                    print(f"Generated tokens not matched for {self.name}")
+                self.batch_top1_error |= torch.eq(baseline_topk, treatment_topk).logical_not()
+            else:
+                if verbose:
+                    print(
+                        f"Top {top_k} tokens not matched for {self.name}. This will lead to wrong beam search results"
+                    )
+                self.batch_topk_error |= (
+                    torch.eq(baseline_topk, treatment_topk).logical_not().sum(1).unsqueeze(dim=1) > 0
+                )
+
+    def end_batch(self):
+        self.top_1_error += self.batch_top1_error.sum()
+        self.top_k_error += self.batch_topk_error.sum()
+
+    def add_latency(self, past_seq_len, latency):
+        key = int(math.log2(past_seq_len)) + 1 if past_seq_len > 0 else 0
+        if key not in self.seq_len_latency:
+            self.seq_len_latency[key] = []
+        self.seq_len_latency[key].append(latency)
+
+
+class Gpt2Tester:
     def __init__(
         self,
         input_ids,
         position_ids,
         attention_mask,
-        beam_select_idx,
-        input_log_probs,
-        input_unfinished_sents,
-        prev_step_results,
-        prev_step_scores,
         num_attention_heads,
         hidden_size,
         num_layer,
-        beam_size,
         device,
         is_fp16=False,
         top_k=20,
         top_k_required_order=False,
     ):
-        super().__init__(input_ids,
-                         position_ids,
-                         attention_mask,
-                         num_attention_heads=num_attention_heads,
-                         hidden_size=hidden_size,
-                         num_layer=num_layer,
-                         device=device,
-                         is_fp16=is_fp16,
-                         top_k=top_k,
-                         top_k_required_order=top_k_required_order)
-        self.input_length = input_ids.shape[-1]
+        self.batch_size = input_ids.shape[0]
+        self.input_length = input_ids.shape[1]
         self.n_layer = num_layer
-        self.beam_size = beam_size
 
-        self.beam_select_idx = beam_select_idx.to(device)
+        self.input_ids = input_ids
+        self.position_ids = position_ids
+        self.attention_mask = attention_mask
 
-        float_type = torch.float16 if is_fp16 else torch.float32
-        self.input_log_probs = input_log_probs.type(float_type).to(device)
-        self.input_unfinished_sents = input_unfinished_sents.to(device)
-
-        self.prev_step_results = prev_step_results.to(device) if prev_step_results is not None else None
-        self.prev_step_scores = prev_step_scores.type(float_type).to(device)
-
-        self.last_state = None
-
-    def get_inputs(self) -> Gpt2BeamSearchInputs:
-        return Gpt2BeamSearchInputs(
-            self.input_ids,
-            self.past,
-            self.position_ids,
-            self.attention_mask,
-            self.beam_select_idx,
-            self.input_log_probs,
-            self.input_unfinished_sents,
-            self.prev_step_results,
-            self.prev_step_scores,
-        )
+        self.has_position_ids = position_ids is not None
+        self.has_attention_mask = attention_mask is not None
+
+        # Emtpy past state for first inference
+        self.past = []
+        past_shape = [
+            2,
+            self.batch_size,
+            num_attention_heads,
+            0,
+            hidden_size // num_attention_heads,
+        ]
+        for _i in range(num_layer):
+            empty_past = torch.empty(past_shape).type(torch.float16 if is_fp16 else torch.float32)
+            self.past.append(empty_past.to(device))
+
+        self.logits = None
+        self.top_1_tokens = None
+        self.top_k_tokens = None
+        self.top_k = top_k
+        self.top_k_required_order = top_k_required_order
+
+    def get_inputs(self) -> Gpt2Inputs:
+        return Gpt2Inputs(self.input_ids, self.position_ids, self.attention_mask, self.past)
+
+    def save_test_data(self, session, output, save_test_data_dir, test_case_id):
+        from onnx import numpy_helper
+
+        path = os.path.join(save_test_data_dir, "test_data_set_" + str(test_case_id))
+        if os.path.exists(path):
+            print(f"Directory {path} existed. Skip saving test data")
+            return
+
+        os.makedirs(path, exist_ok=True)
+
+        def add_tensor(input_tensors, torch_tensor, name):
+            input_tensors.append(numpy_helper.from_array(torch_tensor.clone().cpu().numpy(), name))
+
+        input_tensors = []
+        add_tensor(input_tensors, self.input_ids, "input_ids")
+
+        if self.has_position_ids:
+            add_tensor(input_tensors, self.position_ids, "position_ids")
+
+        if self.has_attention_mask:
+            add_tensor(input_tensors, self.attention_mask, "attention_mask")
+
+        for i in range(self.n_layer):
+            add_tensor(input_tensors, self.past[i], "past_" + str(i))
+
+        for i, tensor in enumerate(input_tensors):
+            with open(os.path.join(path, f"input_{i}.pb"), "wb") as f:
+                f.write(tensor.SerializeToString())
+
+        output_names = [output.name for output in session.get_outputs()]
+        for i, _name in enumerate(output_names):
+            tensor = numpy_helper.from_array(
+                output[i] if isinstance(output[i], numpy.ndarray) else output[i].clone().cpu().numpy()
+            )
+            with open(os.path.join(path, f"output_{i}.pb"), "wb") as f:
+                f.write(tensor.SerializeToString())
+
+        print(f"Test data saved to directory {path}")
 
     def update(self, output, step, device):
         """
         Update the inputs for next inference.
         """
-        self.last_state = (torch.from_numpy(output[0]).to(device)
-                           if isinstance(output[0], numpy.ndarray) else output[0].clone().detach().cpu())
+        self.logits = (
+            torch.from_numpy(output[0]) if isinstance(output[0], numpy.ndarray) else output[0].clone().detach().cpu()
+        )
 
-        self.input_ids = self.last_state.view(self.batch_size * self.beam_size, -1).to(device)
+        self.top_1_tokens = Gpt2Tester.predict_next_token(self.logits)
+        self.top_k_tokens = Gpt2Tester.predict_next_token(self.logits, self.top_k, self.top_k_required_order)
 
-        if self.position_ids is not None:
-            input_unfinished_sents_id = -3
-            self.prev_step_results = (torch.from_numpy(output[-2]).to(device) if isinstance(output[-2], numpy.ndarray)
-                                      else output[-2].clone().detach().to(device))
-            self.position_ids = (torch.tensor([self.input_length + step - 1
-                                               ]).unsqueeze(0).repeat(self.batch_size * self.beam_size, 1).to(device))
+        self.input_ids = self.top_1_tokens.clone().detach().reshape([self.batch_size, 1]).to(device)
 
-            if self.attention_mask.size(0) != (self.batch_size * self.beam_size):
-                self.attention_mask = self.attention_mask.repeat(self.batch_size * self.beam_size, 1)
+        if self.has_position_ids:
+            self.position_ids = (
+                torch.tensor([self.input_length + step - 1]).unsqueeze(0).repeat(self.batch_size, 1).to(device)
+            )
+
+        if self.has_attention_mask:
             self.attention_mask = torch.cat(
                 [
                     self.attention_mask,
-                    torch.ones([self.batch_size * self.beam_size, 1]).type_as(self.attention_mask),
+                    torch.ones([self.batch_size, 1]).type_as(self.attention_mask),
                 ],
                 1,
             ).to(device)
-        else:
-            input_unfinished_sents_id = -2
-
-        self.beam_select_idx = (torch.from_numpy(output[input_unfinished_sents_id - 2]).to(device) if isinstance(
-            output[input_unfinished_sents_id - 2], numpy.ndarray) else output[input_unfinished_sents_id -
-                                                                              2].clone().detach().to(device))
-        self.input_log_probs = (torch.from_numpy(output[input_unfinished_sents_id - 1]).to(device) if isinstance(
-            output[input_unfinished_sents_id - 1], numpy.ndarray) else output[input_unfinished_sents_id -
-                                                                              1].clone().detach().to(device))
-        self.input_unfinished_sents = (torch.from_numpy(output[input_unfinished_sents_id]).to(device) if isinstance(
-            output[input_unfinished_sents_id], numpy.ndarray) else
-                                       output[input_unfinished_sents_id].clone().detach().to(device))
-        self.prev_step_scores = (torch.from_numpy(output[-1]).to(device)
-                                 if isinstance(output[-1], numpy.ndarray) else output[-1].clone().detach().to(device))
-        self.top_1_tokens = self.input_ids[0]
-        self.top_k_tokens = self.last_state
 
         self.past = []
 
         if isinstance(output[1], tuple):  # past in torch output is tuple
             self.past = list(output[1])
         else:
             for i in range(self.n_layer):
-                past_i = (torch.from_numpy(output[i + 1])
-                          if isinstance(output[i + 1], numpy.ndarray) else output[i + 1].clone().detach())
+                past_i = (
+                    torch.from_numpy(output[i + 1])
+                    if isinstance(output[i + 1], numpy.ndarray)
+                    else output[i + 1].clone().detach()
+                )
                 self.past.append(past_i.to(device))
 
+    def diff(self, baseline):
+        """
+        Compare inputs and logits output.
+        """
+
+        print("start diff...")
+        if self.logits is not None:
+            max_io_diff = (self.logits - baseline.logits).abs().max()
+            if max_io_diff > 1e-4:
+                print(f"Max logits difference is too large: {max_io_diff}")
+
+        if not torch.all(self.input_ids == baseline.input_ids):
+            print("Input_ids is different", self.input_ids, baseline.input_ids)
+
+        if self.has_position_ids:
+            if not torch.all(self.position_ids == baseline.position_ids):
+                print(
+                    "position_ids is different",
+                    self.position_ids,
+                    baseline.position_ids,
+                )
+
+        if self.has_attention_mask:
+            if not torch.all(self.attention_mask == baseline.attention_mask):
+                print(
+                    "attention_mask is different",
+                    self.attention_mask,
+                    baseline.attention_mask,
+                )
+
+        assert len(self.past) == len(baseline.past)
+
+        for i, past_i in enumerate(self.past):
+            assert past_i.shape == baseline.past[i].shape
+            if past_i.nelement() > 0:
+                max_past_diff = (past_i - baseline.past[i]).abs().max()
+                if max_past_diff > 1e-4:
+                    print(f"max_past_diff[{i}]={max_past_diff}")
+
     @staticmethod
-    def test_generation(session,
-                        model,
-                        device,
-                        test_inputs,
-                        precision=Precision.FLOAT32,
-                        model_class="GPT2LMHeadModel_BeamSearchStep",
-                        top_k=20,
-                        top_k_no_order=True,
-                        max_steps=24,
-                        max_inputs=0,
-                        verbose=False,
-                        save_test_data=0,
-                        save_test_data_dir="."):
+    def predict_next_token(logits, top_k=1, required_order=False):
+        """
+        Get top k topkens based on logits.
         """
-        Test Generation using beam search to compare PyTorch and ONNX model.
+
+        # logits has shape (batch_size, seq_len, vocab_size)
+        # last token logits has shape (batch_size, vocab_size)
+        lastTokenLogits = logits[:, -1]  # noqa: N806
+        if top_k == 1:
+            generatedTokens = torch.argmax(lastTokenLogits, 1, True)  # noqa: N806
+            return generatedTokens
+        else:
+            topk = torch.argsort(lastTokenLogits, -1, descending=True)[:, :top_k]
+            if not required_order:
+                sorted_topk, _ = topk.sort()
+                return sorted_topk
+            return topk
+
+    @staticmethod
+    def diff_present(onnx_output, onnx_io_output, n_layer):
+        """
+        Compare the present outputs of two outputs from ONNX Runtime.
+        """
+        present_diff_max = []
+        for i in range(n_layer):
+            onnx_present_i = (
+                torch.from_numpy(onnx_output[i + 1])
+                if isinstance(onnx_output[i + 1], numpy.ndarray)
+                else onnx_output[i + 1]
+            )
+            onnx_io_present_i = (
+                torch.from_numpy(onnx_io_output[i + 1])
+                if isinstance(onnx_io_output[i + 1], numpy.ndarray)
+                else onnx_io_output[i + 1]
+            )
+            max_diff = (onnx_present_i - onnx_io_present_i).abs().max()
+            present_diff_max.append(max_diff)
+        print(f"present_diff_max={present_diff_max}")
+
+    @staticmethod
+    def is_quantized_onnx_model(onnx_model_path):
+        """
+        Returns True if the ONNX model is quantized.
+        """
+        from onnx import load
+
+        model = load(onnx_model_path)
+        from onnxruntime.quantization.quantize import __producer__ as quantize_producer
+
+        return model.producer_name == quantize_producer
+
+    @staticmethod
+    def test_generation(
+        session,
+        model,
+        device,
+        test_inputs,
+        precision=Precision.FLOAT32,
+        model_class="Gpt2LMHeadModel",
+        top_k=20,
+        top_k_no_order=True,
+        max_steps=24,
+        max_inputs=0,
+        verbose=False,
+        save_test_data=0,
+        save_test_data_dir=".",
+    ):
+        """
+        Test Generation using greedy beam search (without sampling) to compare PyTorch and ONNX model.
         It will print top 1 and top k errors on the given test inputs.
         """
         print(
             f"start test generation: (top_k={top_k} top_k_no_order={top_k_no_order} max_steps={max_steps} test_inputs={len(test_inputs)} max_inputs={max_inputs})"
         )
         n_layer = model.config.n_layer
         n_head = model.config.n_head
         n_embd = model.config.n_embd
-        beam_size = model.config.beam_size
         eos_token_id = model.config.eos_token_id
         test_data_saved = 0
 
         is_float16 = precision == Precision.FLOAT16
+        if is_float16:
+            assert "float16" in session.get_outputs()[0].type
 
         # We will still use fp32 torch model as baseline when onnx model if fp16
         model.eval().to(device)
 
         # Allocate initial buffers for IO Binding of ONNX Runtimne. The buffer size will automatically increase later.
-        init_output_shapes = Gpt2BeamSearchHelper.get_output_shapes(
+        init_output_shapes = Gpt2Helper.get_output_shapes(
             batch_size=4,
-            context_len=128,
             past_sequence_length=128,
             sequence_length=32,
-            beam_size=1,
-            step=0,
             config=model.config,
             model_class=model_class,
         )
-        output_buffers = Gpt2BeamSearchHelper.get_output_buffers(
-            init_output_shapes,
-            device,
-            is_float16=is_float16,
-        )
+        output_buffers = Gpt2Helper.get_output_buffers(init_output_shapes, device, is_float16=is_float16)
 
         baseline_name = "Torch"
         treatment_name = "Quantized Onnx" if precision == Precision.INT8 else "Onnx"
         torch_metric = Gpt2Metric(baseline_name, baseline_name, top_k)
         onnx_metric = Gpt2Metric(treatment_name, baseline_name, top_k)
         onnx_io_metric = Gpt2Metric(treatment_name + " with IO Binding", baseline_name, top_k)
 
         for i, inputs in enumerate(test_inputs):
             if max_inputs > 0 and i == max_inputs:
                 break
             if i % 10 == 0:
                 print(f"{i}")
             input_ids = inputs["input_ids"]
             position_ids = inputs["position_ids"] if "position_ids" in inputs else None
-            attention_mask = (inputs["attention_mask"] if "attention_mask" in inputs else None)
-            beam_select_idx = (inputs["beam_select_idx"] if "beam_select_idx" in inputs else None)
-            input_log_probs = (inputs["input_log_probs"] if "input_log_probs" in inputs else None)
-            input_unfinished_sents = inputs["input_unfinished_sents"]
-            if model_class == "GPT2LMHeadModel_BeamSearchStep":
-                prev_step_results = inputs["input_ids"]
-            else:
-                prev_step_results = None
-
-            if "prev_step_scores" in inputs:
-                prev_step_scores = inputs["prev_step_scores"]
-            else:
-                prev_step_scores = torch.zeros([input_ids.shape[0], 1])
+            attention_mask = inputs["attention_mask"] if "attention_mask" in inputs else None
 
-            onnx_runner = Gpt2BeamSearchTester(
+            onnx_runner = Gpt2Tester(
                 input_ids,
                 position_ids,
                 attention_mask,
-                beam_select_idx,
-                input_log_probs,
-                input_unfinished_sents,
-                prev_step_results,
-                prev_step_scores,
                 n_head,
                 n_embd,
                 n_layer,
-                beam_size,
                 device,
                 is_float16,
                 top_k,
                 not top_k_no_order,
             )
-            onnx_io_runner = Gpt2BeamSearchTester(
+            onnx_io_runner = Gpt2Tester(
                 input_ids,
                 position_ids,
                 attention_mask,
-                beam_select_idx,
-                input_log_probs,
-                input_unfinished_sents,
-                prev_step_results,
-                prev_step_scores,
                 n_head,
                 n_embd,
                 n_layer,
-                beam_size,
                 device,
                 is_float16,
                 top_k,
                 not top_k_no_order,
             )
-            torch_runner = Gpt2BeamSearchTester(
+            torch_runner = Gpt2Tester(
                 input_ids,
                 position_ids,
                 attention_mask,
-                beam_select_idx,
-                input_log_probs,
-                input_unfinished_sents,
-                prev_step_results,
-                prev_step_scores,
                 n_head,
                 n_embd,
                 n_layer,
-                beam_size,
                 device,
                 False,
                 top_k,
                 not top_k_no_order,
             )  # Torch model baseline is fp32
 
             batch_size = torch_runner.batch_size
             onnx_metric.start_batch(batch_size)
             onnx_io_metric.start_batch(batch_size)
-            context_len = list(onnx_runner.input_ids.size())[-1]
+
             with torch.no_grad():
+                done = torch.zeros(batch_size, dtype=torch.bool)
                 for step in range(max_steps):
-                    print(f"Processing step: {step}")
-                    if model_class == "GPT2LMHeadModel_BeamSearchStep":
-                        num_seq = beam_size
-                        seq_len = list(onnx_runner.input_ids.size())[1]
-                        past_seq_len = list(onnx_runner.past[0].size())[3]
-                    else:
-                        num_seq = sum(onnx_io_runner.input_unfinished_sents.view(-1).long().cpu())
-                        past_seq_len = list(onnx_runner.past[0].size())[3]
-                        seq_len = list(onnx_runner.input_ids.size())[-1] - past_seq_len
+                    seq_len = list(onnx_runner.input_ids.size())[1]
+                    past_seq_len = list(onnx_runner.past[0].size())[3]
 
                     start_time = timeit.default_timer()
-                    pytorch_output = Gpt2BeamSearchHelper.pytorch_inference(model, torch_runner.get_inputs())
+                    pytorch_output = Gpt2Helper.pytorch_inference(model, torch_runner.get_inputs())
                     torch_metric.add_latency(past_seq_len, timeit.default_timer() - start_time)
                     torch_runner.update(pytorch_output, step, device)
 
-                    (
-                        onnx_output,
-                        avg_latency_ms,
-                    ) = Gpt2BeamSearchHelper.onnxruntime_inference(session, onnx_runner.get_inputs(), total_runs=1)
+                    onnx_output, avg_latency_ms = Gpt2Helper.onnxruntime_inference(
+                        session, onnx_runner.get_inputs(), total_runs=1
+                    )
                     onnx_metric.add_latency(past_seq_len, avg_latency_ms / 1000.0)
                     onnx_runner.update(onnx_output, step, device)
 
-                    if model_class == "GPT2LMHeadModel_BeamSearchStep":
-                        num_seq = beam_size
-                    else:
-                        num_seq = sum(onnx_io_runner.input_unfinished_sents.view(-1).long().cpu())
-
-                    output_shapes = Gpt2BeamSearchHelper.get_output_shapes(
+                    output_shapes = Gpt2Helper.get_output_shapes(
                         batch_size,
-                        context_len,
                         past_seq_len,
                         seq_len,
-                        beam_size,
-                        step,
                         model.config,
                         model_class=model_class,
-                        num_seq=num_seq,
                     )
-
-                    Gpt2BeamSearchHelper.auto_increase_buffer_size(output_buffers, output_shapes)
+                    Gpt2Helper.auto_increase_buffer_size(output_buffers, output_shapes)
 
                     (
                         onnx_io_output,
                         avg_latency_ms,
-                    ) = Gpt2BeamSearchHelper.onnxruntime_inference_with_binded_io(
+                    ) = Gpt2Helper.onnxruntime_inference_with_binded_io(
                         session,
                         onnx_io_runner.get_inputs(),
                         output_buffers,
                         output_shapes,
                         total_runs=1,
                         return_numpy=False,
                         include_copy_output_latency=True,
                     )
-
                     onnx_io_metric.add_latency(past_seq_len, avg_latency_ms / 1000.0)
 
                     if test_data_saved < save_test_data:
                         onnx_io_runner.save_test_data(session, onnx_io_output, save_test_data_dir, test_data_saved)
                         test_data_saved += 1
 
                     onnx_io_runner.update(onnx_io_output, step, device)
 
-                    if ((not onnx_runner.input_unfinished_sents.any())
-                            or (not torch_runner.input_unfinished_sents.any())):
-                        print("break at step: ", step)
+                    if verbose:
+                        onnx_runner.diff(onnx_io_runner)
+                        Gpt2Tester.diff_present(onnx_output, onnx_io_output, n_layer)
+
+                        print("Top 1 tokens:")
+                        print("\tTorch", torch_runner.top_1_tokens)
+                        print("\tONNX", onnx_runner.top_1_tokens)
+                        print("\tONNX with IO binding", onnx_io_runner.top_1_tokens)
+
+                    onnx_metric.eval_batch(torch_runner, onnx_runner, past_seq_len, verbose=verbose)
+                    onnx_io_metric.eval_batch(torch_runner, onnx_io_runner, past_seq_len, verbose=verbose)
+
+                    done = done | (torch_runner.top_1_tokens == eos_token_id).any()
+                    if torch.all(done):
                         break
 
-            print(f"Totally {step+1} steps run")
             onnx_metric.end_batch()
             onnx_io_metric.end_batch()
 
         torch_metric.print()
         onnx_metric.print()
         onnx_io_metric.print()
-
-        print("\tONNX")
-        if model_class == "GPT2LMHeadModel_BeamSearchStep":
-            results_onnx = onnx_runner.prev_step_results.view(batch_size * beam_size, -1)
-            results_onnx_io = onnx_io_runner.prev_step_results.view(batch_size * beam_size, -1)
-        else:
-            results_onnx = onnx_runner.input_ids.view(batch_size * beam_size, -1)
-            results_onnx_io = onnx_io_runner.input_ids.view(batch_size * beam_size, -1)
-        Gpt2BeamSearchTester.pprint_results(
-            results_onnx,
-            onnx_runner.prev_step_scores.view(batch_size * beam_size, -1),
-            pad_token_id=eos_token_id,
-            eos_token_id=eos_token_id,
-        )
-        print("\tONNX with IO binding")
-        Gpt2BeamSearchTester.pprint_results(
-            results_onnx_io,
-            onnx_io_runner.prev_step_scores.view(batch_size * beam_size, -1),
-            pad_token_id=eos_token_id,
-            eos_token_id=eos_token_id,
-        )
-
-    @staticmethod
-    def pprint_results(
-        output_ids,
-        output_scores,
-        pad_token_id=None,
-        eos_token_id=None,
-    ):
-        """
-        Print test generation results.
-        """
-        if pad_token_id is None:
-            pad_token_id = 1
-        if eos_token_id is None:
-            eos_token_id = 1
-        if torch.is_tensor(output_ids):
-            output_ids = output_ids.cpu().numpy()
-
-        for i, sample in enumerate(output_ids):
-            for j, seq in enumerate(sample):
-                if isinstance(seq, numpy.ndarray) or isinstance(seq, list):
-                    # remove left padding
-                    for k, t in enumerate(seq):
-                        if t != pad_token_id:
-                            seq = seq[k:]
-                            break
-                    # remove EOS
-                    for k, t in enumerate(seq):
-                        if t == eos_token_id:
-                            seq = seq[:k + 1]
-                            break
-                    print("-" * 40)
-                    result = ",".join([str(token_id) for token_id in sample])
-                    print(f">> Output {j + 1}: \t{[result]}")
-                else:
-                    result = ",".join([str(token_id) for token_id in sample])
-                    print(f">> Output {i}: \t{result}")
-                    print(f">> Scores {i}: \t{output_scores[i]}")
-                    break
-            print("=" * 80)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_helper.py` & `onnxruntime/transformers/onnx_model.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,883 +1,1174 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
-# Licensed under the MIT License.  See License.txt in the project root for
-# license information.
+# Licensed under the MIT License.
 # --------------------------------------------------------------------------
-# This script helps onnx conversion and validation for GPT2 model with past state.
-import os
+
 import logging
-import torch
-import shutil
-import random
-import numpy
-import time
-import re
-import pickle
+import os
+import sys
+from collections import deque
 from pathlib import Path
-from typing import List, Dict, Tuple, Union
-from transformers import GPT2Model, GPT2LMHeadModel, GPT2Config, TFGPT2Model
-from float16 import float_to_float16_max_diff
-from onnx_model import OnnxModel
-from fusion_utils import FusionUtils
-from benchmark_helper import Precision
-from io_binding_helper import IOBindingHelper
-from torch_onnx_export_helper import torch_onnx_export
-
-logger = logging.getLogger(__name__)
-
-PRETRAINED_GPT2_MODELS = ['distilgpt2', 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']
-
-DEFAULT_TOLERANCE = {Precision.FLOAT32: 0.0005, Precision.FLOAT16: 0.2, Precision.INT8: 3.0}
-
-
-class GPT2ModelNoPastState(GPT2Model):
-    """ Here we wrap a class to disable past state output.
-    """
+from typing import Dict, List, Optional, Tuple
 
-    def __init__(self, config):
-        super().__init__(config)
+from float16 import convert_float_to_float16
+from onnx import (
+    AttributeProto,
+    GraphProto,
+    ModelProto,
+    NodeProto,
+    TensorProto,
+    ValueInfoProto,
+    helper,
+    numpy_helper,
+    save_model,
+)
+from shape_infer_helper import SymbolicShapeInferenceHelper
 
-    def forward(self, input_ids):
-        return super().forward(input_ids, use_cache=False, return_dict=False)
-
-
-class TFGPT2ModelNoPastState(TFGPT2Model):
-    """ Here we wrap a class to disable past state output.
-    """
+logger = logging.getLogger(__name__)
 
-    def __init__(self, config):
-        config.use_cache = False
-        super().__init__(config)
 
-    def forward(self, input_ids):
-        return super().call(input_ids, use_cache=False)
+class OnnxModel:
+    def __init__(self, model):
+        self.initialize(model)
+
+    def initialize(self, model):
+        self.model: ModelProto = model
+        self._node_name_suffix: Dict[str, int] = {}  # key is node name prefix, value is the last suffix generated
+        self.shape_infer_helper: SymbolicShapeInferenceHelper = None
+        self.enable_shape_infer: bool = True
+        self.all_graphs: Optional[List[GraphProto]] = None
+
+    def disable_shape_inference(self):
+        self.enable_shape_infer = False
+
+    def infer_runtime_shape(self, dynamic_axis_mapping={}, update=False):  # noqa: B006
+        if self.enable_shape_infer:
+            if self.shape_infer_helper is None or update:
+                self.shape_infer_helper = SymbolicShapeInferenceHelper(self.model)
+
+            try:
+                if self.shape_infer_helper.infer(dynamic_axis_mapping):
+                    return self.shape_infer_helper
+            except Exception:
+                self.enable_shape_infer = False  # disable shape inference to suppress same error message.
+                print("failed in shape inference", sys.exc_info()[0])
+
+        return None
+
+    def input_name_to_nodes(self):
+        input_name_to_nodes = {}
+        for node in self.nodes():
+            for input_name in node.input:
+                if input_name:  # could be empty when it is optional
+                    if input_name not in input_name_to_nodes:
+                        input_name_to_nodes[input_name] = [node]
+                    else:
+                        input_name_to_nodes[input_name].append(node)
+        return input_name_to_nodes
 
+    def output_name_to_node(self):
+        output_name_to_node = {}
+        for node in self.nodes():
+            for output_name in node.output:
+                if output_name:  # could be empty when it is optional
+                    output_name_to_node[output_name] = node
+        return output_name_to_node
+
+    def nodes(self):
+        all_nodes = []
+        for graph in self.graphs():
+            for node in graph.node:
+                all_nodes.append(node)
+        return all_nodes
+
+    def graph(self):
+        return self.model.graph
+
+    def graphs(self):
+        if self.all_graphs is not None:
+            return self.all_graphs
+        self.all_graphs = []
+        graph_queue = [self.model.graph]
+        while graph_queue:
+            graph = graph_queue.pop(0)
+            self.all_graphs.append(graph)
+            for node in graph.node:
+                for attr in node.attribute:
+                    if attr.type == AttributeProto.AttributeType.GRAPH:
+                        assert isinstance(attr.g, GraphProto)
+                        graph_queue.append(attr.g)
+                    if attr.type == AttributeProto.AttributeType.GRAPHS:
+                        for g in attr.graphs:
+                            assert isinstance(g, GraphProto)
+                            graph_queue.append(g)
+        return self.all_graphs
+
+    def get_graphs_input_names(self):
+        input_names = []
+        for graph in self.graphs():
+            for input in graph.input:
+                input_names.append(input.name)
+        return input_names
+
+    def get_graphs_output_names(self):
+        output_names = []
+        for graph in self.graphs():
+            for output in graph.output:
+                output_names.append(output.name)
+        return output_names
+
+    def get_graph_by_node(self, node):
+        for graph in self.graphs():
+            if node in graph.node:
+                return graph
+        return None
+
+    def get_graph_by_name(self, graph_name):
+        for graph in self.graphs():
+            if graph_name == graph.name:
+                return graph
+        return None
 
-class MyGPT2Model(GPT2Model):
-    """ Here we wrap a class for Onnx model conversion for GPT2Model with past state.
-    """
+    def get_topological_insert_id(self, graph, outputs):
+        for idx, node in enumerate(graph.node):
+            for input in node.input:
+                if input in outputs:
+                    return idx
+        return len(graph.node)
+
+    def remove_node(self, node):
+        for graph in self.graphs():
+            if node in graph.node:
+                graph.node.remove(node)
+                return
+        logger.warning("Failed to remove node %s", node)  # It might be a bug to hit this line.
+
+    def remove_nodes(self, nodes_to_remove):
+        for node in nodes_to_remove:
+            self.remove_node(node)
+
+    def add_node(self, node, graph_name=None):
+        if graph_name is None or graph_name == self.model.graph.name:
+            self.model.graph.node.extend([node])
+        else:
+            graph = self.get_graph_by_name(graph_name)
+            insert_idx = self.get_topological_insert_id(graph, node.output)
+            graph.node.insert(insert_idx, node)
+
+    def add_nodes(self, nodes_to_add, node_name_to_graph_name=None):
+        if node_name_to_graph_name is None:
+            self.model.graph.node.extend(nodes_to_add)
+        else:
+            for node in nodes_to_add:
+                graph_name = node_name_to_graph_name[node.name]
+                self.add_node(node, graph_name)
+
+    def add_initializer(self, tensor, graph_name=None):
+        if graph_name is None or graph_name == self.model.graph.name:
+            self.model.graph.initializer.extend([tensor])
+        else:
+            graph = self.get_graph_by_name(graph_name)
+            graph.initializer.extend([tensor])
 
-    def __init__(self, config):
-        super().__init__(config)
+    def add_input(self, input, graph_name=None):
+        if graph_name is None or graph_name == self.model.graph.name:
+            self.model.graph.input.extend([input])
+        else:
+            graph = self.get_graph_by_name(graph_name)
+            graph.input.extend([input])
 
     @staticmethod
-    def post_process(result, num_layer):
-        if isinstance(result[1][0], tuple) or isinstance(result[1][0], list):
-            assert len(result[1]) == num_layer and len(result[1][0]) == 2
-            #assert len(result[1][0][0].shape) == 4 and result[1][0][0].shape == result[1][0][1].shape
-            present = []
-            for i in range(num_layer):
-                # Since transformers v4.*, past key and values are separated outputs.
-                # Here we concate them into one tensor to be compatible with Attention operator.
-                present.append(torch.cat((result[1][i][0].unsqueeze(0), result[1][i][1].unsqueeze(0)), dim=0))
-            return (result[0], tuple(present))
-
-        return result
-
-    def forward(self, input_ids, position_ids, attention_mask, *past):
-        result = super().forward(input_ids,
-                                 position_ids=position_ids,
-                                 attention_mask=attention_mask,
-                                 past_key_values=past,
-                                 return_dict=False)
-        return MyGPT2Model.post_process(result, self.config.n_layer)
-
-
-class MyGPT2LMHeadModel(GPT2LMHeadModel):
-    """ Here we wrap a class for Onnx model conversion for GPT2LMHeadModel with past state.
-    """
-
-    def __init__(self, config):
-        super().__init__(config)
-
-    def forward(self, input_ids, position_ids, attention_mask, *past):
-        result = super().forward(input_ids,
-                                 position_ids=position_ids,
-                                 attention_mask=attention_mask,
-                                 past_key_values=past,
-                                 return_dict=False)
-
-        return MyGPT2Model.post_process(result, self.config.n_layer)
-
-
-class MyGPT2LMHeadModel_NoPadding(GPT2LMHeadModel):
-    """ Here we wrap a class for Onnx model conversion for GPT2LMHeadModel with past state and no padding.
-        When you always use batch_size=1 in inference, there is no padding in inputs. In such case, position_ids
-        and attention_mask need no be in inputs.
-    """
-
-    def __init__(self, config):
-        super().__init__(config)
-
-    def forward(self, input_ids, *past):
-        result = super().forward(input_ids, past_key_values=past, return_dict=False)
-
-        return MyGPT2Model.post_process(result, self.config.n_layer)
-
-
-# Maps model class name to a tuple of model class, name of first output and use padding or not
-MODEL_CLASSES = {
-    'GPT2LMHeadModel': (MyGPT2LMHeadModel, 'logits', True),
-    'GPT2LMHeadModel_NoPadding': (MyGPT2LMHeadModel_NoPadding, 'logits', False),
-    'GPT2Model': (MyGPT2Model, 'last_state', True),
-}
-
+    def replace_node_input(node, old_input_name, new_input_name):
+        assert isinstance(old_input_name, str) and isinstance(new_input_name, str)
+        for j in range(len(node.input)):
+            if node.input[j] == old_input_name:
+                node.input[j] = new_input_name
+
+    def replace_input_of_all_nodes(self, old_input_name, new_input_name):
+        for node in self.model.graph.node:
+            OnnxModel.replace_node_input(node, old_input_name, new_input_name)
+
+    @staticmethod
+    def replace_node_output(node, old_output_name, new_output_name):
+        assert isinstance(old_output_name, str) and isinstance(new_output_name, str)
+        for j in range(len(node.output)):
+            if node.output[j] == old_output_name:
+                node.output[j] = new_output_name
+
+    def replace_output_of_all_nodes(self, old_output_name, new_output_name):
+        # This function shall be used carefully. For example:
+        #       Add --[old_name]--> Cast ---> [new_name]
+        #        |
+        #        +----[old_name]--> Transpose -->
+        # If we want to remove the Cast node: replace output of Add to new_name is not enough;
+        # The input of Transpose shall also be updated to new_name.
+        for node in self.model.graph.node:
+            OnnxModel.replace_node_output(node, old_output_name, new_output_name)
+
+    def get_initializer(self, name):
+        for graph in self.graphs():
+            for tensor in graph.initializer:
+                if tensor.name == name:
+                    return tensor
+        return None
+
+    def get_nodes_by_op_type(self, op_type):
+        nodes = []
+        for node in self.nodes():
+            if node.op_type == op_type:
+                nodes.append(node)
+        return nodes
+
+    def get_children(self, node, input_name_to_nodes=None):
+        if input_name_to_nodes is None:
+            input_name_to_nodes = self.input_name_to_nodes()
+
+        children = []
+        for output in node.output:
+            if output in input_name_to_nodes:
+                for node in input_name_to_nodes[output]:
+                    children.append(node)
+        return children
+
+    def get_parents(self, node, output_name_to_node=None):
+        if output_name_to_node is None:
+            output_name_to_node = self.output_name_to_node()
+
+        parents = []
+        for input in node.input:
+            if input in output_name_to_node:
+                parents.append(output_name_to_node[input])
+        return parents
+
+    def get_parent(self, node, i, output_name_to_node=None):
+        if output_name_to_node is None:
+            output_name_to_node = self.output_name_to_node()
+
+        if len(node.input) <= i:
+            return None
+
+        input = node.input[i]
+        if input not in output_name_to_node:
+            return None
 
-class Gpt2Inputs:
+        return output_name_to_node[input]
 
-    def __init__(self, input_ids, position_ids, attention_mask, past):
-        self.input_ids: torch.LongTensor = input_ids
-        self.position_ids: torch.LongTensor = position_ids
-        self.attention_mask: Union[torch.LongTensor, torch.FloatTensor, torch.HalfTensor] = attention_mask
-        self.past: Union[List[torch.FloatTensor], List[torch.HalfTensor]] = past
-
-    def to_list(self) -> List:
-        input_list = [v for v in [self.input_ids, self.position_ids, self.attention_mask] if v is not None]
-        if self.past:
-            input_list.extend(self.past)
-
-        return input_list
-
-    def to_tuple(self) -> Tuple:
-        return tuple(v for v in [self.input_ids, self.position_ids, self.attention_mask, self.past] if v is not None)
-
-    def to_fp32(self):
-        # For attention mask, only convert fp16 to fp32, and keep the original type if it is integer.
-        attention_mask = None
-        if self.attention_mask is not None:
-            attention_mask = self.attention_mask.to(
-                dtype=torch.float32) if (self.attention_mask.dtype == torch.float16) else self.attention_mask
-
-        past = [p.to(dtype=torch.float32) for p in self.past]
-        return Gpt2Inputs(self.input_ids, self.position_ids, attention_mask, past)
-
-
-class Gpt2Helper:
-    """ A helper class for Gpt2 model conversion, inference and verification.
-    """
+    def match_first_parent(self, node, parent_op_type, output_name_to_node, exclude=[]):  # noqa: B006
+        """
+        Find parent node based on constraints on op_type.
 
-    @staticmethod
-    def get_dummy_inputs(batch_size: int,
-                         past_sequence_length: int,
-                         sequence_length: int,
-                         num_attention_heads: int,
-                         hidden_size: int,
-                         num_layer: int,
-                         vocab_size: int,
-                         device: torch.device,
-                         float16: bool = False,
-                         has_position_ids: bool = True,
-                         has_attention_mask: bool = True,
-                         input_ids_dtype: torch.dtype = torch.int32,
-                         position_ids_dtype: torch.dtype = torch.int32,
-                         attention_mask_dtype: torch.dtype = torch.int32) -> Gpt2Inputs:
-        """ Create random inputs for GPT2 model.
-        Returns torch tensors of input_ids, position_ids, attention_mask and a list of past state tensors.
-        """
-        float_type = torch.float16 if float16 else torch.float32
-        past_shape = [2, batch_size, num_attention_heads, past_sequence_length, int(hidden_size / num_attention_heads)]
-
-        past = [(torch.rand(past_shape, dtype=float_type, device=device) * 2.0 - 1.0) for _ in range(num_layer)]
-        input_ids = torch.randint(low=0,
-                                  high=vocab_size - 1,
-                                  size=(batch_size, sequence_length),
-                                  dtype=input_ids_dtype,
-                                  device=device)
-
-        attention_mask = None
-        if has_attention_mask:
-            total_sequence_length = past_sequence_length + sequence_length
-            attention_mask = torch.ones([batch_size, total_sequence_length], dtype=attention_mask_dtype, device=device)
-            if total_sequence_length >= 2:
-                padding_position = random.randint(0, total_sequence_length - 1)  # test input with padding.
-                attention_mask[:, padding_position] = 0
-
-        # Deduce position_ids from attention mask
-        position_ids = None
-        if has_position_ids:
-            position_ids = (attention_mask.long().cumsum(-1) - 1)
-            position_ids.masked_fill_(position_ids < 0, 0)
-            position_ids = position_ids[:, past_sequence_length:].to(position_ids_dtype)
+        Args:
+            node (str): current node name.
+            parent_op_type (str): constraint of parent node op_type.
+            output_name_to_node (dict): dictionary with output name as key, and node as value.
+            exclude (list): list of nodes that are excluded (not allowed to match as parent).
 
-        return Gpt2Inputs(input_ids, position_ids, attention_mask, past)
+        Returns:
+            parent: The matched parent node. None if not found.
+            index: The input index of matched parent node. None if not found.
+        """
+        for i, input in enumerate(node.input):
+            if input in output_name_to_node:
+                parent = output_name_to_node[input]
+                if parent.op_type == parent_op_type and parent not in exclude:
+                    return parent, i
+                else:
+                    logger.debug(f"To find first {parent_op_type}, current {parent.op_type}")
+        return None, None
+
+    def match_parent(
+        self,
+        node,
+        parent_op_type,
+        input_index=None,
+        output_name_to_node=None,
+        exclude=[],  # noqa: B006
+        return_indice=None,
+    ):
+        """
+        Find parent node based on constraints on op_type and index.
+        When input_index is None, we will find the first parent node based on constraints,
+        and return_indice will be appended the corresponding input index.
 
-    @staticmethod
-    def get_output_shapes(batch_size: int,
-                          past_sequence_length: int,
-                          sequence_length: int,
-                          config: GPT2Config,
-                          model_class: str = "GPT2LMHeadModel") -> Dict[str, List[int]]:
-        """ Returns a dictionary with output name as key, and shape as value.
-        """
-        num_attention_heads = config.num_attention_heads
-        hidden_size = config.hidden_size
-        num_layer = config.num_hidden_layers
-        vocab_size = config.vocab_size
-
-        output_name = MODEL_CLASSES[model_class][1]
-
-        last_state_shape = [batch_size, sequence_length, vocab_size if output_name == "logits" else hidden_size]
-        present_state_shape = [
-            2, batch_size, num_attention_heads, past_sequence_length + sequence_length,
-            int(hidden_size / num_attention_heads)
-        ]
-
-        output_shapes = {output_name: last_state_shape}
-        for i in range(num_layer):
-            output_shapes["present_" + str(i)] = present_state_shape
+        Args:
+            node (str): current node name.
+            parent_op_type (str): constraint of parent node op_type.
+            input_index (int or None): only check the parent given input index of current node.
+            output_name_to_node (dict): dictionary with output name as key, and node as value.
+            exclude (list): list of nodes that are excluded (not allowed to match as parent).
+            return_indice (list): a list to append the input index when input_index is None.
 
-        return output_shapes
+        Returns:
+            parent: The matched parent node.
+        """
+        assert node is not None
+        assert input_index is None or input_index >= 0
 
-    @staticmethod
-    def auto_increase_buffer_size(output_buffers, output_shapes):
-        for key in output_shapes:
-            assert key in output_buffers
-            buffer = output_buffers[key]
-            if numpy.prod(output_shapes[key]) > buffer.nelement():
-                output_buffers[key] = torch.empty(numpy.prod(output_shapes[key]),
-                                                  dtype=buffer.dtype,
-                                                  device=buffer.device)
+        if output_name_to_node is None:
+            output_name_to_node = self.output_name_to_node()
 
-    @staticmethod
-    def get_output_buffers(output_shapes, device, is_float16=False):
-        """ Returns a dictionary of output name as key, and 1D tensor as value. The tensor has enough space for given shape.
+        if input_index is None:
+            parent, index = self.match_first_parent(node, parent_op_type, output_name_to_node, exclude)
+            if return_indice is not None:
+                return_indice.append(index)
+            return parent
+
+        if input_index >= len(node.input):
+            logger.debug(f"input_index {input_index} >= node inputs {len(node.input)}")
+            return None
+
+        parent = self.get_parent(node, input_index, output_name_to_node)
+        if parent is not None and parent.op_type == parent_op_type and parent not in exclude:
+            return parent
+
+        if parent is not None:
+            logger.debug(f"Expect {parent_op_type}, Got {parent.op_type}")
+
+        return None
+
+    def match_parent_paths(self, node, paths, output_name_to_node):
+        for i, path in enumerate(paths):
+            assert isinstance(path, (List, Tuple))
+            return_indice = []
+            matched = self.match_parent_path(node, path[0], path[1], output_name_to_node, return_indice)
+            if matched:
+                return i, matched, return_indice
+        return -1, None, None
+
+    def match_parent_path(
+        self,
+        node,
+        parent_op_types,
+        parent_input_index=None,
+        output_name_to_node=None,
+        return_indice=None,
+    ):
         """
-        data_type = torch.float16 if is_float16 else torch.float32
+        Find a sequence of input edges based on constraints on parent op_type and index.
+        When input_index is None, we will find the first parent node based on constraints,
+        and return_indice will be appended the corresponding input index.
 
-        output_buffers = {}
-        for name, shape in output_shapes.items():
-            output_buffers[name] = torch.empty(numpy.prod(shape), dtype=data_type, device=device)
-        return output_buffers
+        Args:
+            node (str): current node name.
+            parent_op_types (str): constraint of parent node op_type of each input edge.
+            parent_input_index (list): constraint of input index of each input edge. None means no constraint.
+            output_name_to_node (dict): dictionary with output name as key, and node as value.
+            return_indice (list): a list to append the input index
+                                  When there is no constraint on input index of an edge.
 
-    @staticmethod
-    def diff_outputs(torch_outputs, ort_outputs, relative=False):
-        """ Returns the maximum difference between PyTorch and OnnxRuntime outputs.
+        Returns:
+            parents: a list of matched parent node.
         """
-        expected_outputs = torch_outputs[0].cpu().numpy()
-        diff = numpy.abs(expected_outputs - ort_outputs[0])
-        if relative:
-            return numpy.amax(diff / (numpy.abs(expected_outputs) + 1e-6))
-        else:
-            return numpy.amax(diff)
+        if parent_input_index is not None:
+            assert len(parent_input_index) == len(parent_op_types)
 
-    @staticmethod
-    def compare_outputs(torch_outputs, ort_outputs, rtol=1e-03, atol=1e-03):
-        """ Returns True if torch and ORT outputs are close for given thresholds, and False otherwise.
-        """
-        is_close = numpy.allclose(ort_outputs[0], torch_outputs[0].cpu().numpy(), rtol=rtol, atol=atol)
-        logger.debug(f'PyTorch and OnnxRuntime output 0 (last_state) are close: {is_close}')
+        if output_name_to_node is None:
+            output_name_to_node = self.output_name_to_node()
 
-        is_all_close = is_close
-        num_layers = len(ort_outputs) - 1
+        current_node = node
+        matched_parents = []
+        for i, op_type in enumerate(parent_op_types):
+            matched_parent = self.match_parent(
+                current_node,
+                op_type,
+                parent_input_index[i] if parent_input_index is not None else None,
+                output_name_to_node,
+                exclude=[],
+                return_indice=return_indice,
+            )
+            if matched_parent is None:
+                if parent_input_index is not None:
+                    logger.debug(
+                        f"Failed to match index={i} parent_input_index={parent_input_index[i]} op_type={op_type}",
+                        stack_info=True,
+                    )
+                else:
+                    logger.debug(f"Failed to match index={i} op_type={op_type}", stack_info=True)
+                return None
+
+            matched_parents.append(matched_parent)
+            current_node = matched_parent
+
+        return matched_parents
+
+    def find_first_child_by_type(self, node, child_type, input_name_to_nodes=None, recursive=True):
+        children = self.get_children(node, input_name_to_nodes)
+        dq = deque(children)
+        while len(dq) > 0:
+            current_node = dq.pop()
+            if current_node.op_type == child_type:
+                return current_node
+
+            if recursive:
+                children = self.get_children(current_node, input_name_to_nodes)
+                for child in children:
+                    dq.appendleft(child)
+
+        return None
+
+    def find_first_parent_by_type(self, node, parent_type, output_name_to_node=None, recursive=True):
+        if output_name_to_node is None:
+            output_name_to_node = self.output_name_to_node()
+
+        parents = self.get_parents(node, output_name_to_node)
+        dq = deque(parents)
+        while len(dq) > 0:
+            current_node = dq.pop()
+            if current_node.op_type == parent_type:
+                return current_node
+
+            if recursive:
+                parents = self.get_parents(current_node, output_name_to_node)
+                for parent in parents:
+                    dq.appendleft(parent)
+
+        return None
+
+    def get_constant_value(self, output_name):
+        for node in self.get_nodes_by_op_type("Constant"):
+            if node.output[0] == output_name:
+                for att in node.attribute:
+                    if att.name == "value":
+                        return numpy_helper.to_array(att.t)
+
+        # Fall back to intializer since constant folding might have been applied.
+        initializer = self.get_initializer(output_name)
+        if initializer is not None:
+            return numpy_helper.to_array(initializer)
+
+        return None
+
+    def get_constant_input(self, node):
+        for i, input in enumerate(node.input):
+            value = self.get_constant_value(input)
+            if value is not None:
+                return i, value
+
+        return None, None
+
+    def find_constant_input(self, node, expected_value, delta=0.000001):
+        i, value = self.get_constant_input(node)
+        if value is not None and value.size == 1 and abs(value - expected_value) < delta:
+            return i
+
+        return -1
+
+    def is_constant_with_specified_dimension(self, output_name, dimensions, description):
+        value = self.get_constant_value(output_name)
+        if value is None:
+            logger.debug(f"{description} {output_name} is not initializer.")
+            return False
+
+        if len(value.shape) != dimensions:
+            logger.debug(f"{description} {output_name} shall have {dimensions} dimensions. Got shape {value.shape}")
+            return False
+
+        return True
+
+    def has_constant_input(self, node, expected_value, delta=0.000001):
+        return self.find_constant_input(node, expected_value, delta) >= 0
+
+    def get_children_subgraph_nodes(self, root_node, stop_nodes, input_name_to_nodes=None):
+        if input_name_to_nodes is None:
+            input_name_to_nodes = self.input_name_to_nodes()
+
+        children = input_name_to_nodes[root_node.output[0]]
+
+        unique_nodes = []
+
+        dq = deque(children)
+        while len(dq) > 0:
+            current_node = dq.pop()
+            if current_node in stop_nodes:
+                continue
+
+            if current_node not in unique_nodes:
+                unique_nodes.append(current_node)
+
+                for output in current_node.output:
+                    if output in input_name_to_nodes:
+                        children = input_name_to_nodes[output]
+                        for child in children:
+                            dq.appendleft(child)
+
+        return unique_nodes
+
+    def tensor_shape_to_list(self, tensor_type):
+        """Convert tensor shape to list"""
+        shape_list = []
+        for d in tensor_type.shape.dim:
+            if d.HasField("dim_value"):
+                shape_list.append(d.dim_value)  # known dimension
+            elif d.HasField("dim_param"):
+                shape_list.append(d.dim_param)  # unknown dimension with symbolic name
+            else:
+                shape_list.append("?")  # shall not happen
+        return shape_list
 
-        for layer in range(num_layers):
-            is_close = numpy.allclose(ort_outputs[1 + layer],
-                                      torch_outputs[1][layer].cpu().numpy(),
-                                      rtol=rtol,
-                                      atol=atol)
-            logger.debug(f'PyTorch and OnnxRuntime layer {layer} state (present_{layer}) are close:{is_close}')
-            is_all_close = is_all_close and is_close
-
-        if not is_all_close:
-            max_abs_diff = Gpt2Helper.diff_outputs(torch_outputs, ort_outputs)
-            logger.info(f'PyTorch and OnnxRuntime results are not all close: max_abs_diff={max_abs_diff:.5f}')
+    def get_dtype(self, input_or_output: str):
+        """Try get data type given a name (could be initializer, graph input or output)."""
+        tensor_type_map = {obj.name: obj.type for obj in self.model.graph.value_info}
+
+        if input_or_output in tensor_type_map:
+            return tensor_type_map[input_or_output].tensor_type.elem_type
+
+        graph_input = self.find_graph_input(input_or_output)
+        if graph_input:
+            return graph_input.type.tensor_type.elem_type
+
+        graph_output = self.find_graph_output(input_or_output)
+        if graph_output:
+            return graph_output.type.tensor_type.elem_type
+
+        return None
+
+    @staticmethod
+    def get_node_attribute(node: NodeProto, attribute_name: str):
+        for attr in node.attribute:
+            if attr.name == attribute_name:
+                value = helper.get_attribute_value(attr)
+                return value
+        return None
+
+    def remove_cascaded_cast_nodes(self):
+        """Remove Cast node that are followed by another Cast node like  --> Cast --> Cast -->
+        Note that this shall be used carefully since it might introduce semantic change.
+        For example, float -> int -> float could get different value than the original float value.
+        So, it is recommended to used only in post-processing of mixed precision conversion.
+        """
+        output_name_to_node = self.output_name_to_node()
+        removed_count = 0
+        for node in self.nodes():
+            if node.op_type == "Cast":
+                parent = self.get_parent(node, 0, output_name_to_node=output_name_to_node)
+                if parent and parent.op_type == "Cast":
+                    node.input[0] = parent.input[0]
+                    removed_count += 1
+
+        if removed_count > 0:
+            logger.info("Removed %d cascaded Cast nodes", removed_count)
+            self.prune_graph()
+
+    def remove_useless_cast_nodes(self):
+        """Remove cast nodes that are not needed: input and output has same data type."""
+        shape_infer = self.infer_runtime_shape(update=True)
+        if shape_infer is None:
+            logger.info("Skip removing useless cast nodes since shape inference failed.")
+            return
+
+        def get_data_type(input_or_output_name):
+            dtype = self.get_dtype(input_or_output_name)
+            if dtype:
+                return dtype
+            if shape_infer.known_vi_[input_or_output_name].type.tensor_type.HasField("elem_type"):
+                return shape_infer.known_vi_[input_or_output_name].type.tensor_type.elem_type
+            return None
+
+        nodes_to_remove = []
+        for node in self.nodes():
+            if node.op_type == "Cast":
+                input_dtype = get_data_type(node.input[0])
+                output_dtype = get_data_type(node.output[0])
+                if input_dtype and input_dtype == output_dtype:
+                    nodes_to_remove.append(node)
+
+        if nodes_to_remove:
+            graph_input_names = set(self.get_graphs_input_names())
+            graph_output_names = set(self.get_graphs_output_names())
+            for node in nodes_to_remove:
+                if bool(set(node.output) & graph_output_names):
+                    if (not bool(set(node.input) & graph_input_names)) and len(
+                        self.input_name_to_nodes()[node.input[0]]
+                    ) == 1:
+                        self.replace_output_of_all_nodes(node.input[0], node.output[0])
+                    else:
+                        continue
+                else:
+                    self.replace_input_of_all_nodes(node.output[0], node.input[0])
+                self.remove_node(node)
+
+            logger.info("Removed %d Cast nodes with output type same as input", len(nodes_to_remove))
+
+    def convert_model_float32_to_float16(self, cast_input_output=True):
+        logger.warning(
+            "The function convert_model_float32_to_float16 is deprecated. Use convert_float_to_float16 instead!"
+        )
+        self.convert_float_to_float16(use_symbolic_shape_infer=True, keep_io_types=cast_input_output)
 
-        return is_all_close
+    def convert_float_to_float16(self, use_symbolic_shape_infer=True, **kwargs):
+        """Convert a model to half (default) or mixed precision.
+           To use mixed precision, user need specify which graph inputs, outputs, operator type
+           or list of nodes shall keep in float32.
 
-    @staticmethod
-    def compare_outputs_v2(torch_outputs, ort_outputs, atol=1e-06):
-        """Compare outputs from PyTorch and OnnxRuntime
+           Note that the conversion might not proceed without type information for the whole graph.
 
-        Args:
-            torch_outputs (Tuple[Torch.Tensor]): PyTorch model output
-            ort_outputs (List[numpy.ndarray]): OnnxRuntime output
-            atol (float, optional): Absolute tollerance. Defaults to 1e-06.
+           By default, we use symbolic shape inference to get type information. The benefit of symbolic shape inference
+           is that it could handle fused operators in com.microsoft domain. Those operators cannot be handled in onnx shape
+           inference so symbolic shape inference is recommended for optimized model.
 
-        Returns:
-            is_all_close(bool): whether all elements are close.
-            max_abs_diff(float): maximum absolute difference.
-            messages(str): a list of debug message for each output
-        """
-        is_all_close = True
-        is_top1_matched = False
-        max_diffs = []
-        messages = []
-        for i in range(len(ort_outputs)):
-            ort_output = ort_outputs[i]
-            torch_output = (torch_outputs[0] if i == 0 else torch_outputs[1][i - 1]).cpu().numpy()
-            is_close = numpy.allclose(ort_output, torch_output, atol=atol, rtol=0)
-            max_diffs.append(numpy.amax(numpy.abs(torch_output - ort_output)))
-            is_all_close = is_all_close and is_close
-
-            if numpy.isnan(torch_output).any():
-                logger.debug(f'PyTorch output {i} has nan')
-            if numpy.isinf(torch_output).any():
-                logger.debug(f'PyTorch output {i} has inf')
-            if numpy.isnan(ort_output).any():
-                logger.debug(f'ORT output {i} has nan')
-            if numpy.isinf(ort_output).any():
-                logger.debug(f'ORT output {i} has inf')
-
-            diff = numpy.fabs(ort_output - torch_output)
-            idx = numpy.unravel_index(diff.argmax(), diff.shape)
-            messages.append(
-                f'diff={diff[idx]:.9f} index={idx} ort={ort_output[idx]:.9f} torch={float(torch_output[idx]):.9f}')
-
-            if i == 0:  # logits
-                ort_max_index = numpy.unravel_index(numpy.argmax(ort_output, axis=None), ort_output.shape)
-                torch_max_index = numpy.unravel_index(numpy.argmax(torch_output, axis=None), torch_output.shape)
-                is_top1_matched = numpy.array_equal(ort_max_index, torch_max_index)
+           When symbolic shape inference is used (even if it failed), ONNX shape inference will be disabled.
 
-        max_diff_output_index = max_diffs.index(max(max_diffs))
-        return is_all_close, max(max_diffs), max_diff_output_index, messages, is_top1_matched
+           Note that onnx shape inference will fail for model larger than 2GB. For large model, you have to eanble
+           symbolic shape inference. If your model is not optimized, you can also use model path to call
+           convert_float_to_float16 in float16.py (see https://github.com/microsoft/onnxruntime/pull/15067) to
+           avoid the 2GB limit.
 
-    @staticmethod
-    def export_onnx(model,
-                    device,
-                    onnx_model_path: str,
-                    verbose: bool = False,
-                    use_external_data_format: bool = False,
-                    has_position_ids: bool = True,
-                    has_attention_mask: bool = True,
-                    input_ids_dtype: torch.dtype = torch.int32,
-                    position_ids_dtype: torch.dtype = torch.int32,
-                    attention_mask_dtype: torch.dtype = torch.int32):
-        """ Export GPT-2 model with past state to ONNX model.
-        """
-        config: GPT2Config = model.config
-        num_layer = config.n_layer
-        dummy_inputs = Gpt2Helper.get_dummy_inputs(batch_size=1,
-                                                   past_sequence_length=1,
-                                                   sequence_length=1,
-                                                   num_attention_heads=config.num_attention_heads,
-                                                   hidden_size=config.hidden_size,
-                                                   num_layer=num_layer,
-                                                   vocab_size=config.vocab_size,
-                                                   device=device,
-                                                   float16=False,
-                                                   has_position_ids=has_position_ids,
-                                                   has_attention_mask=has_attention_mask,
-                                                   input_ids_dtype=input_ids_dtype,
-                                                   position_ids_dtype=position_ids_dtype,
-                                                   attention_mask_dtype=attention_mask_dtype)
-        input_list = dummy_inputs.to_list()
-
-        with torch.no_grad():
-            outputs = model(*input_list)
-
-        past_names = [f'past_{i}' for i in range(num_layer)]
-        present_names = [f'present_{i}' for i in range(num_layer)]
-
-        # GPT2Model outputs last_state; GPT2LMHeadModel outputs logits (prediction_scores)
-        assert outputs[0].shape[2] == config.vocab_size or outputs[0].shape[2] == config.hidden_size
-        output_names = ["logits" if outputs[0].shape[2] == config.vocab_size else "last_state"] + present_names
-
-        # Shape of input tensors:
-        #    input_ids: (batch_size, seq_len)
-        #    past_{i}:  (2, batch_size, num_heads, past_seq_len, hidden_size/num_heads)
-        #    attention_mask: (batch_size, past_seq_len + seq_len)
-        # Shape of output tensors:
-        #    last_state: (batch_size, seq_len, hidden_size)
-        #      or logits: (batch_size, seq_len, vocab_size)
-        #    present_{i}:  (2, batch_size, num_heads, past_seq_len + seq_len, hidden_size/num_heads)
-        dynamic_axes = {'input_ids': {0: 'batch_size', 1: 'seq_len'}, output_names[0]: {0: 'batch_size', 1: 'seq_len'}}
-        for name in past_names:
-            dynamic_axes[name] = {1: 'batch_size', 3: 'past_seq_len'}
-        for name in present_names:
-            dynamic_axes[name] = {1: 'batch_size', 3: 'total_seq_len'}
-
-        input_names = ['input_ids']
-        if has_position_ids:
-            dynamic_axes['position_ids'] = {0: 'batch_size', 1: 'seq_len'}
-            input_names.append('position_ids')
-        if has_attention_mask:
-            dynamic_axes['attention_mask'] = {0: 'batch_size', 1: 'total_seq_len'}
-            input_names.append('attention_mask')
-        input_names.extend(past_names)
+        Args:
+            use_symbolic_shape_infer (bool, optional): use symbolic shape inference instead of onnx shape inference.
+                                                       Defaults to True.
+            keep_io_types (Union[bool, List[str]], optional): boolean or a list of float32 input/output names.
+                                                              If True, model inputs/outputs should be left as float32.
+                                                              Defaults to True.
+            op_block_list (List[str], optional): List of operator types to leave as float32.
+                                                 Defaults to None, which will use `float16.DEFAULT_OP_BLOCK_LIST`.
+            node_block_list (List[str], optional): List of node names to leave as float32. Defaults to None.
+            force_fp16_initializers(bool): force converting all float initializers to float16.
+                                           Default to false.
+            min_positive_val (float, optional): minimal positive value. Defaults to 1e-7.
+            max_finite_val (float, optional): maximal finite value. Defaults to 1e4.
+        """
+        if "keep_io_types" not in kwargs:
+            kwargs["keep_io_types"] = True
 
-        assert len(outputs) == 2 and len(outputs[1]) == num_layer
+        model = self.model
+        if use_symbolic_shape_infer:
+            # Use symbolic shape inference since custom operators (like Gelu, SkipLayerNormalization etc)
+            # are not recognized by onnx shape inference.
+            shape_infer_helper = SymbolicShapeInferenceHelper(model)
+            try:
+                model_with_shape = shape_infer_helper.infer_shapes(model, auto_merge=True, guess_output_rank=False)
+
+                # auto_merge might cause issue (see https://github.com/microsoft/onnxruntime/issues/15521)
+                # we only merge tensor data type but not shape information back to the original onnx model.
+                # Note that float16 conversion need data type but not shape information.
+                if model_with_shape is not None:
+                    name_vi = {}
+                    for vi in model_with_shape.graph.value_info:
+                        if (
+                            hasattr(vi.type, "tensor_type")
+                            and hasattr(vi.type.tensor_type, "elem_type")
+                            and vi.type.tensor_type.elem_type != TensorProto.UNDEFINED
+                            and vi.name
+                        ):
+                            vi_copy = ValueInfoProto()
+                            vi_copy.CopyFrom(vi)
+                            if hasattr(vi_copy.type.tensor_type, "shape"):
+                                vi_copy.type.tensor_type.ClearField("shape")
+                            name_vi[vi.name] = vi_copy
+                    for vi in model.graph.value_info:
+                        if vi.name in name_vi:
+                            del name_vi[vi.name]
+                    for _, vi in name_vi.items():
+                        model.graph.value_info.append(vi)
+            except Exception:
+                logger.warning(
+                    "Failed to run symbolic shape inference. Please file an issue in https://github.com/microsoft/onnxruntime."
+                )
 
-        logger.info(
-            f"Shapes: input_ids={dummy_inputs.input_ids.shape} past={dummy_inputs.past[0].shape} output={outputs[0].shape} present={outputs[1][0].shape}"
+        parameters = {"disable_shape_infer": use_symbolic_shape_infer}
+        parameters.update(
+            {
+                key: kwargs[key]
+                for key in [
+                    "keep_io_types",
+                    "min_positive_val",
+                    "max_finite_val",
+                    "op_block_list",
+                    "node_block_list",
+                    "force_fp16_initializers",
+                ]
+                if key in kwargs
+            }
         )
 
-        Path(onnx_model_path).parent.mkdir(parents=True, exist_ok=True)
-
-        torch_onnx_export(model,
-                          args=tuple(input_list),
-                          f=onnx_model_path,
-                          input_names=input_names,
-                          output_names=output_names,
-                          example_outputs=outputs,
-                          dynamic_axes=dynamic_axes,
-                          opset_version=11,
-                          do_constant_folding=True,
-                          use_external_data_format=use_external_data_format,
-                          verbose=verbose)
+        fp16_model = convert_float_to_float16(model, **parameters)
+        self.initialize(fp16_model)
 
-    @staticmethod
-    def optimize_onnx(onnx_model_path,
-                      optimized_model_path,
-                      is_float16,
-                      num_attention_heads,
-                      hidden_size,
-                      use_external_data_format=False,
-                      auto_mixed_precision=False,
-                      **kwargs):
-        """ Optimize ONNX model with an option to convert it to use mixed precision.
-        """
-        from optimizer import optimize_model
-
-        from fusion_options import FusionOptions
-        optimization_options = FusionOptions('gpt2')
-        #optimization_options.enable_gelu = False
-        #optimization_options.enable_layer_norm = False
-        #optimization_options.enable_attention = False
-        m = optimize_model(onnx_model_path,
-                           model_type='gpt2',
-                           num_heads=num_attention_heads,
-                           hidden_size=hidden_size,
-                           opt_level=0,
-                           optimization_options=optimization_options,
-                           use_gpu=False)
-
-        if is_float16:
-            if auto_mixed_precision:
-                Gpt2Helper.auto_mixed_precision(m)
-            else:
-                m.convert_float_to_float16(use_symbolic_shape_infer=True, **kwargs)
+        self.remove_cascaded_cast_nodes()
 
-        m.save_model_to_file(optimized_model_path, use_external_data_format)
+        self.remove_useless_cast_nodes()
 
-    @staticmethod
-    def auto_mixed_precision(onnx_model: OnnxModel,
-                             op_block_list: List[str] = ['Add', 'LayerNormalization', 'FastGelu']):
-        """Convert GPT-2 model to mixed precision.
-           It detects whether original model has fp16 precision weights, and set parameters for float16 conversion automatically.
+    def create_node_name(self, op_type, name_prefix=None):
+        """Create a unique node name that starts with a prefix (default is operator type).
+           The name will not be duplicated with any name that generated or existed in current graphs.
         Args:
-            onnx_model (OnnxModel): optimized ONNX model
-            op_block_list (List[str], optional): . Defaults to ['Add', 'LayerNormalization', 'FastGelu']
+            op_type (str): operator type
+            name_prefix (str, optional): prefix of node name. Defaults to None.
+
         Returns:
-            parameters(dict): a dictionary of parameters used in float16 conversion
+            str: node name
         """
-        op_full_set = set([node.op_type for node in onnx_model.nodes()])
-        fp32_op_set = set(op_block_list)
-        fp16_op_set = op_full_set.difference(fp32_op_set)
-        logger.info(f"fp32 op: {fp32_op_set} fp16 op: {fp16_op_set}")
-
-        # logits is the first output
-        logits_output_name = onnx_model.graph().output[0].name
-
-        # We use the weight in last MatMul node to detect whether the model is stored with float16 weights from training.
-        is_weight_fp16_precision = False
-        output_name_to_node = onnx_model.output_name_to_node()
-        assert logits_output_name in output_name_to_node
-        node = output_name_to_node[logits_output_name]
-        last_matmul_node = None
-        if node.op_type == "MatMul":
-            last_matmul_node = node
-            logger.info(f"Found last MatMul node for logits: {node.name}")
-            initializer = None
-            for input in node.input:
-                initializer = onnx_model.get_initializer(input)
-                if initializer is not None:
-                    break
-
-            # when the max difference of value after converting float to float16 is lower than a threshold (1e-6),
-            # we can deduce that the weights are stored in float16 precision.
-            max_diff = float_to_float16_max_diff(initializer)
-            logger.debug(f"max diff of converting weights in last MatMul node {node.name}: {max_diff}")
-            is_weight_fp16_precision = (max_diff < 1E-6)
+
+        if name_prefix:
+            prefix = name_prefix if name_prefix.endswith("_") else (name_prefix + "_")
         else:
-            logger.warning(f"Failed to find MatMul node for logits. Found {node.op_type} of node {node.name}")
+            prefix = op_type + "_"
 
-        if is_weight_fp16_precision:
-            keep_io_types = []
-            node_block_list = []
+        suffix: int = 0
+        if prefix in self._node_name_suffix:
+            suffix = self._node_name_suffix[prefix] + 1
         else:
-            # When original weight is float32 precision, keep logits and last MatMul in float32 could get better precision.
-            keep_io_types = [logits_output_name]
-            node_block_list = [last_matmul_node.name]
-
-        parameters = {
-            "keep_io_types": keep_io_types,
-            "op_block_list": op_block_list,
-            "node_block_list": node_block_list,
-            "force_fp16_initializers": is_weight_fp16_precision
-        }
-
-        logger.info(f"auto_mixed_precision parameters: {parameters}")
-        onnx_model.convert_float_to_float16(use_symbolic_shape_infer=True, **parameters)
-
-        fusion_utils = FusionUtils(onnx_model)
-        fusion_utils.remove_cascaded_cast_nodes()
-        fusion_utils.remove_useless_cast_nodes()
+            # Check existed node name only once for a prefix
+            # as we assume create_node_name is called for every new node in fusion.
+            for node in self.nodes():
+                if node.name and node.name.startswith(prefix):
+                    try:
+                        index = int(node.name[len(prefix) :])
+                        suffix = max(index + 1, suffix)
+                    except ValueError:
+                        continue
+
+        # Record the generated suffix so that we can avoid generating duplicated name.
+        self._node_name_suffix[prefix] = suffix
+
+        return prefix + str(suffix)
+
+    def find_graph_input(self, input_name):
+        for input in self.model.graph.input:
+            if input.name == input_name:
+                return input
+        return None
+
+    def find_graph_output(self, output_name):
+        for output in self.model.graph.output:
+            if output.name == output_name:
+                return output
+        return None
+
+    def get_parent_subgraph_nodes(self, node, stop_nodes, output_name_to_node=None):
+        if output_name_to_node is None:
+            output_name_to_node = self.output_name_to_node()
+
+        unique_nodes = []
+
+        parents = self.get_parents(node, output_name_to_node)
+        dq = deque(parents)
+        while len(dq) > 0:
+            current_node = dq.pop()
+            if current_node in stop_nodes:
+                continue
+
+            if current_node not in unique_nodes:
+                unique_nodes.append(current_node)
+
+                for input in current_node.input:
+                    if input in output_name_to_node:
+                        dq.appendleft(output_name_to_node[input])
 
-        return parameters
+        return unique_nodes
 
-    @staticmethod
-    def pytorch_inference(model, inputs: Gpt2Inputs, total_runs: int = 0):
-        """ Run inference of PyTorch model, and returns average latency in ms when total_runs > 0 besides outputs.
+    def get_graph_inputs(self, current_node, recursive=False):
         """
-        logger.debug("start pytorch_inference")
-
-        # Convert it to fp32 as the PyTroch model cannot deal with half input.
-        input_list = inputs.to_fp32().to_list()
-
-        with torch.no_grad():
-            outputs = model(*input_list)
-
-        if total_runs == 0:
-            return outputs
-
-        latency = []
-        with torch.no_grad():
-            for _ in range(total_runs):
-                start = time.time()
-                outputs = model(*input_list)
-                latency.append(time.time() - start)
+        Find graph inputs that linked to current node.
+        """
+        graph_inputs = []
+        for input in current_node.input:
+            if self.find_graph_input(input) and input not in graph_inputs:
+                graph_inputs.append(input)
+
+        if recursive:
+            parent_nodes = self.get_parent_subgraph_nodes(current_node, [])
+            for node in parent_nodes:
+                for input in node.input:
+                    if self.find_graph_input(input) and input not in graph_inputs:
+                        graph_inputs.append(input)
+        return graph_inputs
+
+    @staticmethod
+    def input_index(node_output, child_node):
+        index = 0
+        for input in child_node.input:
+            if input == node_output:
+                return index
+            index += 1
+        return -1
+
+    def remove_unused_constant(self):
+        input_name_to_nodes = self.input_name_to_nodes()
+
+        # remove unused constant
+        unused_nodes = []
+        nodes = self.nodes()
+        for node in nodes:
+            if node.op_type == "Constant" and node.output[0] not in input_name_to_nodes:
+                unused_nodes.append(node)
 
-        average_latency = sum(latency) * 1000 / len(latency)
-        logger.debug("PyTorch inference time = {} ms".format(format(average_latency, '.2f')))
+        self.remove_nodes(unused_nodes)
 
-        return outputs, average_latency
+        if len(unused_nodes) > 0:
+            logger.debug(f"Removed unused constant nodes: {len(unused_nodes)}")
 
-    @staticmethod
-    def onnxruntime_inference(ort_session, inputs: Gpt2Inputs, total_runs: int = 0):
-        """ Run inference of ONNX model, and returns average latency in ms when total_runs > 0 besides outputs.
+    def prune_graph(self, outputs=None, allow_remove_graph_inputs=True):
         """
-        logger.debug(f"start onnxruntime_inference")
+        Prune graph to keep only required outputs. It removes unnecessary nodes that are not linked
+        (directly or indirectly) to any required output.
 
-        ort_inputs = {'input_ids': numpy.ascontiguousarray(inputs.input_ids.cpu().numpy())}
+        There is also an option to remove graph inputs that are not used to generate any required output.
 
-        if inputs.past is not None:
-            for i, past_i in enumerate(inputs.past):
-                ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())
+        Args:
+            outputs (list): a list of graph outputs to retain. If it is None, all graph outputs will be kept.
+            allow_remove_graph_inputs (bool): allow remove graph inputs.
+        """
 
-        if inputs.attention_mask is not None:
-            ort_inputs['attention_mask'] = numpy.ascontiguousarray(inputs.attention_mask.cpu().numpy())
+        if len(self.graphs()) > 1:
+            logger.debug("Skip prune_graph since graph has subgraph")
+            return
+
+        if outputs is None:
+            outputs = [output.name for output in self.model.graph.output]
+
+        output_name_to_node = self.output_name_to_node()
+        all_nodes = []
+        for output in outputs:
+            if output in output_name_to_node:
+                last_node = output_name_to_node[output]
+                if last_node in all_nodes:
+                    continue
+                nodes = self.get_parent_subgraph_nodes(last_node, [])
+                all_nodes.append(last_node)
+                all_nodes.extend(nodes)
+
+        nodes_to_remove = []
+        for node in self.model.graph.node:
+            if node not in all_nodes:
+                nodes_to_remove.append(node)
+
+        self.remove_nodes(nodes_to_remove)
+
+        # remove outputs not in list
+        output_to_remove = []
+        for output in self.model.graph.output:
+            if output.name not in outputs:
+                output_to_remove.append(output)
+        for output in output_to_remove:
+            self.model.graph.output.remove(output)
+
+        # remove inputs not used by any node.
+        input_to_remove = []
+        if allow_remove_graph_inputs:
+            input_name_to_nodes = self.input_name_to_nodes()
+            for input in self.model.graph.input:
+                if input.name not in input_name_to_nodes:
+                    input_to_remove.append(input)
+            for input in input_to_remove:
+                self.model.graph.input.remove(input)
+
+        if input_to_remove or output_to_remove or nodes_to_remove:
+            removed = []
+            if input_to_remove:
+                removed.append(f"{len(input_to_remove)} inputs")
+            if output_to_remove:
+                removed.append(f"{len(output_to_remove)} outputs")
+            if nodes_to_remove:
+                removed.append(f"{len(nodes_to_remove)} nodes")
+            logger.info("Removed %s", ", ".join(removed))
+
+        self.update_graph()
+
+    def update_graph(self, verbose=False, allow_remove_graph_inputs=False):
+        graph = self.model.graph
+
+        remaining_input_names = []
+        for node in graph.node:
+            if node.op_type in ["Loop", "Scan", "If"]:
+                # TODO: handle inner graph
+                logger.debug(f"Skip update_graph since graph has operator: {node.op_type}")
+                return
+            if node.op_type != "Constant":
+                for input_name in node.input:
+                    if input_name not in remaining_input_names:
+                        remaining_input_names.append(input_name)
+        if verbose:
+            logger.debug(f"remaining input names: {remaining_input_names}")
+
+        # remove graph input that is not used
+        inputs_to_remove = []
+        if allow_remove_graph_inputs:
+            for input in graph.input:
+                if input.name not in remaining_input_names:
+                    inputs_to_remove.append(input)
+            for input in inputs_to_remove:
+                graph.input.remove(input)
+
+        names_to_remove = [input.name for input in inputs_to_remove]
+        logger.debug(f"remove {len(inputs_to_remove)} unused inputs: {names_to_remove}")
+
+        # remove weights that are not used
+        weights_to_remove = []
+        weights_to_keep = []
+        for initializer in graph.initializer:
+            if initializer.name not in remaining_input_names and not self.find_graph_output(initializer.name):
+                weights_to_remove.append(initializer)
+            else:
+                weights_to_keep.append(initializer.name)
+        for initializer in weights_to_remove:
+            graph.initializer.remove(initializer)
+
+        names_to_remove = [initializer.name for initializer in weights_to_remove]
+        logger.debug(f"remove {len(weights_to_remove)} unused initializers: {names_to_remove}")
+        if verbose:
+            logger.debug(f"remaining initializers:{weights_to_keep}")
+
+        self.remove_unused_constant()
+
+    def is_safe_to_fuse_nodes(self, nodes_to_remove, keep_outputs, input_name_to_nodes, output_name_to_node):
+        for node_to_remove in nodes_to_remove:
+            for output_to_remove in node_to_remove.output:
+                if output_to_remove in keep_outputs:
+                    continue
+
+                if output_to_remove in input_name_to_nodes:
+                    for impacted_node in input_name_to_nodes[output_to_remove]:
+                        if impacted_node not in nodes_to_remove:
+                            logger.debug(
+                                "it is not safe to remove nodes since output %s is used by %s",
+                                output_to_remove,
+                                impacted_node,
+                            )
+                            return False
+        return True
+
+    @staticmethod
+    def graph_topological_sort(graph, is_deterministic=False):
+        deps_set = set()  # dependency set of all node
+        sorted_node_set = set()  # sorted node set
+        sorted_nodes = []  # initialize sorted_nodes
+
+        initializer_names = [init.name for init in graph.initializer]
+        graph_input_names = [input.name for input in graph.input]
+        input_names = initializer_names + graph_input_names
+
+        if is_deterministic:
+            input_names.sort()
+
+        for input_name in input_names:
+            deps_set.add(input_name)
+
+        sorted_node_set_len = -1
+        graph_nodes = graph.node if not is_deterministic else sorted(graph.node, key=lambda x: x.name)
+        last_node_name = None
+        while len(sorted_node_set) != len(graph_nodes):
+            if len(sorted_node_set) == sorted_node_set_len:
+                break
+            sorted_node_set_len = len(sorted_node_set)
+            for node_idx, node in enumerate(graph_nodes):
+                if node_idx in sorted_node_set:
+                    continue
+                input_count = sum(1 for _ in node.input if _)
+                if input_count == 0:
+                    sorted_nodes.append(node)
+                    sorted_node_set.add(node_idx)
+                    for output in node.output:
+                        deps_set.add(output)
+                    continue
+                failed = False
+                for input_name in node.input:
+                    if input_name and input_name not in deps_set:
+                        failed = True
+                        last_node_name = node.name
+                if not failed:
+                    sorted_nodes.append(node)
+                    sorted_node_set.add(node_idx)
+                    for output in node.output:
+                        deps_set.add(output)
+                else:
+                    continue
+
+        if len(sorted_node_set) != len(graph.node):
+            raise RuntimeError(
+                f"Graph is not a DAG: len(sorted_node_set)={len(sorted_node_set)}, len(graph.node)={len(graph.node)}, failed at node {last_node_name}"
+            )
+
+        graph.ClearField("node")
+        graph.node.extend(sorted_nodes)
+
+    def topological_sort(self, is_deterministic=False):
+        # TODO: support graph_topological_sort() in subgraphs
+        # for graph in self.graphs():
+        #    self.graph_topological_sort(graph)
+        OnnxModel.graph_topological_sort(self.model.graph, is_deterministic)
+
+    @staticmethod
+    def save(
+        model,
+        output_path,
+        save_as_external_data=False,
+        all_tensors_to_one_file=True,
+        size_threshold=1024,
+        convert_attribute=False,
+    ):
+        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
+
+        # Add ms domain if needed
+        ms_opset = [opset for opset in model.opset_import if opset.domain == "com.microsoft"]
+        # Check whether there is custom op in top level graph (our fusion is on top level right now).
+        # May need to extend to subgraph if our fusion are extended to subgraphs.
+        ms_node = [node for node in model.graph.node if node.domain == "com.microsoft"]
+        if ms_node and not ms_opset:
+            opset = model.opset_import.add()
+            opset.version = 1
+            opset.domain = "com.microsoft"
+
+        if save_as_external_data:
+            # Save model to external data, which is needed for model size > 2GB
+            output_dir = Path(output_path).parent
+            output_dir.mkdir(parents=True, exist_ok=True)
+            external_data_path = output_path + ".data"
+            location = Path(external_data_path).name if all_tensors_to_one_file else None
+
+            if os.path.exists(output_path):
+                logger.info(f"Delete the existed onnx file: {output_path}")
+                os.remove(output_path)
+
+            if all_tensors_to_one_file:
+                if os.path.exists(external_data_path):
+                    # Delete the external data file. Otherwise, data will be appended to existing file.
+                    logger.info(f"Delete the existed external data file: {external_data_path}")
+                    os.remove(external_data_path)
+            else:
+                if os.listdir(output_dir):
+                    raise RuntimeError(f"Output directory ({output_dir}) for external data is not empty.")
 
-        if inputs.position_ids is not None:
-            ort_inputs['position_ids'] = numpy.ascontiguousarray(inputs.position_ids.cpu().numpy())
+            save_model(
+                model,
+                output_path,
+                save_as_external_data=True,
+                all_tensors_to_one_file=all_tensors_to_one_file,
+                location=location,
+                size_threshold=size_threshold,
+                convert_attribute=convert_attribute,
+            )
+        else:
+            save_model(model, output_path)
 
-        ort_outputs = ort_session.run(None, ort_inputs)
-        if total_runs == 0:
-            return ort_outputs
+    def save_model_to_file(self, output_path, use_external_data_format=False, all_tensors_to_one_file=True):
+        logger.info("Sort graphs in topological order")
+        self.topological_sort()
+
+        # Note: After the model is saved to another directory with external data,
+        #       You need reload the onnx model if you want to read tensor from self.model object.
+        #       It is because the base directory is not updated for self.model object so attempt to read tensor data
+        #       might encounter error since external data cannot be located.
+        OnnxModel.save(self.model, output_path, use_external_data_format, all_tensors_to_one_file)
+        logger.info(f"Model saved to {output_path}")
 
-        latency = []
-        for _ in range(total_runs):
-            start = time.time()
-            ort_outputs = ort_session.run(None, ort_inputs)
-            latency.append(time.time() - start)
+    def get_graph_inputs_excluding_initializers(self):
+        """
+        Returns real graph inputs (excluding initializers from older onnx model).
+        """
+        graph_inputs = []
+        for input in self.model.graph.input:
+            if self.get_initializer(input.name) is None:
+                graph_inputs.append(input)
+        return graph_inputs
 
-        average_latency = sum(latency) * 1000 / len(latency)
-        logger.debug("OnnxRuntime Inference time = {} ms".format(format(average_latency, '.2f')))
+    def get_opset_version(self):
+        """Get opset version of onnx domain
 
-        return ort_outputs, average_latency
+        Raises:
+            RuntimeError: ONNX model has no opset for default domain.
 
-    @staticmethod
-    def prepare_io_binding(ort_session, input_ids, position_ids, attention_mask, past, output_buffers, output_shapes):
-        """ Returnas IO binding object for a session.
+        Returns:
+            int: opset version of onnx domain.
         """
-        return IOBindingHelper.prepare_io_binding(ort_session, input_ids, position_ids, attention_mask, past,
-                                                  output_buffers, output_shapes)
+        for opset in self.model.opset_import:
+            if opset.domain in ["", "ai.onnx"]:
+                return opset.version
+        raise RuntimeError("ONNX model has no opset for default domain")
 
-    @staticmethod
-    def get_outputs_from_io_binding_buffer(ort_session, output_buffers, output_shapes, return_numpy=True):
-        """ Copy results to cpu. Returns a list of numpy array.
+    def get_operator_statistics(self, include_domain=False):
         """
-        return IOBindingHelper.get_outputs_from_io_binding_buffer(ort_session, output_buffers, output_shapes,
-                                                                  return_numpy)
-
-    @staticmethod
-    def onnxruntime_inference_with_binded_io(ort_session,
-                                             inputs: Gpt2Inputs,
-                                             output_buffers: Dict[str, torch.Tensor],
-                                             output_shapes: Dict[str, List[int]],
-                                             total_runs: int = 0,
-                                             return_numpy: bool = True,
-                                             include_copy_output_latency: bool = False):
-        """ Inference with IO binding. Returns outputs, and optional latency when total_runs > 0.
-        """
-        logger.debug(f"start onnxruntime_inference_with_binded_io")
-
-        # Bind inputs and outputs to onnxruntime session
-        io_binding = Gpt2Helper.prepare_io_binding(ort_session, inputs.input_ids, inputs.position_ids,
-                                                   inputs.attention_mask, inputs.past, output_buffers, output_shapes)
-
-        # Run onnxruntime with io binding
-        ort_session.run_with_iobinding(io_binding)
-
-        # Copy results to cpu for verification
-        ort_outputs = Gpt2Helper.get_outputs_from_io_binding_buffer(ort_session, output_buffers, output_shapes,
-                                                                    return_numpy)
-
-        if total_runs == 0:
-            return ort_outputs
-
-        latency = []
-        for _ in range(total_runs):
-            start = time.time()
-            # Run onnxruntime with io binding
-            ort_session.run_with_iobinding(io_binding)
-            if include_copy_output_latency:
-                _ = Gpt2Helper.get_outputs_from_io_binding_buffer(ort_session, output_buffers, output_shapes,
-                                                                  return_numpy)
-            latency.append(time.time() - start)
-
-        average_latency = sum(latency) * 1000 / len(latency)
-        logger.debug("OnnxRuntime with IO binding inference time = {} ms".format(format(average_latency, '.2f')))
-
-        return ort_outputs, average_latency
+        Returns node count of operators.
+        """
+        op_count = {}
+        for node in self.nodes():
+            op = (node.domain + ":" if include_domain and node.domain else "") + node.op_type
+            op_count[op] = 1 if op not in op_count else (op_count[op] + 1)
+
+        logger.info(f"Operators:{op_count}")
+        return op_count
 
     @staticmethod
-    def save_outputs(i, ort_outputs, torch_outputs):
-        with open(f'ort_outputs_{i}.pickle', 'wb') as f:
-            pickle.dump(ort_outputs, f)
-        logger.info(f"ORT output are saved to ort_outputs_{i}.pickle")
-
-        with open(f'torch_outputs_{i}.pickle', 'wb') as f:
-            pickle.dump(torch_outputs, f)
-        logger.info(f"Torch output are saved to torch_outputs_{i}.pickle")
+    def has_same_value(tensor1: TensorProto, tensor2: TensorProto) -> bool:
+        """Returns True when two tensors have same value.
+           Note that name can be different.
 
-    @staticmethod
-    def save_inputs(i, dummy_inputs, ort_outputs, torch_outputs):
-        with open(f'dummy_inputs_{i}.pickle', 'wb') as f:
-            pickle.dump(dummy_inputs, f)
-        logger.info(f"inputs are saved to dummy_inputs_{i}.pickle")
+        Args:
+            tensor1 (TensorProto): initializer 1
+            tensor2 (TensorProto): initializer 2
 
-    @staticmethod
-    def test_parity(ort_session,
-                    model,
-                    device,
-                    is_float16=False,
-                    rtol=5e-4,
-                    atol=5e-4,
-                    test_cases_per_run=10000,
-                    total_runs=1,
-                    use_io_binding=True,
-                    model_class="GPT2LMHeadModel",
-                    has_position_ids=True,
-                    has_attention_mask=True,
-                    input_ids_dtype=torch.int32,
-                    position_ids_dtype=torch.int32,
-                    attention_mask_dtype=torch.int32,
-                    verbose=False,
-                    enable_pickle_output=False):
-        """ Generate random inputs and compare the results of PyTorch and Onnx Runtime.
+        Returns:
+            bool: True when two intializers has same value.
         """
+        if tensor1.data_type != tensor2.data_type or tensor1.dims != tensor2.dims:
+            return False
+        if tensor1.HasField("raw_data") and tensor2.HasField("raw_data"):
+            return tensor1.raw_data == tensor2.raw_data
+        return (numpy_helper.to_array(tensor1) == numpy_helper.to_array(tensor2)).all()
+
+    def remove_duplicated_initializer(self):
+        """Remove initializers with duplicated values, and only keep the first one.
+        It could help reduce size of models (like ALBert) with shared weights.
+        Note: this function does not process subgraph.
+        """
+        if len(self.graphs()) > 1:
+            logger.warning("remove_duplicated_initializer does not process subgraphs.")
 
-        config: GPT2Config = model.config
-
-        logger.info(
-            f"Running parity test (atol={atol}, test_cases={test_cases_per_run}, runs={total_runs}, use_io_binding={use_io_binding}, model_class={model_class}, is_float16={is_float16}) ..."
-        )
+        initializer_count = len(self.model.graph.initializer)
 
-        max_batch_size = 8
-        max_past_seq_len = 4  # Do not use large number here for higher chance of hitting empty past (past_seq_len=0)
-        max_seq_len = 2
-
-        output_buffers = None
-        if use_io_binding:
-            max_output_shapes = Gpt2Helper.get_output_shapes(max_batch_size, max_past_seq_len, max_seq_len, config,
-                                                             model_class)
-            output_buffers = Gpt2Helper.get_output_buffers(max_output_shapes, device, is_float16)
-
-        passed_test_cases = 0
-        top1_matched_cases = 0
-
-        max_abs_diff_list = []
-        top1_matched_cases_per_run = [0] * total_runs
-        total_test_cases = test_cases_per_run * total_runs
-        for i in range(total_test_cases):
-            run_id = int(i / test_cases_per_run)
-            sequence_length = random.randint(1, max_seq_len)
-            past_sequence_length = random.randint(0, max_past_seq_len)
-            batch_size = random.randint(1, max_batch_size)
-
-            logger.debug(
-                f"Running parity test for batch_size={batch_size} past_sequence_length={past_sequence_length}...")
-            dummy_inputs = Gpt2Helper.get_dummy_inputs(batch_size,
-                                                       past_sequence_length,
-                                                       sequence_length,
-                                                       config.num_attention_heads,
-                                                       config.hidden_size,
-                                                       config.n_layer,
-                                                       config.vocab_size,
-                                                       device,
-                                                       is_float16,
-                                                       has_position_ids,
-                                                       has_attention_mask,
-                                                       input_ids_dtype=input_ids_dtype,
-                                                       position_ids_dtype=position_ids_dtype,
-                                                       attention_mask_dtype=attention_mask_dtype)
-            outputs = Gpt2Helper.pytorch_inference(model, dummy_inputs)
-            if use_io_binding:
-                ort_outputs = Gpt2Helper.onnxruntime_inference(ort_session, dummy_inputs)
-            else:
-                output_shapes = Gpt2Helper.get_output_shapes(batch_size, past_sequence_length, sequence_length, config,
-                                                             model_class)
-                ort_outputs = Gpt2Helper.onnxruntime_inference_with_binded_io(ort_session, dummy_inputs, output_buffers,
-                                                                              output_shapes)
-
-            is_all_close, max_abs_diff, max_diff_output_index, messages, is_top1_matched = Gpt2Helper.compare_outputs_v2(
-                outputs, ort_outputs, atol=atol)
-            if not numpy.isnan(max_abs_diff):
-                max_abs_diff_list.append(max_abs_diff)
-            if is_all_close:
-                passed_test_cases += 1
-            if is_top1_matched:
-                top1_matched_cases += 1
-                top1_matched_cases_per_run[run_id] += 1
-
-            if verbose and not is_all_close:
-                logger.info(
-                    f"test_case={i} batch_size={batch_size} past_sequence_length={past_sequence_length} sequence_length={sequence_length} MaxDiff={max_abs_diff}"
+        same = [-1] * initializer_count
+        for i in range(initializer_count - 1):
+            if same[i] >= 0:
+                continue
+            for j in range(i + 1, initializer_count):
+                if OnnxModel.has_same_value(self.model.graph.initializer[i], self.model.graph.initializer[j]):
+                    same[j] = i
+
+        count = 0
+        for i in range(initializer_count):
+            if same[i] >= 0:
+                count += 1
+                self.replace_input_of_all_nodes(
+                    self.model.graph.initializer[i].name, self.model.graph.initializer[same[i]].name
                 )
-                for i, message in enumerate(messages):
-                    logger.info(f"\t{i}: Name={ort_session.get_outputs()[i].name}, {message}")
-
-            # Collect data for debugging
-            if enable_pickle_output and (numpy.isnan(max_abs_diff) or max_abs_diff > 100 * atol):
-                Gpt2Helper.save_inputs(i, dummy_inputs)
-                Gpt2Helper.save_outputs(i, ort_outputs, outputs)
-
-        if max_abs_diff_list:
-            result = {
-                f"max_diff_percentile_{p}": "{:.5f}".format(numpy.percentile(max_abs_diff_list, p))
-                for p in [50, 90, 95, 99]
-            }
-        else:
-            result = {f"max_diff_percentile_{p}": "nan" for p in [50, 90, 95, 99]}
-
-        result["top1_match_rate"] = top1_matched_cases * 1.0 / total_test_cases
-        result["top1_match_rate_per_run"] = [x * 1.0 / test_cases_per_run for x in top1_matched_cases_per_run]
-        result["diff_pass_rate"] = passed_test_cases * 1.0 / total_test_cases
-        result["nan_rate"] = (total_test_cases - len(max_abs_diff_list)) * 1.0 / total_test_cases
-
-        logger.info(
-            f"Parity Test Cases={total_test_cases}; Passed={passed_test_cases}; Nan={total_test_cases-len(max_abs_diff_list)}; Top1_Matched={top1_matched_cases}"
-        )
 
-        if passed_test_cases > 0.95 * total_test_cases:
-            logger.info(f"Parity is good: passed rate={int(passed_test_cases*100/total_test_cases):.0f}%")
-
-        return result
-
-    @staticmethod
-    def test_performance(ort_session,
-                         model,
-                         device,
-                         is_float16=False,
-                         total_runs=100,
-                         use_io_binding=True,
-                         model_class="GPT2LMHeadModel",
-                         has_position_ids=True,
-                         has_attention_mask=True,
-                         input_ids_dtype=torch.int32,
-                         position_ids_dtype=torch.int32,
-                         attention_mask_dtype=torch.int32,
-                         batch_size=8,
-                         sequence_length=1,
-                         past_sequence_length=32):
-        """ Generate random inputs and measure average latency of Onnx Runtime.
-        """
-
-        config: GPT2Config = model.config
-
-        output_buffers = None
-        if use_io_binding:
-            output_shapes = Gpt2Helper.get_output_shapes(batch_size, past_sequence_length, sequence_length, config,
-                                                         model_class)
-            output_buffers = Gpt2Helper.get_output_buffers(output_shapes, device, is_float16)
-
-        dummy_inputs = Gpt2Helper.get_dummy_inputs(batch_size,
-                                                   past_sequence_length,
-                                                   sequence_length,
-                                                   config.num_attention_heads,
-                                                   config.hidden_size,
-                                                   config.n_layer,
-                                                   config.vocab_size,
-                                                   device,
-                                                   is_float16,
-                                                   has_position_ids,
-                                                   has_attention_mask,
-                                                   input_ids_dtype=input_ids_dtype,
-                                                   position_ids_dtype=position_ids_dtype,
-                                                   attention_mask_dtype=attention_mask_dtype)
-
-        if use_io_binding:
-            _, latency = Gpt2Helper.onnxruntime_inference(ort_session, dummy_inputs, total_runs)
-        else:
-            _, latency = Gpt2Helper.onnxruntime_inference_with_binded_io(ort_session, dummy_inputs, output_buffers,
-                                                                         output_shapes, total_runs)
-
-        return latency
-
-    @staticmethod
-    def torchscript(model, config, device, has_position_ids=True, has_attention_mask=True):
-        """ JIT trace for TorchScript.
+        if count > 0:
+            self.update_graph()
+            print(f"Removed {count} initializers with duplicated value")
+
+    def add_prefix_to_names(self, prefix: str):
+        """Add prefix to initializer or intermediate outputs in graph. Main graph inputs and outputs are excluded.
+        It could help avoid conflicting in name of node_args when merging two graphs.
+        Note: this function does not process subgraph.
         """
-        input_list = Gpt2Helper.get_dummy_inputs(batch_size=1,
-                                                 past_sequence_length=1,
-                                                 sequence_length=1,
-                                                 num_attention_heads=config.num_attention_heads,
-                                                 hidden_size=config.hidden_size,
-                                                 num_layer=config.n_layer,
-                                                 vocab_size=config.vocab_size,
-                                                 device=device,
-                                                 float16=False,
-                                                 has_position_ids=has_position_ids,
-                                                 has_attention_mask=has_attention_mask).to_list()
-        return torch.jit.trace(model, input_list)
+        if len(self.graphs()) > 1:
+            logger.warning("add_prefix_to_names does not process subgraphs.")
 
-    @staticmethod
-    def get_onnx_paths(output_dir,
-                       model_name_or_path,
-                       model_class: str = 'GPT2LMHeadModel',
-                       has_past=True,
-                       new_folder=False,
-                       remove_existing=["raw", "fp32", "fp16", "int8"]):
-        """ Build a  path name for given model based on given attributes.
-        """
-        model_name = model_name_or_path
-        if os.path.isdir(model_name_or_path):
-            model_name = Path(model_name_or_path).parts[-1]
-        else:
-            model_name.split('/')[-1]
-
-        if model_class != 'GPT2LMHeadModel':
-            model_name += "_" + model_class
-
-        if has_past:
-            model_name += "_past"
-
-        if new_folder:
-            suffix = {"raw": "", "fp32": "_fp32", "fp16": "_fp16", "int8": "_int8"}
-            # Remove the directories if existed.
-            for model_type in ["raw", "fp32", "fp16", "int8"]:
-                new_dir = os.path.join(output_dir, model_name + suffix[model_type])
-                if os.path.exists(new_dir):
-                    if (model_type in remove_existing):
-                        try:
-                            shutil.rmtree(new_dir)
-                            logger.info(f"Removed the existed directory: {new_dir}")
-                        except OSError as e:
-                            logger.info(f"Failed to remove the directory {new_dir}: {e.strerror}")
-                    else:
-                        logger.info(f"Directory for {model_type} existed: {new_dir}")
-
-            # store each model to its own directory (for external data format).
-            return {
-                "raw": os.path.join(os.path.join(output_dir, model_name), model_name + ".onnx"),
-                "fp32": os.path.join(os.path.join(output_dir, model_name + "_fp32"), model_name + "_fp32.onnx"),
-                "fp16": os.path.join(os.path.join(output_dir, model_name + "_fp16"), model_name + "_fp16.onnx"),
-                "int8": os.path.join(os.path.join(output_dir, model_name + "_int8"), model_name + "_int8.onnx")
-            }
+        # Exclude the names of inputs and outputs of main graph (but not subgraphs)
+        # and empty names ("") as they have special meaning to denote missing optional inputs
+        excluded = [i.name for i in self.model.graph.input] + [o.name for o in self.model.graph.output] + [""]
+
+        for initializer in self.model.graph.initializer:
+            if initializer.name not in excluded:
+                if prefix + initializer.name not in excluded:
+                    initializer.name = prefix + initializer.name
+
+        for node in self.model.graph.node:
+            # update name of node inputs
+            for j in range(len(node.input)):
+                if node.input[j] not in excluded:
+                    if prefix + node.input[j] not in excluded:
+                        node.input[j] = prefix + node.input[j]
+
+            # update name of node outputs
+            for j in range(len(node.output)):
+                if node.output[j] not in excluded:
+                    if prefix + node.output[j] not in excluded:
+                        node.output[j] = prefix + node.output[j]
+
+        for value_info in self.model.graph.value_info:
+            if value_info.name not in excluded:
+                value_info.name = prefix + value_info.name
 
-        return {
-            "raw": os.path.join(output_dir, model_name + ".onnx"),
-            "fp32": os.path.join(output_dir, model_name + "_fp32.onnx"),
-            "fp16": os.path.join(output_dir, model_name + "_fp16.onnx"),
-            "int8": os.path.join(output_dir, model_name + "_int8.onnx")
-        }
+    def clean_shape_infer(self):
+        self.model.graph.ClearField("value_info")
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_parity.py` & `onnxruntime/transformers/models/gpt2/gpt2_parity.py`

 * *Files 23% similar despite different names*

```diff
@@ -6,125 +6,174 @@
 
 # This script uses different configurations in mixed precision conversion for GPT-2 model, and
 # measures the inference latency, top 1 match rate (compared to PyTorch FP32 model) and ONNX model size.
 # It outputs a csv file with Mann-Whitney U test and T-Test on each pair of experiments, where
 # pvalue < 0.05 means two experiments have significant difference on top 1 match rate.
 # User could use this script to select the best mixed precision model according to these metrics.
 
-from convert_to_onnx import main, get_latency_name
-import os
 import argparse
-import logging
-from gpt2_helper import PRETRAINED_GPT2_MODELS, Gpt2Helper
-from benchmark_helper import setup_logger
-from onnx_model import OnnxModel
-import onnx
 import csv
 import datetime
+import json
+import logging
+import os
+import sys
+
+import onnx
 import scipy.stats
-import torch
+from convert_to_onnx import main
+from gpt2_helper import PRETRAINED_GPT2_MODELS, Gpt2Helper
+from onnx_model import OnnxModel
 
-logger = logging.getLogger('')
+sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
+
+from benchmark_helper import get_ort_environment_variables, setup_logger  # noqa: E402
+
+logger = logging.getLogger("")
 
 
 def parse_arguments(argv=None):
     parser = argparse.ArgumentParser()
 
-    parser.add_argument('-m',
-                        '--model_name_or_path',
-                        required=True,
-                        type=str,
-                        help='Model path, or pretrained model name in the list: ' + ', '.join(PRETRAINED_GPT2_MODELS))
-
-    parser.add_argument('--csv',
-                        required=False,
-                        type=str,
-                        default='gpt2_parity_results.csv',
-                        help='path of csv file to save the result')
-
-    parser.add_argument('--test_cases', required=False, type=int, default=500, help="number of test cases per run")
+    parser.add_argument(
+        "-m",
+        "--model_name_or_path",
+        required=True,
+        type=str,
+        help="Model path, or pretrained model name in the list: " + ", ".join(PRETRAINED_GPT2_MODELS),
+    )
+
+    parser.add_argument(
+        "--csv",
+        required=False,
+        type=str,
+        default="gpt2_parity_results.csv",
+        help="path of csv file to save the result",
+    )
+
+    parser.add_argument(
+        "--test_cases",
+        required=False,
+        type=int,
+        default=500,
+        help="number of test cases per run",
+    )
 
-    parser.add_argument('--runs', required=False, type=int, default=40, help="number of repeated runs")
+    parser.add_argument("--runs", required=False, type=int, default=40, help="number of repeated runs")
 
-    parser.add_argument('--use_gpu', required=False, action='store_true', help="use GPU for inference")
+    parser.add_argument("--use_gpu", required=False, action="store_true", help="use GPU for inference")
     parser.set_defaults(use_gpu=False)
 
-    parser.add_argument('--all', required=False, action='store_true', help="run all combinations of mixed precision")
+    parser.add_argument(
+        "--all",
+        required=False,
+        action="store_true",
+        help="run all combinations of mixed precision",
+    )
     parser.set_defaults(all=False)
 
-    parser.add_argument('-e', '--use_external_data_format', required=False, action='store_true')
+    parser.add_argument("-e", "--use_external_data_format", required=False, action="store_true")
     parser.set_defaults(use_external_data_format=False)
 
-    parser.add_argument('--verbose', required=False, action='store_true')
+    parser.add_argument("--verbose", required=False, action="store_true")
     parser.set_defaults(verbose=False)
 
-    parser.add_argument('--skip_test',
-                        required=False,
-                        action='store_true',
-                        help="do not run test, and only rank experiments based on existing csv file")
+    parser.add_argument(
+        "--skip_test",
+        required=False,
+        action="store_true",
+        help="do not run test, and only rank experiments based on existing csv file",
+    )
     parser.set_defaults(skip_test=False)
 
+    parser.add_argument(
+        "--overwrite",
+        required=False,
+        action="store_true",
+        help="Overwrite existing csv file",
+    )
+    parser.set_defaults(overwrite=False)
+
     args = parser.parse_args(argv)
 
     return args
 
 
 class ParityTask:
     def __init__(self, test_cases, total_runs, csv_path):
         self.total_runs = total_runs
         self.test_cases = test_cases
         self.csv_path = csv_path
         self.results = []
         self.run_id = 0
 
     def run(self, argv, experiment_name):
-        start_time = datetime.datetime.now().strftime('%Y%m%d%H%M%S')
+        start_time = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
         run_id = f"{start_time}_{self.run_id}"
         self.run_id += 1
 
         try:
-            result = main(argv + ["-t", f"{self.test_cases}", "-r", f"{self.total_runs}"],
-                          experiment_name=experiment_name,
-                          run_id=run_id,
-                          csv_filename=self.csv_path)
-        except:
+            result = main(
+                [*argv, "-t", f"{self.test_cases}", "-r", f"{self.total_runs}"],
+                experiment_name=experiment_name,
+                run_id=run_id,
+                csv_filename=self.csv_path,
+            )
+            if result:
+                self.results.append(result)
+        except Exception:
             logger.exception(f"Failed to run experiment {experiment_name}")
+            result = None
 
-        if result:
-            self.results.append(result)
+        return result
 
 
 def load_results_from_csv(csv_path):
     rows = []
     import csv
-    with open(csv_path, newline='') as csvfile:
+
+    with open(csv_path, newline="") as csvfile:
         reader = csv.DictReader(csvfile)
         for row in reader:
             rows.append(row)
     return rows
 
 
+def get_latency(row):
+    for name in row:
+        if name.startswith("average_latency(batch_size="):
+            return float(row[name])
+
+    raise RuntimeError("Failed to get average_latency from output")
+
+
 def score(row):
     """Scoring function based on 3 metrics. The larger score is better."""
-    latency_in_ms = float(row[get_latency_name()])
+    latency_in_ms = get_latency(row)
     top1_match_rate = float(row["top1_match_rate"])
-    onnx_size_in_MB = float(row["onnx_size_in_MB"])
+    onnx_size_in_MB = float(row["onnx_size_in_MB"])  # noqa: N806
     # A simple scoring function: cost of 0.1ms latency ~ 0.1% match rate ~ 100MB size
-    return (top1_match_rate * 1000 - latency_in_ms * 10 - onnx_size_in_MB / 100)
+    return top1_match_rate * 1000 - latency_in_ms * 10 - onnx_size_in_MB / 100
 
 
 def print_wins(wins, rows, test_name):
     print()
     print("*" * 10)
 
     row_map = {}
     for row in rows:
         row_map[row["run_id"]] = row
 
-    sorted_wins = dict(sorted(wins.items(), key=lambda item: (item[1], score(row_map[item[0]])), reverse=True))
+    sorted_wins = dict(
+        sorted(
+            wins.items(),
+            key=lambda item: (item[1], score(row_map[item[0]])),
+            reverse=True,
+        )
+    )
     logger.debug(f"{test_name} Wins:{sorted_wins}")
     logger.info(f"Based on {test_name} wins and a scoring function, the ranking:")
 
     rank = 0
     previous_value = -1
     count = 0
     for key, value in sorted_wins.items():
@@ -132,98 +181,114 @@
             rank = count
         previous_value = value
         count += 1
 
         for row in rows:
             if row["run_id"] == key:
                 logger.info(
-                    "{:02d}: WINs={:02d}, run_id={}, latency={:5.2f} top1_match={:.4f} size={}_MB experiment={} {}".
-                    format(
-                        rank, value, key, float(row[get_latency_name()]), float(row["top1_match_rate"]),
-                        row["onnx_size_in_MB"], row["experiment"], " (Half2 Disabled)" if
-                        (row['ORT_CUDA_GEMM_OPTIONS'] == "4" and "Half2" not in row["experiment"]) else ""))
+                    "{:02d}: WINs={:02d}, run_id={}, latency={:5.2f}, top1_match={:.4f}, size={}_MB, experiment={}, {}".format(
+                        rank,
+                        value,
+                        key,
+                        get_latency(row),
+                        float(row["top1_match_rate"]),
+                        row["onnx_size_in_MB"],
+                        row["experiment"],
+                        get_ort_environment_variables(),
+                    )
+                )
                 break
 
 
 def run_significance_test(rows, output_csv_path):
-    """Run U test and T test.
-    """
+    """Run U test and T test."""
     utest_wins = {}
     ttest_wins = {}
     for row in rows:
         run_id = row["run_id"]
         utest_wins[run_id] = 0
         ttest_wins[run_id] = 0
 
-    with open(output_csv_path, 'w', newline='') as csvfile:
+    with open(output_csv_path, "w", newline="") as csvfile:
         column_names = [
-            'model_name', 'run_id_1', 'experiment_1', 'top1_match_rate_1', 'run_id_2', 'experiment_2',
-            'top1_match_rate_2', 'U_statistic', 'U_pvalue', "T_statistic", "T_pvalue"
+            "model_name",
+            "run_id_1",
+            "experiment_1",
+            "top1_match_rate_1",
+            "run_id_2",
+            "experiment_2",
+            "top1_match_rate_2",
+            "U_statistic",
+            "U_pvalue",
+            "T_statistic",
+            "T_pvalue",
         ]
 
         writer = csv.DictWriter(csvfile, fieldnames=column_names)
         writer.writeheader()
 
         required_match_columns = ["model_name", "test_cases", "runs"]
         num_results = len(rows)
         for i in range(num_results - 1):
             result1 = rows[i]
 
+            if isinstance(result1["top1_match_rate_per_run"], str):
+                a = json.loads(result1["top1_match_rate_per_run"])
+            else:
+                a = result1["top1_match_rate_per_run"]
+
             for j in range(i + 1, num_results, 1):
                 result2 = rows[j]
 
                 all_matched = True
                 for column in required_match_columns:
-                    if (result1[column] != result2[column]):
+                    if result1[column] != result2[column]:
                         all_matched = False
                         break
                 if not all_matched:
                     continue
 
-                if isinstance(result1["top1_match_rate_per_run"], str):
-                    import json
-                    a = json.loads(result1["top1_match_rate_per_run"])
+                if isinstance(result2["top1_match_rate_per_run"], str):
                     b = json.loads(result2["top1_match_rate_per_run"])
                 else:
-                    a = result1["top1_match_rate_per_run"]
                     b = result2["top1_match_rate_per_run"]
 
                 try:
                     utest_statistic, utest_pvalue = scipy.stats.mannwhitneyu(
                         a, b, use_continuity=True, alternative="two-sided"
-                    )  #TODO: shall we use one-sided: less or greater according to "top1_match_rate"
-                except ValueError:  #ValueError: All numbers are identical in mannwhitneyu
+                    )  # TODO: shall we use one-sided: less or greater according to "top1_match_rate"
+                except ValueError:  # ValueError: All numbers are identical in mannwhitneyu
                     utest_statistic = None
                     utest_pvalue = None
                 ttest_statistic, ttest_pvalue = scipy.stats.ttest_ind(a, b, axis=None, equal_var=True)
 
-                if utest_pvalue < 0.05:
+                if utest_pvalue is not None and utest_pvalue < 0.05:
                     if float(result1["top1_match_rate"]) > float(result2["top1_match_rate"]):
                         utest_wins[result1["run_id"]] += 1
                     else:
                         utest_wins[result2["run_id"]] += 1
 
                 if ttest_pvalue < 0.05:
                     if float(result1["top1_match_rate"]) > float(result2["top1_match_rate"]):
                         ttest_wins[result1["run_id"]] += 1
                     else:
                         ttest_wins[result2["run_id"]] += 1
 
                 row = {
-                    'model_name': result1["model_name"],
-                    'run_id_1': result1["run_id"],
-                    'experiment_1': result1["experiment"],
-                    'top1_match_rate_1': float(result1["top1_match_rate"]),
+                    "model_name": result1["model_name"],
+                    "run_id_1": result1["run_id"],
+                    "experiment_1": result1["experiment"],
+                    "top1_match_rate_1": float(result1["top1_match_rate"]),
                     "run_id_2": result2["run_id"],
                     "experiment_2": result2["experiment"],
-                    'top1_match_rate_2': float(result2["top1_match_rate"]),
-                    'U_statistic': utest_statistic,
-                    'U_pvalue': utest_pvalue,
-                    'T_statistic': ttest_statistic,
-                    'T_pvalue': ttest_pvalue
+                    "top1_match_rate_2": float(result2["top1_match_rate"]),
+                    "U_statistic": utest_statistic,
+                    "U_pvalue": utest_pvalue,
+                    "T_statistic": ttest_statistic,
+                    "T_pvalue": ttest_pvalue,
                 }
 
                 writer.writerow(row)
     logger.info(f"U-Test and T-Test results are output to {output_csv_path}")
     print_wins(utest_wins, rows, "U-Test")
     print_wins(ttest_wins, rows, "T-Test")
 
@@ -244,30 +309,45 @@
 
 
 def get_mixed_precision_parameters(args, last_matmul_node_name, op_block_list):
     model = args.model_name_or_path
     parameters = f"-m {model} -o --use_gpu -p fp16".split()
     if args.use_external_data_format:
         parameters.append("--use_external_data_format")
-    parameters += ["--io_block_list", "logits", "--node_block_list", last_matmul_node_name]
+    parameters += [
+        "--io_block_list",
+        "logits",
+        "--node_block_list",
+        last_matmul_node_name,
+    ]
 
     if op_block_list:
-        parameters.extend(["--op_block_list"] + op_block_list)
+        parameters.extend(["--op_block_list", *op_block_list])
 
     return parameters
 
 
-def run_candidate(task: ParityTask, args, last_matmul_node_name, op_block_list=["FastGelu", "LayerNormalization"]):
+def run_candidate(
+    task: ParityTask,
+    args,
+    last_matmul_node_name,
+    op_block_list=["FastGelu", "LayerNormalization"],  # noqa: B006
+):
     parameters = get_mixed_precision_parameters(args, last_matmul_node_name, op_block_list)
-    op_block_list_str = ','.join(sorted(op_block_list))
-    name_suffix = " (Half2 Disabled)" if os.getenv('ORT_CUDA_GEMM_OPTIONS') == "4" else ""
+    op_block_list_str = ",".join(sorted(op_block_list))
+
     if op_block_list:
-        name = f"Mixed precision baseline + {op_block_list_str} in FP32{name_suffix}"
+        name = f"Mixed precision baseline + {op_block_list_str} in FP32"
     else:
-        name = f"Mixed precision baseline (logits output and last MatMul node {last_matmul_node_name} in FP32){name_suffix}"
+        name = f"Mixed precision baseline (logits output and last MatMul node {last_matmul_node_name} in FP32)"
+
+    env_vars = get_ort_environment_variables()
+    if env_vars:
+        name = name + f" ({env_vars})"
+
     task.run(parameters, name)
 
 
 def get_baselines(args):
     model = args.model_name_or_path
     fp32_baseline = f"-m {model} -o -p fp32".split()
     if args.use_gpu:
@@ -278,129 +358,162 @@
     fp16_baseline = f"-m {model} -o --use_gpu -p fp16".split()
     if args.use_external_data_format:
         fp16_baseline.append("--use_external_data_format")
 
     return fp32_baseline, fp16_baseline
 
 
-def get_all_operators():
-    """All operators in the optimized model"""
-    return "Attention Gather Add LayerNormalization FastGelu MatMul".split()
-
-
-def run_tuning_step0(task, fp16_baseline):
+def run_tuning_step0(task, fp16_baseline, all_ops, optimized_ops):
     """Step 0 is to check which operator in FP16 causes most loss"""
     fp32_logits = ["--io_block_list", "logits"]
     task.run(fp16_baseline + fp32_logits, "FP16 except logits")
 
     fp32_io = ["--keep_io_types"]
     task.run(fp16_baseline + fp32_io, "Graph I/O FP32, Other FP16")
 
-    op_list = get_all_operators()
-    #task.run(fp16_baseline + fp32_io + ["--op_block_list"] + [o for o in op_list], "Everthing in FP32")
-
     # Only weights in FP16
-    task.run(fp16_baseline + fp32_io + ["--op_block_list"] + [o for o in op_list] + ['--force_fp16_initializers'],
-             "FP32 except weights in FP16")
+    task.run(
+        fp16_baseline + fp32_io + ["--op_block_list"] + [o for o in all_ops] + ["--force_fp16_initializers"],
+        "FP32 except weights in FP16",
+    )
 
+    optimized_ops_results = []
+    op_list = optimized_ops
     for op in op_list:
         op_block_list = ["--op_block_list"] + [o for o in op_list if o != op]
-        task.run(fp16_baseline + fp32_io + op_block_list, f"FP32 except {op} in FP16")
+        result = task.run(fp16_baseline + fp32_io + op_block_list, f"FP32 except {op} in FP16")
+        if result:
+            optimized_ops_results.append(result)
+
+    # Check which optimized operator causes the most loss in precision
+    min_result = min(optimized_ops_results, key=lambda y: y["top1_match_rate"])
+    print("step 0: optimized operator causes the most loss in precision", min_result)
 
 
-def run_tuning_step1(task, mixed_precision_baseline):
-    """Step 1 is to figure out which operator in FP32 could benefit most"""
-    for op in get_all_operators():
+def run_tuning_step1(task, mixed_precision_baseline, optimized_ops):
+    """Step 1 is to figure out which optimized operator in FP32 could benefit most"""
+    for op in optimized_ops:
         op_block_list = ["--op_block_list", op]
-        task.run(mixed_precision_baseline + op_block_list, f"Mixed precision baseline + {op} in FP32")
+        task.run(
+            mixed_precision_baseline + op_block_list,
+            f"Mixed precision baseline + {op} in FP32",
+        )
 
 
-def run_tuning_step2(task, mixed_precision_baseline):
-    """Assumed that you have run step 1 to figure out that Logits FP32 and Add FP32 is important,
-    Step 2 is to figure out a combination of two operators (one is Add from step one) to get better result
+def run_tuning_step2(task, mixed_precision_baseline, optimized_ops):
+    """Assumed that you have run step 0 and 1 to figure out that Logits FP32 and some operators shall be in FP32,
+    This step will try add one more operator.
     """
-    for op in get_all_operators():
-        if op not in ['Add']:
-            op_block_list = ["--op_block_list", 'Add', op]
-            task.run(mixed_precision_baseline + op_block_list, f"Mixed precision baseline + Add,{op} in FP32")
-
-
-def run_parity_disable_half2(task: ParityTask, args):
-    onnx_model_paths = Gpt2Helper.get_onnx_paths('onnx_models',
-                                                 args.model_name_or_path,
-                                                 new_folder=args.use_external_data_format,
-                                                 remove_existing=[])
-    last_matmul_node_name = get_last_matmul_node_name(onnx_model_paths["raw"])
-    run_candidate(task, args, last_matmul_node_name, op_block_list=[])
-    run_candidate(task, args, last_matmul_node_name, op_block_list=["Add"])
-    run_candidate(task, args, last_matmul_node_name, op_block_list=["LayerNormalization", "Add"])
+    candidate_fp32_ops = ["FastGelu", "LayerNormalization", "SkipLayerNormalization"]
+    fp32_ops = [x for x in candidate_fp32_ops if x in optimized_ops]
+    for op in optimized_ops:
+        if op not in fp32_ops:
+            op_block_list = [*fp32_ops, op]
+            task.run(
+                [*mixed_precision_baseline, "--op_block_list", *op_block_list],
+                "Mixed precision baseline + {},{} in FP32".format(",".join(fp32_ops), op),
+            )
 
 
 def run_parity(task: ParityTask, args):
-    onnx_model_paths = Gpt2Helper.get_onnx_paths('onnx_models',
-                                                 args.model_name_or_path,
-                                                 new_folder=args.use_external_data_format,
-                                                 remove_existing=[])
+    onnx_model_paths = Gpt2Helper.get_onnx_paths(
+        "onnx_models",
+        args.model_name_or_path,
+        new_folder=args.use_external_data_format,
+        remove_existing=[],
+    )
 
     fp32_baseline, fp16_baseline = get_baselines(args)
 
-    task.run(fp32_baseline, "FP32 baseline")
+    result = task.run(fp32_baseline, "FP32 baseline")
+
+    optimized_ops = []
+    if result and ("optimized_operators" in result) and result["optimized_operators"]:
+        optimized_ops = result["optimized_operators"].split(",")
+    else:
+        raise RuntimeError("Failed to get optimized operators")
+
+    all_ops = []
+    if result and ("operators" in result) and result["operators"]:
+        all_ops = result["operators"].split(",")
+    else:
+        raise RuntimeError("Failed to get operators")
 
     # The following tests for fp16 requires GPU
     if not args.use_gpu:
         logger.info("skip mixed precision since --use_gpu is not specified")
         return
 
     task.run(fp16_baseline, "FP16 baseline")
 
     last_matmul_node_name = get_last_matmul_node_name(onnx_model_paths["raw"])
 
     # Mixed precision baseline
     run_candidate(task, args, last_matmul_node_name, op_block_list=[])
 
-    # Result from tuning step 1
-    run_candidate(task, args, last_matmul_node_name, op_block_list=["Add"])
+    def get_fp32_ops(x):
+        return [op for op in x if op in all_ops]
 
     if args.all:
-        run_tuning_step0(task, fp16_baseline)
+        run_tuning_step0(task, fp16_baseline, all_ops, optimized_ops)
         mixed_precision_baseline = get_mixed_precision_parameters(args, last_matmul_node_name, op_block_list=[])
-        run_tuning_step1(task, mixed_precision_baseline)
-        run_tuning_step2(task, mixed_precision_baseline)
+        run_tuning_step1(task, mixed_precision_baseline, optimized_ops)
+        run_tuning_step2(task, mixed_precision_baseline, optimized_ops)
     else:
-        run_candidate(task, args, last_matmul_node_name, op_block_list=["LayerNormalization", "Add"])
-        run_candidate(task, args, last_matmul_node_name, op_block_list=["FastGelu", "Add"])
+        run_candidate(
+            task,
+            args,
+            last_matmul_node_name,
+            op_block_list=get_fp32_ops(["SkipLayerNormalization", "LayerNormalization", "Add"]),
+        )
+        run_candidate(task, args, last_matmul_node_name, op_block_list=["FastGelu"])
 
     # Run a few good candidates
-    run_candidate(task, args, last_matmul_node_name, op_block_list=["FastGelu", "LayerNormalization", "Add"])
-    run_candidate(task, args, last_matmul_node_name, op_block_list=["FastGelu", "LayerNormalization", "Add", "Gather"])
-    run_candidate(task, args, last_matmul_node_name, \
-                  op_block_list=["FastGelu", "LayerNormalization", "Add", "Gather", "MatMul"])
+    run_candidate(
+        task,
+        args,
+        last_matmul_node_name,
+        op_block_list=get_fp32_ops(["FastGelu", "SkipLayerNormalization", "LayerNormalization", "Add"]),
+    )
+    run_candidate(
+        task,
+        args,
+        last_matmul_node_name,
+        op_block_list=get_fp32_ops(
+            ["FastGelu", "EmbedLayerNormalization", "SkipLayerNormalization", "LayerNormalization", "Add"]
+        ),
+    )
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     args = parse_arguments()
     setup_logger(args.verbose)
 
     if args.test_cases < 100 or args.runs < 20 or args.test_cases * args.runs < 10000:
         logger.warning(
-            "Not enough test cases or runs to get stable results or test significance. Recommend test_cases >= 100, runs >= 20, test_cases * runs >= 10000."
+            "Not enough test cases or runs to get stable results or test significance. "
+            "Recommend test_cases >= 100, runs >= 20, test_cases * runs >= 10000."
         )
 
+    if os.path.exists(args.csv) and not args.skip_test:
+        if not args.overwrite:
+            raise RuntimeError(
+                f"Output file {args.csv} existed. Please remove the file, or use either --skip_test or --overwrite."
+            )
+        else:
+            logger.info("Remove existing file %s since --overwrite is specified", args.csv)
+            os.remove(args.csv)
+
     task = ParityTask(args.test_cases, args.runs, args.csv)
 
     if not args.skip_test:
-        if (os.getenv('ORT_CUDA_GEMM_OPTIONS') == "4" and args.use_gpu):
-            assert torch.cuda.get_device_capability(
-            )[0] >= 7, "half2 kernel is not avaiable in current GPU device. Please set environment variable ORT_CUDA_GEMM_OPTIONS=0 or use supported GPU like V100 or T4"
-            run_parity_disable_half2(task, args)
-        else:
-            run_parity(task, args)
+        run_parity(task, args)
 
     try:
         rows = load_results_from_csv(task.csv_path)
-    except:
+    except Exception:
         logger.exception(f"Failed to load csv {task.csv_path}")
         rows = task.results
 
     logger.info("Start running significance tests...")
-    summary_csv = task.csv_path.replace('.csv', ".stats.csv")
+    summary_csv = task.csv_path.replace(".csv", ".stats.csv")
     run_significance_test(rows, summary_csv)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/gpt2_tester.py` & `onnxruntime/transformers/onnx_model_bert.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,427 +1,490 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
-# Licensed under the MIT License.  See License.txt in the project root for
-# license information.
+# Licensed under the MIT License.
 # --------------------------------------------------------------------------
-# This script helps evaluation of GPT-2 model.
-import os
-import logging
-import torch
-import random
-import numpy
-import time
-import timeit
-import math
-import statistics
-from gpt2_helper import Gpt2Helper, Gpt2Inputs
-from benchmark_helper import Precision
-
-logger = logging.getLogger(__name__)
-
-
-class Gpt2Metric:
-    def __init__(self, treatment_name, baseline_name='Torch', top_k=20):
-        assert top_k > 1 and top_k <= 100
-        self.baseline = baseline_name
-        self.treatment = treatment_name
-        self.name: str = f"{treatment_name} vs {baseline_name}"
-        self.top_k = top_k
-        self.top_1_error: int = 0
-        self.top_k_error: int = 0
-        self.total_samples: int = 0
-        self.max_logits_diff: float = 0  # for non-empty past state
-        self.max_logits_diff_no_past: float = 0  # for empty past state
-        self.batch_top1_error: torch.FloatTensor = None  # top 1 error for current batch
-        self.batch_topk_error: torch.FloatTensor = None  # top k error for current batch
-        self.seq_len_latency = {}
-
-    def print(self):
-        if self.baseline != self.treatment:
-            print("---")
-            print(f"Metrics for {self.treatment} (baseline={self.baseline}):")
-            if self.total_samples > 0:
-                top_1_error_rate = 100.0 * self.top_1_error / self.total_samples
-                top_k_error_rate = 100.0 * self.top_k_error / self.total_samples
-                print(
-                    f"Total={self.total_samples} Top1Error={self.top_1_error} ({top_1_error_rate:.2f}%) Top{self.top_k}Error={self.top_k_error} ({top_k_error_rate:.2f}%)"
+
+from logging import getLogger
+from typing import List, Optional
+
+from convert_to_packing_mode import PackingMode
+from fusion_attention import AttentionMask, FusionAttention
+from fusion_bart_attention import FusionBartAttention
+from fusion_biasgelu import FusionBiasGelu
+from fusion_embedlayer import FusionEmbedLayerNormalization
+from fusion_fastgelu import FusionFastGelu
+from fusion_gelu import FusionGelu
+from fusion_gelu_approximation import FusionGeluApproximation
+from fusion_gemmfastgelu import FusionGemmFastGelu
+from fusion_layernorm import FusionLayerNormalization, FusionLayerNormalizationTF
+from fusion_options import AttentionMaskFormat, FusionOptions
+from fusion_qordered_attention import FusionQOrderedAttention
+from fusion_qordered_gelu import FusionQOrderedGelu
+from fusion_qordered_layernorm import FusionQOrderedLayerNormalization
+from fusion_qordered_matmul import FusionQOrderedMatMul
+from fusion_reshape import FusionReshape
+from fusion_shape import FusionShape
+from fusion_skiplayernorm import FusionBiasSkipLayerNormalization, FusionSkipLayerNormalization
+from fusion_utils import FusionUtils
+from onnx import GraphProto, ModelProto, TensorProto, ValueInfoProto, helper
+from onnx_model import OnnxModel
+
+logger = getLogger(__name__)
+
+
+class BertOptimizationOptions(FusionOptions):
+    """This class is deprecated"""
+
+    def __init__(self, model_type):
+        logger.warning("BertOptimizationOptions is depreciated. Please use FusionOptions instead.")
+        super().__init__(model_type)
+
+
+class BertOnnxModel(OnnxModel):
+    def __init__(self, model: ModelProto, num_heads: int = 0, hidden_size: int = 0):
+        """Initialize BERT ONNX Model.
+
+        Args:
+            model (ModelProto): the ONNX model
+            num_heads (int, optional): number of attention heads. Defaults to 0 (detect the parameter automatically).
+            hidden_size (int, optional): hidden dimension. Defaults to 0 (detect the parameter automatically).
+        """
+        assert (num_heads == 0 and hidden_size == 0) or (num_heads > 0 and hidden_size % num_heads == 0)
+
+        super().__init__(model)
+        self.num_heads = num_heads
+        self.hidden_size = hidden_size
+
+        self.attention_mask = AttentionMask(self)
+        self.attention_fusion = FusionAttention(self, self.hidden_size, self.num_heads, self.attention_mask)
+        self.qordered_attention_fusion = FusionQOrderedAttention(
+            self, self.hidden_size, self.num_heads, self.attention_mask
+        )
+        self.utils = FusionUtils(self)
+
+    def fuse_attention(self):
+        self.attention_fusion.apply()
+        # Only relevant in models with Q-DQ nodes
+        self.qordered_attention_fusion.apply()
+
+    def fuse_gelu(self):
+        fusion = FusionGelu(self)
+        fusion.apply()
+        fusion = FusionFastGelu(self)
+        fusion.apply()
+        # Only relevant in models with Q-DQ nodes
+        fusion = FusionQOrderedGelu(self)
+        fusion.apply()
+
+    def fuse_bias_gelu(self, is_fastgelu):
+        fusion = FusionBiasGelu(self, is_fastgelu)
+        fusion.apply()
+
+    def gelu_approximation(self):
+        fusion = FusionGeluApproximation(self)
+        fusion.apply()
+
+    def fuse_gemm_fast_gelu(self):
+        fusion = FusionGemmFastGelu(self)
+        fusion.apply()
+
+    def fuse_add_bias_skip_layer_norm(self):
+        fusion = FusionBiasSkipLayerNormalization(self)
+        fusion.apply()
+
+    def fuse_reshape(self):
+        fusion = FusionReshape(self)
+        fusion.apply()
+
+    def fuse_shape(self):
+        fusion = FusionShape(self)
+        fusion.apply()
+
+    def fuse_embed_layer(self, use_mask_index):
+        fusion = FusionEmbedLayerNormalization(self, use_mask_index)
+        fusion.apply()
+
+    def fuse_layer_norm(self):
+        fusion = FusionLayerNormalization(self)
+        fusion.apply()
+
+        fusion = FusionLayerNormalizationTF(self)
+        fusion.apply()
+
+        # Only relevant in models with Q-DQ nodes
+        fusion = FusionQOrderedLayerNormalization(self)
+        fusion.apply()
+
+    def fuse_skip_layer_norm(self):
+        fusion = FusionSkipLayerNormalization(self)
+        fusion.apply()
+
+    # Only relevant in models with Q-DQ nodes
+    def fuse_qordered_mamtul(self):
+        fusion = FusionQOrderedMatMul(self)
+        fusion.apply()
+
+    def get_graph_inputs_from_node_type(self, op_type: str, input_indices: List[int], casted: bool):
+        """
+        Get graph inputs that feed into node type (like EmbedLayerNormalization or Attention).
+        Returns a list of the graph input names based on the filter whether it is casted or not.
+        """
+        graph_inputs = []
+
+        output_name_to_node = self.output_name_to_node()
+        nodes = self.get_nodes_by_op_type(op_type)
+        for node in nodes:
+            bert_inputs = [node.input[i] for i in input_indices if i < len(node.input)]
+            for bert_input in bert_inputs:
+                if self.find_graph_input(bert_input):
+                    if not casted:
+                        graph_inputs.append(bert_input)
+                elif bert_input in output_name_to_node:
+                    parent = output_name_to_node[bert_input]
+                    if parent.op_type == "Cast" and self.find_graph_input(parent.input[0]) is not None:
+                        if casted:
+                            graph_inputs.append(parent.input[0])
+        return graph_inputs
+
+    def get_graph_inputs_from_fused_nodes(self, casted: bool):
+        inputs = self.get_graph_inputs_from_node_type("EmbedLayerNormalization", [0, 1, 7], casted)
+        inputs += self.get_graph_inputs_from_node_type("Attention", [3], casted)
+        return inputs
+
+    def change_graph_input_type(
+        self,
+        graph: GraphProto,
+        graph_input: ValueInfoProto,
+        new_type: int = TensorProto.INT32,
+    ):
+        """Change graph input type, and add Cast node if needed.
+
+        Args:
+            graph (GraphProto): graph
+            graph_input (TensorProto): input of the graph
+            new_type (int, optional): new data type. Defaults to TensorProto.INT32.
+
+        Returns:
+            NodeProto: a new Cast node that added. None if Cast node is not added.
+            List[NodeProto]: Cast nodes that have been removed.
+        """
+        assert isinstance(graph, GraphProto)
+        assert isinstance(graph_input, ValueInfoProto)
+        assert self.find_graph_input(graph_input.name)
+
+        if graph_input.type.tensor_type.elem_type == int(new_type):
+            return None, []
+
+        new_cast_node = None
+        nodes_to_remove = []
+
+        input_name_to_nodes = self.input_name_to_nodes()
+        if graph_input.name in input_name_to_nodes:
+            nodes = input_name_to_nodes[graph_input.name]
+
+            # For children that is not Cast node, insert a Cast node to convert int32 to original data type.
+            nodes_not_cast = [node for node in nodes if node.op_type != "Cast"]
+            if nodes_not_cast:
+                node_name = self.create_node_name("Cast")
+                output_name = node_name + "_" + graph_input.name
+                new_value_info = graph.value_info.add()
+                new_value_info.CopyFrom(graph_input)
+                new_value_info.name = output_name
+                new_cast_node = helper.make_node(
+                    "Cast",
+                    [graph_input.name],
+                    [output_name],
+                    to=int(graph_input.type.tensor_type.elem_type),
+                    name=node_name,
                 )
-            print("Max logits diffs:")
-            print(f"\twith past  = {self.max_logits_diff:.6f}")
-            print(f"\tempty past = {self.max_logits_diff_no_past:.6f}")
-        else:
-            print(f"Metrics for {self.treatment} (baseline):")
-
-        if self.seq_len_latency:
-            print("Past sequence length range and average latency:")
-            total = 0
-            count = 0
-            for key in sorted(self.seq_len_latency.keys()):
-                average = statistics.mean(self.seq_len_latency[key]) * 1000.0
-                if key == 0:
-                    print("\t{}:         \t{:.2f} ms".format(key, average))
-                else:
-                    print("\t[{}, {}]:\t{:.2f} ms".format(2**key, 2**(key + 1) - 1, average))
-                total += average * len(self.seq_len_latency[key])
-                count += len(self.seq_len_latency[key])
-            print("Average Latency: {:.2f} ms".format(total / count))
-
-    def diff_logits(self, baseline_logits, treatment_logits, is_empty_past: bool):
-        diff = (baseline_logits - treatment_logits).abs().max()
-        if is_empty_past:
-            self.max_logits_diff_no_past = max(self.max_logits_diff_no_past, diff)
-        else:
-            self.max_logits_diff = max(self.max_logits_diff, diff)
-
-        return diff
-
-    def start_batch(self, batch_size: int):
-        self.total_samples += batch_size
-        self.batch_top1_error = torch.zeros((batch_size, 1), dtype=torch.bool)
-        self.batch_topk_error = torch.zeros((batch_size, 1), dtype=torch.bool)
-
-    def eval_batch(self, baseline, treatment, past_seq_len, verbose=True):
-        self._eval_topk(baseline.top_1_tokens, treatment.top_1_tokens, 1, verbose)
-        self._eval_topk(baseline.top_k_tokens, treatment.top_k_tokens, self.top_k, verbose)
-
-        max_diff = self.diff_logits(baseline.logits, treatment.logits, past_seq_len == 0)
-        if verbose:
-            print(f"Max logits diffs of {self.name}: {max_diff}")
-
-    def _eval_topk(self, baseline_topk, treatment_topk, top_k, verbose=True):
-        if not torch.all(torch.eq(baseline_topk, treatment_topk)):
-            if top_k == 1:
-                if verbose:
-                    print(f"Generated tokens not matched for {self.name}")
-                self.batch_top1_error |= torch.eq(baseline_topk, treatment_topk).logical_not()
-            else:
-                if verbose:
-                    print(
-                        f"Top {top_k} tokens not matched for {self.name}. This will lead to wrong beam search results")
-                self.batch_topk_error |= (torch.eq(baseline_topk, treatment_topk).logical_not().sum(1).unsqueeze(dim=1)
-                                          > 0)
-
-    def end_batch(self):
-        self.top_1_error += self.batch_top1_error.sum()
-        self.top_k_error += self.batch_topk_error.sum()
-
-    def add_latency(self, past_seq_len, latency):
-        key = int(math.log2(past_seq_len)) + 1 if past_seq_len > 0 else 0
-        if key not in self.seq_len_latency:
-            self.seq_len_latency[key] = []
-        self.seq_len_latency[key].append(latency)
-
-
-class Gpt2Tester:
-    def __init__(self,
-                 input_ids,
-                 position_ids,
-                 attention_mask,
-                 num_attention_heads,
-                 hidden_size,
-                 num_layer,
-                 device,
-                 is_fp16=False,
-                 top_k=20,
-                 top_k_required_order=False):
-
-        self.batch_size = input_ids.shape[0]
-        self.input_length = input_ids.shape[1]
-        self.n_layer = num_layer
-
-        self.input_ids = input_ids
-        self.position_ids = position_ids
-        self.attention_mask = attention_mask
-
-        self.has_position_ids = position_ids is not None
-        self.has_attention_mask = attention_mask is not None
-
-        # Emtpy past state for first inference
-        self.past = []
-        past_shape = [2, self.batch_size, num_attention_heads, 0, hidden_size // num_attention_heads]
-        for i in range(num_layer):
-            empty_past = torch.empty(past_shape).type(torch.float16 if is_fp16 else torch.float32)
-            self.past.append(empty_past.to(device))
-
-        self.logits = None
-        self.top_1_tokens = None
-        self.top_k_tokens = None
-        self.top_k = top_k
-        self.top_k_required_order = top_k_required_order
-
-    def get_inputs(self) -> Gpt2Inputs:
-        return Gpt2Inputs(self.input_ids, self.position_ids, self.attention_mask, self.past)
-
-    def save_test_data(self, session, output, save_test_data_dir, test_case_id):
-        from onnx import numpy_helper
-
-        path = os.path.join(save_test_data_dir, 'test_data_set_' + str(test_case_id))
-        if os.path.exists(path):
-            print(f"Directory {path} existed. Skip saving test data")
-            return
-
-        os.makedirs(path, exist_ok=True)
-
-        def add_tensor(input_tensors, torch_tensor, name):
-            input_tensors.append(numpy_helper.from_array(torch_tensor.clone().cpu().numpy(), name))
-
-        input_tensors = []
-        add_tensor(input_tensors, self.input_ids, "input_ids")
-
-        if self.has_position_ids:
-            add_tensor(input_tensors, self.position_ids, "position_ids")
-
-        if self.has_attention_mask:
-            add_tensor(input_tensors, self.attention_mask, "attention_mask")
-
-        for i in range(self.n_layer):
-            add_tensor(input_tensors, self.past[i], 'past_' + str(i))
-
-        for i, tensor in enumerate(input_tensors):
-            with open(os.path.join(path, 'input_{}.pb'.format(i)), 'wb') as f:
-                f.write(tensor.SerializeToString())
-
-        output_names = [output.name for output in session.get_outputs()]
-        for i, name in enumerate(output_names):
-            tensor = numpy_helper.from_array(
-                output[i] if isinstance(output[i], numpy.ndarray) else output[i].clone().cpu().numpy())
-            with open(os.path.join(path, 'output_{}.pb'.format(i)), 'wb') as f:
-                f.write(tensor.SerializeToString())
+                graph.node.extend([new_cast_node])
 
-        print(f"Test data saved to directory {path}")
+                for node in nodes_not_cast:
+                    OnnxModel.replace_node_input(node, graph_input.name, output_name)
 
-    def update(self, output, step, device):
+            # For children that is Cast node, no need to insert Cast.
+            # When the children is Cast to int32, we can remove that Cast node since input type is int32 now.
+            nodes_cast = [node for node in nodes if node.op_type == "Cast"]
+            for node in nodes_cast:
+                if OnnxModel.get_node_attribute(node, "to") == int(new_type):
+                    self.replace_input_of_all_nodes(node.output[0], graph_input.name)
+                if not self.find_graph_output(node.output[0]):
+                    nodes_to_remove.append(node)
+            if nodes_to_remove:
+                self.remove_nodes(nodes_to_remove)
+
+        graph_input.type.tensor_type.elem_type = int(new_type)
+        return new_cast_node, nodes_to_remove
+
+    def change_graph_inputs_to_int32(self):
+        """Change data type of all graph inputs to int32 type, and add Cast node if needed."""
+        graph = self.graph()
+        add_cast_count = 0
+        remove_cast_count = 0
+        for graph_input in graph.input:
+            new_node, removed_nodes = self.change_graph_input_type(graph, graph_input, TensorProto.INT32)
+            if new_node:
+                add_cast_count += 1
+            remove_cast_count += len(removed_nodes)
+        logger.info(
+            f"Graph inputs are changed to int32. Added {add_cast_count} Cast nodes, and removed {remove_cast_count} Cast nodes."
+        )
+
+    def use_dynamic_axes(self, dynamic_batch_dim="batch_size", dynamic_seq_len="max_seq_len"):
         """
-        Update the inputs for next inference.
+        Update input and output shape to use dynamic axes.
         """
-        self.logits = torch.from_numpy(output[0]) if isinstance(output[0],
-                                                                numpy.ndarray) else output[0].clone().detach().cpu()
+        bert_graph_inputs = self.get_graph_inputs_from_fused_nodes(
+            casted=True
+        ) + self.get_graph_inputs_from_fused_nodes(casted=False)
+
+        for input in self.model.graph.input:
+            if input.name in bert_graph_inputs:
+                dim_proto = input.type.tensor_type.shape.dim[0]
+                dim_proto.dim_param = dynamic_batch_dim
+                if dynamic_seq_len is not None:
+                    dim_proto = input.type.tensor_type.shape.dim[1]
+                    dim_proto.dim_param = dynamic_seq_len
+
+        for output in self.model.graph.output:
+            dim_proto = output.type.tensor_type.shape.dim[0]
+            dim_proto.dim_param = dynamic_batch_dim
+
+    def preprocess(self):
+        self.adjust_reshape_and_expand()
+        return
+
+    def adjust_reshape_and_expand(self):
+        nodes_to_remove = []
+        for node in self.nodes():
+            if node.op_type == "Reshape":
+                # Clean up unnecessary reshape nodes.
+                # Find reshape nodes with no actually data in "shape" attribute and remove.
+                reshape_shape = self.get_constant_value(node.input[1])
+                if reshape_shape is not None and reshape_shape.size == 0:
+                    nodes_to_remove.extend([node])
+                    self.replace_input_of_all_nodes(node.output[0], node.input[0])
+                    continue
+
+                # Find path "Slice" -> "Reshape" -> "Expand" -> "Expand" -> current "Reshape", simplify the graph by
+                # changing current reshape's input to output of slice.
+                reshape_path = self.match_parent_path(
+                    node,
+                    ["Expand", "Expand", "Reshape", "Slice"],
+                    [0, 0, 0, 0],
+                    self.output_name_to_node(),
+                )
+                if reshape_path is not None:
+                    expand_node = reshape_path[-3]
+                    expand_shape_value = self.get_constant_value(expand_node.input[1])
+
+                    reshape_before_expand = reshape_path[-2]
+                    shape_value = self.get_constant_value(reshape_before_expand.input[1])
+
+                    slice_node = reshape_path[-1]
+                    if (
+                        expand_shape_value is not None
+                        and shape_value is not None
+                        and len(expand_shape_value) == 2
+                        and len(shape_value) == 1
+                        and expand_shape_value[1] == shape_value[0]
+                    ):
+                        node.input[0] = slice_node.output[0]
+
+        if nodes_to_remove:
+            self.remove_nodes(nodes_to_remove)
+            logger.info(f"Removed Reshape and Expand count: {len(nodes_to_remove)}")
+
+    def clean_graph(self):
+        output_name_to_node = self.output_name_to_node()
+        nodes_to_remove = []
+        for node in self.nodes():
+            # Before:
+            #  input_ids --> Shape --> Gather(indices=0) --> Unsqueeze ------+
+            #          |                                                     |
+            #          |                                                     v
+            #          +----> Shape --> Gather(indices=1) --> Unsqueeze--->  Concat --> ConstantOfShape -->Cast --> EmbedLayerNormaliation/ReduceSum
+            # After:
+            #  input_ids --> Shape                                                  --> ConstantOfShape -->Cast --> EmbedLayerNormaliation/ReduceSum
+            # TODO: merge ConstantOfShape -->Cast to ConstantOfShape (need update the data type of value)
+            op_input_id = {"EmbedLayerNormalization": 1, "ReduceSum": 0, "Attention": 3}
+            if node.op_type in op_input_id:
+                i = op_input_id[node.op_type]
+                parent_nodes = self.match_parent_path(
+                    node,
+                    [
+                        "Cast",
+                        "ConstantOfShape",
+                        "Concat",
+                        "Unsqueeze",
+                        "Gather",
+                        "Shape",
+                    ],
+                    [i, 0, 0, 0, 0, 0],
+                    output_name_to_node,
+                )
+                if parent_nodes is not None:
+                    (
+                        cast,
+                        constantOfShape,  # noqa: N806
+                        concat,
+                        unsqueeze,
+                        gather,
+                        shape,
+                    ) = parent_nodes
+                    if shape.input[0] == self.graph().input[0].name:
+                        constantOfShape.input[0] = shape.output[0]
+                        output_name_to_node = self.output_name_to_node()
+
+            if node.op_type == "Attention":
+                # Before:
+                #   input_ids --> Shape -->ConstantOfShape -->Cast --> ReduceSum --> Attention
+                # After:
+                #   remove this path, and remove the optional mask_index input of Attention node.
+                parent_nodes = self.match_parent_path(
+                    node,
+                    ["ReduceSum", "Cast", "ConstantOfShape", "Shape"],
+                    [3, 0, 0, 0],
+                    output_name_to_node,
+                )
+                if parent_nodes is not None:
+                    if parent_nodes[-1].input[0] == self.graph().input[0].name:
+                        attention_node = helper.make_node(
+                            "Attention",
+                            inputs=node.input[0 : len(node.input) - 1],
+                            outputs=node.output,
+                            name=node.name + "_remove_mask",
+                        )
+                        attention_node.domain = "com.microsoft"
+                        attention_node.attribute.extend([helper.make_attribute("num_heads", self.num_heads)])
+                        self.add_node(attention_node, self.get_graph_by_node(attention_node).name)
+                        nodes_to_remove.append(node)
+        self.remove_nodes(nodes_to_remove)
+
+    def postprocess(self):
+        self.clean_graph()
+        self.prune_graph()
+
+    def optimize(self, options: Optional[FusionOptions] = None, add_dynamic_axes: bool = False):
+        if (options is not None) and not options.enable_shape_inference:
+            self.disable_shape_inference()
+
+        self.utils.remove_identity_nodes()
+
+        # Remove cast nodes that having same data type of input and output based on symbolic shape inference.
+        self.utils.remove_useless_cast_nodes()
+
+        if (options is None) or options.enable_layer_norm:
+            self.fuse_layer_norm()
+
+        if (options is None) or options.enable_gelu:
+            self.fuse_gelu()
+
+        self.preprocess()
+
+        self.fuse_reshape()
+
+        if (options is None) or options.enable_skip_layer_norm:
+            self.fuse_skip_layer_norm()
+
+        if options is not None:
+            self.attention_mask.set_mask_format(options.attention_mask_format)
+            if options.use_multi_head_attention and not isinstance(self.attention_fusion, FusionBartAttention):
+                self.attention_fusion = FusionAttention(
+                    self, self.hidden_size, self.num_heads, self.attention_mask, options.use_multi_head_attention
+                )
 
-        self.top_1_tokens = Gpt2Tester.predict_next_token(self.logits)
-        self.top_k_tokens = Gpt2Tester.predict_next_token(self.logits, self.top_k, self.top_k_required_order)
+        if (options is None) or options.enable_attention:
+            self.fuse_attention()
 
-        self.input_ids = self.top_1_tokens.clone().detach().reshape([self.batch_size, 1]).to(device)
+        # Perform the MatMul fusion after the Attention fusion as we do not
+        # want to fuse the MatMuls inside the Attention subgraphs
+        if (options is None) or options.enable_qordered_matmul:
+            self.fuse_qordered_mamtul()
 
-        if self.has_position_ids:
-            self.position_ids = torch.tensor([self.input_length + step - 1]).unsqueeze(0).repeat(self.batch_size,
-                                                                                                 1).to(device)
-
-        if self.has_attention_mask:
-            self.attention_mask = torch.cat(
-                [self.attention_mask,
-                 torch.ones([self.batch_size, 1]).type_as(self.attention_mask)], 1).to(device)
-
-        self.past = []
-
-        if isinstance(output[1], tuple):  # past in torch output is tuple
-            self.past = list(output[1])
-        else:
-            for i in range(self.n_layer):
-                past_i = torch.from_numpy(output[i + 1]) if isinstance(
-                    output[i + 1], numpy.ndarray) else output[i + 1].clone().detach()
-                self.past.append(past_i.to(device))
+        self.fuse_shape()
 
-    def diff(self, baseline):
-        """
-        Compare inputs and logits output.
-        """
+        if (options is None) or options.enable_embed_layer_norm:
+            use_mask_index = options.attention_mask_format == AttentionMaskFormat.MaskIndexEnd
+            self.fuse_embed_layer(use_mask_index)
 
-        print("start diff...")
-        if self.logits is not None:
-            max_io_diff = (self.logits - baseline.logits).abs().max()
-            if max_io_diff > 1e-4:
-                print(f'Max logits difference is too large: {max_io_diff}')
-
-        if not torch.all(self.input_ids == baseline.input_ids):
-            print('Input_ids is different', self.input_ids, baseline.input_ids)
-
-        if self.has_position_ids:
-            if not torch.all(self.position_ids == baseline.position_ids):
-                print('position_ids is different', self.position_ids, baseline.position_ids)
-
-        if self.has_attention_mask:
-            if not torch.all(self.attention_mask == baseline.attention_mask):
-                print('attention_mask is different', self.attention_mask, baseline.attention_mask)
-
-        assert len(self.past) == len(baseline.past)
-
-        for i, past_i in enumerate(self.past):
-            assert past_i.shape == baseline.past[i].shape
-            if past_i.nelement() > 0:
-                max_past_diff = (past_i - baseline.past[i]).abs().max()
-                if max_past_diff > 1e-4:
-                    print(f"max_past_diff[{i}]={max_past_diff}")
+        # Remove reshape nodes that having same shape of input and output based on symbolic shape inference.
+        self.utils.remove_useless_reshape_nodes()
 
-    @staticmethod
-    def predict_next_token(logits, top_k=1, required_order=False):
-        """
-        Get top k topkens based on logits.
-        """
+        self.postprocess()
 
-        # logits has shape (batch_size, seq_len, vocab_size)
-        # last token logits has shape (batch_size, vocab_size)
-        lastTokenLogits = logits[:, -1]
-        if top_k == 1:
-            generatedTokens = torch.argmax(lastTokenLogits, 1, True)
-            return generatedTokens
-        else:
-            topk = torch.argsort(lastTokenLogits, -1, descending=True)[:, :top_k]
-            if not required_order:
-                sorted_topk, _ = topk.sort()
-                return sorted_topk
-            return topk
+        # Bias fusion is done after postprocess to avoid extra Reshape between bias and Gelu/FastGelu/SkipLayerNormalization
+        if (options is None) or options.enable_bias_gelu:
+            # Fuse Gelu and Add Bias before it.
+            self.fuse_bias_gelu(is_fastgelu=True)
+            self.fuse_bias_gelu(is_fastgelu=False)
 
-    @staticmethod
-    def diff_present(onnx_output, onnx_io_output, n_layer):
-        """
-        Compare the present outputs of two outputs from ONNX Runtime.
-        """
-        present_diff_max = []
-        for i in range(n_layer):
-            onnx_present_i = torch.from_numpy(onnx_output[i + 1]) if isinstance(onnx_output[i + 1],
-                                                                                numpy.ndarray) else onnx_output[i + 1]
-            onnx_io_present_i = torch.from_numpy(onnx_io_output[i + 1]) if isinstance(
-                onnx_io_output[i + 1], numpy.ndarray) else onnx_io_output[i + 1]
-            max_diff = (onnx_present_i - onnx_io_present_i).abs().max()
-            present_diff_max.append(max_diff)
-        print(f"present_diff_max={present_diff_max}")
+        if (options is None) or options.enable_bias_skip_layer_norm:
+            # Fuse SkipLayerNormalization and Add Bias before it.
+            self.fuse_add_bias_skip_layer_norm()
+
+        if options is not None and options.enable_gelu_approximation:
+            self.gelu_approximation()
+
+        if options is not None and options.enable_gemm_fast_gelu:
+            self.fuse_gemm_fast_gelu()
+
+        self.remove_unused_constant()
 
-    @staticmethod
-    def is_quantized_onnx_model(onnx_model_path):
+        # Use symbolic batch dimension in input and output.
+        if add_dynamic_axes:
+            self.use_dynamic_axes()
+
+        logger.info(f"opset version: {self.get_opset_version()}")
+
+    def get_fused_operator_statistics(self):
         """
-        Returns True if the ONNX model is quantized.
+        Returns node count of fused operators.
         """
-        from onnx import load
-        model = load(onnx_model_path)
-        from onnxruntime.quantization.quantize import __producer__ as quantize_producer
-        return model.producer_name == quantize_producer
-
-    @staticmethod
-    def test_generation(session,
-                        model,
-                        device,
-                        test_inputs,
-                        precision=Precision.FLOAT32,
-                        model_class='Gpt2LMHeadModel',
-                        top_k=20,
-                        top_k_no_order=True,
-                        max_steps=24,
-                        max_inputs=0,
-                        verbose=False,
-                        save_test_data=0,
-                        save_test_data_dir='.'):
+        op_count = {}
+        ops = [
+            "EmbedLayerNormalization",
+            "Attention",
+            "MultiHeadAttention",
+            "Gelu",
+            "FastGelu",
+            "BiasGelu",
+            "GemmFastGelu",
+            "LayerNormalization",
+            "SkipLayerNormalization",
+        ]
+        q_ops = ["QOrderedAttention", "QOrderedGelu", "QOrderedLayerNormalization", "QOrderedMatMul"]
+        for op in ops + q_ops:
+            nodes = self.get_nodes_by_op_type(op)
+            op_count[op] = len(nodes)
+
+        logger.info(f"Optimized operators:{op_count}")
+        return op_count
+
+    def is_fully_optimized(self):
         """
-        Test Generation using greedy beam search (without sampling) to compare PyTorch and ONNX model.
-        It will print top 1 and top k errors on the given test inputs.
+        Returns True when the model is fully optimized.
         """
-        print(
-            f"start test generation: (top_k={top_k} top_k_no_order={top_k_no_order} max_steps={max_steps} test_inputs={len(test_inputs)} max_inputs={max_inputs})"
-        )
-        n_layer = model.config.n_layer
-        n_head = model.config.n_head
-        n_embd = model.config.n_embd
-        eos_token_id = model.config.eos_token_id
-        test_data_saved = 0
-
-        is_float16 = (precision == Precision.FLOAT16)
-        if is_float16:
-            assert 'float16' in session.get_outputs()[0].type
-
-        # We will still use fp32 torch model as baseline when onnx model if fp16
-        model.eval().to(device)
-
-        # Allocate initial buffers for IO Binding of ONNX Runtimne. The buffer size will automatically increase later.
-        init_output_shapes = Gpt2Helper.get_output_shapes(batch_size=4,
-                                                          past_sequence_length=128,
-                                                          sequence_length=32,
-                                                          config=model.config,
-                                                          model_class=model_class)
-        output_buffers = Gpt2Helper.get_output_buffers(init_output_shapes, device, is_float16=is_float16)
-
-        baseline_name = 'Torch'
-        treatment_name = 'Quantized Onnx' if precision == Precision.INT8 else "Onnx"
-        torch_metric = Gpt2Metric(baseline_name, baseline_name, top_k)
-        onnx_metric = Gpt2Metric(treatment_name, baseline_name, top_k)
-        onnx_io_metric = Gpt2Metric(treatment_name + ' with IO Binding', baseline_name, top_k)
-
-        for i, inputs in enumerate(test_inputs):
-            if (max_inputs > 0 and i == max_inputs):
-                break
-            if i % 10 == 0:
-                print(f"{i}")
-            input_ids = inputs["input_ids"]
-            position_ids = inputs["position_ids"] if "position_ids" in inputs else None
-            attention_mask = inputs["attention_mask"] if "attention_mask" in inputs else None
-
-            onnx_runner = Gpt2Tester(input_ids, position_ids, attention_mask, n_head, n_embd, n_layer, device,
-                                     is_float16, top_k, not top_k_no_order)
-            onnx_io_runner = Gpt2Tester(input_ids, position_ids, attention_mask, n_head, n_embd, n_layer, device,
-                                        is_float16, top_k, not top_k_no_order)
-            torch_runner = Gpt2Tester(input_ids, position_ids, attention_mask, n_head, n_embd, n_layer, device, False,
-                                      top_k, not top_k_no_order)  # Torch model baseline is fp32
-
-            batch_size = torch_runner.batch_size
-            onnx_metric.start_batch(batch_size)
-            onnx_io_metric.start_batch(batch_size)
-
-            with torch.no_grad():
-                done = torch.zeros(batch_size, dtype=torch.bool)
-                for step in range(max_steps):
-                    seq_len = list(onnx_runner.input_ids.size())[1]
-                    past_seq_len = list(onnx_runner.past[0].size())[3]
-
-                    start_time = timeit.default_timer()
-                    pytorch_output = Gpt2Helper.pytorch_inference(model, torch_runner.get_inputs())
-                    torch_metric.add_latency(past_seq_len, timeit.default_timer() - start_time)
-                    torch_runner.update(pytorch_output, step, device)
-
-                    onnx_output, avg_latency_ms = Gpt2Helper.onnxruntime_inference(session,
-                                                                                   onnx_runner.get_inputs(),
-                                                                                   total_runs=1)
-                    onnx_metric.add_latency(past_seq_len, avg_latency_ms / 1000.0)
-                    onnx_runner.update(onnx_output, step, device)
-
-                    output_shapes = Gpt2Helper.get_output_shapes(batch_size,
-                                                                 past_seq_len,
-                                                                 seq_len,
-                                                                 model.config,
-                                                                 model_class=model_class)
-                    Gpt2Helper.auto_increase_buffer_size(output_buffers, output_shapes)
-
-                    onnx_io_output, avg_latency_ms = Gpt2Helper.onnxruntime_inference_with_binded_io(
-                        session,
-                        onnx_io_runner.get_inputs(),
-                        output_buffers,
-                        output_shapes,
-                        total_runs=1,
-                        return_numpy=False,
-                        include_copy_output_latency=True)
-                    onnx_io_metric.add_latency(past_seq_len, avg_latency_ms / 1000.0)
-
-                    if test_data_saved < save_test_data:
-                        onnx_io_runner.save_test_data(session, onnx_io_output, save_test_data_dir, test_data_saved)
-                        test_data_saved += 1
-
-                    onnx_io_runner.update(onnx_io_output, step, device)
-
-                    if verbose:
-                        onnx_runner.diff(onnx_io_runner)
-                        Gpt2Tester.diff_present(onnx_output, onnx_io_output, n_layer)
-
-                        print("Top 1 tokens:")
-                        print("\tTorch", torch_runner.top_1_tokens)
-                        print("\tONNX", onnx_runner.top_1_tokens)
-                        print("\tONNX with IO binding", onnx_io_runner.top_1_tokens)
-
-                    onnx_metric.eval_batch(torch_runner, onnx_runner, past_seq_len, verbose=verbose)
-                    onnx_io_metric.eval_batch(torch_runner, onnx_io_runner, past_seq_len, verbose=verbose)
-
-                    done = done | (torch_runner.top_1_tokens == eos_token_id).any()
-                    if torch.all(done):
-                        break
-
-            onnx_metric.end_batch()
-            onnx_io_metric.end_batch()
-
-        torch_metric.print()
-        onnx_metric.print()
-        onnx_io_metric.print()
+        op_count = self.get_fused_operator_statistics()
+        embed = op_count["EmbedLayerNormalization"]
+        attention = op_count["Attention"] + op_count["MultiHeadAttention"] + op_count["QOrderedAttention"]
+        gelu = op_count["Gelu"] + op_count["BiasGelu"] + op_count["FastGelu"]
+        layer_norm = op_count["LayerNormalization"] + op_count["SkipLayerNormalization"]
+        is_perfect = (embed > 0) and (attention > 0) and (attention == gelu) and (layer_norm >= 2 * attention)
+
+        if layer_norm == 0:
+            logger.debug("Layer Normalization not fused")
+
+        if gelu == 0:
+            logger.debug("Gelu/FastGelu not fused")
+
+        if embed == 0:
+            logger.debug("Embed Layer not fused")
+
+        if attention == 0:
+            logger.warning("Attention not fused")
+
+        return is_perfect
+
+    def convert_to_packing_mode(self, use_symbolic_shape_infer: bool = False):
+        packing_mode = PackingMode(self)
+        packing_mode.convert(use_symbolic_shape_infer)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/huggingface_models.py` & `onnxruntime/transformers/huggingface_models.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,25 +2,43 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 # Maps model class name to a tuple of model class
 MODEL_CLASSES = [
-    'AutoModel', 'AutoModelWithLMHead', 'AutoModelForSequenceClassification', 'AutoModelForQuestionAnswering',
-    'AutoModelForCausalLM',
+    "AutoModel",
+    "AutoModelWithLMHead",
+    "AutoModelForSequenceClassification",
+    "AutoModelForQuestionAnswering",
+    "AutoModelForCausalLM",
 ]
 
 # List of pretrained models: https://huggingface.co/transformers/pretrained_models.html
 # Pretrained model name to a tuple of input names, opset_version, use_external_data_format, optimization model type
 MODELS = {
     # BERT
-    "bert-base-uncased": (["input_ids", "attention_mask", "token_type_ids"], 12, False, "bert"),
-    "bert-large-uncased": (["input_ids", "attention_mask", "token_type_ids"], 12, False, "bert"),
-    "bert-base-cased": (["input_ids", "attention_mask", "token_type_ids"], 12, False, "bert"),
+    "bert-base-uncased": (
+        ["input_ids", "attention_mask", "token_type_ids"],
+        12,
+        False,
+        "bert",
+    ),
+    "bert-large-uncased": (
+        ["input_ids", "attention_mask", "token_type_ids"],
+        12,
+        False,
+        "bert",
+    ),
+    "bert-base-cased": (
+        ["input_ids", "attention_mask", "token_type_ids"],
+        12,
+        False,
+        "bert",
+    ),
     # "bert-large-cased": (["input_ids", "attention_mask", "token_type_ids"], 12, False, "bert"),
     # "bert-base-multilingual-uncased": (["input_ids", "attention_mask", "token_type_ids"], 12, False, "bert"),
     # "bert-base-multilingual-cased": (["input_ids", "attention_mask", "token_type_ids"], 12, False, "bert"),
     # "bert-base-chinese": (["input_ids", "attention_mask", "token_type_ids"], 12, False, "bert"),
     # "bert-base-german-cased": (["input_ids", "attention_mask", "token_type_ids"], 12, False, "bert"),
     # "bert-large-uncased-whole-word-masking": (["input_ids", "attention_mask", "token_type_ids"], 12, False, "bert"),
     # "bert-large-cased-whole-word-masking": (["input_ids", "attention_mask", "token_type_ids"], 12, False, "bert"),
@@ -51,64 +69,67 @@
     "xlm-mlm-enfr-1024": (["input_ids"], 11, False, "bert"),
     # RoBERTa
     "roberta-base": (["input_ids", "attention_mask"], 12, False, "bert"),
     "roberta-large": (["input_ids", "attention_mask"], 12, False, "bert"),
     "roberta-large-mnli": (["input_ids", "attention_mask"], 12, False, "bert"),
     "deepset/roberta-base-squad2": (["input_ids", "attention_mask"], 11, False, "bert"),
     "distilroberta-base": (["input_ids", "attention_mask"], 12, False, "bert"),
-
     # DistilBERT
     "distilbert-base-uncased": (["input_ids", "attention_mask"], 11, False, "bert"),
-    "distilbert-base-uncased-distilled-squad": (["input_ids", "attention_mask"], 11, False, "bert"),
+    "distilbert-base-uncased-distilled-squad": (
+        ["input_ids", "attention_mask"],
+        11,
+        False,
+        "bert",
+    ),
     # CTRL
     "ctrl": (["input_ids"], 11, True, "bert"),
     # CamemBERT
     "camembert-base": (["input_ids"], 11, False, "bert"),
     # ALBERT
     "albert-base-v1": (["input_ids"], 12, False, "bert"),
     "albert-large-v1": (["input_ids"], 12, False, "bert"),
     "albert-xlarge-v1": (["input_ids"], 12, True, "bert"),
-    #"albert-xxlarge-v1": (["input_ids"], 12, True, "bert"),
+    # "albert-xxlarge-v1": (["input_ids"], 12, True, "bert"),
     "albert-base-v2": (["input_ids"], 12, False, "bert"),
     "albert-large-v2": (["input_ids"], 12, False, "bert"),
     "albert-xlarge-v2": (["input_ids"], 12, True, "bert"),
-    #"albert-xxlarge-v2": (["input_ids"], 12, True, "bert"),
+    # "albert-xxlarge-v2": (["input_ids"], 12, True, "bert"),
     # T5 (use benchmark_t5.py instead)
     # "t5-small": (["input_ids", "decoder_input_ids"], 12, False, "bert"),
     # "t5-base": (["input_ids", "decoder_input_ids"], 12, False, "bert"),
     # "t5-large": (["input_ids", "decoder_input_ids"], 12, True, "bert"),
     # "t5-3b": (["input_ids", "decoder_input_ids"], 12, True, "bert"),
     # "t5-11b": (["input_ids", "decoder_input_ids"], 12, True, "bert"),
-    #"valhalla/t5-small-qa-qg-hl": (["input_ids"], 12, True, "bert"),
+    # "valhalla/t5-small-qa-qg-hl": (["input_ids"], 12, True, "bert"),
     # XLM-RoBERTa
     "xlm-roberta-base": (["input_ids"], 11, False, "bert"),
     "xlm-roberta-large": (["input_ids"], 11, True, "bert"),
     # FlauBERT
     "flaubert/flaubert_small_cased": (["input_ids"], 11, False, "bert"),
-    #"flaubert/flaubert_base_uncased": (["input_ids"], 11, False, "bert"),
+    # "flaubert/flaubert_base_uncased": (["input_ids"], 11, False, "bert"),
     "flaubert/flaubert_base_cased": (["input_ids"], 11, False, "bert"),
-    #"flaubert/flaubert_large_cased": (["input_ids"], 11, False, "bert"),
+    # "flaubert/flaubert_large_cased": (["input_ids"], 11, False, "bert"),
     # Bart
     "facebook/bart-large": (["input_ids", "attention_mask"], 11, False, "bart"),
     "facebook/bart-base": (["input_ids", "attention_mask"], 11, False, "bart"),
     "facebook/bart-large-mnli": (["input_ids", "attention_mask"], 11, False, "bart"),
     "facebook/bart-large-cnn": (["input_ids", "attention_mask"], 11, False, "bart"),
-
     # DialoGPT
     "microsoft/DialoGPT-small": (["input_ids"], 11, False, "gpt2"),
     "microsoft/DialoGPT-medium": (["input_ids"], 11, False, "gpt2"),
-    #"microsoft/DialoGPT-large": (["input_ids"], 11, True, "gpt2"),
+    # "microsoft/DialoGPT-large": (["input_ids"], 11, True, "gpt2"),
     # Reformer
-    #"google/reformer-enwik8": (["input_ids"], 11, False, "bert"),
-    #"google/reformer-crime-and-punishment": (["input_ids"], 11, False, "bert"),
+    # "google/reformer-enwik8": (["input_ids"], 11, False, "bert"),
+    # "google/reformer-crime-and-punishment": (["input_ids"], 11, False, "bert"),
     # MarianMT
-    #"Helsinki-NLP/opus-mt-ROMANCE-en": (["input_ids"], 12, False, "bert"),
+    # "Helsinki-NLP/opus-mt-ROMANCE-en": (["input_ids"], 12, False, "bert"),
     # Longformer (use benchmark_longformer.py instead)
-    #"allenai/longformer-base-4096": (["input_ids"], 12, False, "bert"),
-    #"allenai/longformer-large-4096": (["input_ids"], 12, False, "bert"),
+    # "allenai/longformer-base-4096": (["input_ids"], 12, False, "bert"),
+    # "allenai/longformer-large-4096": (["input_ids"], 12, False, "bert"),
     # MBart
     "facebook/mbart-large-cc25": (["input_ids"], 11, True, "bert"),
     "facebook/mbart-large-en-ro": (["input_ids"], 11, True, "bert"),
     # "Helsinki-NLP/opus-mt-ROMANCE-en": (["input_ids"], 12, False, "bert"),
     # # Longformer
     # "allenai/longformer-base-4096": (["input_ids"], 12, False, "bert"),
     # "allenai/longformer-large-4096": (["input_ids"], 12, True, "bert"),
@@ -125,11 +146,22 @@
     # Layoutlm
     "microsoft/layoutlm-base-uncased": (["input_ids"], 11, False, "bert"),
     "microsoft/layoutlm-large-uncased": (["input_ids"], 11, False, "bert"),
     # Squeezebert
     "squeezebert/squeezebert-uncased": (["input_ids"], 11, False, "bert"),
     "squeezebert/squeezebert-mnli": (["input_ids"], 11, False, "bert"),
     "squeezebert/squeezebert-mnli-headless": (["input_ids"], 11, False, "bert"),
-    "unc-nlp/lxmert-base-uncased": (["input_ids", "visual_feats", "visual_pos"], 11, False, "bert"),
+    "unc-nlp/lxmert-base-uncased": (
+        ["input_ids", "visual_feats", "visual_pos"],
+        11,
+        False,
+        "bert",
+    ),
     # "google/pegasus-xsum": (["input_ids"], 11, False, "bert"),
     # "google/pegasus-large": (["input_ids"], 11, False, "bert"),
+    # ViT
+    "google/vit-base-patch16-224": (["pixel_values"], 12, False, "vit"),
+    # Swin
+    "microsoft/swin-base-patch4-window7-224": (["pixel_values"], 12, False, "swin"),
+    "microsoft/swin-small-patch4-window7-224": (["pixel_values"], 12, False, "swin"),
+    "microsoft/swin-tiny-patch4-window7-224": (["pixel_values"], 12, False, "swin"),
 }
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/io_binding_helper.py` & `onnxruntime/transformers/io_binding_helper.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,79 +1,85 @@
+import logging
+from typing import Dict, List
+
 import numpy
 import torch
-import logging
-from typing import List, Dict, Union
+
 from onnxruntime import InferenceSession
 
 logger = logging.getLogger(__name__)
 
 
 class TypeHelper:
-
     @staticmethod
     def get_input_type(ort_session: InferenceSession, name: str) -> str:
-        for i, input in enumerate(ort_session.get_inputs()):
+        for _i, input in enumerate(ort_session.get_inputs()):
             if input.name == name:
                 return input.type
         raise ValueError(f"input name {name} not found")
 
     @staticmethod
     def get_output_type(ort_session, name: str) -> str:
-        for i, output in enumerate(ort_session.get_outputs()):
+        for _i, output in enumerate(ort_session.get_outputs()):
             if output.name == name:
                 return output.type
 
         raise ValueError(f"output name {name} not found")
 
     @staticmethod
     def ort_type_to_numpy_type(ort_type: str):
         ort_type_to_numpy_type_map = {
             "tensor(int64)": numpy.longlong,
-            "tensor(int32)": numpy.int32,  #numpy.intc?
+            "tensor(int32)": numpy.intc,
             "tensor(float)": numpy.float32,
             "tensor(float16)": numpy.float16,
+            "tensor(bool)": bool,
         }
         if ort_type not in ort_type_to_numpy_type_map:
             raise ValueError(f"{ort_type} not found in map")
 
         return ort_type_to_numpy_type_map[ort_type]
 
     @staticmethod
     def ort_type_to_torch_type(ort_type: str):
         ort_type_to_torch_type_map = {
             "tensor(int64)": torch.int64,
             "tensor(int32)": torch.int32,
             "tensor(float)": torch.float32,
             "tensor(float16)": torch.float16,
+            "tensor(bool)": torch.bool,
         }
         if ort_type not in ort_type_to_torch_type_map:
             raise ValueError(f"{ort_type} not found in map")
 
         return ort_type_to_torch_type_map[ort_type]
 
     @staticmethod
     def numpy_type_to_torch_type(numpy_type: numpy.dtype):
         numpy_type_to_torch_type_map = {
             numpy.longlong: torch.int64,
+            numpy.intc: torch.int32,
             numpy.int32: torch.int32,
             numpy.float32: torch.float32,
             numpy.float16: torch.float16,
+            bool: torch.bool,
         }
         if numpy_type not in numpy_type_to_torch_type_map:
             raise ValueError(f"{numpy_type} not found in map")
 
         return numpy_type_to_torch_type_map[numpy_type]
 
     @staticmethod
     def torch_type_to_numpy_type(torch_type: torch.dtype):
         torch_type_to_numpy_type_map = {
             torch.int64: numpy.longlong,
-            torch.int32: numpy.int32,
+            torch.int32: numpy.intc,
             torch.float32: numpy.float32,
             torch.float16: numpy.float16,
+            torch.bool: bool,
         }
         if torch_type not in torch_type_to_numpy_type_map:
             raise ValueError(f"{torch_type} not found in map")
 
         return torch_type_to_numpy_type_map[torch_type]
 
     @staticmethod
@@ -85,89 +91,117 @@
 
         for output in ort_session.get_outputs():
             name_to_numpy_type[output.name] = TypeHelper.ort_type_to_numpy_type(output.type)
         return name_to_numpy_type
 
 
 class IOBindingHelper:
-
     @staticmethod
     def get_output_buffers(ort_session: InferenceSession, output_shapes, device):
-        """ Returns a dictionary of output name as key, and 1D tensor as value. The tensor has enough space for given shape.
-        """
+        """Returns a dictionary of output name as key, and 1D tensor as value. The tensor has enough space for given shape."""
         output_buffers = {}
         for name, shape in output_shapes.items():
             ort_type = TypeHelper.get_output_type(ort_session, name)
             torch_type = TypeHelper.ort_type_to_torch_type(ort_type)
             output_buffers[name] = torch.empty(numpy.prod(shape), dtype=torch_type, device=device)
         return output_buffers
 
     @staticmethod
-    def prepare_io_binding(ort_session,
-                           input_ids: torch.Tensor,
-                           position_ids: torch.Tensor,
-                           attention_mask: torch.Tensor,
-                           past: List[torch.Tensor],
-                           output_buffers,
-                           output_shapes,
-                           name_to_np_type=None):
-        """ Returnas IO binding object for a session.
-        """
+    def prepare_io_binding(
+        ort_session,
+        input_ids: torch.Tensor,
+        position_ids: torch.Tensor,
+        attention_mask: torch.Tensor,
+        past: List[torch.Tensor],
+        output_buffers,
+        output_shapes,
+        name_to_np_type=None,
+    ):
+        """Returnas IO binding object for a session."""
         if name_to_np_type is None:
             name_to_np_type = TypeHelper.get_io_numpy_type_map(ort_session)
 
         # Bind inputs and outputs to onnxruntime session
         io_binding = ort_session.io_binding()
 
         # Bind inputs
         assert input_ids.is_contiguous()
-        io_binding.bind_input('input_ids', input_ids.device.type, 0, name_to_np_type['input_ids'],
-                              list(input_ids.size()), input_ids.data_ptr())
+        io_binding.bind_input(
+            "input_ids",
+            input_ids.device.type,
+            0,
+            name_to_np_type["input_ids"],
+            list(input_ids.size()),
+            input_ids.data_ptr(),
+        )
 
         if past is not None:
             for i, past_i in enumerate(past):
                 assert past_i.is_contiguous()
 
                 data_ptr = past_i.data_ptr()
                 if data_ptr == 0:
                     # When past_sequence_length is 0, its data_ptr will be zero. IO Binding asserts that data_ptr shall not be zero.
                     # Here we workaround and pass data pointer of input_ids. Actual data is not used for past so it does not matter.
                     data_ptr = input_ids.data_ptr()
 
-                io_binding.bind_input(f'past_{i}', past_i.device.type, 0, name_to_np_type[f'past_{i}'],
-                                      list(past_i.size()), data_ptr)
+                io_binding.bind_input(
+                    f"past_{i}",
+                    past_i.device.type,
+                    0,
+                    name_to_np_type[f"past_{i}"],
+                    list(past_i.size()),
+                    data_ptr,
+                )
 
         if attention_mask is not None:
             assert attention_mask.is_contiguous()
-            io_binding.bind_input('attention_mask', attention_mask.device.type, 0, name_to_np_type['attention_mask'],
-                                  list(attention_mask.size()), attention_mask.data_ptr())
+            io_binding.bind_input(
+                "attention_mask",
+                attention_mask.device.type,
+                0,
+                name_to_np_type["attention_mask"],
+                list(attention_mask.size()),
+                attention_mask.data_ptr(),
+            )
 
         if position_ids is not None:
             assert position_ids.is_contiguous()
-            io_binding.bind_input('position_ids', position_ids.device.type, 0, name_to_np_type['position_ids'],
-                                  list(position_ids.size()), position_ids.data_ptr())
+            io_binding.bind_input(
+                "position_ids",
+                position_ids.device.type,
+                0,
+                name_to_np_type["position_ids"],
+                list(position_ids.size()),
+                position_ids.data_ptr(),
+            )
 
         # Bind outputs
         for output in ort_session.get_outputs():
             output_name = output.name
             output_buffer = output_buffers[output_name]
             logger.debug(f"{output_name} device type={output_buffer.device.type} shape={list(output_buffer.size())}")
-            io_binding.bind_output(output_name, output_buffer.device.type, 0, name_to_np_type[output_name],
-                                   output_shapes[output_name], output_buffer.data_ptr())
+            io_binding.bind_output(
+                output_name,
+                output_buffer.device.type,
+                0,
+                name_to_np_type[output_name],
+                output_shapes[output_name],
+                output_buffer.data_ptr(),
+            )
 
         return io_binding
 
     @staticmethod
     def get_outputs_from_io_binding_buffer(ort_session, output_buffers, output_shapes, return_numpy=True):
-        """ Copy results to cpu. Returns a list of numpy array.
-        """
+        """Copy results to cpu. Returns a list of numpy array."""
         ort_outputs = []
         for output in ort_session.get_outputs():
             output_name = output.name
             buffer = output_buffers[output_name]
             shape = output_shapes[output_name]
-            copy_tensor = buffer[0:numpy.prod(shape)].reshape(shape).clone().detach()
+            copy_tensor = buffer[0 : numpy.prod(shape)].reshape(shape).clone().detach()
             if return_numpy:
                 ort_outputs.append(copy_tensor.cpu().numpy())
             else:
                 ort_outputs.append(copy_tensor)
         return ort_outputs
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/machine_info.py` & `onnxruntime/transformers/machine_info.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,34 +1,47 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 # It is used to dump machine information for Notebooks
 
 import argparse
-import logging
-from typing import List, Dict, Union, Tuple
-import cpuinfo
-import psutil
 import json
-import sys
+import logging
 import platform
+import sys  # noqa: F401
 from os import environ
-from py3nvml.py3nvml import nvmlInit, nvmlSystemGetDriverVersion, nvmlDeviceGetCount, nvmlDeviceGetHandleByIndex, \
-                            nvmlDeviceGetMemoryInfo, nvmlDeviceGetName, nvmlShutdown, NVMLError
+from typing import Dict, List, Tuple, Union  # noqa: F401
+
+import cpuinfo
+import psutil
+from py3nvml.py3nvml import (
+    NVMLError,
+    nvmlDeviceGetCount,
+    nvmlDeviceGetHandleByIndex,
+    nvmlDeviceGetMemoryInfo,
+    nvmlDeviceGetName,
+    nvmlInit,
+    nvmlShutdown,
+    nvmlSystemGetDriverVersion,
+)
 
 
-class MachineInfo():
-    """ Class encapsulating Machine Info logic. """
+class MachineInfo:
+    """Class encapsulating Machine Info logic."""
+
     def __init__(self, silent=False, logger=None):
         self.silent = silent
 
         if logger is None:
-            logging.basicConfig(format="%(asctime)s - %(name)s - %(levelname)s: %(message)s", level=logging.INFO)
+            logging.basicConfig(
+                format="%(asctime)s - %(name)s - %(levelname)s: %(message)s",
+                level=logging.INFO,
+            )
             self.logger = logging.getLogger(__name__)
         else:
             self.logger = logger
 
         self.machine_info = None
         try:
             self.machine_info = self.get_machine_info()
@@ -46,15 +59,15 @@
             "cpu": self.get_cpu_info(),
             "memory": self.get_memory_info(),
             "os": platform.platform(),
             "python": self._try_get(cpu_info, ["python_version"]),
             "packages": self.get_related_packages(),
             "onnxruntime": self.get_onnxruntime_info(),
             "pytorch": self.get_pytorch_info(),
-            "tensorflow": self.get_tensorflow_info()
+            "tensorflow": self.get_tensorflow_info(),
         }
         return machine_info
 
     def get_memory_info(self) -> Dict:
         """Get memory info"""
         mem = psutil.virtual_memory()
         return {"total": mem.total, "available": mem.available}
@@ -75,25 +88,25 @@
         return {
             "brand": self._try_get(cpu_info, ["brand", "brand_raw"]),
             "cores": psutil.cpu_count(logical=False),
             "logical_cores": psutil.cpu_count(logical=True),
             "hz": self._try_get(cpu_info, ["hz_actual"]),
             "l2_cache": self._try_get(cpu_info, ["l2_cache_size"]),
             "flags": self._try_get(cpu_info, ["flags"]),
-            "processor": platform.uname().processor
+            "processor": platform.uname().processor,
         }
 
     def get_gpu_info_by_nvml(self) -> Dict:
         """Get GPU info using nvml"""
         gpu_info_list = []
         driver_version = None
         try:
             nvmlInit()
             driver_version = nvmlSystemGetDriverVersion()
-            deviceCount = nvmlDeviceGetCount()
+            deviceCount = nvmlDeviceGetCount()  # noqa: N806
             for i in range(deviceCount):
                 handle = nvmlDeviceGetHandleByIndex(i)
                 info = nvmlDeviceGetMemoryInfo(handle)
                 gpu_info = {}
                 gpu_info["memory_total"] = info.total
                 gpu_info["memory_available"] = info.free
                 gpu_info["name"] = nvmlDeviceGetName(handle)
@@ -102,86 +115,110 @@
         except NVMLError as error:
             if not self.silent:
                 self.logger.error("Error fetching GPU information using nvml: %s", error)
             return None
 
         result = {"driver_version": driver_version, "devices": gpu_info_list}
 
-        if 'CUDA_VISIBLE_DEVICES' in environ:
-            result["cuda_visible"] = environ['CUDA_VISIBLE_DEVICES']
+        if "CUDA_VISIBLE_DEVICES" in environ:
+            result["cuda_visible"] = environ["CUDA_VISIBLE_DEVICES"]
         return result
 
     def get_related_packages(self) -> List[str]:
         import pkg_resources
+
         installed_packages = pkg_resources.working_set
         related_packages = [
-            'onnxruntime-gpu', 'onnxruntime', 'ort-nightly-gpu', 'ort-nightly', 'onnx', 'transformers', 'protobuf',
-            'sympy', 'torch', 'tensorflow', 'flatbuffers', 'numpy', 'onnxconverter-common'
+            "onnxruntime-gpu",
+            "onnxruntime",
+            "ort-nightly-gpu",
+            "ort-nightly",
+            "onnx",
+            "transformers",
+            "protobuf",
+            "sympy",
+            "torch",
+            "tensorflow",
+            "flatbuffers",
+            "numpy",
+            "onnxconverter-common",
         ]
         related_packages_list = {i.key: i.version for i in installed_packages if i.key in related_packages}
         return related_packages_list
 
     def get_onnxruntime_info(self) -> Dict:
         try:
             import onnxruntime
+
             return {
                 "version": onnxruntime.__version__,
-                "support_gpu": 'CUDAExecutionProvider' in onnxruntime.get_available_providers()
+                "support_gpu": "CUDAExecutionProvider" in onnxruntime.get_available_providers(),
             }
         except ImportError as error:
             if not self.silent:
                 self.logger.exception(error)
             return None
         except Exception as exception:
             if not self.silent:
                 self.logger.exception(exception, False)
             return None
 
     def get_pytorch_info(self) -> Dict:
         try:
             import torch
-            return {"version": torch.__version__, "support_gpu": torch.cuda.is_available(), "cuda": torch.version.cuda}
+
+            return {
+                "version": torch.__version__,
+                "support_gpu": torch.cuda.is_available(),
+                "cuda": torch.version.cuda,
+            }
         except ImportError as error:
             if not self.silent:
                 self.logger.exception(error)
             return None
         except Exception as exception:
             if not self.silent:
                 self.logger.exception(exception, False)
             return None
 
     def get_tensorflow_info(self) -> Dict:
         try:
             import tensorflow as tf
+
             return {
                 "version": tf.version.VERSION,
                 "git_version": tf.version.GIT_VERSION,
-                "support_gpu": tf.test.is_built_with_cuda()
+                "support_gpu": tf.test.is_built_with_cuda(),
             }
         except ImportError as error:
             if not self.silent:
                 self.logger.exception(error)
             return None
         except ModuleNotFoundError as error:
             if not self.silent:
                 self.logger.exception(error)
             return None
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
 
-    parser.add_argument('--silent', required=False, action='store_true', help="Do not print error message")
+    parser.add_argument(
+        "--silent",
+        required=False,
+        action="store_true",
+        help="Do not print error message",
+    )
     parser.set_defaults(silent=False)
 
     args = parser.parse_args()
     return args
 
 
 def get_machine_info(silent=True) -> str:
     machine = MachineInfo(silent)
     return json.dumps(machine.machine_info, indent=2)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     args = parse_arguments()
     print(get_machine_info(args.silent))
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_exporter.py` & `onnxruntime/transformers/onnx_exporter.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,75 +1,84 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
-import numpy
 import os
-import torch
+import sys
 from pathlib import Path
-from transformers import AutoConfig, AutoTokenizer, LxmertConfig, TransfoXLConfig
+
+import numpy
+import torch
 from affinity_helper import AffinitySetting
-from benchmark_helper import create_onnxruntime_session, Precision, OptimizerInfo
-from gpt2_helper import GPT2ModelNoPastState, PRETRAINED_GPT2_MODELS, TFGPT2ModelNoPastState
-from quantize_helper import QuantizeHelper
+from benchmark_helper import OptimizerInfo, Precision, create_onnxruntime_session
 from huggingface_models import MODEL_CLASSES
+from quantize_helper import QuantizeHelper
 from torch_onnx_export_helper import torch_onnx_export
+from transformers import AutoConfig, AutoFeatureExtractor, AutoTokenizer, LxmertConfig, TransfoXLConfig
 
-os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
+sys.path.append(os.path.join(os.path.dirname(__file__), "models", "gpt2"))
+from gpt2_helper import PRETRAINED_GPT2_MODELS, GPT2ModelNoPastState, TFGPT2ModelNoPastState  # noqa: E402
+
+os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
 
 logger = logging.getLogger(__name__)
 
-# Walkaround by replacing torch.triu using self-defined op
+# Workaround by replacing torch.triu using self-defined op
 # Since torch.triu cannot be exported to ONNX. See https://github.com/pytorch/pytorch/issues/32968
 torch_func = {"triu": torch.triu}
 
 
 def triu_onnx(x, diagonal=0, out=None):
     assert out is None
     assert len(x.shape) == 2 and x.size(0) == x.size(1)
 
     torch_triu = torch_func["triu"]
     template = torch_triu(torch.ones((1024, 1024), dtype=torch.uint8), diagonal)
-    mask = template[:x.size(0), :x.size(1)]
+    mask = template[: x.size(0), : x.size(1)]
     return torch.where(mask.bool(), x, torch.zeros_like(x))
 
 
 def replace_torch_functions():
     torch.triu = triu_onnx
 
 
 def restore_torch_functions():
     torch.triu = torch_func["triu"]
 
 
 def create_onnxruntime_input(vocab_size, batch_size, sequence_length, input_names, config, data_type=numpy.int64):
-    input_ids = numpy.random.randint(low=0, high=vocab_size - 1, size=(batch_size, sequence_length), dtype=data_type)
+    if config.model_type in ["vit", "swin"]:
+        input_ids = numpy.random.rand(batch_size, 3, config.image_size, config.image_size).astype(numpy.float32)
+        inputs = {"pixel_values": input_ids}
+        return inputs
 
-    inputs = {'input_ids': input_ids}
+    input_ids = numpy.random.randint(low=0, high=vocab_size - 1, size=(batch_size, sequence_length), dtype=data_type)
+    inputs = {"input_ids": input_ids}
 
     if "attention_mask" in input_names:
         attention_mask = numpy.ones([batch_size, sequence_length], dtype=data_type)
-        inputs['attention_mask'] = attention_mask
+        inputs["attention_mask"] = attention_mask
 
     if "token_type_ids" in input_names:
         segment_ids = numpy.zeros([batch_size, sequence_length], dtype=data_type)
-        inputs['token_type_ids'] = segment_ids
+        inputs["token_type_ids"] = segment_ids
 
     if config.is_encoder_decoder:
-        inputs['decoder_input_ids'] = input_ids
+        inputs["decoder_input_ids"] = input_ids
 
     if isinstance(config, LxmertConfig):
         inputs["visual_feats"] = numpy.random.randn(1, 1, config.visual_feat_dim).astype(numpy.float32)
         inputs["visual_pos"] = numpy.random.randn(1, 1, config.visual_pos_dim).astype(numpy.float32)
     if isinstance(config, TransfoXLConfig):
-        inputs["tf_transfo_xl_model/transformer/pos_emb/einsum/Einsum/inputs_1:0"] = numpy.zeros([config.hidden_size],
-                                                                                                 dtype=numpy.float32)
+        inputs["tf_transfo_xl_model/transformer/pos_emb/einsum/Einsum/inputs_1:0"] = numpy.zeros(
+            [config.hidden_size], dtype=numpy.float32
+        )
     return inputs
 
 
 def filter_inputs(inputs, input_names):
     remaining_model_inputs = {}
     for input_name in input_names:
         if input_name in inputs:
@@ -86,70 +95,92 @@
         res_list.append(i) if not isinstance(i, (list, tuple)) else update_flatten_list(i, res_list)
     return res_list
 
 
 def build_dynamic_axes(example_inputs, outputs_flatten):
     sequence_length = example_inputs["input_ids"].shape[-1]
 
-    dynamic_axes = {key: {0: 'batch_size', 1: 'seq_len'} for key in example_inputs.keys()}
+    dynamic_axes = {key: {0: "batch_size", 1: "seq_len"} for key in example_inputs}
 
-    output_names = ['output_' + str(i + 1) for i in range(len(outputs_flatten))]
+    output_names = ["output_" + str(i + 1) for i in range(len(outputs_flatten))]
     for i, output_name in enumerate(output_names):
-        dynamic_axes[output_name] = {0: 'batch_size'}
+        dynamic_axes[output_name] = {0: "batch_size"}
         dims = outputs_flatten[i].shape
         for j, dim in enumerate(dims):
             if dim == sequence_length:
-                dynamic_axes[output_name].update({j: 'seq_len'})
+                dynamic_axes[output_name].update({j: "seq_len"})
     return dynamic_axes, output_names
 
 
-def validate_onnx_model(onnx_model_path, example_inputs, example_outputs_flatten, use_gpu, fp16, output_names=None):
+def validate_onnx_model(
+    onnx_model_path,
+    example_inputs,
+    example_outputs_flatten,
+    use_gpu,
+    fp16,
+    output_names=None,
+):
     test_session = create_onnxruntime_session(onnx_model_path, use_gpu, enable_all_optimization=False)
     if test_session is None:
         logger.error(f"{onnx_model_path} is an invalid ONNX model")
         return False
 
     logger.info(f"{onnx_model_path} is a valid ONNX model")
 
     # Compare the inference result with PyTorch or Tensorflow
     example_ort_inputs = {k: t.numpy() for k, t in example_inputs.items()}
     example_ort_outputs = test_session.run(output_names, example_ort_inputs)
     if len(example_outputs_flatten) != len(example_ort_outputs):
         logger.error(
-            f"Number of output tensors expected {len(example_outputs_flatten)}, got {len(example_ort_outputs)}")
+            f"Number of output tensors expected {len(example_outputs_flatten)}, got {len(example_ort_outputs)}"
+        )
         return False
 
     for i in range(len(example_outputs_flatten)):
         abs_diff = numpy.amax(numpy.abs(example_ort_outputs[i] - example_outputs_flatten[i].cpu().numpy()))
         if abs_diff > 1e-4:
             logger.info(f"Max absolute diff={abs_diff} for output tensor {i}")
 
         rtol = 5e-02 if fp16 else 1e-4
         atol = 1e-01 if fp16 else 1e-4
-        if not numpy.allclose(example_ort_outputs[i], example_outputs_flatten[i].cpu().numpy(), rtol=rtol, atol=atol):
+        if not numpy.allclose(
+            example_ort_outputs[i],
+            example_outputs_flatten[i].cpu().numpy(),
+            rtol=rtol,
+            atol=atol,
+        ):
             logger.error(f"Output tensor {i} is not close: rtol={rtol}, atol={atol}")
             return False
 
     logger.info(f"inference result of onnxruntime is validated on {onnx_model_path}")
     return True
 
 
-def get_onnx_file_path(onnx_dir: str, model_name: str, input_count: int, optimized_by_script: bool, use_gpu: bool,
-                       precision: Precision, optimized_by_onnxruntime: bool, use_external_data: bool):
+def get_onnx_file_path(
+    onnx_dir: str,
+    model_name: str,
+    input_count: int,
+    optimized_by_script: bool,
+    use_gpu: bool,
+    precision: Precision,
+    optimized_by_onnxruntime: bool,
+    use_external_data: bool,
+):
     from re import sub
-    normalized_model_name = sub(r'[^a-zA-Z0-9_]', '_', model_name)
+
+    normalized_model_name = sub(r"[^a-zA-Z0-9_]", "_", model_name)
 
     if not optimized_by_script:
         filename = f"{normalized_model_name}_{input_count}"
     else:
         device = "gpu" if use_gpu else "cpu"
         filename = f"{normalized_model_name}_{input_count}_{precision}_{device}"
 
     if optimized_by_onnxruntime:
-        filename += f"_ort"
+        filename += "_ort"
 
     directory = onnx_dir
     # ONNXRuntime will not write external data so the raw and optimized models shall be in same directory.
     if use_external_data and not optimized_by_onnxruntime:
         directory = os.path.join(onnx_dir, filename)
         if not os.path.exists(directory):
             os.makedirs(directory)
@@ -168,81 +199,104 @@
     path = Path(file_path)
     return str(path.parent.joinpath(path.stem + suffix).with_suffix(path.suffix))
 
 
 def optimize_onnx_model_by_ort(onnx_model_path, ort_model_path, use_gpu, overwrite, model_fusion_statistics):
     if overwrite or not os.path.exists(ort_model_path):
         Path(ort_model_path).parent.mkdir(parents=True, exist_ok=True)
-        from optimizer import optimize_by_onnxruntime, get_fusion_statistics
+        from optimizer import get_fusion_statistics, optimize_by_onnxruntime
+
         # Use onnxruntime to optimize model, which will be saved to *_ort.onnx
-        opt_model = optimize_by_onnxruntime(onnx_model_path,
-                                            use_gpu=use_gpu,
-                                            optimized_model_path=ort_model_path,
-                                            opt_level=99)
+        _ = optimize_by_onnxruntime(
+            onnx_model_path,
+            use_gpu=use_gpu,
+            optimized_model_path=ort_model_path,
+            opt_level=99,
+        )
         model_fusion_statistics[ort_model_path] = get_fusion_statistics(ort_model_path)
     else:
         logger.info(f"Skip optimization since model existed: {ort_model_path}")
 
 
-def optimize_onnx_model(model_name, onnx_model_path, optimized_model_path, model_type, num_attention_heads, hidden_size,
-                        use_gpu, precision, use_raw_attention_mask, overwrite, model_fusion_statistics,
-                        use_external_data_format, optimization_options=None):
+def optimize_onnx_model(
+    onnx_model_path,
+    optimized_model_path,
+    model_type,
+    num_attention_heads,
+    hidden_size,
+    use_gpu,
+    precision,
+    use_raw_attention_mask,
+    overwrite,
+    model_fusion_statistics,
+    use_external_data_format,
+    optimization_options=None,
+):
     if overwrite or not os.path.exists(optimized_model_path):
         Path(optimized_model_path).parent.mkdir(parents=True, exist_ok=True)
 
-        from optimizer import optimize_model
         from fusion_options import FusionOptions
-        if optimization_options == None:
-          optimization_options = FusionOptions(model_type)
+        from optimizer import optimize_model
+
+        if optimization_options is None:
+            optimization_options = FusionOptions(model_type)
         optimization_options.use_raw_attention_mask(use_raw_attention_mask)
-        if Precision.FLOAT16 == precision:
+        if precision == Precision.FLOAT16:
             optimization_options.enable_gelu_approximation = True
-        if Precision.INT8 == precision:
+        if precision == Precision.INT8:
             optimization_options.enable_embed_layer_norm = False
 
+        # For swin models, the num_attention_heads is a list, which isn't supported yet, so set to 0 for now
+        if model_type == "swin":
+            num_attention_heads = 0
+            hidden_size = 0
+
         # Use script to optimize model.
         # Use opt_level <= 1 for models to be converted to fp16, because some fused op (like FusedGemm) has only fp32 and no fp16.
         # It is better to be conservative so we use opt_level=0 here, in case MemcpyFromHost is added to the graph by OnnxRuntime.
-        opt_model = optimize_model(onnx_model_path,
-                                   model_type,
-                                   num_heads=num_attention_heads,
-                                   hidden_size=hidden_size,
-                                   opt_level=0,
-                                   optimization_options=optimization_options,
-                                   use_gpu=use_gpu,
-                                   only_onnxruntime=False)
-        if model_type == 'bert_keras' or model_type == "bert_tf":
+        opt_model = optimize_model(
+            onnx_model_path,
+            model_type,
+            num_heads=num_attention_heads,
+            hidden_size=hidden_size,
+            opt_level=0,
+            optimization_options=optimization_options,
+            use_gpu=use_gpu,
+            only_onnxruntime=False,
+        )
+        if model_type == "bert_keras" or model_type == "bert_tf":
             opt_model.use_dynamic_axes()
 
         model_fusion_statistics[optimized_model_path] = opt_model.get_fused_operator_statistics()
 
-        if Precision.FLOAT16 == precision:
+        if precision == Precision.FLOAT16:
             opt_model.convert_float_to_float16(keep_io_types=True)
 
         opt_model.save_model_to_file(optimized_model_path, use_external_data_format)
     else:
         logger.info(f"Skip optimization since model existed: {optimized_model_path}")
 
 
 def modelclass_dispatcher(model_name, custom_model_class):
-    if (custom_model_class != None):
-        if (custom_model_class in MODEL_CLASSES):
+    if custom_model_class is not None:
+        if custom_model_class in MODEL_CLASSES:
             return custom_model_class
         else:
-            raise Exception("Valid model class: " + ' '.join(MODEL_CLASSES))
+            raise Exception("Valid model class: " + " ".join(MODEL_CLASSES))
 
     if model_name in PRETRAINED_GPT2_MODELS:
         return "GPT2ModelNoPastState"
 
     import re
-    if (re.search('-squad$', model_name) != None):
+
+    if re.search("-squad$", model_name) is not None:
         return "AutoModelForQuestionAnswering"
-    elif (re.search('-mprc$', model_name) != None):
+    elif re.search("-mprc$", model_name) is not None:
         return "AutoModelForSequenceClassification"
-    elif (re.search('gpt2', model_name) != None):
+    elif re.search("gpt2", model_name) is not None:
         return "AutoModelWithLMHead"
 
     return "AutoModel"
 
 
 def load_pretrained_model(model_name, config, cache_dir, custom_model_class, is_tf_model=False):
     model_class_name = modelclass_dispatcher(model_name, custom_model_class)
@@ -250,26 +304,26 @@
     if model_class_name == "GPT2ModelNoPastState":
         if is_tf_model:
             return TFGPT2ModelNoPastState.from_pretrained(model_name, config=config, cache_dir=cache_dir)
         else:
             return GPT2ModelNoPastState.from_pretrained(model_name, config=config, cache_dir=cache_dir)
 
     if is_tf_model:
-        model_class_name = 'TF' + model_class_name
+        model_class_name = "TF" + model_class_name
 
     transformers_module = __import__("transformers", fromlist=[model_class_name])
     logger.info(f"Model class name: {model_class_name}")
     model_class = getattr(transformers_module, model_class_name)
 
     return model_class.from_pretrained(model_name, config=config, cache_dir=cache_dir)
 
 
 def load_pt_model(model_name, model_class, cache_dir, config_modifier):
     config = AutoConfig.from_pretrained(model_name, cache_dir=cache_dir)
-    if hasattr(config, 'return_dict'):
+    if hasattr(config, "return_dict"):
         config.return_dict = False
 
     config_modifier.modify(config)
 
     model = load_pretrained_model(model_name, config=config, cache_dir=cache_dir, custom_model_class=model_class)
 
     return config, model
@@ -277,227 +331,390 @@
 
 def load_tf_model(model_name, model_class, cache_dir, config_modifier):
     config = AutoConfig.from_pretrained(model_name, cache_dir=cache_dir)
 
     config_modifier.modify(config)
     # Loading tf model from transformers limits the cpu affinity to {0} when KMP_AFFINITY is set
     # Restore the affinity after model loading for expected ORT performance
-    affi_helper = AffinitySetting()
-    affi_helper.get_affinity()
-    model = load_pretrained_model(model_name,
-                                  config=config,
-                                  cache_dir=cache_dir,
-                                  custom_model_class=model_class,
-                                  is_tf_model=True)
-    affi_helper.set_affinity()
+    affinity_setting = AffinitySetting()
+    affinity_setting.get_affinity()
+    model = load_pretrained_model(
+        model_name,
+        config=config,
+        cache_dir=cache_dir,
+        custom_model_class=model_class,
+        is_tf_model=True,
+    )
+    affinity_setting.set_affinity()
 
     return config, model
 
 
 # For test only
 def load_pt_model_from_tf(model_name):
     # Note that we could get pt model from tf, but model source and its structure in this case is different from directly using
     # load_pt_model() and load_tf_model() even with the same name. Therefore it should not be used for comparing with them
     from convert_tf_models_to_pytorch import tf2pt_pipeline
+
     config, model = tf2pt_pipeline(model_name)
 
     return config, model
 
 
-def validate_and_optimize_onnx(model_name,
-                               use_external_data_format,
-                               model_type,
-                               onnx_dir,
-                               input_names,
-                               use_gpu,
-                               precision,
-                               optimize_info,
-                               validate_onnx,
-                               use_raw_attention_mask,
-                               overwrite,
-                               config,
-                               model_fusion_statistics,
-                               onnx_model_path,
-                               example_inputs,
-                               example_outputs_flatten,
-                               output_names,
-                               fusion_options):
+def validate_and_optimize_onnx(
+    model_name,
+    use_external_data_format,
+    model_type,
+    onnx_dir,
+    input_names,
+    use_gpu,
+    precision,
+    optimize_info,
+    validate_onnx,
+    use_raw_attention_mask,
+    overwrite,
+    config,
+    model_fusion_statistics,
+    onnx_model_path,
+    example_inputs,
+    example_outputs_flatten,
+    output_names,
+    fusion_options,
+):
     is_valid_onnx_model = True
     if validate_onnx:
-        is_valid_onnx_model = validate_onnx_model(onnx_model_path, example_inputs, example_outputs_flatten, use_gpu,
-                                                  False, output_names)
+        is_valid_onnx_model = validate_onnx_model(
+            onnx_model_path,
+            example_inputs,
+            example_outputs_flatten,
+            use_gpu,
+            False,
+            output_names,
+        )
     if optimize_info == OptimizerInfo.NOOPT:
         return onnx_model_path, is_valid_onnx_model, config.vocab_size
 
-    if optimize_info == OptimizerInfo.BYSCRIPT or precision == Precision.FLOAT16 or precision == Precision.INT8:  # Use script (optimizer.py) to optimize
-        optimized_model_path = get_onnx_file_path(onnx_dir, model_name, len(input_names), True, use_gpu, precision,
-                                                  False, use_external_data_format)
-        optimize_onnx_model(model_name, onnx_model_path, optimized_model_path, model_type, config.num_attention_heads,
-                            config.hidden_size, use_gpu, precision, use_raw_attention_mask, overwrite,
-                            model_fusion_statistics, use_external_data_format, fusion_options)
+    if (
+        optimize_info == OptimizerInfo.BYSCRIPT or precision == Precision.FLOAT16 or precision == Precision.INT8
+    ):  # Use script (optimizer.py) to optimize
+        optimized_model_path = get_onnx_file_path(
+            onnx_dir,
+            model_name,
+            len(input_names),
+            True,
+            use_gpu,
+            precision,
+            False,
+            use_external_data_format,
+        )
+        optimize_onnx_model(
+            onnx_model_path,
+            optimized_model_path,
+            model_type,
+            config.num_attention_heads,
+            config.hidden_size,
+            use_gpu,
+            precision,
+            use_raw_attention_mask,
+            overwrite,
+            model_fusion_statistics,
+            use_external_data_format,
+            fusion_options,
+        )
 
         onnx_model_path = optimized_model_path
         if validate_onnx:
-            is_valid_onnx_model = validate_onnx_model(onnx_model_path, example_inputs, example_outputs_flatten, use_gpu,
-                                                      precision == Precision.FLOAT16, output_names)
+            is_valid_onnx_model = validate_onnx_model(
+                onnx_model_path,
+                example_inputs,
+                example_outputs_flatten,
+                use_gpu,
+                precision == Precision.FLOAT16,
+                output_names,
+            )
 
         if precision == Precision.INT8:
             logger.info(f"Quantizing model: {onnx_model_path}")
             QuantizeHelper.quantize_onnx_model(onnx_model_path, onnx_model_path, use_external_data_format)
             logger.info(f"Finished quantizing model: {onnx_model_path}")
 
     if optimize_info == OptimizerInfo.BYORT:  # Use OnnxRuntime to optimize
         if is_valid_onnx_model:
-            ort_model_path = add_filename_suffix(onnx_model_path, '_ort')
-            optimize_onnx_model_by_ort(onnx_model_path, ort_model_path, use_gpu, overwrite, model_fusion_statistics)
-
-    return onnx_model_path, is_valid_onnx_model, config.vocab_size
-
-
-def export_onnx_model_from_pt(model_name, opset_version, use_external_data_format, model_type, model_class,
-                              config_modifier, cache_dir, onnx_dir, input_names, use_gpu, precision, optimizer_info,
-                              validate_onnx, use_raw_attention_mask, overwrite, model_fusion_statistics, fusion_options):
-
+            ort_model_path = add_filename_suffix(onnx_model_path, "_ort")
+            optimize_onnx_model_by_ort(
+                onnx_model_path,
+                ort_model_path,
+                use_gpu,
+                overwrite,
+                model_fusion_statistics,
+            )
+
+    return (
+        onnx_model_path,
+        is_valid_onnx_model,
+        config.num_labels if model_type in ["vit", "swin"] else config.vocab_size,
+    )
+
+
+def export_onnx_model_from_pt(
+    model_name,
+    opset_version,
+    use_external_data_format,
+    model_type,
+    model_class,
+    config_modifier,
+    cache_dir,
+    onnx_dir,
+    input_names,
+    use_gpu,
+    precision,
+    optimizer_info,
+    validate_onnx,
+    use_raw_attention_mask,
+    overwrite,
+    model_fusion_statistics,
+    fusion_options,
+):
     config, model = load_pt_model(model_name, model_class, cache_dir, config_modifier)
     # config, model = load_pt_model_from_tf(model_name)
     model.cpu()
 
-    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
-    max_input_size = tokenizer.max_model_input_sizes[
-        model_name] if model_name in tokenizer.max_model_input_sizes else 1024
+    example_inputs = None
+    max_input_size = None
 
-    example_inputs = tokenizer.encode_plus("This is a sample input", return_tensors="pt")
+    if model_type in ["vit", "swin"]:
+        image_processor = AutoFeatureExtractor.from_pretrained(model_name, cache_dir=cache_dir)
+        data = numpy.random.randint(
+            low=0, high=256, size=config.image_size * config.image_size * 3, dtype=numpy.uint8
+        ).reshape(config.image_size, config.image_size, 3)
+
+        example_inputs = image_processor(data, return_tensors="pt")
+    else:
+        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
+        max_input_size = (
+            tokenizer.max_model_input_sizes[model_name] if model_name in tokenizer.max_model_input_sizes else 1024
+        )
+
+        example_inputs = tokenizer.encode_plus("This is a sample input", return_tensors="pt")
 
     example_inputs = filter_inputs(example_inputs, input_names)
 
     example_outputs = model(**example_inputs)
 
     assert isinstance(example_outputs, (list, tuple)), f"type of output is not list or tuple: {type(example_outputs)}"
 
     # Flatten is needed for gpt2 and distilgpt2.
     example_outputs_flatten = flatten(example_outputs)
     example_outputs_flatten = update_flatten_list(example_outputs_flatten, [])
 
-    onnx_model_path = get_onnx_file_path(onnx_dir, model_name, len(input_names), False, use_gpu, precision, False,
-                                         use_external_data_format)
+    onnx_model_path = get_onnx_file_path(
+        onnx_dir,
+        model_name,
+        len(input_names),
+        False,
+        use_gpu,
+        precision,
+        False,
+        use_external_data_format,
+    )
 
     if overwrite or not os.path.exists(onnx_model_path):
-        logger.info("Exporting ONNX model to {}".format(onnx_model_path))
+        logger.info(f"Exporting ONNX model to {onnx_model_path}")
         Path(onnx_model_path).parent.mkdir(parents=True, exist_ok=True)
 
-        dynamic_axes, output_names = build_dynamic_axes(example_inputs, example_outputs_flatten)
+        dynamic_axes = None
+        output_names = None
+
+        if model_type in ["vit", "swin"]:
+            dynamic_axes, output_names = {key: {0: "pixel_values"} for key in example_inputs}, ["logits"]
+        else:
+            dynamic_axes, output_names = build_dynamic_axes(example_inputs, example_outputs_flatten)
 
         replace_torch_functions()
-        torch_onnx_export(model=model,
-                          args=tuple(example_inputs.values()),
-                          f=onnx_model_path,
-                          input_names=list(example_inputs.keys()),
-                          output_names=output_names,
-                          example_outputs=example_outputs,
-                          dynamic_axes=dynamic_axes,
-                          do_constant_folding=True,
-                          opset_version=opset_version,
-                          use_external_data_format=use_external_data_format)
+        torch_onnx_export(
+            model=model,
+            args=tuple(example_inputs.values()),
+            f=onnx_model_path,
+            input_names=list(example_inputs.keys()),
+            output_names=output_names,
+            dynamic_axes=dynamic_axes,
+            do_constant_folding=True,
+            opset_version=opset_version,
+            use_external_data_format=use_external_data_format,
+        )
         restore_torch_functions()
     else:
         logger.info(f"Skip export since model existed: {onnx_model_path}")
 
     onnx_model_file, is_valid_onnx_model, vocab_size = validate_and_optimize_onnx(
-        model_name, use_external_data_format, model_type, onnx_dir, input_names, use_gpu, precision, optimizer_info,
-        validate_onnx, use_raw_attention_mask, overwrite, config, model_fusion_statistics, onnx_model_path,
-        example_inputs, example_outputs_flatten, None, fusion_options)
+        model_name,
+        use_external_data_format,
+        model_type,
+        onnx_dir,
+        input_names,
+        use_gpu,
+        precision,
+        optimizer_info,
+        validate_onnx,
+        use_raw_attention_mask,
+        overwrite,
+        config,
+        model_fusion_statistics,
+        onnx_model_path,
+        example_inputs,
+        example_outputs_flatten,
+        None,
+        fusion_options,
+    )
 
     return onnx_model_file, is_valid_onnx_model, vocab_size, max_input_size
 
 
-def export_onnx_model_from_tf(model_name, opset_version, use_external_data_format, model_type, model_class,
-                              config_modifier, cache_dir, onnx_dir, input_names, use_gpu, precision, optimizer_info,
-                              validate_onnx, use_raw_attention_mask, overwrite, model_fusion_statistics, fusion_options):
+def export_onnx_model_from_tf(
+    model_name,
+    opset_version,
+    use_external_data_format,
+    model_type,
+    model_class,
+    config_modifier,
+    cache_dir,
+    onnx_dir,
+    input_names,
+    use_gpu,
+    precision,
+    optimizer_info,
+    validate_onnx,
+    use_raw_attention_mask,
+    overwrite,
+    model_fusion_statistics,
+    fusion_options,
+):
     # Use CPU to export
     import tensorflow as tf
-    tf.config.set_visible_devices([], 'GPU')
+
+    tf.config.set_visible_devices([], "GPU")
 
     tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
     # Fix "Using pad_token, but it is not set yet" error.
     if tokenizer.pad_token is None:
-        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
-    max_input_size = tokenizer.max_model_input_sizes[
-        model_name] if model_name in tokenizer.max_model_input_sizes else 1024
+        tokenizer.add_special_tokens({"pad_token": "[PAD]"})
+    max_input_size = (
+        tokenizer.max_model_input_sizes[model_name] if model_name in tokenizer.max_model_input_sizes else 1024
+    )
 
     config, model = load_tf_model(model_name, model_class, cache_dir, config_modifier)
     model.resize_token_embeddings(len(tokenizer))
 
-    example_inputs = tokenizer.encode_plus("This is a sample input",
-                                           return_tensors="tf",
-                                           max_length=max_input_size,
-                                           padding="max_length",
-                                           truncation=True)
+    example_inputs = tokenizer.encode_plus(
+        "This is a sample input",
+        return_tensors="tf",
+        max_length=max_input_size,
+        padding="max_length",
+        truncation=True,
+    )
     example_inputs = filter_inputs(example_inputs, input_names)
 
     if config.is_encoder_decoder:
-        example_inputs["decoder_input_ids"] = tokenizer.encode_plus("This is a sample input",
-                                                                    return_tensors="tf",
-                                                                    max_length=max_input_size,
-                                                                    padding="max_length",
-                                                                    truncation=True).input_ids
+        example_inputs["decoder_input_ids"] = tokenizer.encode_plus(
+            "This is a sample input",
+            return_tensors="tf",
+            max_length=max_input_size,
+            padding="max_length",
+            truncation=True,
+        ).input_ids
     if model_name == "unc-nlp/lxmert-base-uncased":
         example_inputs["visual_feats"] = tf.random.normal([1, 1, config.visual_feat_dim])
         example_inputs["visual_pos"] = tf.random.normal([1, 1, config.visual_pos_dim])
 
     try:
         # Use no past state for these models
         if config.use_cache:
             config.use_cache = False
-    except:
+    except Exception:
         pass
 
     example_outputs = model(example_inputs, training=False)
     output_names = None
 
     # For xlnet models, only compare the last_hidden_state output.
     if model_name == "xlnet-base-cased" or model_name == "xlnet-large-cased":
         output_names = ["last_hidden_state"]
         example_outputs = example_outputs["last_hidden_state"]
 
     # Flatten is needed for gpt2 and distilgpt2. Output name sorting is needed for tf2onnx outputs to match onnx outputs.
     from tensorflow.python.util import nest
+
     example_outputs_flatten = nest.flatten(example_outputs)
 
-    onnx_model_path = get_onnx_file_path(onnx_dir, model_name, len(input_names), False, use_gpu, precision, False,
-                                         use_external_data_format)
+    onnx_model_path = get_onnx_file_path(
+        onnx_dir,
+        model_name,
+        len(input_names),
+        False,
+        use_gpu,
+        precision,
+        False,
+        use_external_data_format,
+    )
     tf_internal_model_path = onnx_model_path[:-5] if use_external_data_format else onnx_model_path
 
     if overwrite or not os.path.exists(tf_internal_model_path):
-        logger.info("Exporting ONNX model to {}".format(onnx_model_path))
+        logger.info(f"Exporting ONNX model to {onnx_model_path}")
         if not use_external_data_format:
             Path(tf_internal_model_path).parent.mkdir(parents=True, exist_ok=True)
 
-        import tf2onnx, zipfile
+        import zipfile
+
+        import tf2onnx
+
         tf2onnx.logging.set_level(tf2onnx.logging.ERROR)
         specs = []
         for name, value in example_inputs.items():
             dims = [None] * len(value.shape)
             specs.append(tf.TensorSpec(tuple(dims), value.dtype, name=name))
-        _, _ = tf2onnx.convert.from_keras(model,
-                                          input_signature=tuple(specs),
-                                          opset=opset_version,
-                                          large_model=use_external_data_format,
-                                          output_path=tf_internal_model_path)
+        _, _ = tf2onnx.convert.from_keras(
+            model,
+            input_signature=tuple(specs),
+            opset=opset_version,
+            large_model=use_external_data_format,
+            output_path=tf_internal_model_path,
+        )
         if use_external_data_format:
             # need to unpack the zip for run_onnxruntime()
-            with zipfile.ZipFile(tf_internal_model_path, 'r') as z:
+            with zipfile.ZipFile(tf_internal_model_path, "r") as z:
                 z.extractall(os.path.dirname(tf_internal_model_path))
             tf_internal_model_path = os.path.join(os.path.dirname(tf_internal_model_path), "__MODEL_PROTO.onnx")
             if os.path.exists(onnx_model_path):
                 os.remove(onnx_model_path)
             os.rename(tf_internal_model_path, onnx_model_path)
 
     else:
         logger.info(f"Skip export since model existed: {onnx_model_path}")
 
-    model_type = model_type + '_tf'
-    opt_onnx_model_file, onnx_model_file, is_valid_onnx_model, vocab_size = validate_and_optimize_onnx(
-        model_name, use_external_data_format, model_type, onnx_dir, input_names, use_gpu, precision, optimizer_info,
-        validate_onnx, use_raw_attention_mask, overwrite, config, model_fusion_statistics, onnx_model_path,
-        example_inputs, example_outputs_flatten, output_names, fusion_options)
-
-    return opt_onnx_model_file, onnx_model_file, is_valid_onnx_model, vocab_size, max_input_size
+    model_type = model_type + "_tf"
+    optimized_onnx_path, is_valid_onnx_model, vocab_size = validate_and_optimize_onnx(
+        model_name,
+        use_external_data_format,
+        model_type,
+        onnx_dir,
+        input_names,
+        use_gpu,
+        precision,
+        optimizer_info,
+        validate_onnx,
+        use_raw_attention_mask,
+        overwrite,
+        config,
+        model_fusion_statistics,
+        onnx_model_path,
+        example_inputs,
+        example_outputs_flatten,
+        output_names,
+        fusion_options,
+    )
+
+    return (
+        optimized_onnx_path,
+        is_valid_onnx_model,
+        vocab_size,
+        max_input_size,
+    )
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_bert.py` & `onnxruntime/transformers/fusion_qordered_attention.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,393 +1,421 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 from logging import getLogger
-from typing import List
-from onnx import GraphProto, ModelProto, TensorProto, ValueInfoProto, helper
+from typing import Tuple
+
+import numpy as np
+from fusion_attention import AttentionMask
+from fusion_base import Fusion
+from fusion_utils import FusionUtils, NumpyHelper
+from onnx import NodeProto, helper
 from onnx_model import OnnxModel
-from fusion_reshape import FusionReshape
-from fusion_shape import FusionShape
-from fusion_layernorm import FusionLayerNormalization, FusionLayerNormalizationTF
-from fusion_skiplayernorm import FusionSkipLayerNormalization, FusionBiasSkipLayerNormalization
-from fusion_embedlayer import FusionEmbedLayerNormalization
-from fusion_attention import FusionAttention, AttentionMask
-from fusion_gelu import FusionGelu
-from fusion_fastgelu import FusionFastGelu
-from fusion_biasgelu import FusionBiasGelu
-from fusion_gelu_approximation import FusionGeluApproximation
-from fusion_utils import FusionUtils
-from fusion_options import FusionOptions
 
 logger = getLogger(__name__)
 
 
-class BertOptimizationOptions(FusionOptions):
-    """ This class is deprecated
-    """
-    def __init__(self, model_type):
-        logger.warning(f"BertOptimizationOptions is depreciated. Please use FusionOptions instead.")
-        super().__init__(model_type)
-
+class FusionQOrderedAttention(Fusion):
+    def __init__(
+        self,
+        model: OnnxModel,
+        hidden_size: int,
+        num_heads: int,
+        attention_mask: AttentionMask,
+    ):
+        self.hidden_size = hidden_size
+        self.num_heads = num_heads
+        self.attention_mask = attention_mask
 
-class BertOnnxModel(OnnxModel):
-    def __init__(self, model: ModelProto, num_heads: int = 0, hidden_size: int = 0):
-        """Initialize BERT ONNX Model.
+        super().__init__(model, "QOrderedAttention", "QOrderedLayerNormalization")
 
+    def get_num_heads_and_hidden_size(self, reshape_q: NodeProto) -> Tuple[int, int]:
+        """Detect num_heads and hidden_size from a reshape node.
         Args:
-            model (ModelProto): the ONNX model
-            num_heads (int, optional): number of attentioin heads. Defaults to 0, and we will detect the parameter automatically.
-            hidden_size (int, optional): hidden dimension. Defaults to 0, and we will detect the parameter automatically.
+            reshape_q (NodeProto): reshape node for Q
+        Returns:
+            Tuple[int, int]: num_heads and hidden_size
         """
-        assert (num_heads == 0 and hidden_size == 0) or (num_heads > 0 and hidden_size % num_heads == 0)
 
-        super().__init__(model)
-        self.num_heads = num_heads
-        self.hidden_size = hidden_size
+        # we assume that reshape fusion has done, so the shape is a tensor like [0, 0, num_heads, head_size]
+        q_shape = self.model.get_initializer(reshape_q.input[1])
+        if q_shape is None:
+            logger.debug(f"{reshape_q.input[1]} is not initializer.")
+
+            # Check if the second input to Reshape flows through a Constant node
+            # TODO: Investigate why FusionAttention doesn't have such logic
+            constant_node = self.model.match_parent_path(reshape_q, ["Constant"], [1])
+
+            if constant_node is None:
+                return self.num_heads, self.hidden_size  # Fall back to user specified value
+            else:
+                constant_node = constant_node[0]
+
+                if len(constant_node.attribute) != 1:
+                    return self.num_heads, self.hidden_size  # Fall back to user specified value
+
+                # This is assuming it is a Tensor attribute (this is a safe assumption)
+                q_shape = constant_node.attribute[0].t
+
+        q_shape_value = NumpyHelper.to_array(q_shape)
+        if len(q_shape_value) != 4 or (q_shape_value[2] <= 0 or q_shape_value[3] <= 0):
+            logger.debug(f"q_shape_value={q_shape_value}. Expected value are like [0, 0, num_heads, head_size].")
+            return self.num_heads, self.hidden_size  # Fall back to user specified value
+
+        num_heads = q_shape_value[2]
+        head_size = q_shape_value[3]
+        hidden_size = num_heads * head_size
+
+        if self.num_heads > 0 and num_heads != self.num_heads:
+            if self.num_heads_warning:
+                logger.warning(f"--num_heads is {self.num_heads}. Detected value is {num_heads}. Using detected value.")
+                self.num_heads_warning = False  # Do not show the warning more than once
+
+        if self.hidden_size > 0 and hidden_size != self.hidden_size:
+            if self.hidden_size_warning:
+                logger.warning(
+                    f"--hidden_size is {self.hidden_size}. Detected value is {hidden_size}. Using detected value."
+                )
+                self.hidden_size_warning = False  # Do not show the warning more than once
+
+        return num_heads, hidden_size
+
+    def fuse(self, normalize_node, input_name_to_nodes, output_name_to_node):
+        add_before_layernorm = self.model.match_parent_path(
+            normalize_node,
+            ["QuantizeLinear", "Add"],
+            [0, 0],
+        )
 
-        self.attention_mask = AttentionMask(self)
-        self.attention_fusion = FusionAttention(self, self.hidden_size, self.num_heads, self.attention_mask)
-        self.utils = FusionUtils(self)
-
-    def fuse_attention(self):
-        self.attention_fusion.apply()
-
-    def fuse_gelu(self):
-        fusion = FusionGelu(self)
-        fusion.apply()
-        fusion = FusionFastGelu(self)
-        fusion.apply()
-
-    def fuse_bias_gelu(self, is_fastgelu):
-        fusion = FusionBiasGelu(self, is_fastgelu)
-        fusion.apply()
-
-    def gelu_approximation(self):
-        fusion = FusionGeluApproximation(self)
-        fusion.apply()
-
-    def fuse_add_bias_skip_layer_norm(self):
-        fusion = FusionBiasSkipLayerNormalization(self)
-        fusion.apply()
-
-    def fuse_reshape(self):
-        fusion = FusionReshape(self)
-        fusion.apply()
-
-    def fuse_shape(self):
-        fusion = FusionShape(self)
-        fusion.apply()
-
-    def fuse_embed_layer(self):
-        fusion = FusionEmbedLayerNormalization(self)
-        fusion.apply()
-
-    def fuse_layer_norm(self):
-        fusion = FusionLayerNormalization(self)
-        fusion.apply()
-
-        fusion = FusionLayerNormalizationTF(self)
-        fusion.apply()
-
-    def fuse_skip_layer_norm(self):
-        fusion = FusionSkipLayerNormalization(self)
-        fusion.apply()
+        if add_before_layernorm is not None:
+            start_node = add_before_layernorm[-1]
+        else:
+            return
+
+        # Input QDQ nodes
+        dequantize_input = self.model.match_parent_path(
+            start_node,
+            ["DequantizeLinear"],
+            [None],
+        )
 
-    def get_graph_inputs_from_node_type(self, op_type: str, input_indices: List[int], casted: bool):
-        """
-        Get graph inputs that feed into node type (like EmbedLayerNormalization or Attention).
-        Returns a list of the graph input names based on the filter whether it is casted or not.
-        """
-        graph_inputs = []
+        if dequantize_input is None:
+            logger.debug("fuse_qordered_attention: failed to match input qdq nodes path")
+            return
+
+        dequantize_input = dequantize_input[-1]
+
+        # QKV nodes
+        qkv_nodes = self.model.match_parent_path(
+            start_node,
+            ["Add", "MatMul", "Reshape", "Transpose", "DequantizeLinear", "QuantizeLinear", "MatMul"],
+            [None, None, 0, 0, 0, 0, 0],
+        )
 
-        output_name_to_node = self.output_name_to_node()
-        nodes = self.get_nodes_by_op_type(op_type)
-        for node in nodes:
-            bert_inputs = [node.input[i] for i in input_indices if i < len(node.input)]
-            for bert_input in bert_inputs:
-                if self.find_graph_input(bert_input):
-                    if not casted:
-                        graph_inputs.append(bert_input)
-                elif bert_input in output_name_to_node:
-                    parent = output_name_to_node[bert_input]
-                    if parent.op_type == 'Cast' and self.find_graph_input(parent.input[0]) is not None:
-                        if casted:
-                            graph_inputs.append(parent.input[0])
-        return graph_inputs
-
-    def get_graph_inputs_from_fused_nodes(self, casted: bool):
-        inputs = self.get_graph_inputs_from_node_type('EmbedLayerNormalization', [0, 1, 7], casted)
-        inputs += self.get_graph_inputs_from_node_type('Attention', [3], casted)
-        return inputs
-
-    def change_graph_input_type(self,
-                                graph: GraphProto,
-                                graph_input: ValueInfoProto,
-                                new_type: int = TensorProto.INT32):
-        """Change graph input type, and add Cast node if needed.
+        if qkv_nodes is None:
+            logger.debug("fuse_qordered_attention: failed to match qkv path")
+            return
 
-        Args:
-            graph (GraphProto): graph
-            graph_input (TensorProto): input of the graph
-            new_type (int, optional): new data type. Defaults to TensorProto.INT32.
+        (_, projection_matmul, reshape_qkv, transpose_qkv, dequantize_qkv, quantize_qkv, matmul_qkv) = qkv_nodes
 
-        Returns:
-            NodeProto: a new Cast node that added. None if Cast node is not added.
-            List[NodeProto]: Cast nodes that have been removed.
-        """
-        assert isinstance(graph, GraphProto)
-        assert isinstance(graph_input, ValueInfoProto)
-        assert self.find_graph_input(graph_input.name)
-
-        if graph_input.type.tensor_type.elem_type == int(new_type):
-            return None, []
-
-        new_cast_node = None
-        nodes_to_remove = []
-
-        input_name_to_nodes = self.input_name_to_nodes()
-        if graph_input.name in input_name_to_nodes:
-            nodes = input_name_to_nodes[graph_input.name]
-
-            # For children that is not Cast node, insert a Cast node to convert int32 to original data type.
-            nodes_not_cast = [node for node in nodes if node.op_type != 'Cast']
-            if nodes_not_cast:
-                node_name = self.create_node_name('Cast')
-                output_name = node_name + '_' + graph_input.name
-                new_value_info = graph.value_info.add()
-                new_value_info.CopyFrom(graph_input)
-                new_value_info.name = output_name
-                new_cast_node = helper.make_node('Cast', [graph_input.name], [output_name],
-                                                 to=int(graph_input.type.tensor_type.elem_type),
-                                                 name=node_name)
-                graph.node.extend([new_cast_node])
-
-                for node in nodes_not_cast:
-                    OnnxModel.replace_node_input(node, graph_input.name, output_name)
-
-            # For children that is Cast node, no need to insert Cast.
-            # When the children is Cast to int32, we can remove that Cast node since input type is int32 now.
-            nodes_cast = [node for node in nodes if node.op_type == 'Cast']
-            for node in nodes_cast:
-                if OnnxModel.get_node_attribute(node, "to") == int(new_type):
-                    self.replace_input_of_all_nodes(node.output[0], graph_input.name)
-                if not self.find_graph_output(node.output[0]):
-                    nodes_to_remove.append(node)
-            if nodes_to_remove:
-                self.remove_nodes(nodes_to_remove)
+        # Make sure the Q/DQ has the proper zero points and constant per-tensor scales
+        if not FusionUtils.check_qdq_node_for_fusion(quantize_qkv, self.model):
+            return
 
-        graph_input.type.tensor_type.elem_type = int(new_type)
-        return new_cast_node, nodes_to_remove
+        if not FusionUtils.check_qdq_node_for_fusion(dequantize_qkv, self.model):
+            return
 
-    def change_graph_inputs_to_int32(self):
-        """Change data type of all graph inputs to int32 type, and add Cast node if needed.
-        """
-        graph = self.graph()
-        add_cast_count = 0
-        remove_cast_count = 0
-        for graph_input in graph.input:
-            new_node, removed_nodes = self.change_graph_input_type(graph, graph_input, TensorProto.INT32)
-            if new_node:
-                add_cast_count += 1
-            remove_cast_count += len(removed_nodes)
-        logger.info(
-            f"Graph inputs are changed to int32. Added {add_cast_count} Cast nodes, and removed {remove_cast_count} Cast nodes."
+        # Identify the root input to the Attention node
+        other_inputs = []
+        for _i, input in enumerate(start_node.input):
+            if input not in output_name_to_node:
+                continue
+
+            if input == qkv_nodes[0].output[0]:
+                continue
+
+            other_inputs.append(input)
+
+        if len(other_inputs) != 1:
+            return
+
+        root_input = other_inputs[0]
+
+        # V nodes
+        v_nodes = self.model.match_parent_path(
+            matmul_qkv,
+            ["Transpose", "Reshape", "DequantizeLinear", "QuantizeLinear", "Add", "MatMul"],
+            [1, 0, 0, 0, 0, None],
         )
 
-    def use_dynamic_axes(self, dynamic_batch_dim='batch_size', dynamic_seq_len='max_seq_len'):
-        """
-        Update input and output shape to use dynamic axes.
-        """
-        bert_graph_inputs = self.get_graph_inputs_from_fused_nodes(
-            casted=True) + self.get_graph_inputs_from_fused_nodes(casted=False)
+        if v_nodes is None:
+            logger.debug("fuse_qordered_attention: failed to match v path")
+            return
+
+        (_, _, dequantize_v, quantize_v, add_v, matmul_v) = v_nodes
+
+        # Make sure the Q/DQ has the proper zero points and constant per-tensor scales
+        if not FusionUtils.check_qdq_node_for_fusion(quantize_v, self.model):
+            return
+
+        if not FusionUtils.check_qdq_node_for_fusion(dequantize_v, self.model):
+            return
+
+        # V MatMul weight
+        dequantize_v_matmul_weight = self.model.match_parent_path(matmul_v, ["DequantizeLinear"], [1])
+
+        if dequantize_v_matmul_weight is None:
+            logger.debug("fuse_qordered_attention: failed to match v path")
+            return
+
+        dequantize_v_matmul_weight = dequantize_v_matmul_weight[0]
+
+        if self.model.get_constant_value(dequantize_v_matmul_weight.input[0]) is None:
+            return
+
+        # Make sure the upstream DequantizeLinear-1 has the proper zero points and scales
+        # Per-channel scales are supported for weights alone
+        if not FusionUtils.check_qdq_node_for_fusion(dequantize_v_matmul_weight, self.model, False):
+            return
+
+        # QK nodes
+        qk_nodes = self.model.match_parent_path(
+            matmul_qkv,
+            [
+                "DequantizeLinear",
+                "QuantizeLinear",
+                "Softmax",
+                "Add",
+                "Div",
+                "DequantizeLinear",
+                "QuantizeLinear",
+                "MatMul",
+            ],
+            [0, 0, 0, 0, None, 0, 0, 0],
+        )
 
-        dynamic_batch_inputs = {}
-        for input in self.model.graph.input:
-            if input.name in bert_graph_inputs:
-                dim_proto = input.type.tensor_type.shape.dim[0]
-                dim_proto.dim_param = dynamic_batch_dim
-                if dynamic_seq_len is not None:
-                    dim_proto = input.type.tensor_type.shape.dim[1]
-                    dim_proto.dim_param = dynamic_seq_len
-
-        for output in self.model.graph.output:
-            dim_proto = output.type.tensor_type.shape.dim[0]
-            dim_proto.dim_param = dynamic_batch_dim
-
-    def preprocess(self):
-        self.adjust_reshape_and_expand()
-        return
-
-    def adjust_reshape_and_expand(self):
-        nodes_to_remove = []
-        for node in self.nodes():
-            if node.op_type == 'Reshape':
-                # Clean up unneccessary reshape nodes.
-                # Find reshape nodes with no actually data in "shape" attribute and remove.
-                reshape_shape = self.get_constant_value(node.input[1])
-                if reshape_shape is not None and reshape_shape.size == 0:
-                    nodes_to_remove.extend([node])
-                    self.replace_input_of_all_nodes(node.output[0], node.input[0])
-                    continue
-
-                # Find path "Slice" -> "Reshape" -> "Expand" -> "Expand" -> current "Reshape", simplify the graph by
-                # changing current reshape's input to output of slice.
-                reshape_path = self.match_parent_path(node, ['Expand', 'Expand', 'Reshape', 'Slice'], [0, 0, 0, 0],
-                                                      self.output_name_to_node())
-                if reshape_path is not None:
-                    expand_node = reshape_path[-3]
-                    expand_shape_value = self.get_constant_value(expand_node.input[1])
-
-                    reshape_before_expand = reshape_path[-2]
-                    shape_value = self.get_constant_value(reshape_before_expand.input[1])
-
-                    slice_node = reshape_path[-1]
-                    if expand_shape_value is not None and shape_value is not None and len(
-                            expand_shape_value) == 2 and len(
-                                shape_value) == 1 and expand_shape_value[1] == shape_value[0]:
-                        node.input[0] = slice_node.output[0]
-
-        if nodes_to_remove:
-            self.remove_nodes(nodes_to_remove)
-            logger.info(f"Removed Reshape and Expand count: {len(nodes_to_remove)}")
-
-    def clean_graph(self):
-        output_name_to_node = self.output_name_to_node()
-        nodes_to_remove = []
-        for node in self.nodes():
-            # Before:
-            #  input_ids --> Shape --> Gather(indices=0) --> Unsqueeze ------+
-            #          |                                                     |
-            #          |                                                     v
-            #          +----> Shape --> Gather(indices=1) --> Unsqueeze--->  Concat --> ConstantOfShape -->Cast --> EmbedLayerNormaliation/ReduceSum
-            # After:
-            #  input_ids --> Shape                                                  --> ConstantOfShape -->Cast --> EmbedLayerNormaliation/ReduceSum
-            # TODO: merge ConstantOfShape -->Cast to ConstantOfShape (need update the data type of value)
-            op_input_id = {"EmbedLayerNormalization": 1, "ReduceSum": 0, "Attention": 3}
-            if node.op_type in op_input_id:
-                i = op_input_id[node.op_type]
-                parent_nodes = self.match_parent_path(
-                    node, ['Cast', 'ConstantOfShape', 'Concat', 'Unsqueeze', 'Gather', 'Shape'], [i, 0, 0, 0, 0, 0],
-                    output_name_to_node)
-                if parent_nodes is not None:
-                    cast, constantOfShape, concat, unsqueeze, gather, shape = parent_nodes
-                    if shape.input[0] == self.graph().input[0].name:
-                        constantOfShape.input[0] = shape.output[0]
-                        output_name_to_node = self.output_name_to_node()
-
-            if node.op_type == 'Attention':
-                # Before:
-                #   input_ids --> Shape -->ConstantOfShape -->Cast --> ReduceSum --> Attention
-                # After:
-                #   remove this path, and remove the optional mask_index input of Attention node.
-                parent_nodes = self.match_parent_path(node, ['ReduceSum', 'Cast', 'ConstantOfShape', 'Shape'],
-                                                      [3, 0, 0, 0], output_name_to_node)
-                if parent_nodes is not None:
-                    if parent_nodes[-1].input[0] == self.graph().input[0].name:
-                        attention_node = helper.make_node('Attention',
-                                                          inputs=node.input[0:len(node.input) - 1],
-                                                          outputs=node.output,
-                                                          name=node.name + "_remove_mask")
-                        attention_node.domain = "com.microsoft"
-                        attention_node.attribute.extend([helper.make_attribute("num_heads", self.num_heads)])
-                        self.add_node(attention_node, self.get_graph_by_node(attention_node).name)
-                        nodes_to_remove.append(node)
-        self.remove_nodes(nodes_to_remove)
-
-    def postprocess(self):
-        self.clean_graph()
-        self.prune_graph()
-
-    def optimize(self, options: FusionOptions = None, add_dynamic_axes=False):
-        # Remove cast nodes that having same data type of input and output based on symbolic shape inference.
-        self.utils.remove_useless_cast_nodes()
-
-        if (options is None) or options.enable_layer_norm:
-            self.fuse_layer_norm()
-
-        if (options is None) or options.enable_gelu:
-            self.fuse_gelu()
-
-        self.preprocess()
-
-        self.fuse_reshape()
-
-        if (options is None) or options.enable_skip_layer_norm:
-            self.fuse_skip_layer_norm()
-
-        if (options is None) or options.enable_attention:
-            if options is not None:
-                self.attention_mask.set_mask_format(options.attention_mask_format)
-            self.fuse_attention()
-
-        self.fuse_shape()
-
-        if (options is None) or options.enable_embed_layer_norm:
-            self.fuse_embed_layer()
-
-        # Remove reshape nodes that having same shape of input and output based on symbolic shape inference.
-        self.utils.remove_useless_reshape_nodes()
-
-        self.postprocess()
-
-        # Bias fusion is done after postprocess to avoid extra Reshape between bias and Gelu/FastGelu/SkipLayerNormalization
-        if (options is None) or options.enable_bias_gelu:
-            # Fuse Gelu and Add Bias before it.
-            self.fuse_bias_gelu(is_fastgelu=True)
-            self.fuse_bias_gelu(is_fastgelu=False)
-
-        if (options is None) or options.enable_bias_skip_layer_norm:
-            # Fuse SkipLayerNormalization and Add Bias before it.
-            self.fuse_add_bias_skip_layer_norm()
-
-        if (options is not None and options.enable_gelu_approximation):
-            self.gelu_approximation()
-
-        self.remove_unused_constant()
-
-        # Use symbolic batch dimension in input and output.
-        if add_dynamic_axes:
-            self.use_dynamic_axes()
+        if qk_nodes is None:
+            logger.debug("fuse_qordered_attention: failed to match qk path")
+            return
+
+        (
+            dequantize_qk_softmax,
+            quantize_qk_softmax,
+            softmax_qk,
+            add_qk,
+            div_qk,
+            dequantize_qk,
+            quantize_qk,
+            matmul_qk,
+        ) = qk_nodes
+
+        # Make sure the Q/DQ has the proper zero points and constant per-tensor scales
+        if not FusionUtils.check_qdq_node_for_fusion(quantize_qk_softmax, self.model):
+            return
+
+        if not FusionUtils.check_qdq_node_for_fusion(dequantize_qk_softmax, self.model):
+            return
+
+        if not FusionUtils.check_qdq_node_for_fusion(quantize_qk, self.model):
+            return
+
+        if not FusionUtils.check_qdq_node_for_fusion(dequantize_qk, self.model):
+            return
+
+        # Q nodes
+        q_nodes = self.model.match_parent_path(
+            matmul_qk,
+            ["Transpose", "Reshape", "DequantizeLinear", "QuantizeLinear", "Add", "MatMul"],
+            [0, 0, 0, 0, 0, None],
+        )
 
-        logger.info(f"opset verion: {self.get_opset_version()}")
+        if q_nodes is None:
+            logger.debug("fuse_qordered_attention: failed to match q path")
+            return
 
-    def get_fused_operator_statistics(self):
-        """
-        Returns node count of fused operators.
-        """
-        op_count = {}
-        ops = [
-            'EmbedLayerNormalization', 'Attention', 'Gelu', 'FastGelu', 'BiasGelu', 'LayerNormalization',
-            'SkipLayerNormalization'
-        ]
-        for op in ops:
-            nodes = self.get_nodes_by_op_type(op)
-            op_count[op] = len(nodes)
-        logger.info(f"Optimized operators:{op_count}")
-        return op_count
+        (_, reshape_q, dequantize_q, quantize_q, add_q, matmul_q) = q_nodes
 
-    def is_fully_optimized(self):
-        """
-        Returns True when the model is fully optimized.
-        """
-        op_count = self.get_fused_operator_statistics()
-        embed = op_count['EmbedLayerNormalization']
-        attention = op_count['Attention']
-        gelu = op_count['Gelu'] + op_count['BiasGelu'] + op_count['FastGelu']
-        layer_norm = op_count['LayerNormalization'] + op_count['SkipLayerNormalization']
-        is_perfect = (embed > 0) and (attention > 0) and (attention == gelu) and (layer_norm >= 2 * attention)
-
-        if layer_norm == 0:
-            logger.debug("Layer Normalization not fused")
-
-        if gelu == 0:
-            logger.debug("Gelu/FastGelu not fused")
+        # Make sure the Q/DQ has the proper zero points and constant per-tensor scales
+        if not FusionUtils.check_qdq_node_for_fusion(quantize_q, self.model):
+            return
 
-        if embed == 0:
-            logger.debug("Embed Layer not fused")
+        if not FusionUtils.check_qdq_node_for_fusion(dequantize_q, self.model):
+            return
 
-        if attention == 0:
-            logger.warning("Attention not fused")
+        # Q MatMul weight
+        dequantize_q_matmul_weight = self.model.match_parent_path(matmul_q, ["DequantizeLinear"], [1])
+
+        if dequantize_q_matmul_weight is None:
+            logger.debug("fuse_qordered_attention: failed to match q path")
+            return
+
+        dequantize_q_matmul_weight = dequantize_q_matmul_weight[0]
+
+        if self.model.get_constant_value(dequantize_q_matmul_weight.input[0]) is None:
+            return
+
+        # Make sure the upstream DequantizeLinear-1 has the proper zero points and scales
+        # Per-channel scales are supported for weights alone
+        if not FusionUtils.check_qdq_node_for_fusion(dequantize_q_matmul_weight, self.model, False):
+            return
+
+        # K nodes
+        k_nodes = self.model.match_parent_path(
+            matmul_qk,
+            ["Transpose", "Reshape", "DequantizeLinear", "QuantizeLinear", "Add", "MatMul"],
+            [1, 0, 0, 0, 0, None],
+        )
+
+        if k_nodes is None:
+            logger.debug("fuse_qordered_attention: failed to match k path")
+            return
+
+        (_, _, dequantize_k, quantize_k, add_k, matmul_k) = k_nodes
+
+        # Make sure the Q/DQ has the proper zero points and constant per-tensor scales
+        if not FusionUtils.check_qdq_node_for_fusion(quantize_k, self.model):
+            return
+
+        if not FusionUtils.check_qdq_node_for_fusion(dequantize_k, self.model):
+            return
+
+        # K MatMul weight
+        dequantize_k_matmul_weight = self.model.match_parent_path(matmul_k, ["DequantizeLinear"], [1])
+
+        if dequantize_k_matmul_weight is None:
+            logger.debug("fuse_qordered_attention: failed to match k path")
+            return
+
+        dequantize_k_matmul_weight = dequantize_k_matmul_weight[0]
+
+        if self.model.get_constant_value(dequantize_k_matmul_weight.input[0]) is None:
+            return
+
+        # Make sure the upstream DequantizeLinear-1 has the proper zero points and scales
+        # Per-channel scales are supported for weights alone
+        if not FusionUtils.check_qdq_node_for_fusion(dequantize_k_matmul_weight, self.model, False):
+            return
+
+        # Mask nodes
+        mask_nodes = self.model.match_parent_path(
+            add_qk, ["Mul", "Sub", "Cast", "Unsqueeze", "Unsqueeze"], [None, 0, 1, 0, 0]
+        )
 
-        return is_perfect
+        if mask_nodes is None:
+            logger.debug("fuse_qordered_attention: failed to match mask_nodes path")
+            return
+
+        # Ascertain `qkv_hidden_sizes` attribute value
+        q_weight = self.model.get_initializer(dequantize_q_matmul_weight.input[0])
+        k_weight = self.model.get_initializer(dequantize_k_matmul_weight.input[0])
+        v_weight = self.model.get_initializer(dequantize_v_matmul_weight.input[0])
+
+        qw = NumpyHelper.to_array(q_weight)
+        kw = NumpyHelper.to_array(k_weight)
+        vw = NumpyHelper.to_array(v_weight)
+
+        qw_out_size = np.prod(qw.shape[1:])
+        kw_out_size = np.prod(kw.shape[1:])
+        vw_out_size = np.prod(vw.shape[1:])
+
+        # Form QOrderedAttention node
+        if matmul_v.input[0] == root_input and matmul_q.input[0] == root_input and matmul_k.input[0] == root_input:
+            mask_index = self.attention_mask.process_mask(mask_nodes[-1].input[0])
+
+            # Ascertain `num_heads` and `hidden_size`
+            num_heads, hidden_size = self.get_num_heads_and_hidden_size(reshape_q)
+
+            # Formulate the inputs
+            # Actual quantized input
+            attention_inputs = [dequantize_input.input[0]]
+            attention_inputs.append(dequantize_input.input[1])
+
+            attention_inputs.append(dequantize_q.input[1])
+            attention_inputs.append(dequantize_k.input[1])
+            attention_inputs.append(dequantize_v.input[1])
+
+            attention_inputs.append(dequantize_q_matmul_weight.input[0])
+            attention_inputs.append(dequantize_k_matmul_weight.input[0])
+            attention_inputs.append(dequantize_v_matmul_weight.input[0])
+
+            attention_inputs.append(dequantize_q_matmul_weight.input[1])
+            attention_inputs.append(dequantize_k_matmul_weight.input[1])
+            attention_inputs.append(dequantize_v_matmul_weight.input[1])
+
+            if self.model.get_initializer(add_q.input[0]):
+                attention_inputs.append(add_q.input[0])
+            else:  # second input is the constant bias
+                attention_inputs.append(add_q.input[1])
+
+            if self.model.get_initializer(add_k.input[0]):
+                attention_inputs.append(add_k.input[0])
+            else:  # second input is the constant bias
+                attention_inputs.append(add_k.input[1])
+
+            if self.model.get_initializer(add_v.input[0]):
+                attention_inputs.append(add_v.input[0])
+            else:  # second input is the constant bias
+                attention_inputs.append(add_v.input[1])
+
+            attention_inputs.append(quantize_qk.input[1])
+            attention_inputs.append(quantize_qk_softmax.input[1])
+            attention_inputs.append(dequantize_qkv.input[1])
+
+            # Mask input
+            if mask_index is not None:
+                attention_inputs.append(mask_index)
+            else:
+                attention_inputs.append("")
+
+            # The MatMul weight 'B' and 'bias' need some post-processing
+            # Transpose weight 'B' from order ROW to order COL
+            # This offline transpose is needed only while using the CUDA EP
+            # TODO: Make this fusion logic EP-agnostic ?
+            q_weight_tensor = self.model.get_initializer(dequantize_q_matmul_weight.input[0])
+            FusionUtils.transpose_2d_int8_tensor(q_weight_tensor)
+
+            k_weight_tensor = self.model.get_initializer(dequantize_k_matmul_weight.input[0])
+            FusionUtils.transpose_2d_int8_tensor(k_weight_tensor)
+
+            v_weight_tensor = self.model.get_initializer(dequantize_v_matmul_weight.input[0])
+            FusionUtils.transpose_2d_int8_tensor(v_weight_tensor)
+
+            # Name and create Attention node
+            attention_node_name = self.model.create_node_name("QOrderedAttention")
+
+            attention_node = helper.make_node(
+                "QOrderedAttention",
+                inputs=attention_inputs,
+                outputs=[reshape_qkv.output[0]],
+                name=attention_node_name,
+            )
+
+            self.model.replace_node_input(dequantize_qkv, dequantize_qkv.input[0], attention_node.output[0])
+            self.model.replace_node_input(projection_matmul, projection_matmul.input[0], dequantize_qkv.output[0])
+
+            attention_node.attribute.extend([helper.make_attribute("num_heads", num_heads)])
+            attention_node.attribute.extend([helper.make_attribute("order_input", 1)])
+            attention_node.attribute.extend([helper.make_attribute("order_weight", 0)])
+            attention_node.attribute.extend([helper.make_attribute("order_output", 1)])
+            attention_node.attribute.extend(
+                [helper.make_attribute("qkv_hidden_sizes", [qw_out_size, kw_out_size, vw_out_size])]
+            )
+
+            attention_node.domain = "com.microsoft"
+
+            self.nodes_to_add.append(attention_node)
+            self.node_name_to_graph_name[attention_node.name] = self.this_graph_name
+
+            self.nodes_to_remove.extend([reshape_qkv, transpose_qkv, quantize_qkv, matmul_qkv])
+            self.nodes_to_remove.extend(qk_nodes)
+            self.nodes_to_remove.extend(q_nodes)
+            self.nodes_to_remove.extend(k_nodes)
+            self.nodes_to_remove.extend(v_nodes)
+            self.nodes_to_remove.extend(
+                [dequantize_q_matmul_weight, dequantize_k_matmul_weight, dequantize_v_matmul_weight]
+            )
+
+            # Use prune graph to remove mask nodes since they are shared by all attention nodes.
+            # self.nodes_to_remove.extend(mask_nodes)
+            self.prune_graph = True
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_bert_keras.py` & `onnxruntime/transformers/onnx_model_bert_keras.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,151 +1,204 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
+import argparse  # noqa: F401
 import logging
+import sys  # noqa: F401
+from collections import deque  # noqa: F401
+
+import numpy as np  # noqa: F401
 import onnx
-import sys
-import argparse
-import numpy as np
-from collections import deque
-from onnx import ModelProto, TensorProto, numpy_helper
+from onnx import ModelProto, TensorProto, numpy_helper  # noqa: F401
 from onnx_model_bert_tf import BertOnnxModelTF
 
 logger = logging.getLogger(__name__)
 
 
 class BertOnnxModelKeras(BertOnnxModelTF):
     def __init__(self, model, num_heads, hidden_size):
         super().__init__(model, num_heads, hidden_size)
 
     def match_mask_path(self, add_or_sub_before_softmax):
-        mask_nodes = self.match_parent_path(add_or_sub_before_softmax, ['Mul', 'Sub', 'Reshape', 'Cast'],
-                                            [1, None, 1, 0])
+        mask_nodes = self.match_parent_path(
+            add_or_sub_before_softmax,
+            ["Mul", "Sub", "Reshape", "Cast"],
+            [1, None, 1, 0],
+        )
         if mask_nodes is not None:
             return mask_nodes
 
-        mask_nodes = self.match_parent_path(add_or_sub_before_softmax, ['Mul', 'Sub', 'Cast', 'Slice', 'Unsqueeze'],
-                                            [1, 1, 1, 0, 0])
+        mask_nodes = self.match_parent_path(
+            add_or_sub_before_softmax,
+            ["Mul", "Sub", "Cast", "Slice", "Unsqueeze"],
+            [1, 1, 1, 0, 0],
+        )
         if mask_nodes is not None:
             return mask_nodes
 
-        mask_nodes = self.match_parent_path(add_or_sub_before_softmax, ['Mul', 'Sub', 'Cast', 'Unsqueeze', 'Unsqueeze'],
-                                            [1, None, 1, 0, 0])
+        mask_nodes = self.match_parent_path(
+            add_or_sub_before_softmax,
+            ["Mul", "Sub", "Cast", "Unsqueeze", "Unsqueeze"],
+            [1, None, 1, 0, 0],
+        )
         return mask_nodes
 
     def check_attention_input(self, matmul_q, matmul_k, matmul_v, parent, output_name_to_node):
         reshape_nodes = []
 
         for x in [matmul_q, matmul_k, matmul_v]:
             root_input = x.input[0]
             root_node = output_name_to_node[root_input]
             if root_node == parent:
                 continue
-            if root_node.op_type == 'Reshape' and root_node.input[0] == parent.output[0]:
+            if root_node.op_type == "Reshape" and root_node.input[0] == parent.output[0]:
                 reshape_nodes.append(root_node)
                 continue
             logger.debug(f"Check attention input failed:{root_input}, {parent.output[0]}")
             return False, []
 
         return True, reshape_nodes
 
     def fuse_attention(self):
-        input_name_to_nodes = self.input_name_to_nodes()
+        self.input_name_to_nodes()
         output_name_to_node = self.output_name_to_node()
 
         nodes_to_remove = []
         attention_count = 0
 
         skip_layer_norm_nodes = self.get_nodes_by_op_type("SkipLayerNormalization")
         for normalize_node in skip_layer_norm_nodes:
             # SkipLayerNormalization has two inputs, and one of them is the root input for attention.
             parent = self.get_parent(normalize_node, 0)
-            if parent is None or parent.op_type not in ["SkipLayerNormalization", "EmbedLayerNormalization"]:
-                if parent.op_type == 'Add':
+            if parent is None or parent.op_type not in [
+                "SkipLayerNormalization",
+                "EmbedLayerNormalization",
+            ]:
+                if parent.op_type == "Add":
                     parent = self.get_parent(normalize_node, 1)
-                    if parent is None or parent.op_type not in ["SkipLayerNormalization", "EmbedLayerNormalization"]:
-                        logger.debug(
-                            "First input for skiplayernorm: {}".format(parent.op_type if parent is not None else None))
+                    if parent is None or parent.op_type not in [
+                        "SkipLayerNormalization",
+                        "EmbedLayerNormalization",
+                    ]:
+                        logger.debug(f"First input for skiplayernorm: {parent.op_type if parent is not None else None}")
                         continue
                 else:
-                    logger.debug(
-                        "First input for skiplayernorm: {}".format(parent.op_type if parent is not None else None))
+                    logger.debug(f"First input for skiplayernorm: {parent.op_type if parent is not None else None}")
                     continue
             else:
                 # TODO: shall we add back the checking of children op types.
                 pass
 
-            qkv_nodes = self.match_parent_path(normalize_node,
-                                               ['Add', 'Reshape', 'MatMul', 'Reshape', 'Transpose', 'MatMul'],
-                                               [None, 0, 0, 0, 0, 0])
+            qkv_nodes = self.match_parent_path(
+                normalize_node,
+                ["Add", "Reshape", "MatMul", "Reshape", "Transpose", "MatMul"],
+                [None, 0, 0, 0, 0, 0],
+            )
             if qkv_nodes is None:
                 logger.debug("Failed to match qkv nodes")
                 continue
-            (add, extra_reshape_0, matmul, reshape_qkv, transpose_qkv, matmul_qkv) = qkv_nodes
+            (
+                add,
+                extra_reshape_0,
+                matmul,
+                reshape_qkv,
+                transpose_qkv,
+                matmul_qkv,
+            ) = qkv_nodes
             logger.debug("Matched qkv nodes")
 
-            v_nodes = self.match_parent_path(matmul_qkv, ['Transpose', 'Reshape', 'Add', 'Reshape', 'MatMul'],
-                                             [1, 0, 0, 0, 0])
+            v_nodes = self.match_parent_path(
+                matmul_qkv,
+                ["Transpose", "Reshape", "Add", "Reshape", "MatMul"],
+                [1, 0, 0, 0, 0],
+            )
             if v_nodes is None:
                 logger.debug("Failed to match v path")
                 continue
             (transpose_v, reshape_v, add_v, extra_reshape_1, matmul_v) = v_nodes
 
-            qk_nodes = self.match_parent_path(matmul_qkv, ['Softmax', 'Sub', 'MatMul'], [0, 0, 0])
+            qk_nodes = self.match_parent_path(matmul_qkv, ["Softmax", "Sub", "MatMul"], [0, 0, 0])
             if qk_nodes is not None:
                 (softmax_qk, sub_qk, matmul_qk) = qk_nodes
-                q_nodes = self.match_parent_path(matmul_qk, ['Mul', 'Transpose', 'Reshape', 'Add', 'Reshape', 'MatMul'],
-                                                 [0, None, 0, 0, 0, 0])
+                q_nodes = self.match_parent_path(
+                    matmul_qk,
+                    ["Mul", "Transpose", "Reshape", "Add", "Reshape", "MatMul"],
+                    [0, None, 0, 0, 0, 0],
+                )
                 if q_nodes is not None:
-                    (mul_q, transpose_q, reshape_q, add_q, extra_reshape_2, matmul_q) = q_nodes
+                    (
+                        mul_q,
+                        transpose_q,
+                        reshape_q,
+                        add_q,
+                        extra_reshape_2,
+                        matmul_q,
+                    ) = q_nodes
 
             else:
-                qk_nodes = self.match_parent_path(matmul_qkv, ['Softmax', 'Add', 'Mul', 'MatMul'], [0, 0, 0, None])
+                qk_nodes = self.match_parent_path(matmul_qkv, ["Softmax", "Add", "Mul", "MatMul"], [0, 0, 0, None])
                 if qk_nodes is None:
-                    qk_nodes = self.match_parent_path(matmul_qkv, ['Softmax', 'Add', 'Div', 'MatMul'], [0, 0, 0, None])
+                    qk_nodes = self.match_parent_path(matmul_qkv, ["Softmax", "Add", "Div", "MatMul"], [0, 0, 0, None])
                     if qk_nodes is None:
                         logger.debug("Failed to match qk path")
                         continue
                 (softmax_qk, add_qk, mul_qk, matmul_qk) = qk_nodes
 
-                q_nodes = self.match_parent_path(matmul_qk, ['Transpose', 'Reshape', 'Add', 'Reshape', 'MatMul'],
-                                                 [0, 0, 0, 0, 0])
+                q_nodes = self.match_parent_path(
+                    matmul_qk,
+                    ["Transpose", "Reshape", "Add", "Reshape", "MatMul"],
+                    [0, 0, 0, 0, 0],
+                )
                 if q_nodes is not None:
                     (transpose_q, reshape_q, add_q, extra_reshape_2, matmul_q) = q_nodes
 
             if q_nodes is None:
                 logger.debug("Failed to match q path")
                 continue
 
-            k_nodes = self.match_parent_path(matmul_qk, ['Transpose', 'Reshape', 'Add', 'Reshape', 'MatMul'],
-                                             [1, 0, 0, 0, 0])
+            k_nodes = self.match_parent_path(
+                matmul_qk,
+                ["Transpose", "Reshape", "Add", "Reshape", "MatMul"],
+                [1, 0, 0, 0, 0],
+            )
             if k_nodes is None:
                 logger.debug("Failed to match k path")
                 continue
             (transpose_k, reshape_k, add_k, extra_reshape_3, matmul_k) = k_nodes
 
             mask_nodes = self.match_mask_path(qk_nodes[1])
             if mask_nodes is None:
                 logger.debug("Failed to match mask path")
                 continue
             if not self.has_constant_input(mask_nodes[1], 1):
                 logger.debug("Sub node expected to have an input with constant value 1.0.")
                 continue
 
-            is_same_root, reshape_nodes = self.check_attention_input(matmul_q, matmul_k, matmul_v, parent,
-                                                                     output_name_to_node)
+            is_same_root, reshape_nodes = self.check_attention_input(
+                matmul_q, matmul_k, matmul_v, parent, output_name_to_node
+            )
             if is_same_root:
                 mask_index = self.attention_mask.process_mask(mask_nodes[-1].input[0])
                 logger.debug("Create an Attention node.")
-                attention_node = self.attention_fusion.create_attention_node(mask_index, matmul_q, matmul_k, matmul_v,
-                                                                             add_q, add_k, add_v, self.num_heads,
-                                                                             self.hidden_size, parent.output[0],
-                                                                             reshape_qkv.output[0], None)
+                attention_node = self.attention_fusion.create_attention_node(
+                    mask_index,
+                    matmul_q,
+                    matmul_k,
+                    matmul_v,
+                    add_q,
+                    add_k,
+                    add_v,
+                    self.num_heads,
+                    self.hidden_size,
+                    parent.output[0],
+                    reshape_qkv.output[0],
+                    None,
+                )
                 if attention_node is None:
                     continue
 
                 self.add_node(attention_node)
                 attention_count += 1
 
                 nodes_to_remove.extend([reshape_qkv, transpose_qkv, matmul_qkv])
@@ -166,154 +219,154 @@
 
     def preprocess(self):
         self.process_embedding()
         self.fuse_mask()
         self.skip_reshape()
 
     def skip_reshape(self):
-        input_name_to_nodes = self.input_name_to_nodes()
-        output_name_to_node = self.output_name_to_node()
-
-        nodes_to_remove = []
-        attention_count = 0
+        self.input_name_to_nodes()
+        self.output_name_to_node()
 
         count = 0
         reshape_nodes = self.get_nodes_by_op_type("Reshape")
         for reshape_node in reshape_nodes:
             parent = self.get_parent(reshape_node, 0)
             if parent is not None and parent.op_type == "Reshape":
                 reshape_node.input[0] = parent.input[0]
                 count += 1
 
         if count > 0:
             logger.info(f"Skip consequent Reshape count: {count}")
 
     def fuse_embedding(self, node, output_name_to_node):
-        assert node.op_type == 'LayerNormalization'
+        assert node.op_type == "LayerNormalization"
         logger.debug(f"start fusing embedding from node with output={node.output[0]}...")
-        word_embed_path = self.match_parent_path(node, ['Add', 'Add', 'Gather'], [0, 0, 0], output_name_to_node)
+        word_embed_path = self.match_parent_path(node, ["Add", "Add", "Gather"], [0, 0, 0], output_name_to_node)
         if word_embed_path is None:
             logger.debug("failed to match word_embed_path")
             return False
 
         skip_node, add_node, gather_node = word_embed_path
 
         word_initializer = self.get_initializer(gather_node.input[0])
         if word_initializer is None:
             logger.debug("failed to get word initializer")
             return False
 
         temp = numpy_helper.to_array(word_initializer)
         if len(temp.shape) == 2:
-            logger.info("Found word embedding. name:{}, shape:{}".format(word_initializer.name, temp.shape))
+            logger.info(f"Found word embedding. name:{word_initializer.name}, shape:{temp.shape}")
             word_embedding = word_initializer.name
         else:
-            logger.info("Failed to find word embedding. name:{}, shape:{}".format(word_initializer.name, temp.shape))
+            logger.info(f"Failed to find word embedding. name:{word_initializer.name}, shape:{temp.shape}")
             return False
 
         pos_initializer = self.get_initializer(add_node.input[1])
         if pos_initializer is not None:
             temp = numpy_helper.to_array(pos_initializer)
             if len(temp.shape) == 3 and temp.shape[0] == 1:
                 tensor = numpy_helper.from_array(temp.reshape((temp.shape[1], temp.shape[2])), "position_embedding")
                 self.add_initializer(tensor)
-                logger.info("Found position embedding. name:{}, shape:{}".format(pos_initializer.name, temp.shape[1:]))
+                logger.info(f"Found position embedding. name:{pos_initializer.name}, shape:{temp.shape[1:]}")
                 position_embedding = "position_embedding"
             else:
-                logger.info("Failed to find position embedding. name:{}, shape:{}".format(
-                    pos_initializer.name, temp.shape))
+                logger.info(f"Failed to find position embedding. name:{pos_initializer.name}, shape:{temp.shape}")
                 return False
         else:
-            pos_embed_path = self.match_parent_path(add_node, ['Gather', 'Slice'], [1, 1], output_name_to_node)
+            pos_embed_path = self.match_parent_path(add_node, ["Gather", "Slice"], [1, 1], output_name_to_node)
             if pos_embed_path is None:
                 logger.debug("failed to match pos_embed_path")
                 return False
 
             pos_gather, pos_slice = pos_embed_path
             pos_initializer = self.get_initializer(pos_gather.input[0])
             if pos_initializer is None:
                 logger.debug("failed to get pos initializer")
                 return False
 
             temp = numpy_helper.to_array(pos_initializer)
             if len(temp.shape) == 2:
-                logger.info("Found word embedding. name:{}, shape:{}".format(pos_initializer.name, temp.shape))
+                logger.info(f"Found word embedding. name:{pos_initializer.name}, shape:{temp.shape}")
                 position_embedding = pos_initializer.name
             else:
-                logger.info("Failed to find position embedding. name:{}, shape:{}".format(
-                    pos_initializer.name, temp.shape))
+                logger.info(f"Failed to find position embedding. name:{pos_initializer.name}, shape:{temp.shape}")
                 return False
 
         gather = self.get_parent(skip_node, 1, output_name_to_node)
         if gather is None or gather.op_type != "Gather":
             logger.debug("failed to get gather")
             return False
 
         segment_initializer = self.get_initializer(gather.input[0])
         if segment_initializer is None:
             logger.debug("failed to get segment initializer")
             return False
 
         temp = numpy_helper.to_array(segment_initializer)
         if len(temp.shape) == 2:
-            logger.info("Found segment embedding. name:{}, shape:{}".format(segment_initializer.name, temp.shape))
+            logger.info(f"Found segment embedding. name:{segment_initializer.name}, shape:{temp.shape}")
             segment_embedding = segment_initializer.name
         else:
-            logger.info("Failed to find segment embedding. name:{}, shape:{}".format(
-                segment_initializer.name, temp.shape))
+            logger.info(f"Failed to find segment embedding. name:{segment_initializer.name}, shape:{temp.shape}")
             return False
 
         logger.info("Create Embedding node")
         self.create_embedding_subgraph(node, word_embedding, segment_embedding, position_embedding)
         return True
 
     def process_embedding(self):
         """
         Automatically detect word, segment and position embeddings.
         """
         logger.info("start processing embedding layer...")
         output_name_to_node = self.output_name_to_node()
         for node in self.nodes():
-            if node.op_type == 'LayerNormalization':
+            if node.op_type == "LayerNormalization":
                 if self.fuse_embedding(node, output_name_to_node):
                     return
                 break
 
     def fuse_mask(self):
         nodes_to_remove = []
         for node in self.nodes():
-            if node.op_type == 'Mul' and self.has_constant_input(node, -10000):
-                mask_path = self.match_parent_path(node, ['Sub', 'Cast', 'Slice', 'Unsqueeze'], [0, 1, 0, 0])
+            if node.op_type == "Mul" and self.has_constant_input(node, -10000):
+                mask_path = self.match_parent_path(node, ["Sub", "Cast", "Slice", "Unsqueeze"], [0, 1, 0, 0])
                 if mask_path is None:
                     continue
                 sub_node, cast_node, slice_node, unsqueeze_node = mask_path
 
                 mask_input_name = self.attention_mask.get_first_mask()
                 if unsqueeze_node.input[0] != mask_input_name:
-                    print("Cast input {} is not mask input {}".format(unsqueeze_node.input[0], mask_input_name))
+                    print(f"Cast input {unsqueeze_node.input[0]} is not mask input {mask_input_name}")
                     continue
 
-                unsqueeze_added_1 = onnx.helper.make_node('Unsqueeze',
-                                                          inputs=[mask_input_name],
-                                                          outputs=['mask_fuse_unsqueeze1_output'],
-                                                          name='Mask_UnSqueeze_1',
-                                                          axes=[1])
-
-                unsqueeze_added_2 = onnx.helper.make_node('Unsqueeze',
-                                                          inputs=['mask_fuse_unsqueeze1_output'],
-                                                          outputs=['mask_fuse_unsqueeze2_output'],
-                                                          name='Mask_UnSqueeze_2',
-                                                          axes=[2])
-
-                #self.replace_node_input(cast_node, cast_node.input[0], 'mask_fuse_unsqueeze2_output')
-                cast_node_2 = onnx.helper.make_node('Cast',
-                                                    inputs=['mask_fuse_unsqueeze2_output'],
-                                                    outputs=['mask_fuse_cast_output'])
+                unsqueeze_added_1 = onnx.helper.make_node(
+                    "Unsqueeze",
+                    inputs=[mask_input_name],
+                    outputs=["mask_fuse_unsqueeze1_output"],
+                    name="Mask_UnSqueeze_1",
+                    axes=[1],
+                )
+
+                unsqueeze_added_2 = onnx.helper.make_node(
+                    "Unsqueeze",
+                    inputs=["mask_fuse_unsqueeze1_output"],
+                    outputs=["mask_fuse_unsqueeze2_output"],
+                    name="Mask_UnSqueeze_2",
+                    axes=[2],
+                )
+
+                # self.replace_node_input(cast_node, cast_node.input[0], 'mask_fuse_unsqueeze2_output')
+                cast_node_2 = onnx.helper.make_node(
+                    "Cast",
+                    inputs=["mask_fuse_unsqueeze2_output"],
+                    outputs=["mask_fuse_cast_output"],
+                )
                 cast_node_2.attribute.extend([onnx.helper.make_attribute("to", 1)])
-                self.replace_node_input(sub_node, sub_node.input[1], 'mask_fuse_cast_output')
+                self.replace_node_input(sub_node, sub_node.input[1], "mask_fuse_cast_output")
 
                 nodes_to_remove.extend([slice_node, unsqueeze_node, cast_node])
                 self.add_node(unsqueeze_added_1)
                 self.add_node(unsqueeze_added_2)
                 self.add_node(cast_node_2)
 
         self.remove_nodes(nodes_to_remove)
@@ -326,20 +379,41 @@
 
     def remove_extra_reshape(self):
         skiplayernorm_nodes = self.get_nodes_by_op_type("SkipLayerNormalization")
         reshape_removed = 0
         for skiplayernorm_node in skiplayernorm_nodes:
             path = self.match_parent_path(
                 skiplayernorm_node,
-                ['Add', 'Reshape', 'MatMul', 'Reshape', 'Gelu', 'Add', 'Reshape', 'MatMul', 'SkipLayerNormalization'],
-                [0, 0, 0, 0, 0, 0, 0, 0, 0])
+                [
+                    "Add",
+                    "Reshape",
+                    "MatMul",
+                    "Reshape",
+                    "Gelu",
+                    "Add",
+                    "Reshape",
+                    "MatMul",
+                    "SkipLayerNormalization",
+                ],
+                [0, 0, 0, 0, 0, 0, 0, 0, 0],
+            )
             if path is None:
                 continue
 
-            add_1, reshape_1, matmul_1, reshape_2, gelu, add_2, reshape_3, matmul_2, skiplayernorm = path
+            (
+                add_1,
+                reshape_1,
+                matmul_1,
+                reshape_2,
+                gelu,
+                add_2,
+                reshape_3,
+                matmul_2,
+                skiplayernorm,
+            ) = path
             add_2.input[0] = matmul_2.output[0]
             self.remove_node(reshape_3)
             matmul_1.input[0] = gelu.output[0]
             self.remove_node(reshape_2)
             add_1.input[0] = matmul_1.output[0]
             self.remove_node(reshape_1)
             reshape_removed += 3
@@ -348,20 +422,43 @@
 
     def remove_extra_reshape_2(self):
         skiplayernorm_nodes = self.get_nodes_by_op_type("SkipLayerNormalization")
         reshape_removed = 0
         for skiplayernorm_node in skiplayernorm_nodes:
             path = self.match_parent_path(
                 skiplayernorm_node,
-                ['Add', 'Reshape', 'MatMul', 'Reshape', 'Gelu', 'Add', 'Reshape', 'MatMul', 'Reshape', 'SkipLayerNormalization'],
-                [None, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # yapf: disable
+                [
+                    "Add",
+                    "Reshape",
+                    "MatMul",
+                    "Reshape",
+                    "Gelu",
+                    "Add",
+                    "Reshape",
+                    "MatMul",
+                    "Reshape",
+                    "SkipLayerNormalization",
+                ],
+                [None, 0, 0, 0, 0, 0, 0, 0, 0, 0],
+            )  # yapf: disable
             if path is None:
                 continue
 
-            add_1, reshape_1, matmul_1, reshape_2, gelu, add_2, reshape_3, matmul_2, reshape_4, skiplayernorm = path
+            (
+                add_1,
+                reshape_1,
+                matmul_1,
+                reshape_2,
+                gelu,
+                add_2,
+                reshape_3,
+                matmul_2,
+                reshape_4,
+                skiplayernorm,
+            ) = path
 
             matmul_2.input[0] = skiplayernorm.output[0]
             self.remove_node(reshape_4)
 
             add_2.input[0] = matmul_2.output[0]
             self.remove_node(reshape_3)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_bert_tf.py` & `onnxruntime/transformers/onnx_model_bert_tf.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,51 +1,61 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
+import argparse  # noqa: F401
 import logging
-import onnx
-import sys
-import argparse
+import sys  # noqa: F401
+from collections import deque  # noqa: F401
+
 import numpy as np
-from collections import deque
-from onnx import ModelProto, TensorProto, numpy_helper, helper
+import onnx
+from onnx import ModelProto, TensorProto, helper, numpy_helper  # noqa: F401
 from onnx_model_bert import BertOnnxModel
 
 logger = logging.getLogger(__name__)
 
 
 class BertOnnxModelTF(BertOnnxModel):
     def __init__(self, model, num_heads, hidden_size):
         super().__init__(model, num_heads, hidden_size)
 
     def remove_identity(self):
         nodes_to_remove = []
         for node in self.nodes():
-            if node.op_type == 'Identity':
+            if node.op_type == "Identity":
                 if not self.find_graph_output(node.output[0]):
                     self.replace_input_of_all_nodes(node.output[0], node.input[0])
                     nodes_to_remove.append(node)
         self.remove_nodes(nodes_to_remove)
         logger.info(f"Removed Identity count: {len(nodes_to_remove)}")
 
     def match_mask_path(self, add_or_sub_before_softmax):
-        mask_nodes = self.match_parent_path(add_or_sub_before_softmax, ['Mul', 'Sub', 'Reshape', 'Cast'],
-                                            [1, None, 1, 0])
+        mask_nodes = self.match_parent_path(
+            add_or_sub_before_softmax,
+            ["Mul", "Sub", "Reshape", "Cast"],
+            [1, None, 1, 0],
+        )
         if mask_nodes is not None:
             return mask_nodes
 
-        mask_nodes = self.match_parent_path(add_or_sub_before_softmax, ['Mul', 'Sub', 'Cast', 'Slice', 'Unsqueeze'],
-                                            [1, 0, 1, 0, 0])
+        mask_nodes = self.match_parent_path(
+            add_or_sub_before_softmax,
+            ["Mul", "Sub", "Cast", "Slice", "Unsqueeze"],
+            [1, 0, 1, 0, 0],
+        )
         if mask_nodes is not None:
             return mask_nodes
 
-        mask_nodes = self.match_parent_path(add_or_sub_before_softmax, ['Mul', 'Sub', 'Cast', 'Unsqueeze', 'Unsqueeze'],
-                                            [1, None, 1, 0, 0])
+        mask_nodes = self.match_parent_path(
+            add_or_sub_before_softmax,
+            ["Mul", "Sub", "Cast", "Unsqueeze", "Unsqueeze"],
+            [1, None, 1, 0, 0],
+        )
 
         return mask_nodes
 
     def get_2d_initializers_from_parent_subgraphs(self, current_node):
         """
         Find initializers that is 2D. Returns a dictionary with name as key and shape as value.
         """
@@ -77,29 +87,55 @@
         # Find segment ids in graph inputs. The segment id input must not be the same as input_ids.
         if len(graph_inputs) == 1 and graph_inputs[0] != input_ids:
             return graph_inputs[0]
 
         # If the segment id candidate is the same as the input_ids, try to assign alternative segment ids and simplify the graph if needed.
         segment_ids = nodes[0].input[1]
         _, segment_id_path, _ = self.match_parent_paths(
-            nodes[0], [(["ConstantOfShape", "Cast", "Concat", "Slice", "Cast", "Shape"], [1, 0, 0, 0, 0, 0]),
-                       (["ConstantOfShape", "Cast", "Concat", "Unsqueeze", "Squeeze", "Slice", "Cast", "Shape"
-                         ], [1, 0, 0, 0, 0, 0, 0, 0])], None)
+            nodes[0],
+            [
+                (
+                    ["ConstantOfShape", "Cast", "Concat", "Slice", "Cast", "Shape"],
+                    [1, 0, 0, 0, 0, 0],
+                ),
+                (
+                    [
+                        "ConstantOfShape",
+                        "Cast",
+                        "Concat",
+                        "Unsqueeze",
+                        "Squeeze",
+                        "Slice",
+                        "Cast",
+                        "Shape",
+                    ],
+                    [1, 0, 0, 0, 0, 0, 0, 0],
+                ),
+            ],
+            None,
+        )
 
         if segment_id_path and input_ids and input_ids == segment_id_path[-1].input[0]:
             logger.debug("Simplify semgent id path...")
             constantofshape_node = segment_id_path[0]
             graph_name = self.get_graph_by_node(constantofshape_node).name
-            self.add_node(helper.make_node('Shape', inputs=[input_ids], outputs=["input_shape"]), graph_name)
+            self.add_node(
+                helper.make_node("Shape", inputs=[input_ids], outputs=["input_shape"]),
+                graph_name,
+            )
             constantofshape_value = helper.get_attribute_value(constantofshape_node.attribute[0])
             self.add_node(
-                helper.make_node('ConstantOfShape',
-                                 inputs=["input_shape"],
-                                 outputs=["zeros_for_input_shape"],
-                                 value=constantofshape_value), graph_name)
+                helper.make_node(
+                    "ConstantOfShape",
+                    inputs=["input_shape"],
+                    outputs=["zeros_for_input_shape"],
+                    value=constantofshape_value,
+                ),
+                graph_name,
+            )
             segment_ids = "zeros_for_input_shape"
         return segment_ids
 
     def find_input_ids(self, word_embedding):
         input_name_to_nodes = self.input_name_to_nodes()
         if word_embedding not in input_name_to_nodes:
             return None
@@ -113,49 +149,82 @@
             return graph_inputs[0]
 
         print("Found multiple candidates of input_ids", graph_inputs)
         return None
 
     def find_mask_input(self, excluded_graph_inputs):
         for node in self.nodes():
-            if node.op_type == 'Softmax':
-                mask_path = self.match_parent_path(node, ['Add', 'Mul', 'Sub', 'Cast', 'Slice', 'Unsqueeze'],
-                                                   [0, 1, None, 1, 0, 0])
+            if node.op_type == "Softmax":
+                mask_path = self.match_parent_path(
+                    node,
+                    ["Add", "Mul", "Sub", "Cast", "Slice", "Unsqueeze"],
+                    [0, 1, None, 1, 0, 0],
+                )
                 if mask_path is None:
                     continue
-                add_node, mul_node, sub_node, cast_node, slice_node, unsqueeze_node = mask_path
+                (
+                    add_node,
+                    mul_node,
+                    sub_node,
+                    cast_node,
+                    slice_node,
+                    unsqueeze_node,
+                ) = mask_path
                 if self.has_constant_input(mul_node, -10000) and self.has_constant_input(sub_node, 1):
                     graph_inputs = self.get_graph_inputs(sub_node, recursive=True)
                     inputs = [input for input in graph_inputs if input not in excluded_graph_inputs]
                     if len(inputs) > 1:
                         print("Found multiple candidates of mask input", inputs)
                         return None
                     if len(inputs) == 1:
                         return inputs[0]
                     # Duplicated input found. Try to simplify the graph.
                     path_to_be_simplified = self.match_parent_path(
                         mask_path[-1],
-                        ["ConstantOfShape", "Cast", "Concat", "Unsqueeze", "Squeeze", "Slice", "Cast", "Shape"],
-                        [0, 0, 0, 0, 0, 0, 0, 0])
+                        [
+                            "ConstantOfShape",
+                            "Cast",
+                            "Concat",
+                            "Unsqueeze",
+                            "Squeeze",
+                            "Slice",
+                            "Cast",
+                            "Shape",
+                        ],
+                        [0, 0, 0, 0, 0, 0, 0, 0],
+                    )
                     duplicated_inputs = [input for input in graph_inputs if input in excluded_graph_inputs]
                     # Simplify graph for dynamic axes.
-                    if path_to_be_simplified and duplicated_inputs and len(
-                            duplicated_inputs) == 1 and duplicated_inputs[0] == path_to_be_simplified[-1].input[0]:
+                    if (
+                        path_to_be_simplified
+                        and duplicated_inputs
+                        and len(duplicated_inputs) == 1
+                        and duplicated_inputs[0] == path_to_be_simplified[-1].input[0]
+                    ):
                         logger.debug("Simplify semgent id path...")
                         constantofshape_node = path_to_be_simplified[0]
                         constantofshape_value = helper.get_attribute_value(constantofshape_node.attribute[0])
                         graph_name = self.get_graph_by_node(constantofshape_node).name
                         self.add_node(
-                            helper.make_node('Shape', inputs=[duplicated_inputs[0]], outputs=["input_shape_for_mask"]),
-                            graph_name)
+                            helper.make_node(
+                                "Shape",
+                                inputs=[duplicated_inputs[0]],
+                                outputs=["input_shape_for_mask"],
+                            ),
+                            graph_name,
+                        )
                         self.add_node(
-                            helper.make_node('ConstantOfShape',
-                                             inputs=["input_shape_for_mask"],
-                                             outputs=[unsqueeze_node.input[0]],
-                                             value=constantofshape_value), graph_name)
+                            helper.make_node(
+                                "ConstantOfShape",
+                                inputs=["input_shape_for_mask"],
+                                outputs=[unsqueeze_node.input[0]],
+                                value=constantofshape_value,
+                            ),
+                            graph_name,
+                        )
                     return unsqueeze_node.input[0]
         return None
 
     def create_embedding_subgraph(self, normalize_node, word_embedding, segment_embedding, position_embedding):
         input_ids = self.find_input_ids(word_embedding)
         if input_ids is None:
             logger.info("Failed to find input_ids. Cannot fuse embedding layer.")
@@ -169,15 +238,15 @@
         mask_input = self.find_mask_input([segment_ids, input_ids])
         if mask_input is None:
             logger.info("Failed to find input_mask. Cannot fuse embedding layer.")
             return False
 
         self.bert_inputs = [input_ids, segment_ids, mask_input]
 
-        mask_index = self.create_node_name('mask_index')
+        mask_index = self.create_node_name("mask_index")
         self.attention_mask.set_mask_indice(mask_input, mask_index)
 
         if self.find_graph_input(input_ids).type.tensor_type.elem_type != TensorProto.INT32:
             casted, input_ids = self.utils.cast_graph_input_to_int32(input_ids)
 
         if self.find_graph_input(segment_ids):
             casted, segment_ids = self.utils.cast_graph_input_to_int32(segment_ids)
@@ -185,84 +254,95 @@
             segment_ids, segment_id_cast_node = self.utils.cast_input_to_int32(segment_ids)
 
         if self.find_graph_input(mask_input):
             casted, mask_input = self.utils.cast_graph_input_to_int32(mask_input)
         else:
             mask_input, mask_input_cast_node = self.utils.cast_input_to_int32(mask_input)
 
-        embed_output = self.create_node_name('embed_output')
+        embed_output = self.create_node_name("embed_output")
         embed_node = onnx.helper.make_node(
-            'EmbedLayerNormalization',
+            "EmbedLayerNormalization",
             inputs=[
                 input_ids,
                 segment_ids,
                 word_embedding,
                 position_embedding,
                 segment_embedding,
                 normalize_node.input[1],  # gamma
                 normalize_node.input[2],  # beta
-                mask_input
+                mask_input,
             ],
             outputs=[embed_output, mask_index],
-            name="EmbedLayer")
+            name="EmbedLayer",
+        )
         embed_node.domain = "com.microsoft"
         self.replace_input_of_all_nodes(normalize_node.output[0], embed_output)
         self.add_node(embed_node, self.get_graph_by_node(normalize_node).name)
 
     def process_embedding(self):
         """
         Automatically detect word, segment and position embeddings.
         """
         logger.info("start processing embedding layer...")
         output_name_to_node = self.output_name_to_node()
 
         layer_norm_nodes = self.get_nodes_by_op_type("LayerNormalization")
         for layer_norm_node in layer_norm_nodes:
-            pos_embed_path = self.match_parent_path(layer_norm_node, ['Add', 'Reshape', 'Slice'], [0, 1, 0],
-                                                    output_name_to_node)
+            pos_embed_path = self.match_parent_path(
+                layer_norm_node,
+                ["Add", "Reshape", "Slice"],
+                [0, 1, 0],
+                output_name_to_node,
+            )
             if pos_embed_path is None:
                 continue
 
             add_node, reshape_node, slice_node = pos_embed_path
             initializer = self.get_initializer(slice_node.input[0])
             if initializer is None:
                 continue
 
             temp = numpy_helper.to_array(initializer)
             if len(temp.shape) == 2:
-                logger.info("Found position embedding. name:{}, shape:{}".format(initializer.name, temp.shape))
+                logger.info(f"Found position embedding. name:{initializer.name}, shape:{temp.shape}")
                 position_embedding = initializer.name
             else:
-                logger.info("Failed to find position embedding. name:{}, shape:{}".format(initializer.name, temp.shape))
+                logger.info(f"Failed to find position embedding. name:{initializer.name}, shape:{temp.shape}")
                 return
 
             first_parent = self.get_parent(add_node, 0, output_name_to_node)
             if first_parent is not None and first_parent.op_type == "Add":
                 embeddings = self.get_2d_initializers_from_parent_subgraphs(first_parent)
                 if len(embeddings) != 2:
                     logger.warning(
-                        "Failed to find two embeddings (word and segment) from Add node. Found {}".format(embeddings))
+                        f"Failed to find two embeddings (word and segment) from Add node. Found {embeddings}"
+                    )
                     return
 
                 word_embedding = None
                 segment_embedding = None
                 for name, shape in embeddings.items():
                     if shape[0] == 2:
                         segment_embedding = name
-                        logger.info("Found segment embedding. name:{}, shape:{}".format(name, shape))
+                        logger.info(f"Found segment embedding. name:{name}, shape:{shape}")
                     else:
                         word_embedding = name
-                        logger.info("Found words embedding. name:{}, shape:{}".format(name, shape))
+                        logger.info(f"Found words embedding. name:{name}, shape:{shape}")
 
                 if word_embedding is None or segment_embedding is None:
                     logger.info("Failed to find both word and segment embedding")
                     return
 
                 logger.info("Create Embedding node")
-                self.create_embedding_subgraph(layer_norm_node, word_embedding, segment_embedding, position_embedding)
+                self.create_embedding_subgraph(
+                    layer_norm_node,
+                    word_embedding,
+                    segment_embedding,
+                    position_embedding,
+                )
                 # Prune graph to remove those original embedding nodes.
                 self.prune_graph()
                 break
 
     def check_attention_input(self, matmul_q, matmul_k, matmul_v, parent, output_name_to_node):
         for x in [matmul_q, matmul_k, matmul_v]:
             root_input = x.input[0]
@@ -287,69 +367,83 @@
         # Conceptually we treat add before layernorm as skiplayernorm node since they share the same pattern
         start_nodes.extend(skip_layer_norm_nodes)
         start_nodes.extend(layer_norm_nodes)
 
         for normalize_node in start_nodes:
             graph_name = self.get_graph_by_node(normalize_node).name
             # SkipLayerNormalization has two inputs, and one of them is the root input for attention.
-            if normalize_node.op_type == 'LayerNormalization':
-                add_before_layernorm = self.match_parent(normalize_node, 'Add', 0)
+            if normalize_node.op_type == "LayerNormalization":
+                add_before_layernorm = self.match_parent(normalize_node, "Add", 0)
                 if add_before_layernorm is not None:
-                    normalize_node = add_before_layernorm
+                    normalize_node = add_before_layernorm  # noqa: PLW2901
                 else:
                     continue
             parent = self.get_parent(normalize_node, 1)
-            if parent is None or parent.op_type not in ["SkipLayerNormalization", "LayerNormalization", "Reshape"]:
+            if parent is None or parent.op_type not in [
+                "SkipLayerNormalization",
+                "LayerNormalization",
+                "Reshape",
+            ]:
                 parent = self.get_parent(normalize_node, 0)
-                if parent is None or parent.op_type not in ["SkipLayerNormalization", "LayerNormalization", "Reshape"]:
+                if parent is None or parent.op_type not in [
+                    "SkipLayerNormalization",
+                    "LayerNormalization",
+                    "Reshape",
+                ]:
                     logger.debug("Failed to match parent of normalize_node")
                     continue
 
-            qkv_nodes = self.match_parent_path(normalize_node, ['Add', 'MatMul', 'Reshape', 'Transpose', 'MatMul'],
-                                               [0, 0, 0, 0, 0])
+            qkv_nodes = self.match_parent_path(
+                normalize_node,
+                ["Add", "MatMul", "Reshape", "Transpose", "MatMul"],
+                [0, 0, 0, 0, 0],
+            )
             if qkv_nodes is None:
-                qkv_nodes = self.match_parent_path(normalize_node, ['MatMul', 'Reshape', 'Transpose', 'MatMul'],
-                                                   [1, 0, 0, 0])
+                qkv_nodes = self.match_parent_path(
+                    normalize_node,
+                    ["MatMul", "Reshape", "Transpose", "MatMul"],
+                    [1, 0, 0, 0],
+                )
                 if qkv_nodes is None:
-                    qkv_nodes = self.match_parent_path(normalize_node, ['Add', 'Einsum', 'Einsum'], [0, 0, 0])
+                    qkv_nodes = self.match_parent_path(normalize_node, ["Add", "Einsum", "Einsum"], [0, 0, 0])
                     if qkv_nodes is None:
                         logger.debug("Failed to match qkv nodes")
                         continue
 
             matmul_qkv = qkv_nodes[-1]
-            v_nodes = self.match_parent_path(matmul_qkv, ['Transpose', 'Reshape', 'Add', 'MatMul'], [1, 0, 0, 0])
+            v_nodes = self.match_parent_path(matmul_qkv, ["Transpose", "Reshape", "Add", "MatMul"], [1, 0, 0, 0])
             if v_nodes is None:
-                v_nodes = self.match_parent_path(matmul_qkv, ['Add', 'Einsum'], [1, 0])
+                v_nodes = self.match_parent_path(matmul_qkv, ["Add", "Einsum"], [1, 0])
                 if v_nodes is None:
                     logger.debug("Failed to match v path")
                     continue
 
             add_v = v_nodes[-2]
             matmul_v = v_nodes[-1]
-            qk_nodes = self.match_parent_path(matmul_qkv, ['Softmax', 'Add', "Mul", 'MatMul'], [0, 0, 0, 0])
+            qk_nodes = self.match_parent_path(matmul_qkv, ["Softmax", "Add", "Mul", "MatMul"], [0, 0, 0, 0])
             if qk_nodes is None:
-                qk_nodes = self.match_parent_path(matmul_qkv, ['Softmax', 'Add', 'Einsum'], [0, 0, 0])
+                qk_nodes = self.match_parent_path(matmul_qkv, ["Softmax", "Add", "Einsum"], [0, 0, 0])
                 if qk_nodes is None:
                     logger.debug("Failed to match qk_paths")
                     continue
             matmul_qk = qk_nodes[-1]
 
-            q_nodes = self.match_parent_path(matmul_qk, ['Transpose', 'Reshape', 'Add', 'MatMul'], [0, 0, 0, 0])
+            q_nodes = self.match_parent_path(matmul_qk, ["Transpose", "Reshape", "Add", "MatMul"], [0, 0, 0, 0])
             if q_nodes is None:
-                q_nodes = self.match_parent_path(matmul_qk, ['Add', 'Einsum'], [0, 0])
+                q_nodes = self.match_parent_path(matmul_qk, ["Add", "Einsum"], [0, 0])
                 if q_nodes is None:
                     logger.debug("Failed to match q path")
                     continue
 
             add_q = q_nodes[-2]
             matmul_q = q_nodes[-1]
 
-            k_nodes = self.match_parent_path(matmul_qk, ['Transpose', 'Reshape', 'Add', 'MatMul'], [1, 0, 0, 0])
+            k_nodes = self.match_parent_path(matmul_qk, ["Transpose", "Reshape", "Add", "MatMul"], [1, 0, 0, 0])
             if k_nodes is None:
-                k_nodes = self.match_parent_path(matmul_qk, ['Mul', 'Add', 'Einsum'], [1, 0, 0])
+                k_nodes = self.match_parent_path(matmul_qk, ["Mul", "Add", "Einsum"], [1, 0, 0])
                 if k_nodes is None:
                     logger.debug("Failed to match k path")
                     continue
             add_k = k_nodes[-2]
             matmul_k = k_nodes[-1]
 
             mask_nodes = self.match_mask_path(qk_nodes[1])
@@ -359,62 +453,96 @@
                 continue
 
             if not self.has_constant_input(mask_nodes[1], 1):
                 logger.debug("Sub node expected to have an input with constant value 1.0.")
                 continue
 
             # add a squeeze node to convert a 3-d mask to 2-d
-            squeeze_node = self.match_parent_path(mask_nodes[-1], ['Squeeze'], [0]) or self.match_parent_path(
-                mask_nodes[-1], ['Expand'], [0])
+            squeeze_node = self.match_parent_path(mask_nodes[-1], ["Squeeze"], [0]) or self.match_parent_path(
+                mask_nodes[-1], ["Expand"], [0]
+            )
             squeeze_node_name = "Squeeze_3d_to_2d_mask"
             squeeze_output_name = squeeze_node_name + "_output"
             if squeeze_node is None and len(mask_nodes) == 5 and self.find_graph_input(mask_nodes[-1].input[0]) is None:
                 mask_input = mask_nodes[-1].input[1]
                 self.add_node(
-                    helper.make_node("Squeeze", [mask_input], [squeeze_output_name], squeeze_node_name, axes=[1]),
-                    graph_name)
+                    helper.make_node(
+                        "Squeeze",
+                        [mask_input],
+                        [squeeze_output_name],
+                        squeeze_node_name,
+                        axes=[1],
+                    ),
+                    graph_name,
+                )
                 mask_nodes[-1].input[0] = squeeze_output_name
 
             is_same_root = self.check_attention_input(matmul_q, matmul_k, matmul_v, parent, output_name_to_node)
             if is_same_root:
                 mask_index = self.attention_mask.process_mask(mask_nodes[-1].input[0])
                 logger.debug("Create an Attention node.")
 
                 # For tf models, q and v are flipped.
-                attention_node = self.attention_fusion.create_attention_node(mask_index, matmul_k, matmul_q, matmul_v,
-                                                                             add_k, add_q, add_v, self.num_heads,
-                                                                             self.hidden_size, parent.output[0],
-                                                                             qkv_nodes[2].output[0], None)
+                attention_node = self.attention_fusion.create_attention_node(
+                    mask_index,
+                    matmul_k,
+                    matmul_q,
+                    matmul_v,
+                    add_k,
+                    add_q,
+                    add_v,
+                    self.num_heads,
+                    self.hidden_size,
+                    parent.output[0],
+                    qkv_nodes[2].output[0],
+                    None,
+                )
                 if attention_node is None:
                     continue
 
-                if qkv_nodes[1].op_type == 'Einsum':
+                if qkv_nodes[1].op_type == "Einsum":
                     # add reshape before einsum
-                    tensor = helper.make_tensor(name=qkv_nodes[1].name + "_newshape",
-                                                data_type=TensorProto.INT64,
-                                                dims=[4],
-                                                vals=np.int64(
-                                                    [[0, 0, self.num_heads,
-                                                      int(self.hidden_size / self.num_heads)]]).tobytes(),
-                                                raw=True)
+                    tensor = helper.make_tensor(
+                        name=qkv_nodes[1].name + "_newshape",
+                        data_type=TensorProto.INT64,
+                        dims=[4],
+                        vals=np.int64(
+                            [
+                                [
+                                    0,
+                                    0,
+                                    self.num_heads,
+                                    int(self.hidden_size / self.num_heads),
+                                ]
+                            ]
+                        ).tobytes(),
+                        raw=True,
+                    )
                     self.add_initializer(tensor, graph_name)
-                    reshape_ = helper.make_node("Reshape",
-                                                inputs=[attention_node.output[0], qkv_nodes[1].name + "_newshape"],
-                                                outputs=[qkv_nodes[1].name + "_reshape_output"],
-                                                name=qkv_nodes[1].name + "_reshape")
+                    reshape_ = helper.make_node(
+                        "Reshape",
+                        inputs=[
+                            attention_node.output[0],
+                            qkv_nodes[1].name + "_newshape",
+                        ],
+                        outputs=[qkv_nodes[1].name + "_reshape_output"],
+                        name=qkv_nodes[1].name + "_reshape",
+                    )
                     qkv_nodes[1].input[0] = qkv_nodes[1].name + "_reshape_output"
                     self.add_node(reshape_, graph_name)
-                if parent.op_type == 'Reshape':
+                if parent.op_type == "Reshape":
                     # Temporary work around: we require the skiplayernorm and attention op be fed with 3-d input
                     hidden_size = numpy_helper.to_array(self.get_initializer(parent.input[1]))[1]
-                    tensor = helper.make_tensor(name=parent.name + "_modified",
-                                                data_type=TensorProto.INT64,
-                                                dims=[3],
-                                                vals=np.int64([[1, -1, hidden_size]]).tobytes(),
-                                                raw=True)
+                    tensor = helper.make_tensor(
+                        name=parent.name + "_modified",
+                        data_type=TensorProto.INT64,
+                        dims=[3],
+                        vals=np.int64([[1, -1, hidden_size]]).tobytes(),
+                        raw=True,
+                    )
                     self.add_initializer(tensor, graph_name)
                     parent.input[1] = parent.name + "_modified"
 
                 self.add_node(attention_node, graph_name)
                 attention_count += 1
 
                 nodes_to_remove.extend(qkv_nodes[2:])
@@ -446,15 +574,15 @@
 
         if count > 0:
             logger.info(f"Skip consequent Reshape count: {count}")
 
     def remove_reshape_before_first_attention(self):
         attention_nodes = self.get_nodes_by_op_type("Attention")
         for attention_node in attention_nodes:
-            path = self.match_parent_path(attention_node, ['Reshape', 'EmbedLayerNormalization'], [0, 0])
+            path = self.match_parent_path(attention_node, ["Reshape", "EmbedLayerNormalization"], [0, 0])
             if path is None:
                 continue
             logger.info("Remove Reshape before first Attention node.")
             reshape, _ = path
             self.replace_input_of_all_nodes(reshape.output[0], reshape.input[0])
             self.remove_node(reshape)
             break
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_gpt2.py` & `onnxruntime/transformers/onnx_model_gpt2.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,18 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 import logging
+
 import onnx
-from onnx_model_bert import BertOnnxModel
-from fusion_gpt_attention_no_past import FusionGptAttentionNoPast
 from fusion_gpt_attention import FusionGptAttention
 from fusion_gpt_attention_megatron import FusionGptAttentionMegatron
+from fusion_gpt_attention_no_past import FusionGptAttentionNoPast
+from onnx_model_bert import BertOnnxModel
 
 logger = logging.getLogger(__name__)
 
 
 class Gpt2OnnxModel(BertOnnxModel):
     def __init__(self, model, num_heads, hidden_size):
         super().__init__(model, num_heads, hidden_size)
@@ -26,46 +27,62 @@
             fusion = FusionGptAttentionMegatron(self, self.num_heads)
             fusion.apply()
 
     def postprocess(self):
         """
         Remove extra reshape nodes.
         """
-        logger.debug(f"start postprocessing...")
+        logger.debug("start postprocessing...")
 
         input_name_to_nodes = self.input_name_to_nodes()
         output_name_to_node = self.output_name_to_node()
 
         reshape_count = 0
         for gemm_node in self.get_nodes_by_op_type("Gemm"):
-            reshape_after_gemm = self.find_first_child_by_type(gemm_node,
-                                                               'Reshape',
-                                                               input_name_to_nodes,
-                                                               recursive=False)
+            reshape_after_gemm = self.find_first_child_by_type(
+                gemm_node, "Reshape", input_name_to_nodes, recursive=False
+            )
 
-            return_indice = []
-            nodes = self.match_parent_path(gemm_node, ['Reshape', 'FastGelu'], [0, 0], output_name_to_node)
+            nodes = self.match_parent_path(gemm_node, ["Reshape", "FastGelu"], [0, 0], output_name_to_node)
             if nodes is None:
-                nodes = self.match_parent_path(gemm_node, ['Reshape', 'LayerNormalization'], [0, 0],
-                                               output_name_to_node)
+                nodes = self.match_parent_path(
+                    gemm_node,
+                    ["Reshape", "LayerNormalization"],
+                    [0, 0],
+                    output_name_to_node,
+                )
+
                 if nodes is None:
-                    continue
+                    nodes = self.match_parent_path(
+                        gemm_node,
+                        ["Reshape", "SkipLayerNormalization"],
+                        [0, 0],
+                        output_name_to_node,
+                    )
+
+                    if nodes is None:
+                        continue
+
             (reshape_before_gemm, root_node) = nodes
 
-            matmul_node_name = self.create_node_name('MatMul', 'FullyConnect_MatMul')
-            matmul_node = onnx.helper.make_node('MatMul',
-                                                inputs=[matmul_node_name + "_input", gemm_node.input[1]],
-                                                outputs=[matmul_node_name + "_output"],
-                                                name=matmul_node_name)
-
-            add_node_name = self.create_node_name('Add', 'FullyConnect_Add')
-            add_node = onnx.helper.make_node('Add',
-                                             inputs=[matmul_node_name + "_output", gemm_node.input[2]],
-                                             outputs=[add_node_name + "_output"],
-                                             name=add_node_name)
+            matmul_node_name = self.create_node_name("MatMul", "FullyConnect_MatMul")
+            matmul_node = onnx.helper.make_node(
+                "MatMul",
+                inputs=[matmul_node_name + "_input", gemm_node.input[1]],
+                outputs=[matmul_node_name + "_output"],
+                name=matmul_node_name,
+            )
+
+            add_node_name = self.create_node_name("Add", "FullyConnect_Add")
+            add_node = onnx.helper.make_node(
+                "Add",
+                inputs=[matmul_node_name + "_output", gemm_node.input[2]],
+                outputs=[add_node_name + "_output"],
+                name=add_node_name,
+            )
 
             self.replace_input_of_all_nodes(reshape_after_gemm.output[0], add_node_name + "_output")
 
             # Link root node output with MatMul
             self.replace_input_of_all_nodes(root_node.output[0], matmul_node_name + "_input")
             root_node.output[0] = matmul_node_name + "_input"
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/onnx_model_tnlr.py` & `onnxruntime/transformers/onnx_model_tnlr.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,176 +1,226 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 import logging
-from fusion_attention import FusionAttention, AttentionMask
+from typing import Union
+
+import numpy as np
+from fusion_attention import AttentionMask, FusionAttention
 from fusion_utils import NumpyHelper
-from onnx import helper, numpy_helper, TensorProto, NodeProto
+from onnx import NodeProto, TensorProto, helper, numpy_helper
 from onnx_model import OnnxModel
 from onnx_model_bert import BertOnnxModel
-from typing import Union
 
 logger = logging.getLogger(__name__)
 
 
 class FusionTnlrAttention(FusionAttention):
     """
     Fuse TNLR Attention subgraph into one Attention node.
     TNLR Attention has extra addtion after qk nodes and adopts [S, B, NH] as I/O shape.
     """
-    def __init__(self, model: OnnxModel, hidden_size: int, num_heads: int, attention_mask: AttentionMask):
-        super().__init__(model, hidden_size, num_heads, attention_mask)
 
-    def create_attention_node(self, mask_index: str, matmul: NodeProto, add: NodeProto, num_heads: int,
-                              hidden_size: int, input: str, output: str, add_qk_str: str) -> Union[NodeProto, None]:
+    def __init__(
+        self,
+        model: OnnxModel,
+        hidden_size: int,
+        num_heads: int,
+        attention_mask: AttentionMask,
+    ):
+        super().__init__(model, hidden_size, num_heads, attention_mask)
 
+    def create_attention_node(
+        self,
+        mask_index: str,
+        matmul: NodeProto,
+        add: NodeProto,
+        num_heads: int,
+        hidden_size: int,
+        input: str,
+        output: str,
+        add_qk_str: str,
+    ) -> Union[NodeProto, None]:
         assert num_heads > 0
         if hidden_size > 0 and (hidden_size % num_heads) != 0:
             logger.debug(f"input hidden size {hidden_size} is not a multiple of num of heads {num_heads}")
             return None
 
         weight = self.model.get_initializer(matmul.input[1])
         bias = self.model.get_initializer(add.input[1]) or self.model.get_initializer(add.input[0])
 
         if weight is None or bias is None:
             return None
 
         qkv_weight = NumpyHelper.to_array(weight)
         qkv_bias = NumpyHelper.to_array(bias)
 
-        attention_node_name = self.model.create_node_name('Attention')
+        attention_node_name = self.model.create_node_name("Attention")
 
-        weight = helper.make_tensor(name=attention_node_name + '_qkv_weight',
-                                    data_type=TensorProto.FLOAT,
-                                    dims=[hidden_size, 3 * hidden_size],
-                                    vals=qkv_weight.flatten().tolist())
+        weight = helper.make_tensor(
+            name=attention_node_name + "_qkv_weight",
+            data_type=TensorProto.FLOAT,
+            dims=[hidden_size, 3 * hidden_size],
+            vals=qkv_weight.flatten().tolist(),
+        )
 
         # Sometimes weights and bias are stored in fp16
         if weight.data_type == 10:
             weight.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(weight).astype(np.float16), weight.name))
         self.model.add_initializer(weight, self.this_graph_name)
 
-        bias = helper.make_tensor(name=attention_node_name + '_qkv_bias',
-                                  data_type=TensorProto.FLOAT,
-                                  dims=[3 * hidden_size],
-                                  vals=qkv_bias.flatten().tolist())
+        bias = helper.make_tensor(
+            name=attention_node_name + "_qkv_bias",
+            data_type=TensorProto.FLOAT,
+            dims=[3 * hidden_size],
+            vals=qkv_bias.flatten().tolist(),
+        )
         if bias.data_type == 10:
             bias.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(bias).astype(np.float16), bias.name))
         self.model.add_initializer(bias, self.this_graph_name)
 
-        attention_inputs = [input, attention_node_name + '_qkv_weight', attention_node_name + '_qkv_bias']
+        attention_inputs = [
+            input,
+            attention_node_name + "_qkv_weight",
+            attention_node_name + "_qkv_bias",
+        ]
         if mask_index is not None:
             attention_inputs.append(mask_index)
         else:
             attention_inputs.append("")
 
         if add_qk_str is not None:
             attention_inputs.append("")
             attention_inputs.append(add_qk_str)
 
-        attention_node = helper.make_node('Attention',
-                                          inputs=attention_inputs,
-                                          outputs=[output],
-                                          name=attention_node_name)
+        attention_node = helper.make_node(
+            "Attention",
+            inputs=attention_inputs,
+            outputs=[output],
+            name=attention_node_name,
+        )
         attention_node.domain = "com.microsoft"
         attention_node.attribute.extend([helper.make_attribute("num_heads", num_heads)])
 
         return attention_node
 
     def fuse(self, normalize_node, input_name_to_nodes, output_name_to_node):
         # Sometimes we can not fuse skiplayernormalization since the add before layernorm has an output that used by nodes outside skiplayernorm
         # Conceptually we treat add before layernorm as skiplayernorm node since they share the same pattern
         start_node = normalize_node
-        if normalize_node.op_type != 'SkipLayerNormalization':
+        if normalize_node.op_type != "SkipLayerNormalization":
             return
 
         # SkipLayerNormalization has two inputs, and one of them is the root input for attention.
-        qkv_nodes = self.model.match_parent_path(start_node,
-                                                 ['Where', 'Add', 'MatMul', 'Reshape', 'Transpose', 'MatMul'],
-                                                 [1, 1, 1, 0, 0, 0])
+        qkv_nodes = self.model.match_parent_path(
+            start_node,
+            ["Where", "Add", "MatMul", "Reshape", "Transpose", "MatMul"],
+            [1, 1, 1, 0, 0, 0],
+        )
         if qkv_nodes is not None:
             (_, _, matmul_below, reshape_qkv, transpose_qkv, matmul_qkv) = qkv_nodes
         else:
             return
 
         other_inputs = []
-        for i, input in enumerate(start_node.input):
+        for _i, input in enumerate(start_node.input):
             if input not in output_name_to_node:
                 continue
 
             if input == qkv_nodes[0].output[0]:
                 continue
             other_inputs.append(input)
         if len(other_inputs) != 1:
             return
 
         root_input = other_inputs[0]
 
-        v_nodes = self.model.match_parent_path(matmul_qkv, ['Transpose', 'Reshape', 'Slice', 'Add', 'MatMul'],
-                                               [1, 0, 0, 0, 1])
+        v_nodes = self.model.match_parent_path(
+            matmul_qkv,
+            ["Transpose", "Reshape", "Slice", "Add", "MatMul"],
+            [1, 0, 0, 0, 1],
+        )
         if v_nodes is None:
             return
         (_, _, _, add, matmul) = v_nodes
 
-        upper_nodes = self.model.match_parent_path(matmul, ['Transpose'], [0])
+        upper_nodes = self.model.match_parent_path(matmul, ["Transpose"], [0])
         transpose = upper_nodes[0]
 
-        qk_nodes = self.model.match_parent_path(matmul_qkv, ['Softmax', 'Add', 'MatMul'], [0, 0, 0])
+        qk_nodes = self.model.match_parent_path(matmul_qkv, ["Softmax", "Add", "MatMul"], [0, 0, 0])
         if qk_nodes is None:
             return
         (_, add_qk, matmul_qk) = qk_nodes
 
-        q_nodes = self.model.match_parent_path(matmul_qk, ['Mul', 'Transpose', 'Reshape', 'Slice', 'Add', 'MatMul'],
-                                               [0, 0, 0, 0, 0, 1])
+        q_nodes = self.model.match_parent_path(
+            matmul_qk,
+            ["Mul", "Transpose", "Reshape", "Slice", "Add", "MatMul"],
+            [0, 0, 0, 0, 0, 1],
+        )
         if q_nodes is None:
             return
         add = q_nodes[-2]
         matmul = q_nodes[-1]
 
-        k_nodes = self.model.match_parent_path(matmul_qk, ['Transpose', 'Reshape', 'Slice', 'Add', 'MatMul'],
-                                               [1, 0, 0, 0, 1])
+        k_nodes = self.model.match_parent_path(
+            matmul_qk,
+            ["Transpose", "Reshape", "Slice", "Add", "MatMul"],
+            [1, 0, 0, 0, 1],
+        )
         if k_nodes is None:
             return
         add = k_nodes[-2]
         matmul = k_nodes[-1]
 
-        extra_add_qk_nodes = self.model.match_parent_path(add_qk, ['Reshape', 'Where'], [1, 0])
-        if extra_add_qk_nodes is None:
+        relative_position_bias_nodes = self.model.match_parent_path(add_qk, ["Reshape", "Where"], [1, 0])
+        if relative_position_bias_nodes is None:
             return
 
         if matmul.input[0] == root_input:
             mask_index = None
             attention_last_node = reshape_qkv
             # number of heads are same for all the paths, hence to create attention node, we pass the q_num_heads
             # the input_hidden_size represents the input hidden size, this is used as needed but hidden sizes for Q, K are extracted appropriately
-            new_node = self.create_attention_node(mask_index, matmul, add, self.num_heads, self.hidden_size, root_input,
-                                                  attention_last_node.output[0], extra_add_qk_nodes[0].input[0])
+            new_node = self.create_attention_node(
+                mask_index,
+                matmul,
+                add,
+                self.num_heads,
+                self.hidden_size,
+                root_input,
+                attention_last_node.output[0],
+                relative_position_bias_nodes[0].input[0],
+            )
             if new_node is None:
                 return
 
             self.nodes_to_add.append(new_node)
             self.node_name_to_graph_name[new_node.name] = self.this_graph_name
 
             # Add a transpose node after the attention node
-            back_transpose = helper.make_node("Transpose", ["back_transpose_in_" + new_node.name], [new_node.output[0]],
-                                              "back_transpose_" + new_node.name,
-                                              perm=[1, 0, 2])
+            back_transpose = helper.make_node(
+                "Transpose",
+                ["back_transpose_in_" + new_node.name],
+                [new_node.output[0]],
+                "back_transpose_" + new_node.name,
+                perm=[1, 0, 2],
+            )
             self.model.add_node(back_transpose, self.this_graph_name)
             new_node.input[0] = transpose.input[0]
             new_node.output[0] = "back_transpose_in_" + new_node.name
 
             self.nodes_to_remove.extend([attention_last_node, transpose_qkv, matmul_qkv])
             self.nodes_to_remove.extend(qk_nodes)
             self.nodes_to_remove.extend(q_nodes)
             self.nodes_to_remove.extend(k_nodes)
             self.nodes_to_remove.extend(v_nodes)
 
             # Use prune graph to remove mask nodes since they are shared by all attention nodes.
-            #self.nodes_to_remove.extend(mask_nodes)
+            # self.nodes_to_remove.extend(mask_nodes)
             self.prune_graph = True
 
 
 class TnlrOnnxModel(BertOnnxModel):
     def __init__(self, model, num_heads, hidden_size):
         super().__init__(model, num_heads, hidden_size)
         self.attention_mask = AttentionMask(self)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/optimizer.py` & `onnxruntime/transformers/optimizer.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,244 +1,306 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 # Convert Bert ONNX model converted from TensorFlow or exported from PyTorch to use Attention, Gelu,
 # SkipLayerNormalization and EmbedLayerNormalization ops to optimize
 # performance on NVidia GPU and CPU.
 #
 # For Bert model exported from PyTorch, OnnxRuntime has bert model optimization support internally.
 # You can use the option --use_onnxruntime to check optimizations from OnnxRuntime.
 # For Bert model file like name.onnx, optimized model for GPU or CPU from OnnxRuntime will output as
 # name_ort_gpu.onnx or name_ort_cpu.onnx in the same directory.
 #
-# This script is retained for experiment purpose. Useful senarios like the following:
+# This script is retained for experiment purpose. Useful scenarios like the following:
 #  (1) Change model from fp32 to fp16 for mixed precision inference in GPU with Tensor Core.
 #  (2) Change input data type from int64 to int32.
 #  (3) Some model cannot be handled by OnnxRuntime, and you can modify this script to get optimized model.
 
+import argparse
 import logging
-import coloredlogs
 import os
-import argparse
 from typing import Dict, Optional
-from onnx import load_model, ModelProto
+
+import coloredlogs
+from fusion_options import FusionOptions
+from onnx import ModelProto, load_model
 from onnx_model_bart import BartOnnxModel
 from onnx_model_bert import BertOnnxModel
-from onnx_model_bert_tf import BertOnnxModelTF
 from onnx_model_bert_keras import BertOnnxModelKeras
+from onnx_model_bert_tf import BertOnnxModelTF
+from onnx_model_clip import ClipOnnxModel
 from onnx_model_gpt2 import Gpt2OnnxModel
+from onnx_model_t5 import T5OnnxModel
 from onnx_model_tnlr import TnlrOnnxModel
-from fusion_options import FusionOptions
+from onnx_model_unet import UnetOnnxModel
+from onnx_model_vae import VaeOnnxModel
 
 logger = logging.getLogger(__name__)
 
 # Map model type to tuple: optimizer class, export tools (pytorch, tf2onnx, keras2onnx), and default opt_level
 MODEL_TYPES = {
     "bart": (BartOnnxModel, "pytorch", 1),
     "bert": (BertOnnxModel, "pytorch", 1),
     "bert_tf": (BertOnnxModelTF, "tf2onnx", 0),
     "bert_keras": (BertOnnxModelKeras, "keras2onnx", 0),
     "gpt2": (Gpt2OnnxModel, "pytorch", 1),
-    "gpt2_tf": (Gpt2OnnxModel, 'tf2onnx', 0),  # might add a class for GPT2OnnxModel for TF later.
+    "gpt2_tf": (
+        Gpt2OnnxModel,
+        "tf2onnx",
+        0,
+    ),  # might add a class for GPT2OnnxModel for TF later.
     "tnlr": (TnlrOnnxModel, "pytorch", 1),
+    "t5": (T5OnnxModel, "pytorch", 2),
+    # Stable Diffusion models
+    "unet": (UnetOnnxModel, "pytorch", 1),
+    "vae": (VaeOnnxModel, "pytorch", 1),
+    "clip": (ClipOnnxModel, "pytorch", 1),
+    "vit": (BertOnnxModel, "pytorch", 1),
+    "swin": (BertOnnxModel, "pytorch", 1),
 }
 
 
-def optimize_by_onnxruntime(onnx_model_path: str,
-                            use_gpu: bool = False,
-                            optimized_model_path: Optional[str] = None,
-                            opt_level: Optional[int] = 99,
-                            disabled_optimizers=[]) -> str:
+def optimize_by_onnxruntime(
+    onnx_model_path: str,
+    use_gpu: bool = False,
+    optimized_model_path: Optional[str] = None,
+    opt_level: Optional[int] = 99,
+    disabled_optimizers=[],  # noqa: B006
+    verbose=False,
+) -> str:
     """
     Use onnxruntime to optimize model.
 
     Args:
         onnx_model_path (str): the path of input onnx model.
         use_gpu (bool): whether the optimized model is targeted to run in GPU.
         optimized_model_path (str or None): the path of optimized model.
         opt_level (int): graph optimization level.
         disabled_optimizers (List[str]): a list of names of disabled optimizers
     Returns:
         optimized_model_path (str): the path of optimized model
     """
     assert opt_level in [1, 2, 99]
+    from torch import version as torch_version
+
     import onnxruntime
 
-    if use_gpu and 'CUDAExecutionProvider' not in onnxruntime.get_available_providers():
+    if use_gpu and set(onnxruntime.get_available_providers()).isdisjoint(
+        ["CUDAExecutionProvider", "ROCMExecutionProvider", "MIGraphXExecutionProvider"]
+    ):
         logger.error("There is no gpu for onnxruntime to do optimization.")
         return onnx_model_path
 
     sess_options = onnxruntime.SessionOptions()
     if opt_level == 1:
         sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_BASIC
     elif opt_level == 2:
         sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_EXTENDED
     else:
         sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
 
     if optimized_model_path is None:
-        path_prefix = onnx_model_path[:-5]  #remove .onnx suffix
+        path_prefix = onnx_model_path[:-5]  # remove .onnx suffix
         optimized_model_path = "{}_o{}_{}.onnx".format(path_prefix, opt_level, "gpu" if use_gpu else "cpu")
 
     sess_options.optimized_model_filepath = optimized_model_path
 
+    if verbose:
+        print("Using onnxruntime to optimize model - Debug level Set to verbose")
+        sess_options.log_severity_level = 0
+
     kwargs = {}
     if disabled_optimizers:
         kwargs["disabled_optimizers"] = disabled_optimizers
 
     if not use_gpu:
-        session = onnxruntime.InferenceSession(onnx_model_path,
-                                               sess_options,
-                                               providers=['CPUExecutionProvider'],
-                                               **kwargs)
+        onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=["CPUExecutionProvider"], **kwargs)
     else:
-        session = onnxruntime.InferenceSession(onnx_model_path, sess_options, **kwargs)
-        assert 'CUDAExecutionProvider' in session.get_providers()  # Make sure there is GPU
+        gpu_ep = []
+
+        if torch_version.cuda:
+            gpu_ep.append("CUDAExecutionProvider")
+        elif torch_version.hip:
+            gpu_ep.append("MIGraphXExecutionProvider")
+            gpu_ep.append("ROCMExecutionProvider")
+        onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=gpu_ep, **kwargs)
+        assert not set(onnxruntime.get_available_providers()).isdisjoint(
+            ["CUDAExecutionProvider", "ROCMExecutionProvider", "MIGraphXExecutionProvider"]
+        )
 
     assert os.path.exists(optimized_model_path) and os.path.isfile(optimized_model_path)
-    logger.debug("Save optimized model by onnxruntime to {}".format(optimized_model_path))
+    logger.debug("Save optimized model by onnxruntime to %s", optimized_model_path)
     return optimized_model_path
 
 
-def optimize_by_fusion(model: ModelProto,
-                       model_type: str = 'bert',
-                       num_heads: int = 0,
-                       hidden_size: int = 0,
-                       optimization_options: Optional[FusionOptions] = None):
-    """ Optimize Model by graph fusion logic.
+def optimize_by_fusion(
+    model: ModelProto,
+    model_type: str = "bert",
+    num_heads: int = 0,
+    hidden_size: int = 0,
+    optimization_options: Optional[FusionOptions] = None,
+):
+    """Optimize Model by graph fusion logic.
 
     Note that ONNXRuntime graph optimizations (like constant folding) will not be applied. So it is better to enable
     constant folding during exporting ONNX model, or run optimize_by_onnxruntime on the model first like optimize_model.
 
-    For BERT model, num_heads and hidden_size are optional. For other model types, you need specify these parameters.
+    For BERT model, num_heads and hidden_size are optional. For other model types, you need to specify these parameters.
 
     Args:
         model (ModelProto): model object
         model_type (str, optional): model type - like bert, bert_tf, bert_keras or gpt2. Defaults to 'bert'.
         num_heads (int, optional): number of attention heads. Defaults to 0.
-                                   0 allows detect the parameter from graph automatically (for model_type "bert" only).
+                                   0 allows detect the parameter from graph automatically.
         hidden_size (int, optional): hidden size. Defaults to 0.
-                                     0 allows detect the parameter from graph automatically (for model_type "bert" only).
-        optimization_options (FusionOptions, optional): optimization options that turn on/off some fusions. Defaults to None.
+                                     0 allows detect the parameter from graph automatically.
+        optimization_options (FusionOptions, optional): optimization options that turn on/off some fusions.
+                                                        Defaults to None.
 
      Returns:
         object of an optimizer class.
     """
-    if model_type != "bert" and (num_heads == 0 or hidden_size == 0):
-        logger.warning("Please specify parameters of num_heads and hidden_size when model_type is not 'bert'")
+    if model_type not in ["bert", "swin", "unet", "vae", "clip"] and (num_heads == 0 or hidden_size == 0):
+        logger.warning(f"Please specify parameters of num_heads and hidden_size for model_type {model_type}")
 
     (optimizer_class, producer, _) = MODEL_TYPES[model_type]
 
     if model.producer_name and producer != model.producer_name:
         logger.warning(
-            f"Model producer not matched: Expect {producer}, Got {model.producer_name} {model.producer_version}. Please specify correct --model_type parameter."
+            f'Model producer not matched: Expected "{producer}", Got "{model.producer_name}".'
+            "Please specify correct --model_type parameter."
         )
 
     if optimization_options is None:
         optimization_options = FusionOptions(model_type)
 
     optimizer = optimizer_class(model, num_heads, hidden_size)
 
     optimizer.optimize(optimization_options)
 
     optimizer.topological_sort()
 
     optimizer.model.producer_name = "onnxruntime.transformers"
     from onnxruntime import __version__ as onnxruntime_version
+
     optimizer.model.producer_version = onnxruntime_version
 
     return optimizer
 
 
-def optimize_model(input: str,
-                   model_type: str = 'bert',
-                   num_heads: int = 0,
-                   hidden_size: int = 0,
-                   optimization_options: Optional[FusionOptions] = None,
-                   opt_level: int = None,
-                   use_gpu: bool = False,
-                   only_onnxruntime: bool = False):
-    """ Optimize Model by OnnxRuntime and/or python fusion logic.
+def optimize_model(
+    input: str,
+    model_type: str = "bert",
+    num_heads: int = 0,
+    hidden_size: int = 0,
+    optimization_options: Optional[FusionOptions] = None,
+    opt_level: Optional[int] = None,
+    use_gpu: bool = False,
+    only_onnxruntime: bool = False,
+    verbose=False,
+):
+    """Optimize Model by OnnxRuntime and/or python fusion logic.
 
-    ONNX Runtime has graph optimizations (https://onnxruntime.ai/docs/resources/graph-optimizations.html).
+    ONNX Runtime has graph optimizations (https://onnxruntime.ai/docs/performance/graph-optimizations.html).
     However, the coverage is limited. We also have graph fusions that implemented in Python to improve the coverage.
     They can combined: ONNX Runtime will run first when opt_level > 0, then graph fusions in Python will be applied.
 
     To use ONNX Runtime only and no Python fusion logic, use only_onnxruntime flag and a positive opt_level like
         optimize_model(input, opt_level=1, use_gpu=False, only_onnxruntime=True)
 
     When opt_level is None, we will choose default optimization level according to model type.
 
     When opt_level is 0 and only_onnxruntime is False, only python fusion logic is used and onnxruntime is disabled.
 
-    When opt_level > 1, use_gpu shall set properly since the optimized graph might contain operators for GPU or CPU only.
+    When opt_level > 1, use_gpu shall set properly
+    since the optimized graph might contain operators for GPU or CPU only.
+
     If your model is intended for GPU inference only (especially float16 or mixed precision model), it is recommended to
     set use_gpu to be True, otherwise the model is not optimized for GPU inference.
 
     For BERT model, num_heads and hidden_size are optional. For other model types, you need specify these parameters.
 
     Args:
         input (str): input model path.
         model_type (str, optional): model type - like bert, bert_tf, bert_keras or gpt2. Defaults to 'bert'.
         num_heads (int, optional): number of attention heads. Defaults to 0.
-                                   0 allows detect the parameter from graph automatically (for model_type "bert" only).
+            0 allows detect the parameter from graph automatically.
         hidden_size (int, optional): hidden size. Defaults to 0.
-                                     0 allows detect the parameter from graph automatically (for model_type "bert" only).
-        optimization_options (FusionOptions, optional): optimization options that turn on/off some fusions. Defaults to None.
+            0 allows detect the parameter from graph automatically.
+        optimization_options (FusionOptions, optional): optimization options that turn on/off some fusions.
+            Defaults to None.
         opt_level (int, optional): onnxruntime graph optimization level (0, 1, 2 or 99) or None. Defaults to None.
-                                   When the value is None, default value (1 for bert and gpt2, 0 for other model types) will be used.
-                                   When the level > 0, onnxruntime will be used to optimize model first.
+            When the value is None, default value (1 for bert and gpt2, 0 for other model types) will be used.
+            When the level > 0, onnxruntime will be used to optimize model first.
         use_gpu (bool, optional): use gpu or not for onnxruntime. Defaults to False.
-        only_onnxruntime (bool, optional): only use onnxruntime to optimize model, and no python fusion. Defaults to False.
+        only_onnxruntime (bool, optional): only use onnxruntime to optimize model, and no python fusion.
+            Defaults to False.
 
      Returns:
         object of an optimizer class.
     """
     assert opt_level is None or opt_level in [0, 1, 2, 99]
 
-    if model_type != "bert" and (num_heads == 0 or hidden_size == 0):
-        logger.warning("Please specify parameters of num_heads and hidden_size when model_type is not 'bert'")
-
-    (optimizer_class, producer, default_opt_level) = MODEL_TYPES[model_type]
+    (optimizer_class, _producer, default_opt_level) = MODEL_TYPES[model_type]
 
     if opt_level is None:
         opt_level = default_opt_level
 
+    # Disable constant sharing to avoid model proto str mismatch in test. Ideally the optimizer should not
+    # affect other fusions. We can update the expected model proto once the ConstantSharing optimizer logic becomes
+    # stable.
+    disabled_optimizers = ["ConstantSharing"]
     temp_model_path = None
     if opt_level > 1:
         # Disable some optimizers that might cause failure in symbolic shape inference or attention fusion.
-        disabled_optimizers = [] if only_onnxruntime else [
-            'MatMulScaleFusion', 'MatMulAddFusion'
-            'SimplifiedLayerNormFusion', 'GemmActivationFusion', 'BiasSoftmaxFusion'
-        ]
-        temp_model_path = optimize_by_onnxruntime(input,
-                                                  use_gpu=use_gpu,
-                                                  opt_level=opt_level,
-                                                  disabled_optimizers=disabled_optimizers)
+        disabled_optimizers += (
+            []
+            if only_onnxruntime
+            else [
+                "MatMulScaleFusion",
+                "MatMulAddFusion",
+                "MatmulTransposeFusion",
+                "GemmActivationFusion",
+                "BiasSoftmaxFusion",
+            ]
+        )
+        temp_model_path = optimize_by_onnxruntime(
+            input,
+            use_gpu=use_gpu,
+            opt_level=opt_level,
+            disabled_optimizers=disabled_optimizers,
+            verbose=verbose,
+        )
     elif opt_level == 1:
-        # basic optimizations (like constant folding and cast elimation) are not specified to exection provider.
+        # basic optimizations (like constant folding and cast elimination) are not specified to execution provider.
         # CPU provider is used here so that there is no extra node for GPU memory copy.
-        temp_model_path = optimize_by_onnxruntime(input, use_gpu=False, opt_level=1)
+        temp_model_path = optimize_by_onnxruntime(
+            input,
+            use_gpu=False,
+            opt_level=1,
+            disabled_optimizers=disabled_optimizers,
+            verbose=verbose,
+        )
 
     if only_onnxruntime and not temp_model_path:
         logger.warning("Please specify a positive value for opt_level when only_onnxruntime is True")
 
     model = load_model(temp_model_path or input)
 
     if only_onnxruntime:
         optimizer = optimizer_class(model, num_heads, hidden_size)
     else:
         optimizer = optimize_by_fusion(model, model_type, num_heads, hidden_size, optimization_options)
 
     # Remove the temporary model.
     if temp_model_path:
         os.remove(temp_model_path)
-        logger.debug("Remove tempoary model: {}".format(temp_model_path))
+        logger.debug(f"Remove temporary model: {temp_model_path}")
 
     return optimizer
 
 
 def get_fusion_statistics(optimized_model_path: str) -> Dict[str, int]:
     """
     Get counter of fused operators in optimized model.
@@ -252,140 +314,178 @@
     model = load_model(optimized_model_path, format=None, load_external_data=True)
     optimizer = BertOnnxModel(model)
     return optimizer.get_fused_operator_statistics()
 
 
 def _parse_arguments():
     parser = argparse.ArgumentParser(
-        description=
-        'Graph optimization tool for ONNX Runtime. It transforms ONNX graph to use optimized operators for Transformer models.'
+        description="Graph optimization tool for ONNX Runtime."
+        "It transforms ONNX graph to use optimized operators for Transformer models."
     )
-    parser.add_argument('--input', required=True, type=str, help="input onnx model path")
+    parser.add_argument("--input", required=True, type=str, help="input onnx model path")
 
-    parser.add_argument('--output', required=True, type=str, help="optimized onnx model path")
+    parser.add_argument("--output", required=True, type=str, help="optimized onnx model path")
 
-    parser.add_argument('--model_type',
-                        required=False,
-                        type=str.lower,
-                        default="bert",
-                        choices=list(MODEL_TYPES.keys()),
-                        help="Model type selected in the list: " + ", ".join(MODEL_TYPES.keys()))
+    parser.add_argument(
+        "--model_type",
+        required=False,
+        type=str.lower,
+        default="bert",
+        choices=list(MODEL_TYPES.keys()),
+        help="Model type selected in the list: " + ", ".join(MODEL_TYPES.keys()),
+    )
 
     parser.add_argument(
-        '--num_heads',
+        "--num_heads",
         required=False,
         type=int,
         default=0,
-        help=
-        "number of attention heads like 12 for bert-base and 16 for bert-large. Default is 0 to detect automatically for BERT. For other model type, this parameter need specify correctly."
+        help="number of attention heads like 12 for bert-base and 16 for bert-large. "
+        "Default is 0 to detect automatically for BERT."
+        "For other model type, this parameter need specify correctly.",
     )
 
     parser.add_argument(
-        '--hidden_size',
+        "--hidden_size",
         required=False,
         type=int,
         default=0,
-        help=
-        "hidden size like 768 for bert-base and 1024 for bert-large. Default is 0 to detect automatically for BERT. For other model type, this parameter need specify correctly."
+        help="hidden size like 768 for bert-base and 1024 for bert-large. "
+        "Default is 0 to detect automatically for BERT. "
+        "For other model type, this parameter need specify correctly.",
     )
 
     parser.add_argument(
-        '--input_int32',
+        "--input_int32",
         required=False,
-        action='store_true',
-        help=
-        "Use int32 (instead of int64) inputs. It could avoid unnecessary data cast when EmbedLayerNormalization is fused for BERT."
+        action="store_true",
+        help="Use int32 (instead of int64) inputs. "
+        "It could avoid unnecessary data cast when EmbedLayerNormalization is fused for BERT.",
     )
     parser.set_defaults(input_int32=False)
 
     parser.add_argument(
-        '--float16',
+        "--float16",
         required=False,
-        action='store_true',
-        help=
-        "Convert all weights and nodes in float32 to float16. It has potential loss in precision compared to mixed precision conversion (see convert_float_to_float16)."
+        action="store_true",
+        help="Convert all weights and nodes in float32 to float16. "
+        "It has potential loss in precision compared to mixed precision conversion.",
     )
     parser.set_defaults(float16=False)
 
     FusionOptions.add_arguments(parser)
 
-    parser.add_argument('--verbose', required=False, action='store_true', help="show debug information.")
+    parser.add_argument("--verbose", required=False, action="store_true", help="show debug information.")
     parser.set_defaults(verbose=False)
 
     parser.add_argument(
-        '--use_gpu',
+        "--use_gpu",
         required=False,
-        action='store_true',
-        help="Use GPU for inference. Set this flag if your model is intended for GPU when opt_level > 1.")
+        action="store_true",
+        help="Use GPU for inference. Set this flag if your model is intended for GPU when opt_level > 1.",
+    )
     parser.set_defaults(use_gpu=False)
 
-    parser.add_argument('--only_onnxruntime',
-                        required=False,
-                        action='store_true',
-                        help="optimized by onnxruntime only, and no graph fusion in Python")
+    parser.add_argument(
+        "--only_onnxruntime",
+        required=False,
+        action="store_true",
+        help="optimized by onnxruntime only, and no graph fusion in Python",
+    )
     parser.set_defaults(only_onnxruntime=False)
 
     parser.add_argument(
-        '--opt_level',
+        "--opt_level",
         required=False,
         type=int,
         choices=[0, 1, 2, 99],
         default=None,
-        help=
-        "onnxruntime optimization level. 0 will disable onnxruntime graph optimization. The recommended value is 1. When opt_level > 1 is used, optimized model for GPU might not run in CPU. Level 2 and 99 are intended for --only_onnxruntime."
+        help="onnxruntime optimization level. 0 will disable onnxruntime graph optimization. "
+        "The recommended value is 1. When opt_level > 1 is used, optimized model for GPU might not run in CPU. "
+        "Level 2 and 99 are intended for --only_onnxruntime.",
     )
 
-    parser.add_argument('--use_external_data_format',
-                        required=False,
-                        action='store_true',
-                        help="use external data format to store large model (>2GB)")
+    parser.add_argument(
+        "--use_external_data_format",
+        required=False,
+        action="store_true",
+        help="use external data format to store large model (>2GB)",
+    )
     parser.set_defaults(use_external_data_format=False)
 
+    parser.add_argument(
+        "--disable_symbolic_shape_infer",
+        required=False,
+        action="store_true",
+        help="diable symoblic shape inference",
+    )
+    parser.set_defaults(disable_symbolic_shape_infer=False)
+
+    parser.add_argument(
+        "--convert_to_packing_mode",
+        required=False,
+        action="store_true",
+        help="convert the model to packing mode. Only available for BERT like model",
+    )
+    parser.set_defaults(convert_to_packing_mode=False)
+
     args = parser.parse_args()
 
     return args
 
 
 def _setup_logger(verbose):
     if verbose:
-        coloredlogs.install(level='DEBUG', fmt='[%(filename)s:%(lineno)s - %(funcName)20s()] %(message)s')
+        coloredlogs.install(
+            level="DEBUG",
+            fmt="[%(filename)s:%(lineno)s - %(funcName)20s()] %(message)s",
+        )
     else:
-        coloredlogs.install(fmt='%(funcName)20s: %(message)s')
+        coloredlogs.install(fmt="%(funcName)20s: %(message)s")
 
 
 def main():
     args = _parse_arguments()
 
     _setup_logger(args.verbose)
 
     logger.debug(f"arguments:{args}")
 
     if os.path.realpath(args.input) == os.path.realpath(args.output):
-        logger.warning(f"Specified the same input and output path. Note that this may overwrite the original model")
+        logger.warning("Specified the same input and output path. Note that this may overwrite the original model")
 
     optimization_options = FusionOptions.parse(args)
 
-    optimizer = optimize_model(args.input,
-                               args.model_type,
-                               args.num_heads,
-                               args.hidden_size,
-                               opt_level=args.opt_level,
-                               optimization_options=optimization_options,
-                               use_gpu=args.use_gpu,
-                               only_onnxruntime=args.only_onnxruntime)
+    optimizer = optimize_model(
+        args.input,
+        args.model_type,
+        args.num_heads,
+        args.hidden_size,
+        opt_level=args.opt_level,
+        optimization_options=optimization_options,
+        use_gpu=args.use_gpu,
+        only_onnxruntime=args.only_onnxruntime,
+    )
 
     if args.float16:
         optimizer.convert_float_to_float16(keep_io_types=True)
 
     if args.input_int32:
         optimizer.change_graph_inputs_to_int32()
 
-    optimizer.save_model_to_file(args.output, args.use_external_data_format)
+    if args.model_type in ["bert", "gpt2"]:
+        if optimizer.is_fully_optimized():
+            logger.info("The model has been fully optimized.")
+        else:
+            logger.info("The model has been optimized.")
+
+    if args.convert_to_packing_mode:
+        if args.model_type == "bert":
+            optimizer.convert_to_packing_mode(not args.disable_symbolic_shape_infer)
+        else:
+            logger.warning("Packing mode only supports BERT like models")
 
-    if optimizer.is_fully_optimized():
-        logger.info("The model has been fully optimized.")
-    else:
-        logger.info("The model has been optimized.")
+    optimizer.save_model_to_file(args.output, args.use_external_data_format)
 
 
 if __name__ == "__main__":
     main()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/parity_check_helper.py` & `onnxruntime/transformers/models/gpt2/parity_check_helper.py`

 * *Files 18% similar despite different names*

```diff
@@ -4,21 +4,26 @@
 # license information.
 # --------------------------------------------------------------------------
 # This script helps debugging parity issue for two same onnx models with fp16 and fp32 format
 # Please build ORT with --cmake_extra_defines onnxruntime_DEBUG_NODE_INPUTS_OUTPUTS=ON
 
 import math
 import multiprocessing
-import numpy
 import os
-import torch
+import sys
 from pathlib import Path
-from onnx import numpy_helper, TensorProto
+
+import numpy
+import torch
 from gpt2_helper import Gpt2Helper
-from benchmark_helper import create_onnxruntime_session
+from onnx import TensorProto, numpy_helper
+
+sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
+
+from benchmark_helper import create_onnxruntime_session  # noqa: E402
 
 NON_ZERO_VALUE = str(1)
 ZERO_VALUE = str(0)
 
 
 def environ_setting_nodes(node_name_filter=None, node_type_filter=None):
     # Set I/O data as default
@@ -37,18 +42,22 @@
     # Set dumping values to files as default
     os.environ["ORT_DEBUG_NODE_IO_DUMP_DATA_DESTINATION"] = "files"
     os.environ["ORT_DEBUG_NODE_IO_OUTPUT_DIR"] = output_path
 
 
 def environ_reset():
     for flag in [
-            "ORT_DEBUG_NODE_IO_DUMP_SHAPE_DATA", "ORT_DEBUG_NODE_IO_DUMP_INPUT_DATA",
-            "ORT_DEBUG_NODE_IO_DUMP_OUTPUT_DATA", "ORT_DEBUG_NODE_IO_NAME_FILTER", "ORT_DEBUG_NODE_IO_OP_TYPE_FILTER",
-            "ORT_DEBUG_NODE_IO_DUMP_DATA_TO_FILES", "ORT_DEBUG_NODE_IO_OUTPUT_DIR",
-            "ORT_DEBUG_NODE_IO_DUMPING_DATA_TO_FILES_FOR_ALL_NODES_IS_OK"
+        "ORT_DEBUG_NODE_IO_DUMP_SHAPE_DATA",
+        "ORT_DEBUG_NODE_IO_DUMP_INPUT_DATA",
+        "ORT_DEBUG_NODE_IO_DUMP_OUTPUT_DATA",
+        "ORT_DEBUG_NODE_IO_NAME_FILTER",
+        "ORT_DEBUG_NODE_IO_OP_TYPE_FILTER",
+        "ORT_DEBUG_NODE_IO_DUMP_DATA_TO_FILES",
+        "ORT_DEBUG_NODE_IO_OUTPUT_DIR",
+        "ORT_DEBUG_NODE_IO_DUMPING_DATA_TO_FILES_FOR_ALL_NODES_IS_OK",
     ]:
         if flag in os.environ:
             del os.environ[flag]
 
 
 def inference(model_path, dummy_inputs, outputs_path, use_gpu):
     environ_reset()
@@ -58,78 +67,82 @@
     Gpt2Helper.onnxruntime_inference(session, dummy_inputs)
 
 
 def generate_outputs_files(model_path, dummy_inputs, outputs_path, use_gpu):
     dir_path = Path(outputs_path)
     if dir_path.exists() and dir_path.is_dir():
         import shutil
+
         shutil.rmtree(outputs_path)
     dir_path.mkdir(parents=True, exist_ok=True)
 
     process = multiprocessing.Process(target=inference, args=(model_path, dummy_inputs, outputs_path, use_gpu))
     process.start()
     process.join()
 
 
 def post_processing(outputs_path, outputs_path_other):
     # Compare outputs with e.g. fp16 and fp32
     record = {}
     if_close = {}
 
     import glob
-    for filename in glob.glob(os.path.join(outputs_path, '*.tensorproto')):
+
+    for filename in glob.glob(os.path.join(outputs_path, "*.tensorproto")):
         filename_other = os.path.join(outputs_path_other, Path(filename).name)
         if not os.path.exists(filename_other):
             continue
-        with open(filename, 'rb') as f:
+        with open(filename, "rb") as f:
             tensor = TensorProto()
             tensor.ParseFromString(f.read())
             array = numpy_helper.to_array(tensor)
-            with open(filename_other, 'rb') as f:
+            with open(filename_other, "rb") as f:  # noqa: PLW2901
                 tensor_other = TensorProto()
                 tensor_other.ParseFromString(f.read())
                 array_other = numpy_helper.to_array(tensor_other)
                 if array_other.size == 0:
                     continue
                 diff = numpy.average(numpy.abs(array_other - array) / (numpy.abs(array_other) + 1e-6))
                 if math.isnan(diff):
                     continue
                 record[Path(filename).name.split(".")[0]] = diff
                 if_close[Path(filename).name.split(".")[0]] = numpy.allclose(array, array_other, rtol=1e-04, atol=1e-04)
 
-    results = [f"Node\tDiff\tClose"]
+    results = ["Node\tDiff\tClose"]
     for k, v in sorted(record.items(), key=lambda x: x[1], reverse=True):
         results.append(f"{k}\t{v}\t{if_close[k]}")
     for line in results:
         print(line)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     # Below example shows how to use this helper to investigate parity issue of gpt-2 fp32 and fp16 onnx model
     # Please build ORT with --cmake_extra_defines onnxruntime_DEBUG_NODE_INPUTS_OUTPUTS=ON !!
-    multiprocessing.set_start_method('spawn')
+    multiprocessing.set_start_method("spawn")
 
     # Generate Inputs
     sequence_length = 8
     past_sequence_length = 8
     batch_size = 5
-    dummy_inputs_fp16 = Gpt2Helper.get_dummy_inputs(batch_size,
-                                                    past_sequence_length,
-                                                    sequence_length,
-                                                    12,
-                                                    768,
-                                                    12,
-                                                    50257,
-                                                    device=torch.device("cpu"),
-                                                    float16=True)
+    dummy_inputs_fp16 = Gpt2Helper.get_dummy_inputs(
+        batch_size,
+        past_sequence_length,
+        sequence_length,
+        12,
+        768,
+        12,
+        50257,
+        device=torch.device("cpu"),
+        float16=True,
+    )
     dummy_inputs_fp32 = dummy_inputs_fp16.to_fp32()
 
     # Get GPT-2 model from huggingface using convert_to_onnx.py
-    os.system('python convert_to_onnx.py -m gpt2 --output gpt2_fp32.onnx -o -p fp32 --use_gpu')
-    os.system('python convert_to_onnx.py -m gpt2 --output gpt2_fp16.onnx -o -p fp16 --use_gpu')
+    os.system("python convert_to_onnx.py -m gpt2 --output gpt2_fp32.onnx -o -p fp32 --use_gpu")
+    os.system("python convert_to_onnx.py -m gpt2 --output gpt2_fp16.onnx -o -p fp16 --use_gpu")
 
     # Specify the directory to dump the node's I/O
     outputs_path_fp32_gpu = "./fp32_gpu"
     outputs_path_fp16_gpu = "./fp16_gpu"
     generate_outputs_files("./gpt2_fp32.onnx", dummy_inputs_fp32, outputs_path_fp32_gpu, use_gpu=True)
     generate_outputs_files("./gpt2_fp16.onnx", dummy_inputs_fp16, outputs_path_fp16_gpu, use_gpu=True)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/profiler.py` & `onnxruntime/transformers/profiler.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,141 +1,189 @@
-import os
 import argparse
 import json
-import psutil
+import os
+
 import numpy
+import psutil
 from onnx import TensorProto
+
 """
 This profiler tool could run a transformer model and print out the kernel time spent on each Node of the model.
 Example of profiling of longformer model:
     python profiler.py --model longformer-base-4096_fp32.onnx --batch_size 1 --sequence_length 4096 --global_length 8 --samples 1000 --thread_num 8 --dummy_inputs longformer --use_gpu
 Example of importing profile result file from onnxruntime_perf_test:
     python profiler.py --input profile_2021-10-25_12-02-41.json
 """
 
-NODES_TYPE_CONTAINING_SUBGRAPH = ['Scan', 'Loop', 'If']
+NODES_TYPE_CONTAINING_SUBGRAPH = ["Scan", "Loop", "If"]
 
 
 def parse_arguments(argv=None):
     parser = argparse.ArgumentParser()
 
-    parser.add_argument('-i', '--input', required=False,
-                        type=str,
-                        help="Set the input file for reading the profile results")
-
-    parser.add_argument('-m', '--model', required=False, type=str, help="onnx model path to run profiling. Required when --input is not specified.")
-
-    parser.add_argument('-b', '--batch_size', required=False, type=int, default=1, help="batch size of input")
-
-    parser.add_argument('-s',
-                        '--sequence_length',
-                        required=False,
-                        type=int,
-                        default=32,
-                        help="sequence length of input")
-
-    parser.add_argument('--past_sequence_length',
-                        required=False,
-                        type=int,
-                        default=1,
-                        help="past sequence length for gpt2")
-
-    parser.add_argument('--global_length',
-                        required=False,
-                        type=int,
-                        default=1,
-                        help="number of global tokens for longformer")
+    parser.add_argument(
+        "-i",
+        "--input",
+        required=False,
+        type=str,
+        help="Set the input file for reading the profile results",
+    )
+
+    parser.add_argument(
+        "-m",
+        "--model",
+        required=False,
+        type=str,
+        help="onnx model path to run profiling. Required when --input is not specified.",
+    )
+
+    parser.add_argument(
+        "-b",
+        "--batch_size",
+        required=False,
+        type=int,
+        default=1,
+        help="batch size of input",
+    )
+
+    parser.add_argument(
+        "-s",
+        "--sequence_length",
+        required=False,
+        type=int,
+        default=32,
+        help="sequence length of input",
+    )
+
+    parser.add_argument(
+        "--past_sequence_length",
+        required=False,
+        type=int,
+        default=1,
+        help="past sequence length for gpt2",
+    )
 
     parser.add_argument(
-        '--samples',
+        "--global_length",
+        required=False,
+        type=int,
+        default=1,
+        help="number of global tokens for longformer",
+    )
+
+    parser.add_argument(
+        "--samples",
         required=False,
         type=int,
         default=1000,
-        help="number of samples to test. Set it large enough to reduce the variance of performance result.")
+        help="number of samples to test. Set it large enough to reduce the variance of performance result.",
+    )
 
     parser.add_argument(
-        '--threshold',
+        "--threshold",
         required=False,
         type=float,
         default=0.01,
-        help="Threshold of run time ratio among all nodes. Nodes with larger ratio will show in top expensive nodes.")
+        help="Threshold of run time ratio among all nodes. Nodes with larger ratio will show in top expensive nodes.",
+    )
 
-    parser.add_argument("--thread_num", required=False, type=int, default=-1, help="number of threads to use")
+    parser.add_argument(
+        "--thread_num",
+        required=False,
+        type=int,
+        default=-1,
+        help="number of threads to use",
+    )
 
-    parser.add_argument('--input_ids_name',
-                        required=False,
-                        type=str,
-                        default=None,
-                        help="input name for input IDs, for bert")
-    parser.add_argument('--segment_ids_name',
-                        required=False,
-                        type=str,
-                        default=None,
-                        help="input name for segment IDs, for bert")
-    parser.add_argument('--input_mask_name',
-                        required=False,
-                        type=str,
-                        default=None,
-                        help="input name for attention mask, for bert")
-
-    parser.add_argument('--dummy_inputs',
-                        required=False,
-                        default='default',
-                        choices=['bert', 'gpt2', 'longformer', 'default'],
-                        help="Type of model inputs. The default will create dummy inputs with ones.")
+    parser.add_argument(
+        "--input_ids_name",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for input IDs, for bert",
+    )
+    parser.add_argument(
+        "--segment_ids_name",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for segment IDs, for bert",
+    )
+    parser.add_argument(
+        "--input_mask_name",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for attention mask, for bert",
+    )
 
-    parser.add_argument('-g', '--use_gpu', required=False, action='store_true', help="use GPU")
+    parser.add_argument(
+        "--dummy_inputs",
+        required=False,
+        default="default",
+        choices=["bert", "gpt2", "longformer", "default"],
+        help="Type of model inputs. The default will create dummy inputs with ones.",
+    )
+
+    parser.add_argument("-g", "--use_gpu", required=False, action="store_true", help="use GPU")
     parser.set_defaults(use_gpu=False)
 
-    parser.add_argument('--provider',
-                        required=False,
-                        type=str,
-                        default='cuda',
-                        help="Execution provider to use")
+    parser.add_argument(
+        "--provider",
+        required=False,
+        type=str,
+        default="cuda",
+        help="Execution provider to use",
+    )
 
     parser.add_argument(
-        '--basic_optimization',
+        "--basic_optimization",
         required=False,
-        action='store_true',
-        help="Enable only basic graph optimizations. By default, all optimizations are enabled in OnnxRuntime")
+        action="store_true",
+        help="Enable only basic graph optimizations. By default, all optimizations are enabled in OnnxRuntime",
+    )
     parser.set_defaults(basic_optimization=False)
 
-    parser.add_argument('--kernel_time_only',
-                        required=False,
-                        action='store_true',
-                        help="Only include the kernel time and no fence time")
+    parser.add_argument(
+        "--kernel_time_only",
+        required=False,
+        action="store_true",
+        help="Only include the kernel time and no fence time",
+    )
     parser.set_defaults(kernel_time_only=False)
 
-    parser.add_argument('-v', '--verbose', required=False, action='store_true')
+    parser.add_argument("-v", "--verbose", required=False, action="store_true")
     parser.set_defaults(verbose=False)
 
     return parser.parse_args(argv)
 
 
 def run_profile(onnx_model_path, use_gpu, provider, basic_optimization, thread_num, all_inputs):
     from benchmark_helper import create_onnxruntime_session
 
-    session = create_onnxruntime_session(onnx_model_path,
-                                         use_gpu,
-                                         provider,
-                                         enable_all_optimization=not basic_optimization,
-                                         num_threads=thread_num,
-                                         enable_profiling=True)
+    session = create_onnxruntime_session(
+        onnx_model_path,
+        use_gpu,
+        provider,
+        enable_all_optimization=not basic_optimization,
+        num_threads=thread_num,
+        enable_profiling=True,
+    )
 
     for inputs in all_inputs:
         _ = session.run(None, inputs)
 
     profile_file = session.end_profiling()
     return profile_file
 
 
 def load_profile_json(profile_file):
     print(f"loading profile output {profile_file} ...")
 
-    with open(profile_file, "r") as opened_file:
+    with open(profile_file) as opened_file:
         sess_time = json.load(opened_file)
 
     assert isinstance(sess_time, list)
     return sess_time
 
 
 def parse_kernel_results(sess_time, threshold=0):
@@ -195,25 +243,24 @@
         if ratio < threshold:
             continue
 
         calls = kernel_freq[kernel_name]
         avg_time = duration / float(calls)
         lines.append(f"{duration:10d}\t{ratio * 100.0:5.2f}\t{calls:5d}\t{avg_time:8.1f}\t{kernel_name}")
 
-
     # Group by operator
     op_time = {}
     for kernel_name, op_name in kernel_name_to_op_name.items():
         duration = kernel_time[kernel_name]
         if op_name in op_time:
             op_time[op_name] += duration
         else:
             op_time[op_name] = duration
 
-    lines.append(f"\nGroup kernel time by operator:")
+    lines.append("\nGroup kernel time by operator:")
     lines.append("-" * 64)
     lines.append("Total(μs)\tTime%\tOperator")
     for op_name, duration in sorted(op_time.items(), key=lambda x: x[1], reverse=True):
         ratio = duration / total
         lines.append(f"{duration:10d}\t{ratio * 100.0:5.2f}\t{op_name}")
 
     return lines
@@ -233,16 +280,17 @@
     node_name_list = []
     node_time = {}
     node_freq = {}
     node_provider = {}
     total = 0
     for item in sess_time:
         if item["cat"] == "Node" and "dur" in item and "args" in item and "op_name" in item["args"]:
-            node_name = item["name"].replace("_kernel_time", "").replace("_fence_before",
-                                                                         "").replace("_fence_after", "")
+            node_name = (
+                item["name"].replace("_kernel_time", "").replace("_fence_before", "").replace("_fence_after", "")
+            )
 
             if "provider" in item["args"]:
                 if item["args"]["provider"] == "CPUExecutionProvider":
                     device = "CPU"
                 elif item["args"]["provider"] == "CUDAExecutionProvider":
                     device = "CUDA"
                 elif item["args"]["provider"] == "DmlExecutionProvider":
@@ -267,29 +315,29 @@
                 node_freq[node_name] = 1
                 node_name_list.append(node_name)
 
             total += item["dur"]
 
     # Output items in the original order.
     lines = [
-        "\nNodes in the original order:", "-" * 64,
-        "Total(μs)\tTime%\tAcc %\tAvg(μs)\tCalls\tProvider\tNode"
+        "\nNodes in the original order:",
+        "-" * 64,
+        "Total(μs)\tTime%\tAcc %\tAvg(μs)\tCalls\tProvider\tNode",
     ]
     before_percentage = 0.0
     for node_name in node_name_list:
         duration = node_time[node_name]
         calls = node_freq[node_name]
         avg_time = duration / float(calls)
         percentage = (duration / total) * 100.0
         provider = node_provider[node_name] if node_name in node_provider else ""
         before_percentage += percentage
         lines.append(
             f"{duration:10d}\t{percentage:5.2f}\t{before_percentage:5.2f}\t{avg_time:8.1f}\t{calls:5d}\t{provider:8s}\t{node_name}"
         )
-        
 
     # Output items with run time ratio > thresholds, and sorted by duration in the descending order.
     lines.append(f"\nTop expensive nodes with Time% >= {threshold*100:.2f}:")
     lines.append("-" * 64)
     lines.append("Total(μs)\tTime%\tAvg(μs)\tCalls\tProvider\tNode")
     for node_name, duration in sorted(node_time.items(), key=lambda x: x[1], reverse=True):
         ratio = duration / total
@@ -379,34 +427,38 @@
     for op_name, kernel_time in sorted(op_kernel_time.items(), key=lambda x: x[1], reverse=True):
         fence_time = op_fence_time[op_name] if op_name in op_fence_time else 0
         kernel_time_ratio = kernel_time / total_kernel_time
         total_time = kernel_time + fence_time
         time_ratio = total_time / (total_kernel_time + total_fence_time)
         kernel_calls = op_kernel_records[op_name]
         avg_kernel_time = kernel_time / kernel_calls
-        lines.append(f"{total_time:10d}\t{time_ratio * 100.0:5.2f}\t{kernel_time:11d}\t{kernel_time_ratio * 100.0:5.2f}\t{kernel_calls:5d}\t{avg_kernel_time:14.1f}\t{fence_time:10d}\t{op_name}")
+        lines.append(
+            f"{total_time:10d}\t{time_ratio * 100.0:5.2f}\t{kernel_time:11d}\t{kernel_time_ratio * 100.0:5.2f}\t{kernel_calls:5d}\t{avg_kernel_time:14.1f}\t{fence_time:10d}\t{op_name}"
+        )
 
     lines += ["", "Grouped by provider + operator"]
     lines.append("-" * 64)
     lines.append("Kernel(μs)\tProvider%\tCalls\tAvgKernel(μs)\tProvider\tOperator")
     for key, kernel_time in sorted(provider_op_kernel_time.items(), key=lambda x: x[1], reverse=True):
-        parts = key.split(':')
+        parts = key.split(":")
         provider = parts[0]
-        op_name = parts[1] 
-        short_ep = provider.replace("ExecutionProvider", "")     
+        op_name = parts[1]
+        short_ep = provider.replace("ExecutionProvider", "")
         calls = provider_op_kernel_records[key]
         avg_kernel_time = kernel_time / calls
         provider_time_ratio = kernel_time / provider_kernel_time[provider]
-        lines.append(f"{kernel_time:10d}\t{provider_time_ratio * 100.0:9.2f}\t{calls:5d}\t{avg_kernel_time:14.1f}\t{short_ep:8s}\t{op_name}")
+        lines.append(
+            f"{kernel_time:10d}\t{provider_time_ratio * 100.0:9.2f}\t{calls:5d}\t{avg_kernel_time:14.1f}\t{short_ep:8s}\t{op_name}"
+        )
 
     return lines
 
 
 def get_dim_from_type_proto(dim):
-    return getattr(dim, dim.WhichOneof('value')) if type(dim.WhichOneof('value')) == str else None
+    return getattr(dim, dim.WhichOneof("value")) if type(dim.WhichOneof("value")) == str else None
 
 
 def get_shape_from_type_proto(type_proto):
     return [get_dim_from_type_proto(d) for d in type_proto.tensor_type.shape.dim]
 
 
 def create_dummy_inputs(onnx_model, batch_size, sequence_length, samples):
@@ -435,30 +487,35 @@
         if len(symbol_dims) > 0:
             shape[symbol_dims[0]] = batch_size
         if len(symbol_dims) > 1:
             shape[symbol_dims[1]] = sequence_length
 
         elem_type = graph_input.type.tensor_type.elem_type
         assert elem_type in [TensorProto.FLOAT, TensorProto.INT32, TensorProto.INT64]
-        data_type = numpy.float32 if elem_type == TensorProto.FLOAT else (
-            numpy.int64 if elem_type == TensorProto.INT64 else numpy.int32)
+        data_type = (
+            numpy.float32
+            if elem_type == TensorProto.FLOAT
+            else (numpy.int64 if elem_type == TensorProto.INT64 else numpy.int32)
+        )
         data = numpy.ones(shape, dtype=data_type)
         dummy_inputs[graph_input.name] = data
 
     all_inputs = [dummy_inputs for _ in range(samples)]
     return all_inputs
 
 
-def create_bert_inputs(onnx_model,
-                       batch_size,
-                       sequence_length,
-                       samples,
-                       input_ids_name=None,
-                       segment_ids_name=None,
-                       input_mask_name=None):
+def create_bert_inputs(
+    onnx_model,
+    batch_size,
+    sequence_length,
+    samples,
+    input_ids_name=None,
+    segment_ids_name=None,
+    input_mask_name=None,
+):
     """Create dummy inputs for BERT model.
 
     Args:
         onnx_model (OnnxModel): ONNX model
         batch_size (int): batch size
         sequence_length (int): sequence length
         samples (int): number of samples
@@ -466,24 +523,27 @@
         segment_ids_name (str, optional): Name of graph input for segment IDs. Defaults to None.
         input_mask_name (str, optional): Name of graph input for attention mask. Defaults to None.
 
     Returns:
         List[Dict]: list of inputs
     """
     from bert_test_data import find_bert_inputs, generate_test_data
+
     input_ids, segment_ids, input_mask = find_bert_inputs(onnx_model, input_ids_name, segment_ids_name, input_mask_name)
-    all_inputs = generate_test_data(batch_size,
-                                    sequence_length,
-                                    test_cases=samples,
-                                    seed=123,
-                                    verbose=False,
-                                    input_ids=input_ids,
-                                    segment_ids=segment_ids,
-                                    input_mask=input_mask,
-                                    random_mask_length=False)
+    all_inputs = generate_test_data(
+        batch_size,
+        sequence_length,
+        test_cases=samples,
+        seed=123,
+        verbose=False,
+        input_ids=input_ids,
+        segment_ids=segment_ids,
+        input_mask=input_mask,
+        random_mask_length=False,
+    )
 
     return all_inputs
 
 
 def create_gpt2_inputs(onnx_model, batch_size, sequence_length, past_sequence_length, samples):
     """Create dummy inputs for GPT-2 model.
 
@@ -498,34 +558,37 @@
         RuntimeError: symbolic is not supported. Use the tool convert_to_onnx.py to export ONNX model instead.
 
     Returns:
         List[Dict]: list of inputs
     """
     # The symbolic names shall be same as those used in Gpt2Helper.export_onnx(...) function.
     symbols = {
-        'batch_size': batch_size,
-        'seq_len': sequence_length,
-        'past_seq_len': past_sequence_length,
-        'total_seq_len': sequence_length + past_sequence_length
+        "batch_size": batch_size,
+        "seq_len": sequence_length,
+        "past_seq_len": past_sequence_length,
+        "total_seq_len": sequence_length + past_sequence_length,
     }
 
     dummy_inputs = {}
     for graph_input in onnx_model.get_graph_inputs_excluding_initializers():
         shape = get_shape_from_type_proto(graph_input.type)
         for i, dim in enumerate(shape):
             if isinstance(dim, str):
                 if dim not in symbols.keys():
                     raise RuntimeError(f"symbol is not supported: {dim}")
                 else:
                     shape[i] = symbols[dim]
 
         elem_type = graph_input.type.tensor_type.elem_type
         assert elem_type in [TensorProto.FLOAT, TensorProto.INT32, TensorProto.INT64]
-        data_type = numpy.float32 if elem_type == TensorProto.FLOAT else (
-            numpy.int64 if elem_type == TensorProto.INT64 else numpy.int32)
+        data_type = (
+            numpy.float32
+            if elem_type == TensorProto.FLOAT
+            else (numpy.int64 if elem_type == TensorProto.INT64 else numpy.int32)
+        )
         data = numpy.ones(shape, dtype=data_type)
         dummy_inputs[graph_input.name] = data
 
     all_inputs = [dummy_inputs for _ in range(samples)]
     return all_inputs
 
 
@@ -541,86 +604,117 @@
 
     Raises:
         RuntimeError: symbolic is not supported. Use the tool convert_longformer_to_onnx.py to export ONNX model instead.
 
     Returns:
         List[Dict]: list of inputs
     """
-    symbols = {'batch_size': batch_size, 'sequence_length': sequence_length}
+    symbols = {"batch_size": batch_size, "sequence_length": sequence_length}
 
     dummy_inputs = {}
     for graph_input in onnx_model.get_graph_inputs_excluding_initializers():
         shape = get_shape_from_type_proto(graph_input.type)
         for i, dim in enumerate(shape):
             if isinstance(dim, str):
                 if dim not in symbols.keys():
                     raise RuntimeError(f"symbol is not supported: {dim}")
                 else:
                     shape[i] = symbols[dim]
 
         elem_type = graph_input.type.tensor_type.elem_type
         assert elem_type in [TensorProto.FLOAT, TensorProto.INT32, TensorProto.INT64]
-        data_type = numpy.float32 if elem_type == TensorProto.FLOAT else (
-            numpy.int64 if elem_type == TensorProto.INT64 else numpy.int32)
+        data_type = (
+            numpy.float32
+            if elem_type == TensorProto.FLOAT
+            else (numpy.int64 if elem_type == TensorProto.INT64 else numpy.int32)
+        )
 
         if "global" in graph_input.name:
             data = numpy.zeros(shape, dtype=data_type)
             data[:, :global_length] = 1
         else:
             data = numpy.ones(shape, dtype=data_type)
         dummy_inputs[graph_input.name] = data
 
     all_inputs = [dummy_inputs for _ in range(samples)]
     return all_inputs
 
+
 def process_results(profile_file, args):
     profile_records = load_profile_json(profile_file)
 
     lines = parse_kernel_results(profile_records, args.threshold)
 
     lines += parse_node_results(profile_records, args.kernel_time_only, args.threshold)
 
     lines += group_node_results(profile_records, args.kernel_time_only, args.use_gpu)
 
     return lines
 
+
 def run(args):
     num_threads = args.thread_num if args.thread_num > 0 else psutil.cpu_count(logical=False)
 
     # Set OMP environment variable before importing onnxruntime. Needed for cpu only, and no impact for onnxruntime-gpu package.
     if "OMP_NUM_THREADS" not in os.environ:
         os.environ["OMP_NUM_THREADS"] = str(num_threads)
 
     from onnx import load
     from onnx_model import OnnxModel
+
     onnx_model = OnnxModel(load(args.model))
 
     all_inputs = None
-    if args.dummy_inputs == 'bert':
-        all_inputs = create_bert_inputs(onnx_model, args.batch_size, args.sequence_length, args.samples,
-                                        args.input_ids_name, args.segment_ids_name, args.input_mask_name)
-    elif args.dummy_inputs == 'gpt2':
-        all_inputs = create_gpt2_inputs(onnx_model, args.batch_size, args.sequence_length, args.past_sequence_length,
-                                        args.samples)
-    elif args.dummy_inputs == 'longformer':
-        all_inputs = create_longformer_inputs(onnx_model, args.batch_size, args.sequence_length, args.global_length,
-                                              args.samples)
+    if args.dummy_inputs == "bert":
+        all_inputs = create_bert_inputs(
+            onnx_model,
+            args.batch_size,
+            args.sequence_length,
+            args.samples,
+            args.input_ids_name,
+            args.segment_ids_name,
+            args.input_mask_name,
+        )
+    elif args.dummy_inputs == "gpt2":
+        all_inputs = create_gpt2_inputs(
+            onnx_model,
+            args.batch_size,
+            args.sequence_length,
+            args.past_sequence_length,
+            args.samples,
+        )
+    elif args.dummy_inputs == "longformer":
+        all_inputs = create_longformer_inputs(
+            onnx_model,
+            args.batch_size,
+            args.sequence_length,
+            args.global_length,
+            args.samples,
+        )
     else:  # default
         all_inputs = create_dummy_inputs(onnx_model, args.batch_size, args.sequence_length, args.samples)
 
-    profile_file = run_profile(args.model, args.use_gpu, args.provider, args.basic_optimization, args.thread_num, all_inputs)
+    profile_file = run_profile(
+        args.model,
+        args.use_gpu,
+        args.provider,
+        args.basic_optimization,
+        args.thread_num,
+        all_inputs,
+    )
 
     return profile_file
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     arguments = parse_arguments()
     print("Arguments", arguments)
 
     from benchmark_helper import setup_logger
+
     setup_logger(arguments.verbose)
 
     if not arguments.input:
         assert arguments.model, "requires either --model to run profiling or --input to read profiling results"
         profile_file = run(arguments)
     else:
         profile_file = arguments.input
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/quantize_helper.py` & `onnxruntime/transformers/quantize_helper.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,79 +1,75 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
-import torch
-import onnx
 import os
+
+import onnx  # noqa: F401
+import torch
 from transformers.modeling_utils import Conv1D
 
 logger = logging.getLogger(__name__)
 
 
 def _conv1d_to_linear(module):
     in_size, out_size = module.weight.shape
     linear = torch.nn.Linear(in_size, out_size)
     linear.weight.data = module.weight.data.T.contiguous()
     linear.bias.data = module.bias.data
     return linear
 
 
 def conv1d_to_linear(model):
-    '''in-place
+    """in-place
     This is for Dynamic Quantization, as Conv1D is not recognized by PyTorch, convert it to nn.Linear
-    '''
+    """
     logger.debug("replace Conv1D with Linear")
     for name in list(model._modules):
         module = model._modules[name]
         if isinstance(module, Conv1D):
             linear = _conv1d_to_linear(module)
             model._modules[name] = linear
         else:
             conv1d_to_linear(module)
 
 
 def _get_size_of_pytorch_model(model):
     torch.save(model.state_dict(), "temp.p")
     size = os.path.getsize("temp.p") / (1024 * 1024)
-    os.remove('temp.p')
+    os.remove("temp.p")
     return size
 
 
 class QuantizeHelper:
     @staticmethod
     def quantize_torch_model(model, dtype=torch.qint8):
-        '''
+        """
         Usage: model = quantize_model(model)
 
         TODO: mix of in-place and return, but results are different
-        '''
+        """
         conv1d_to_linear(model)
         quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=dtype)
-        logger.info(f'Size of full precision Torch model(MB):{_get_size_of_pytorch_model(model)}')
-        logger.info(f'Size of quantized Torch model(MB):{_get_size_of_pytorch_model(quantized_model)}')
+        logger.info(f"Size of full precision Torch model(MB):{_get_size_of_pytorch_model(model)}")
+        logger.info(f"Size of quantized Torch model(MB):{_get_size_of_pytorch_model(quantized_model)}")
         return quantized_model
 
     @staticmethod
     def quantize_onnx_model(onnx_model_path, quantized_model_path, use_external_data_format=False):
-        from onnxruntime.quantization import quantize, QuantizationMode
-        logger.info(f'Size of full precision ONNX model(MB):{os.path.getsize(onnx_model_path)/(1024*1024)}')
-        onnx_opt_model = onnx.load_model(onnx_model_path)
-        quantized_onnx_model = quantize(onnx_opt_model,
-                                        quantization_mode=QuantizationMode.IntegerOps,
-                                        symmetric_weight=True,
-                                        force_fusions=True)
-
-        if use_external_data_format:
-            from pathlib import Path
-            Path(quantized_model_path).parent.mkdir(parents=True, exist_ok=True)
-            onnx.external_data_helper.convert_model_to_external_data(quantized_onnx_model,
-                                                                     all_tensors_to_one_file=True,
-                                                                     location=Path(quantized_model_path).name + ".data")
-        onnx.save_model(quantized_onnx_model, quantized_model_path)
+        from pathlib import Path
+
+        from onnxruntime.quantization import quantize_dynamic
 
+        Path(quantized_model_path).parent.mkdir(parents=True, exist_ok=True)
+        logger.info(f"Size of full precision ONNX model(MB):{os.path.getsize(onnx_model_path)/(1024*1024)}")
+        quantize_dynamic(
+            onnx_model_path,
+            quantized_model_path,
+            use_external_data_format=use_external_data_format,
+        )
         logger.info(f"quantized model saved to:{quantized_model_path}")
-        #TODO: inlcude external data in total model size.
-        logger.info(f'Size of quantized ONNX model(MB):{os.path.getsize(quantized_model_path)/(1024*1024)}')
+        # TODO: inlcude external data in total model size.
+        logger.info(f"Size of quantized ONNX model(MB):{os.path.getsize(quantized_model_path)/(1024*1024)}")
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/shape_optimizer.py` & `onnxruntime/transformers/shape_optimizer.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,112 +1,116 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 # This tool is not used directly in bert optimization. It could assist developing the optimization script on the following senarios:
 # (1) It could simplify graph by removing many sub-graphs related to reshape.
 # (2) It could reduce extra inputs and outputs to fit other tools. The script compare_bert_results.py or bert_perf_test.py requires 3 inputs.
 
-import sys
 import argparse
-import numpy as np
-from collections import deque
-from typing import List
-import onnx
-import re
-import tempfile
-import os
 import logging
+import os
+import re  # noqa: F401
+import sys
+import tempfile
+from collections import deque  # noqa: F401
 from datetime import datetime
-from pathlib import Path
+from pathlib import Path  # noqa: F401
+from typing import List
+
+import numpy as np
+import onnx
 from onnx import ModelProto, TensorProto, numpy_helper
-import onnxruntime
 from onnx_model import OnnxModel
 
+import onnxruntime
+
 logger = logging.getLogger(__name__)
 
-CONSTANT_SHAPE_NAME_PREFIX = 'constant_shape_opt__'
-RESHAPE_INPUT_SHAPE_PREFIX = 'reshape_input_shape__'
+CONSTANT_SHAPE_NAME_PREFIX = "constant_shape_opt__"
+RESHAPE_INPUT_SHAPE_PREFIX = "reshape_input_shape__"
 
 
 class BertOnnxModelShapeOptimizer(OnnxModel):
     """
     This optimizer will replace Shape output or the shape input of Reshape node by initializer. Currently, it requires
     model inputs to have static shape.
     """
+
     def __init__(self, onnx_model):
         super().__init__(onnx_model.model)
 
     def add_shape_initializer(self, shape):
         """
         Add an initializer for constant shape.
         """
         shape_value = np.asarray(shape, dtype=np.int64)
-        constant_shape_name = self.create_node_name('Constant', CONSTANT_SHAPE_NAME_PREFIX)
-        tensor = onnx.helper.make_tensor(name=constant_shape_name,
-                                         data_type=TensorProto.INT64,
-                                         dims=shape_value.shape,
-                                         vals=shape_value)
+        constant_shape_name = self.create_node_name("Constant", CONSTANT_SHAPE_NAME_PREFIX)
+        tensor = onnx.helper.make_tensor(
+            name=constant_shape_name,
+            data_type=TensorProto.INT64,
+            dims=shape_value.shape,
+            vals=shape_value,
+        )
         self.add_initializer(tensor)
         return tensor
 
     def get_shape_outputs(self):
         """
         Returns a list of output names of all Shape nodes.
         """
         input_name_to_nodes = self.input_name_to_nodes()
 
         outputs = []
         for node in self.model.graph.node:
-            if node.op_type == 'Shape':
+            if node.op_type == "Shape":
                 if node.output[0] in input_name_to_nodes:
                     outputs.append(node.output[0])
 
         return outputs
 
     def get_reshape_shape_inputs(self):
         """
         Returns a list of shape input names of Reshape nodes.
         """
-        output_name_to_node = self.output_name_to_node()
+        self.output_name_to_node()
 
         shape_inputs = []
         for node in self.model.graph.node:
-            if node.op_type == 'Reshape':
+            if node.op_type == "Reshape":
                 shape_inputs.append(node.input[1])
 
         return shape_inputs
 
     def add_shape_for_reshape_input(self):
         """
         For each Reshape node, create a Shape node for its first input.
         Returns the output names of these Shape nodes.
         """
         output_names = []
         nodes_to_add = []
         for node in self.model.graph.node:
-            if node.op_type == 'Reshape':
+            if node.op_type == "Reshape":
                 input = node.input[0]
-                output_name = self.create_node_name('Reshape_Input', RESHAPE_INPUT_SHAPE_PREFIX)
-                shape_node = onnx.helper.make_node('Shape', inputs=[input], outputs=[output_name])
+                output_name = self.create_node_name("Reshape_Input", RESHAPE_INPUT_SHAPE_PREFIX)
+                shape_node = onnx.helper.make_node("Shape", inputs=[input], outputs=[output_name])
                 nodes_to_add.append(shape_node)
                 output_names.append(output_name)
 
         self.add_nodes(nodes_to_add)
         return output_names
 
     def add_extra_graph_output(self, extra_outputs):
         """
         Add a list of output names to graph output.
         """
         names_to_evaluate = []
         output_names = [output.name for output in self.model.graph.output]
         for name in extra_outputs:
-
             if self.get_initializer(name) is not None:  # already a constant
                 continue
             names_to_evaluate.append(name)
 
             if name not in output_names:
                 output_info = onnx.helper.ValueInfoProto()
                 output_info.name = name
@@ -121,54 +125,69 @@
         Update the model to use static axes instead of dynamic axes for graph inputs.
         """
         for input in self.model.graph.input:
             if input.name in inputs:
                 dim_proto = input.type.tensor_type.shape.dim[0]
                 dim_proto.dim_value = batch_size
                 dim_proto = input.type.tensor_type.shape.dim[1]
-                if dim_proto.HasField('dim_param'):
+                if dim_proto.HasField("dim_param"):
                     dim_proto.dim_value = max_seq_len
-                elif dim_proto.HasField('dim_value') and dim_proto.dim_value != max_seq_len:
+                elif dim_proto.HasField("dim_value") and dim_proto.dim_value != max_seq_len:
                     raise ValueError(
-                        'Unable to set dimension value to {} for axis {} of {}. Contradicts existing dimension value {}.'
-                        .format(max_seq_len, 1, input.name, dim_proto.dim_value))
-
-    def create_dummy_inputs(self,
-                            input_ids,
-                            segment_ids,
-                            input_mask,
-                            batch_size,
-                            sequence_length,
-                            elem_type,
-                            dictionary_size=8):
+                        "Unable to set dimension value to {} for axis {} of {}. Contradicts existing dimension value {}.".format(
+                            max_seq_len, 1, input.name, dim_proto.dim_value
+                        )
+                    )
+
+    def create_dummy_inputs(
+        self,
+        input_ids,
+        segment_ids,
+        input_mask,
+        batch_size,
+        sequence_length,
+        elem_type,
+        dictionary_size=8,
+    ):
         """
         Create dummy data for model inputs. If the model has more than 3 inputs, please update this function accordingly before running the tool.
         """
         assert elem_type in [1, 6, 7]  # only int32, int64 and float32 are supported.
 
         # Create dummy inputs
         input_1 = np.random.randint(dictionary_size, size=(batch_size, sequence_length), dtype=np.int32)
         input_2 = np.ones((batch_size, sequence_length), dtype=np.int32)
         input_3 = np.zeros((batch_size, sequence_length), dtype=np.int32)
 
         # Here we assume that 3 inputs have same data type
-        if elem_type == 1:  #float32
+        if elem_type == 1:  # float32
             input_1 = np.float32(input_1)
             input_2 = np.float32(input_2)
             input_3 = np.float32(input_3)
-        elif elem_type == 7:  #int64
+        elif elem_type == 7:  # int64
             input_1 = np.int64(input_1)
             input_2 = np.int64(input_2)
             input_3 = np.int64(input_3)
 
         inputs = {input_ids: input_1, input_mask: input_2, segment_ids: input_3}
         return inputs
 
-    def shape_optimization(self, temp_model_path, input_ids, segment_ids, input_mask, output_names, batch_size,
-                           sequence_length, enable_shape_opt, enable_reshape_opt, verbose):
+    def shape_optimization(
+        self,
+        temp_model_path,
+        input_ids,
+        segment_ids,
+        input_mask,
+        output_names,
+        batch_size,
+        sequence_length,
+        enable_shape_opt,
+        enable_reshape_opt,
+        verbose,
+    ):
         self.bert_inputs = [input_ids, segment_ids, input_mask]
 
         extra_outputs = []
         if enable_shape_opt:
             extra_outputs.extend(self.get_shape_outputs())
 
         if enable_reshape_opt:
@@ -185,17 +204,19 @@
         # This tool does not support dynamic axes right now.
         self.use_static_input(self.bert_inputs, batch_size, sequence_length)
 
         with open(temp_model_path, "wb") as out:
             out.write(self.model.SerializeToString())
         sess_options = onnxruntime.SessionOptions()
         sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL
-        session = onnxruntime.InferenceSession(temp_model_path,
-                                               sess_options,
-                                               providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
+        session = onnxruntime.InferenceSession(
+            temp_model_path,
+            sess_options,
+            providers=["CUDAExecutionProvider", "CPUExecutionProvider"],
+        )
 
         elem_type = 7
         for input in self.model.graph.input:
             if input.name == input_ids:
                 elem_type = input.type.tensor_type.elem_type
         inputs = self.create_dummy_inputs(input_ids, segment_ids, input_mask, batch_size, sequence_length, elem_type)
 
@@ -246,115 +267,137 @@
         shapes[shape_input] = new_target_shape
 
         logger.debug(f"source_shape={source_shape}, target_shape={target_shape}, new_target_shape={new_target_shape}")
 
     def validate_input(self, input: str):
         if not self.find_graph_input(input):
             valid_names = [input.name for input in self.model.graph.input]
-            raise Exception("Input {} does not exist in the graph inputs: {}".format(input, valid_names))
+            raise Exception(f"Input {input} does not exist in the graph inputs: {valid_names}")
 
     def validate_outputs(self, output_names: List[str]):
         valid_names = [output.name for output in self.model.graph.output]
         for name in output_names:
             if name not in valid_names:
-                raise Exception("Output {} does not exist in the graph outputs: {}".format(name, valid_names))
+                raise Exception(f"Output {name} does not exist in the graph outputs: {valid_names}")
 
-    def optimize(self,
-                 output_path: str,
-                 input_ids: str,
-                 segment_ids: str,
-                 input_mask: str,
-                 enable_shape_opt: bool,
-                 enable_reshape_opt: bool,
-                 output_names: List[str] = None,
-                 batch_size=1,
-                 sequence_length=128,
-                 verbose=False):
+    def optimize(
+        self,
+        output_path: str,
+        input_ids: str,
+        segment_ids: str,
+        input_mask: str,
+        enable_shape_opt: bool,
+        enable_reshape_opt: bool,
+        output_names: List[str] = None,
+        batch_size=1,
+        sequence_length=128,
+        verbose=False,
+    ):
         # Skip if shape optimization has been done before.
         for tensor in self.model.graph.initializer:
             if tensor.name.startswith(CONSTANT_SHAPE_NAME_PREFIX):
-                logger.info('Skip shape optimization since it has been done before')
+                logger.info("Skip shape optimization since it has been done before")
                 return
 
         self.validate_input(input_ids)
         self.validate_input(segment_ids)
         self.validate_input(input_mask)
 
         if output_names is not None:
             self.validate_outputs(output_names)
             self.prune_graph(output_names)
 
         remaining_outputs = [output.name for output in self.model.graph.output]
 
         if enable_shape_opt or enable_reshape_opt:
             if len(self.get_graph_inputs_excluding_initializers()) != 3:
-                logger.info('Skip shape optimization since graph input number is not 3')
+                logger.info("Skip shape optimization since graph input number is not 3")
                 return
 
             with tempfile.TemporaryDirectory() as temp_dir:
-                temp_file_name = 'temp_{}.onnx'.format(datetime.now().strftime("%m_%d-%H_%M_%S"))
+                temp_file_name = "temp_{}.onnx".format(datetime.now().strftime("%m_%d-%H_%M_%S"))
                 dir = "." if verbose else temp_dir
                 temp_file = os.path.join(dir, temp_file_name)
-                self.shape_optimization(temp_file, input_ids, segment_ids, input_mask, remaining_outputs, batch_size,
-                                        sequence_length, enable_shape_opt, enable_reshape_opt, verbose)
+                self.shape_optimization(
+                    temp_file,
+                    input_ids,
+                    segment_ids,
+                    input_mask,
+                    remaining_outputs,
+                    batch_size,
+                    sequence_length,
+                    enable_shape_opt,
+                    enable_reshape_opt,
+                    verbose,
+                )
             logger.debug(f"Temp model with additional outputs: {temp_file}")
             logger.warning(
-                f'Shape optimization is done. The optimized model might only work for input with batch_size={batch_size} sequence_length={sequence_length}'
+                f"Shape optimization is done. The optimized model might only work for input with batch_size={batch_size} sequence_length={sequence_length}"
             )
 
         if output_path is not None:
             with open(output_path, "wb") as out:
                 out.write(self.model.SerializeToString())
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
-    parser.add_argument('--input', required=True, type=str)
-    parser.add_argument('--output', required=True, type=str)
-    parser.add_argument('--input_ids', required=True, type=str)
-    parser.add_argument('--segment_ids', required=True, type=str)
-    parser.add_argument('--input_mask', required=True, type=str)
-    parser.add_argument('--output_names', required=False, type=str, default=None)
-    parser.add_argument('--batch_size', required=False, type=int, default=1)
-    parser.add_argument('--sequence_length', required=False, type=int, default=128)
-    parser.add_argument('--enable_shape_opt', required=False, action='store_true')
+    parser.add_argument("--input", required=True, type=str)
+    parser.add_argument("--output", required=True, type=str)
+    parser.add_argument("--input_ids", required=True, type=str)
+    parser.add_argument("--segment_ids", required=True, type=str)
+    parser.add_argument("--input_mask", required=True, type=str)
+    parser.add_argument("--output_names", required=False, type=str, default=None)
+    parser.add_argument("--batch_size", required=False, type=int, default=1)
+    parser.add_argument("--sequence_length", required=False, type=int, default=128)
+    parser.add_argument("--enable_shape_opt", required=False, action="store_true")
     parser.set_defaults(enable_shape_opt=False)
-    parser.add_argument('--enable_reshape_opt', required=False, action='store_true')
+    parser.add_argument("--enable_reshape_opt", required=False, action="store_true")
     parser.set_defaults(enable_reshape_opt=False)
-    parser.add_argument('--verbose', required=False, action='store_true')
+    parser.add_argument("--verbose", required=False, action="store_true")
     parser.set_defaults(verbose=False)
     args = parser.parse_args()
     return args
 
 
 def setup_logging(verbose):
     log_handler = logging.StreamHandler(sys.stdout)
     if verbose:
-        log_handler.setFormatter(logging.Formatter('[%(filename)s:%(lineno)s - %(funcName)20s()] %(message)s'))
+        log_handler.setFormatter(logging.Formatter("[%(filename)s:%(lineno)s - %(funcName)20s()] %(message)s"))
         logging_level = logging.DEBUG
     else:
-        log_handler.setFormatter(logging.Formatter('%(filename)20s: %(message)s'))
+        log_handler.setFormatter(logging.Formatter("%(filename)20s: %(message)s"))
         logging_level = logging.INFO
     log_handler.setLevel(logging_level)
     logger.addHandler(log_handler)
     logger.setLevel(logging_level)
 
 
 def main():
     args = parse_arguments()
     setup_logging(args.verbose)
 
-    output_names = None if args.output_names is None else args.output_names.split(';')
+    output_names = None if args.output_names is None else args.output_names.split(";")
 
     model = ModelProto()
     with open(args.input, "rb") as input_file:
         model.ParseFromString(input_file.read())
     onnx_model = OnnxModel(model)
 
     optimizer = BertOnnxModelShapeOptimizer(onnx_model)
 
-    optimizer.optimize(args.output, args.input_ids, args.segment_ids, args.input_mask, args.enable_shape_opt,
-                       args.enable_reshape_opt, output_names, args.batch_size, args.sequence_length, args.verbose)
+    optimizer.optimize(
+        args.output,
+        args.input_ids,
+        args.segment_ids,
+        args.input_mask,
+        args.enable_shape_opt,
+        args.enable_reshape_opt,
+        output_names,
+        args.batch_size,
+        args.sequence_length,
+        args.verbose,
+    )
 
 
 if __name__ == "__main__":
     main()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/torch_onnx_export_helper.py` & `onnxruntime/transformers/torch_onnx_export_helper.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,37 +1,40 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 import torch
+
 TrainingMode = torch.onnx.TrainingMode
-from packaging.version import Version
+from packaging.version import Version  # noqa: E402
+
 
 def torch_onnx_export(
-        model,
-        args,
-        f,
-        export_params=True,
-        verbose=False,
-        training=TrainingMode.EVAL,
-        input_names=None,
-        output_names=None,
-        operator_export_type=None,
-        opset_version=None,
-        _retain_param_name=None,
-        do_constant_folding=True,
-        example_outputs=None,
-        strip_doc_string=None,
-        dynamic_axes=None,
-        keep_initializers_as_inputs=None,
-        custom_opsets=None,
-        enable_onnx_checker=None,
-        use_external_data_format=None,
-        export_modules_as_functions=False):
+    model,
+    args,
+    f,
+    export_params=True,
+    verbose=False,
+    training=TrainingMode.EVAL,
+    input_names=None,
+    output_names=None,
+    operator_export_type=None,
+    opset_version=None,
+    _retain_param_name=None,
+    do_constant_folding=True,
+    example_outputs=None,
+    strip_doc_string=None,
+    dynamic_axes=None,
+    keep_initializers_as_inputs=None,
+    custom_opsets=None,
+    enable_onnx_checker=None,
+    use_external_data_format=None,
+    export_modules_as_functions=False,
+):
     if Version(torch.__version__) >= Version("1.11.0"):
         torch.onnx.export(
             model=model,
             args=args,
             f=f,
             export_params=export_params,
             verbose=verbose,
@@ -40,15 +43,16 @@
             output_names=output_names,
             operator_export_type=operator_export_type,
             opset_version=opset_version,
             do_constant_folding=do_constant_folding,
             dynamic_axes=dynamic_axes,
             keep_initializers_as_inputs=keep_initializers_as_inputs,
             custom_opsets=custom_opsets,
-            export_modules_as_functions=export_modules_as_functions)
+            export_modules_as_functions=export_modules_as_functions,
+        )
     else:
         torch.onnx.export(
             model=model,
             args=args,
             f=f,
             export_params=export_params,
             verbose=verbose,
@@ -61,8 +65,9 @@
             do_constant_folding=do_constant_folding,
             example_outputs=example_outputs,
             strip_doc_string=strip_doc_string,
             dynamic_axes=dynamic_axes,
             keep_initializers_as_inputs=keep_initializers_as_inputs,
             custom_opsets=custom_opsets,
             enable_onnx_checker=enable_onnx_checker,
-            use_external_data_format=use_external_data_format)
+            use_external_data_format=use_external_data_format,
+        )
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/longformer/convert_longformer_to_onnx.py` & `onnxruntime/transformers/models/longformer/convert_to_onnx.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,104 +1,158 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 # This script converts Longformer model from huggingface transformers 4.0 or later to ONNX.
-# Unlike normal ONNX model exporting, it will directly translate LongformerSelfAttention to the LongformerAttention operator in ONNX Runtime.
+# It translates LongformerSelfAttention to the LongformerAttention operator in ONNX Runtime.
 #
-# Before running this script, please run "python setup.py install" in ../torch_extensions under Linux with PyTorch installed.
-# Then you can update the path of longformer_attention.cpython-*.so and run this script in same environment.
+# Before running this script, prepare a python environment in Linux with PyTorch 1.9.0 and other packages installed.
+# Then run "python setup.py install" in ./torch_extensions directory. If your python version is not 3.8, you will need
+# update this script with correct name of longformer_attention.cpython-*.so (search TODO below).
+#
+# It is tested in Ubuntu 18.04 with python 3.8, onnxruntime-gpu 1.11.0, PyTorch 1.9.0, transformers 4.18.0.
+# Warning: Using  PyTorch 1.10 or newer version might encounter issue in exporting, but they are fine for benchmarking.
+#
+# Example commands to export longformer base model in Linux:
+#   conda create -n longformer python=3.8
+#   conda activate longformer
+#   python3 -m pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html
+#   python3 -m pip install coloredlogs flatbuffers numpy packaging sympy protobuf==3.20.1 onnx==1.12.0 transformers==4.18.0
+#   python3 -m pip install -i https://test.pypi.org/simple/ ort-nightly-gpu
+#   cd ./torch_extensions
+#   rm -rf build
+#   python setup.py install
+#   cd ..
+#   python convert_to_onnx.py --model longformer-base-4096 --precision fp16 --optimize_onnx
+#   python convert_to_onnx.py --model longformer-base-4096 --precision fp16 --optimize_onnx --no_merge_qkv
 #
-# It is tested in Ubuntu 18.04, python 3.6, PyTorch 1.7.1, transformers 4.3.0 or 4.3.2.
 # GPU is not needed for this script. You can run it in CPU. For --optimize_onnx, you can use either onnxruntime or onnxruntime-gpu package.
 #
-# For inference of the onnx model, you will need onnxruntime-gpu 1.7.0 or above.
+# For inference of the onnx model, you will need onnxruntime-gpu 1.7.0 or newer version.
 
-import sys
+import argparse
+import inspect
 import os
+import sys
+from pathlib import Path
+
 import torch
-import numpy as np
-import argparse
 import transformers
+from longformer_helper import PRETRAINED_LONGFORMER_MODELS
+from onnx import load_model
+from packaging import version
 from torch.onnx import register_custom_op_symbolic
 from torch.onnx.symbolic_helper import parse_args
-from packaging import version
-from pathlib import Path
-from longformer_helper import LongformerHelper, PRETRAINED_LONGFORMER_MODELS
-
-sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
-from torch_onnx_export_helper import torch_onnx_export
-
+from transformers import LongformerModel, LongformerSelfAttention
 
-@parse_args('v', 'v', 'v', 'v', 'v', 'v', 'v', 'i', 'i')
-def my_longformer_attention(g, input, weight, bias, mask, global_weight, global_bias, global_mask, num_heads, window):
-    return g.op("com.microsoft::LongformerAttention",
-                input,
-                weight,
-                bias,
-                mask,
-                global_weight,
-                global_bias,
-                global_mask,
-                num_heads_i=num_heads,
-                window_i=window)
+sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
+from onnx_model_bert import BertOnnxModel  # noqa: E402
+from torch_onnx_export_helper import torch_onnx_export  # noqa: E402
+
+# Supports format 0 or 1
+weight_bias_format = 0
+
+
+@parse_args("v", "v", "v", "v", "v", "v", "v", "i", "i")
+def my_longformer_attention(
+    g,
+    input,
+    weight,
+    bias,
+    mask,
+    global_weight,
+    global_bias,
+    global_mask,
+    num_heads,
+    window,
+):
+    return g.op(
+        "com.microsoft::LongformerAttention",
+        input,
+        weight,
+        bias,
+        mask,
+        global_weight,
+        global_bias,
+        global_mask,
+        num_heads_i=num_heads,
+        window_i=window,
+    )
 
 
 # namespace is onnxruntime which is registered in longformer_attention.cpp
-register_custom_op_symbolic('onnxruntime::LongformerAttention', my_longformer_attention, 9)
+register_custom_op_symbolic("onnxruntime::LongformerAttention", my_longformer_attention, 9)
 
-# TODO: search the directory to find correct output filename of "python setup.py install" when python version is not 3.6
+# TODO: search the directory to find correct output filename of "python setup.py install" when python version is not 3.8
 torch.ops.load_library(
-    r'../torch_extensions/build/lib.linux-x86_64-3.6/longformer_attention.cpython-36m-x86_64-linux-gnu.so')
+    r"./torch_extensions/build/lib.linux-x86_64-3.8/longformer_attention.cpython-38-x86_64-linux-gnu.so"
+)
 
 
 def parse_arguments():
+    """Parse arguments
+
+    Returns:
+        args: Namespace
+    """
     parser = argparse.ArgumentParser()
 
-    parser.add_argument("-m",
-                        "--model",
-                        required=False,
-                        type=str,
-                        default="longformer-base-4096",
-                        help="Checkpoint directory or pre-trained model names in the list: " +
-                        ", ".join(PRETRAINED_LONGFORMER_MODELS.keys()))
+    parser.add_argument(
+        "-m",
+        "--model",
+        required=False,
+        type=str,
+        default="longformer-base-4096",
+        help="Checkpoint directory or pre-trained model names in the list: "
+        + ", ".join(PRETRAINED_LONGFORMER_MODELS.keys()),
+    )
 
     parser.add_argument(
-        '--export_padding',
+        "--export_padding",
         required=False,
-        action='store_true',
-        help=
-        'Export padding logic to ONNX graph. If not enabled, user need pad input so that sequence length is multiple of window size.'
+        action="store_true",
+        help="Export padding logic to ONNX graph. If not enabled, user need pad input so that sequence length is multiple of window size.",
     )
     parser.set_defaults(export_padding=False)
 
-    parser.add_argument('-o',
-                        '--optimize_onnx',
-                        required=False,
-                        action='store_true',
-                        help='Use optimizer.py to optimize onnx model.')
+    parser.add_argument(
+        "--no_merge_qkv",
+        required=False,
+        action="store_true",
+        help="Stack the weights of q, k and v on dimension 0 instead of dimension 1.",
+    )
+    parser.set_defaults(no_merge_qkv=False)
+
+    parser.add_argument(
+        "-o",
+        "--optimize_onnx",
+        required=False,
+        action="store_true",
+        help="Use optimizer.py to optimize onnx model.",
+    )
     parser.set_defaults(optimize_onnx=False)
 
-    parser.add_argument("-p",
-                        "--precision",
-                        required=False,
-                        type=str,
-                        default='fp32',
-                        choices=['fp32', 'fp16'],
-                        help="Precision of model to run: fp32 for full precision, fp16 for mixed precision")
+    parser.add_argument(
+        "-p",
+        "--precision",
+        required=False,
+        type=str,
+        default="fp32",
+        choices=["fp32", "fp16"],
+        help="Precision of model to run: fp32 for full precision, fp16 for mixed precision",
+    )
 
     args = parser.parse_args()
     return args
 
 
 # Create a dummy input for ONNX export.
 def get_dummy_inputs(config, export_padding, device):
-
     # When sequence length is multiple of windows size, there is no padding logic in ONNX graph
     sequence_length = config.attention_window[0] + 1 if export_padding else config.attention_window[0]
 
     # Create dummy inputs
     input_ids = torch.arange(sequence_length).unsqueeze(0).to(device)
 
     attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)
@@ -108,169 +162,228 @@
     global_attention_mask[:, 0] = 1  # first token is global token
 
     return input_ids, attention_mask, global_attention_mask
 
 
 # A new function to replace LongformerSelfAttention.forward
 # For transformers 4.0.0
-def my_longformer_self_attention_forward_4(self,
-                                           hidden_states,
-                                           attention_mask=None,
-                                           is_index_masked=None,
-                                           is_index_global_attn=None,
-                                           is_global_attn=None):
+def my_longformer_self_attention_forward_4(
+    self,
+    hidden_states,
+    attention_mask=None,
+    is_index_masked=None,
+    is_index_global_attn=None,
+    is_global_attn=None,
+):
     global_mask = is_index_global_attn.int()
     # The following check is based on the dummy inputs (only the first token is global).
-    assert len(global_mask.shape) == 2 and global_mask.shape[0] == 1 and global_mask.count_nonzero().item(
-    ) == 1 and global_mask.tolist()[0][0] == 1
+    assert (
+        len(global_mask.shape) == 2
+        and global_mask.shape[0] == 1
+        and global_mask.count_nonzero().item() == 1
+        and global_mask.tolist()[0][0] == 1
+    )
 
     input_mask = is_index_masked.float()
+    # TODO: The filtering value may be -10000.0 or -inf. Check the huggingface implementation.
     input_mask = input_mask.masked_fill(is_index_masked, -10000.0)
     # Yet another way to generate input_mask = torch.masked_fill(attention_mask, is_index_global_attn, 0.0)
 
-    # TODO: add postprocess of ONNX model to calculate based on graph input: input_mask = (attention_mask - 1) * 10000.0
-    # TODO: add postprocess of ONNX model to use graph input directly: glboal_mask = global_attention_mask
+    # TODO: add postprocessing of ONNX model to calculate based on graph input: input_mask = (attention_mask - 1) * 10000.0
+    # TODO: add postprocessing of ONNX model to use graph input directly: global_mask = global_attention_mask
 
     # The following check is based on the dummy inputs (only the last token is masked).
-    assert len(input_mask.shape) == 2 and input_mask.shape[0] == 1 and input_mask.count_nonzero().item(
-    ) == 1 and input_mask.tolist()[0][-1] == -10000.0
+    assert (
+        len(input_mask.shape) == 2
+        and input_mask.shape[0] == 1
+        and input_mask.count_nonzero().item() == 1
+        and input_mask.tolist()[0][-1] == -10000.0
+    )
 
     weight = torch.stack(
-        (self.query.weight.transpose(0, 1), \
-         self.key.weight.transpose(0, 1), \
-         self.value.weight.transpose(0, 1)), dim=1)
-    weight = weight.reshape(self.embed_dim, 3 * self.embed_dim)
-
-    bias = torch.stack((self.query.bias, self.key.bias, self.value.bias), dim=0)
-    bias = bias.reshape(3 * self.embed_dim)
-
-    global_weight = torch.stack((self.query_global.weight.transpose(0, 1), \
-                                 self.key_global.weight.transpose(0, 1), \
-                                 self.value_global.weight.transpose(0, 1)),
-                                dim=1)
-    global_weight = global_weight.reshape(self.embed_dim, 3 * self.embed_dim)
-
-    global_bias = torch.stack((self.query_global.bias, self.key_global.bias, self.value_global.bias), dim=0)
-    global_bias = global_bias.reshape(3 * self.embed_dim)
-
-    attn_output = torch.ops.onnxruntime.LongformerAttention(hidden_states, weight, bias, input_mask, global_weight,
-                                                            global_bias, global_mask, self.num_heads,
-                                                            self.one_sided_attn_window_size)
+        (
+            self.query.weight.transpose(0, 1),
+            self.key.weight.transpose(0, 1),
+            self.value.weight.transpose(0, 1),
+        ),
+        dim=weight_bias_format,
+    )
+
+    if weight_bias_format == 1:
+        # shape is (hidden_size, 3*hidden_size) for format 1, otherwise (3, hidden_size, hidden_size) by default
+        weight = weight.reshape(self.embed_dim, 3 * self.embed_dim)
+
+    global_weight = torch.stack(
+        (
+            self.query_global.weight.transpose(0, 1),
+            self.key_global.weight.transpose(0, 1),
+            self.value_global.weight.transpose(0, 1),
+        ),
+        dim=weight_bias_format,
+    )
+
+    if weight_bias_format == 1:
+        global_weight = global_weight.reshape(self.embed_dim, 3 * self.embed_dim)
+
+    if weight_bias_format == 1:
+        bias = torch.stack((self.query.bias, self.key.bias, self.value.bias), dim=0)
+        bias = bias.reshape(3 * self.embed_dim)
+        global_bias = torch.stack((self.query_global.bias, self.key_global.bias, self.value_global.bias), dim=0)
+        global_bias = global_bias.reshape(3 * self.embed_dim)
+    else:
+        bias = torch.stack(
+            (self.query.bias, self.key.bias, self.value.bias, self.key_global.bias, self.value_global.bias), dim=0
+        )
+        bias = bias.reshape(5 * self.embed_dim)
+        global_bias = self.query_global.bias
+        global_bias = global_bias.reshape(1 * self.embed_dim)
+
+    attn_output = torch.ops.onnxruntime.LongformerAttention(
+        hidden_states,
+        weight,
+        bias,
+        input_mask,
+        global_weight,
+        global_bias,
+        global_mask,
+        self.num_heads,
+        self.one_sided_attn_window_size,
+    )
 
     assert attn_output.size() == hidden_states.size(), "Unexpected size"
 
-    outputs = (attn_output, )
+    outputs = (attn_output,)
     return outputs
 
 
 # For transformers 4.3.0
-def my_longformer_self_attention_forward_4_3(self,
-                                             hidden_states,
-                                             attention_mask=None,
-                                             is_index_masked=None,
-                                             is_index_global_attn=None,
-                                             is_global_attn=None,
-                                             output_attentions=False):
-    assert output_attentions == False
-    return my_longformer_self_attention_forward_4(self, hidden_states, attention_mask, is_index_masked,
-                                                  is_index_global_attn, is_global_attn)
-
-
-# For transformers 4.3.2
-def my_longformer_self_attention_forward_4_3_2(self,
-                                               hidden_states,
-                                               attention_mask=None,
-                                               layer_head_mask=None,
-                                               is_index_masked=None,
-                                               is_index_global_attn=None,
-                                               is_global_attn=None,
-                                               output_attentions=False):
-    assert output_attentions == False
+def my_longformer_self_attention_forward_4_3(
+    self,
+    hidden_states,
+    attention_mask=None,
+    is_index_masked=None,
+    is_index_global_attn=None,
+    is_global_attn=None,
+    output_attentions=False,
+):
+    assert output_attentions is False
+    return my_longformer_self_attention_forward_4(
+        self,
+        hidden_states,
+        attention_mask,
+        is_index_masked,
+        is_index_global_attn,
+        is_global_attn,
+    )
+
+
+# For transformers 4.3.2 or later versions
+def my_longformer_self_attention_forward_4_3_2(
+    self,
+    hidden_states,
+    attention_mask=None,
+    layer_head_mask=None,
+    is_index_masked=None,
+    is_index_global_attn=None,
+    is_global_attn=None,
+    output_attentions=False,
+):
+    assert output_attentions is False
     assert layer_head_mask is None
-    return my_longformer_self_attention_forward_4(self, hidden_states, attention_mask, is_index_masked,
-                                                  is_index_global_attn, is_global_attn)
+    return my_longformer_self_attention_forward_4(
+        self,
+        hidden_states,
+        attention_mask,
+        is_index_masked,
+        is_index_global_attn,
+        is_global_attn,
+    )
+
 
+def export_longformer(model: LongformerModel, onnx_model_path: str, export_padding: bool):
+    """Export longformer model to ONNX
 
-def export_longformer(model, onnx_model_path, export_padding):
-    input_ids, attention_mask, global_attention_mask = get_dummy_inputs(model.config,
-                                                                        export_padding,
-                                                                        device=torch.device('cpu'))
+    Args:
+        model (LongformerModel): longformer model
+        onnx_model_path (str): output onnx path
+        export_padding (bool): whether export padding logic to ONNX so that input string can be any length.
+
+    Raises:
+        RuntimeError: This tool requires transformers 4.0.0 or later.
+        RuntimeError: LongformerSelfAttention.forward arguments are different.
+    """
+    input_ids, attention_mask, global_attention_mask = get_dummy_inputs(
+        model.config, export_padding, device=torch.device("cpu")
+    )
 
-    example_outputs = model(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)
+    _ = model(
+        input_ids,
+        attention_mask=attention_mask,
+        global_attention_mask=global_attention_mask,
+    )
 
     if version.parse(transformers.__version__) < version.parse("4.0.0"):
         raise RuntimeError("This tool requires transformers 4.0.0 or later.")
 
-    # Here we replace LongformerSelfAttention.forward using our implmentation for exporting ONNX model
-    from transformers import LongformerSelfAttention
-    import inspect
-    key = ' '.join(inspect.getfullargspec(LongformerSelfAttention.forward).args)
+    # Here we replace LongformerSelfAttention.forward using our implementation for exporting ONNX model
+    key = " ".join(inspect.getfullargspec(LongformerSelfAttention.forward).args)
     args_to_func = {
-        'self hidden_states attention_mask layer_head_mask is_index_masked is_index_global_attn is_global_attn output_attentions':
-        my_longformer_self_attention_forward_4_3_2,
-        'self hidden_states attention_mask is_index_masked is_index_global_attn is_global_attn output_attentions':
-        my_longformer_self_attention_forward_4_3,
-        'self hidden_states attention_mask is_index_masked is_index_global_attn is_global_attn':
-        my_longformer_self_attention_forward_4,
+        "self hidden_states attention_mask layer_head_mask is_index_masked is_index_global_attn is_global_attn output_attentions": my_longformer_self_attention_forward_4_3_2,
+        "self hidden_states attention_mask is_index_masked is_index_global_attn is_global_attn output_attentions": my_longformer_self_attention_forward_4_3,
+        "self hidden_states attention_mask is_index_masked is_index_global_attn is_global_attn": my_longformer_self_attention_forward_4,
     }
 
     if key not in args_to_func:
-        print("Current arguments", inspect.getfullargspec(LongformerSelfAttention.forward).args)
+        print(
+            "Current arguments",
+            inspect.getfullargspec(LongformerSelfAttention.forward).args,
+        )
         raise RuntimeError(
             "LongformerSelfAttention.forward arguments are different. Please install supported version (like transformers 4.3.0)."
         )
 
     # Store for restoring later
     original_forward = LongformerSelfAttention.forward
 
     LongformerSelfAttention.forward = args_to_func[key]
 
     example_inputs = (input_ids, attention_mask, global_attention_mask)
 
     Path(onnx_model_path).parent.mkdir(parents=True, exist_ok=True)
 
-    torch_onnx_export(model,
-                      example_inputs,
-                      onnx_model_path,
-                      opset_version=11,
-                      example_outputs=example_outputs,
-                      input_names=["input_ids", "attention_mask", "global_attention_mask"],
-                      output_names=["last_state", "pooler"],
-                      dynamic_axes={
-                          'input_ids': {
-                              0: 'batch_size',
-                              1: 'sequence_length'
-                          },
-                          'attention_mask': {
-                              0: 'batch_size',
-                              1: 'sequence_length'
-                          },
-                          'global_attention_mask': {
-                              0: 'batch_size',
-                              1: 'sequence_length'
-                          },
-                          'last_state': {
-                              0: 'batch_size',
-                              1: 'sequence_length'
-                          },
-                          'pooler': {
-                              0: 'batch_size',
-                              1: 'sequence_length'
-                          }
-                      },
-                      custom_opsets={"com.microsoft": 1})
+    torch_onnx_export(
+        model,
+        example_inputs,
+        onnx_model_path,
+        opset_version=12,
+        input_names=["input_ids", "attention_mask", "global_attention_mask"],
+        output_names=["last_state", "pooler"],
+        dynamic_axes={
+            "input_ids": {0: "batch_size", 1: "sequence_length"},
+            "attention_mask": {0: "batch_size", 1: "sequence_length"},
+            "global_attention_mask": {0: "batch_size", 1: "sequence_length"},
+            "last_state": {0: "batch_size", 1: "sequence_length"},
+            "pooler": {0: "batch_size", 1: "sequence_length"},
+        },
+        custom_opsets={"com.microsoft": 1},
+    )
     print(f"ONNX model exported to {onnx_model_path}")
 
-    # Restore original implementaiton:
+    # Restore original implementation:
     LongformerSelfAttention.forward = original_forward
 
 
-def optimize_longformer(onnx_model_path, fp32_model_path, fp16_model_path=None):
-    from onnx import load_model
-    from onnxruntime.transformers.onnx_model_bert import BertOnnxModel
+def optimize_longformer(onnx_model_path: str, fp32_model_path: str, fp16_model_path=None):
+    """Optimize longformer onnx model
+
+    Args:
+        onnx_model_path (str): path of original ONNX model.
+        fp32_model_path (str): path of optimized fp32 model.
+        fp16_model_path (str, optional): path of optimized fp16 model. Defaults to None.
+    """
     model = load_model(onnx_model_path, format=None, load_external_data=True)
     optimizer = BertOnnxModel(model)
     optimizer.optimize()
 
     use_external_data_format = False
     if fp32_model_path:
         optimizer.save_model_to_file(fp32_model_path, use_external_data_format)
@@ -282,21 +395,23 @@
         print(f"optimized fp16 model saved to {fp16_model_path}")
 
 
 def main(args):
     model_name = args.model
     onnx_model_path = model_name + ".onnx"
 
-    from transformers import LongformerModel
+    global weight_bias_format  # noqa: PLW0603
+    weight_bias_format = 0 if args.no_merge_qkv else 1
+
     model = LongformerModel.from_pretrained(PRETRAINED_LONGFORMER_MODELS[model_name])
 
     export_longformer(model, onnx_model_path, args.export_padding)
 
-    if args.optimize_onnx or args.precision != 'fp32':
-        fp32_model_path = model_name + "_fp32.onnx"
-        fp16_model_path = model_name + "_fp16.onnx" if args.precision == 'fp16' else None
+    if args.optimize_onnx or args.precision != "fp32":
+        fp32_model_path = model_name + f"_f{weight_bias_format}" + "_fp32.onnx"
+        fp16_model_path = model_name + f"_f{weight_bias_format}" + "_fp16.onnx" if args.precision == "fp16" else None
         optimize_longformer(onnx_model_path, fp32_model_path, fp16_model_path)
 
 
 if __name__ == "__main__":
     args = parse_arguments()
     main(args)
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/longformer/generate_test_data.py` & `onnxruntime/transformers/models/longformer/generate_test_data.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,67 +1,99 @@
-#-------------------------------------------------------------------------
+# -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
-#--------------------------------------------------------------------------
+# --------------------------------------------------------------------------
 
 # Generate test data for a longformer model, so that we can use onnxruntime_perf_test.exe to evaluate the inference latency.
 
-import sys
 import argparse
-import numpy as np
 import os
 import random
+import sys
 from pathlib import Path
-from onnx import ModelProto, TensorProto, numpy_helper
 
-sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
-from onnx_model import OnnxModel
-from bert_test_data import fake_input_ids_data, fake_input_mask_data, output_test_data
+import numpy as np
+from onnx import ModelProto, TensorProto, numpy_helper  # noqa: F401
+
+sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
+from bert_test_data import fake_input_ids_data, fake_input_mask_data, output_test_data  # noqa: E402
+from onnx_model import OnnxModel  # noqa: E402
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
 
-    parser.add_argument('--model', required=True, type=str, help="bert onnx model path.")
-
-    parser.add_argument('--output_dir',
-                        required=False,
-                        type=str,
-                        default=None,
-                        help="output test data path. If not specified, .")
-
-    parser.add_argument('--batch_size', required=False, type=int, default=1, help="batch size of input")
-
-    parser.add_argument('--sequence_length',
-                        required=False,
-                        type=int,
-                        default=128,
-                        help="maximum sequence length of input")
+    parser.add_argument("--model", required=True, type=str, help="bert onnx model path.")
 
-    parser.add_argument('--global_tokens', required=False, type=int, default=10, help="number of global tokens")
-
-    parser.add_argument('--input_ids_name', required=False, type=str, default=None, help="input name for input ids")
-
-    parser.add_argument('--input_mask_name',
-                        required=False,
-                        type=str,
-                        default=None,
-                        help="input name for attention mask")
-
-    parser.add_argument('--global_mask_name',
-                        required=False,
-                        type=str,
-                        default=None,
-                        help="input name for global attention mask")
-
-    parser.add_argument('--samples', required=False, type=int, default=1, help="number of test cases to be generated")
-
-    parser.add_argument('--seed', required=False, type=int, default=3, help="random seed")
-
-    parser.add_argument('--verbose', required=False, action='store_true', help="print verbose information")
+    parser.add_argument(
+        "--output_dir",
+        required=False,
+        type=str,
+        default=None,
+        help="output test data path. If not specified, .",
+    )
+
+    parser.add_argument("--batch_size", required=False, type=int, default=1, help="batch size of input")
+
+    parser.add_argument(
+        "--sequence_length",
+        required=False,
+        type=int,
+        default=128,
+        help="maximum sequence length of input",
+    )
+
+    parser.add_argument(
+        "--global_tokens",
+        required=False,
+        type=int,
+        default=10,
+        help="number of global tokens",
+    )
+
+    parser.add_argument(
+        "--input_ids_name",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for input ids",
+    )
+
+    parser.add_argument(
+        "--input_mask_name",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for attention mask",
+    )
+
+    parser.add_argument(
+        "--global_mask_name",
+        required=False,
+        type=str,
+        default=None,
+        help="input name for global attention mask",
+    )
+
+    parser.add_argument(
+        "--samples",
+        required=False,
+        type=int,
+        default=1,
+        help="number of test cases to be generated",
+    )
+
+    parser.add_argument("--seed", required=False, type=int, default=3, help="random seed")
+
+    parser.add_argument(
+        "--verbose",
+        required=False,
+        action="store_true",
+        help="print verbose information",
+    )
     parser.set_defaults(verbose=False)
 
     args = parser.parse_args()
     return args
 
 
 def get_longformer_inputs(onnx_file, input_ids_name=None, input_mask_name=None, global_mask_name=None):
@@ -95,15 +127,15 @@
         expected_inputs = 1 + (1 if input_mask else 0) + (1 if global_mask else 0)
         if len(graph_inputs) != expected_inputs:
             raise ValueError(f"Expect the graph to have {expected_inputs} inputs. Got {len(graph_inputs)}")
 
         return input_ids, input_mask, global_mask
 
     if len(graph_inputs) != 3:
-        raise ValueError("Expect the graph to have 3 inputs. Got {}".format(len(graph_inputs)))
+        raise ValueError(f"Expect the graph to have 3 inputs. Got {len(graph_inputs)}")
 
     # Try guess the inputs based on naming.
     input_ids = None
     input_mask = None
     global_mask = None
     for input in graph_inputs:
         input_name_lower = input.name.lower()
@@ -131,37 +163,39 @@
     data_type = global_mask.type.tensor_type.elem_type
     assert data_type in [TensorProto.FLOAT, TensorProto.INT32, TensorProto.INT64]
 
     if num_global_tokens > 0:
         assert num_global_tokens <= sequence_length
         data = np.zeros((batch_size, sequence_length), dtype=np.int32)
         temp = np.ones((batch_size, num_global_tokens), dtype=np.int32)
-        data[:temp.shape[0], :temp.shape[1]] = temp
+        data[: temp.shape[0], : temp.shape[1]] = temp
     else:
         data = np.zeros((batch_size, sequence_length), dtype=np.int32)
 
     if data_type == TensorProto.FLOAT:
         data = np.float32(data)
     elif data_type == TensorProto.INT64:
         data = np.int64(data)
 
     return data
 
 
-def fake_test_data(batch_size,
-                   sequence_length,
-                   test_cases,
-                   dictionary_size,
-                   verbose,
-                   random_seed,
-                   input_ids,
-                   input_mask,
-                   global_mask,
-                   num_global_tokens,
-                   random_mask_length=False):
+def fake_test_data(
+    batch_size,
+    sequence_length,
+    test_cases,
+    dictionary_size,
+    verbose,
+    random_seed,
+    input_ids,
+    input_mask,
+    global_mask,
+    num_global_tokens,
+    random_mask_length=False,
+):
     """
     Generate fake input data for test.
     """
     assert input_ids is not None
 
     np.random.seed(random_seed)
     random.seed(random_seed)
@@ -171,71 +205,116 @@
         input_1 = fake_input_ids_data(input_ids, batch_size, sequence_length, dictionary_size)
         inputs = {input_ids.name: input_1}
 
         if input_mask:
             inputs[input_mask.name] = fake_input_mask_data(input_mask, batch_size, sequence_length, random_mask_length)
 
         if global_mask:
-            inputs[global_mask.name] = fake_global_mask_data(global_mask, batch_size, sequence_length,
-                                                             num_global_tokens)
+            inputs[global_mask.name] = fake_global_mask_data(
+                global_mask, batch_size, sequence_length, num_global_tokens
+            )
 
         if verbose and len(all_inputs) == 0:
             print("Example inputs", inputs)
         all_inputs.append(inputs)
 
     return all_inputs
 
 
-def generate_test_data(batch_size,
-                       sequence_length,
-                       test_cases,
-                       seed,
-                       verbose,
-                       input_ids,
-                       input_mask,
-                       global_mask,
-                       num_global_tokens,
-                       random_mask_length=False):
+def generate_test_data(
+    batch_size,
+    sequence_length,
+    test_cases,
+    seed,
+    verbose,
+    input_ids,
+    input_mask,
+    global_mask,
+    num_global_tokens,
+    random_mask_length=False,
+):
     dictionary_size = 10000
-    all_inputs = fake_test_data(batch_size, sequence_length, test_cases, dictionary_size, verbose, seed, input_ids,
-                                input_mask, global_mask, num_global_tokens, random_mask_length)
+    all_inputs = fake_test_data(
+        batch_size,
+        sequence_length,
+        test_cases,
+        dictionary_size,
+        verbose,
+        seed,
+        input_ids,
+        input_mask,
+        global_mask,
+        num_global_tokens,
+        random_mask_length,
+    )
     if len(all_inputs) != test_cases:
         print("Failed to create test data for test.")
     return all_inputs
 
 
-def create_longformer_test_data(model, output_dir, batch_size, sequence_length, test_cases, seed, verbose,
-                                input_ids_name, input_mask_name, global_mask_name, num_global_tokens):
-
+def create_longformer_test_data(
+    model,
+    output_dir,
+    batch_size,
+    sequence_length,
+    test_cases,
+    seed,
+    verbose,
+    input_ids_name,
+    input_mask_name,
+    global_mask_name,
+    num_global_tokens,
+):
     input_ids, input_mask, global_mask = get_longformer_inputs(model, input_ids_name, input_mask_name, global_mask_name)
-    all_inputs = generate_test_data(batch_size, sequence_length, test_cases, seed, verbose, input_ids, input_mask,
-                                    global_mask, num_global_tokens)
+    all_inputs = generate_test_data(
+        batch_size,
+        sequence_length,
+        test_cases,
+        seed,
+        verbose,
+        input_ids,
+        input_mask,
+        global_mask,
+        num_global_tokens,
+    )
 
     for i, inputs in enumerate(all_inputs):
         output_test_data(output_dir, i, inputs)
 
 
 def main():
     args = parse_arguments()
 
     output_dir = args.output_dir
     if output_dir is None:
         # Default output directory is a sub-directory under the directory of model.
         output_dir = os.path.join(
-            Path(args.model).parent, "b{}_s{}_g{}".format(args.batch_size, args.sequence_length, args.global_tokens))
+            Path(args.model).parent,
+            f"b{args.batch_size}_s{args.sequence_length}_g{args.global_tokens}",
+        )
 
     if output_dir is not None:
         # create the output directory if not existed
         path = Path(output_dir)
         path.mkdir(parents=True, exist_ok=True)
     else:
         print("Directory existed. test data files will be overwritten.")
 
-    create_longformer_test_data(args.model, output_dir, args.batch_size, args.sequence_length, args.samples, args.seed,
-                                args.verbose, args.input_ids_name, args.input_mask_name, args.global_mask_name,
-                                args.global_tokens)
+    create_longformer_test_data(
+        args.model,
+        output_dir,
+        args.batch_size,
+        args.sequence_length,
+        args.samples,
+        args.seed,
+        args.verbose,
+        args.input_ids_name,
+        args.input_mask_name,
+        args.global_mask_name,
+        args.global_tokens,
+    )
 
     print("Test data is saved to directory:", output_dir)
 
 
 if __name__ == "__main__":
     main()
```

## Comparing `ort_nightly-1.11.0.dev20220320001.data/purelib/onnxruntime/transformers/longformer/longformer_helper.py` & `onnxruntime/transformers/models/longformer/longformer_helper.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,31 +1,26 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 # This script helps creating dummy inputs for Longformer model.
 
-import os
 import logging
-import torch
-import onnx
-import random
+from typing import Dict, List, Tuple, Union
+
 import numpy
-import time
-import re
-from pathlib import Path
-from typing import List, Dict, Tuple, Union
+import torch
 
 logger = logging.getLogger(__name__)
 
 PRETRAINED_LONGFORMER_MODELS = {
     "longformer-base-4096": "allenai/longformer-base-4096",
     "longformer-large-4096": "allenai/longformer-large-4096",
-    "longformer-random-tiny": "patrickvonplaten/longformer-random-tiny"  # A tiny model for debugging
+    "longformer-random-tiny": "patrickvonplaten/longformer-random-tiny",  # A tiny model for debugging
 }
 
 
 class LongformerInputs:
     def __init__(self, input_ids, attention_mask, global_attention_mask):
         self.input_ids: torch.LongTensor = input_ids
         self.attention_mask: Union[torch.FloatTensor, torch.HalfTensor] = attention_mask
@@ -42,35 +37,41 @@
             "input_ids": numpy.ascontiguousarray(self.input_ids.cpu().numpy()),
             "attention_mask": numpy.ascontiguousarray(self.attention_mask.cpu().numpy()),
             "global_attention_mask": numpy.ascontiguousarray(self.global_attention_mask.cpu().numpy()),
         }
 
 
 class LongformerHelper:
-    """ A helper class for Longformer model conversion, inference and verification.
-    """
+    """A helper class for Longformer model conversion, inference and verification."""
+
     @staticmethod
-    def get_dummy_inputs(batch_size: int,
-                         sequence_length: int,
-                         num_global_tokens: int,
-                         device: torch.device,
-                         vocab_size: int = 100) -> LongformerInputs:
-        """ Create random inputs for Longformer model.
+    def get_dummy_inputs(
+        batch_size: int,
+        sequence_length: int,
+        num_global_tokens: int,
+        device: torch.device,
+        vocab_size: int = 100,
+    ) -> LongformerInputs:
+        """Create random inputs for Longformer model.
         Returns torch tensors of input_ids, attention_mask and global_attention_mask tensors.
         """
 
-        input_ids = torch.randint(low=0,
-                                  high=vocab_size - 1,
-                                  size=(batch_size, sequence_length),
-                                  dtype=torch.long,
-                                  device=device)
+        input_ids = torch.randint(
+            low=0,
+            high=vocab_size - 1,
+            size=(batch_size, sequence_length),
+            dtype=torch.long,
+            device=device,
+        )
         attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)
         global_attention_mask = torch.zeros(input_ids.shape, dtype=torch.long, device=device)
         global_token_index = list(range(num_global_tokens))
         global_attention_mask[:, global_token_index] = 1
         return LongformerInputs(input_ids, attention_mask, global_attention_mask)
 
     @staticmethod
     def get_output_shapes(batch_size: int, sequence_length: int, hidden_size: int) -> Dict[str, List[int]]:
-        """ Returns a dictionary with output name as key, and shape as value.
-        """
-        return {"last_state": [batch_size, sequence_length, hidden_size], "pooler": [batch_size, sequence_length]}
+        """Returns a dictionary with output name as key, and shape as value."""
+        return {
+            "last_state": [batch_size, sequence_length, hidden_size],
+            "pooler": [batch_size, sequence_length],
+        }
```

## Comparing `ort_nightly-1.11.0.dev20220320001.dist-info/METADATA` & `ort_nightly-1.16.0.dev20230516002.dist-info/METADATA`

 * *Files 24% similar despite different names*

```diff
@@ -1,140 +1,168 @@
-Metadata-Version: 2.1
-Name: ort-nightly
-Version: 1.11.0.dev20220320001
-Summary: ONNX Runtime is a runtime accelerator for Machine Learning models
-Home-page: https://onnxruntime.ai
-Author: Microsoft Corporation
-Author-email: onnxruntime@microsoft.com
-License: MIT License
-Download-URL: https://github.com/microsoft/onnxruntime/tags
-Keywords: onnx machine learning
-Platform: UNKNOWN
-Classifier: Development Status :: 5 - Production/Stable
-Classifier: Intended Audience :: Developers
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Operating System :: POSIX :: Linux
-Classifier: Topic :: Scientific/Engineering
-Classifier: Topic :: Scientific/Engineering :: Mathematics
-Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
-Classifier: Topic :: Software Development
-Classifier: Topic :: Software Development :: Libraries
-Classifier: Topic :: Software Development :: Libraries :: Python Modules
-Classifier: Programming Language :: Python
-Classifier: Programming Language :: Python :: 3 :: Only
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Operating System :: Microsoft :: Windows
-Classifier: Operating System :: MacOS
-Requires-Dist: numpy (>=1.22.3)
-Requires-Dist: protobuf
-Requires-Dist: flatbuffers
-
-ONNX Runtime
-============
-
-ONNX Runtime is a performance-focused scoring engine for Open Neural Network Exchange (ONNX) models.
-For more information on ONNX Runtime, please see `aka.ms/onnxruntime <https://aka.ms/onnxruntime/>`_ or the `Github project <https://github.com/microsoft/onnxruntime/>`_.
-
-
-Changes
--------
-
-1.11.0
-^^^^^^
-
-Release Notes : TBD
-
-1.10.0
-^^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.10.0
-
-1.9.0
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.9.0
-
-1.8.2
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.2
-
-1.8.1
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.1
-
-1.8.0
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.0
-
-1.7.0
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.7.0
-
-1.6.0
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.6.0
-
-1.5.3
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.3
-
-1.5.2
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.2
-
-1.5.1
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.1
-
-
-1.4.0
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.4.0
-
-1.3.1
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.3.1
-
-1.3.0
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.3.0
-
-1.2.0
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.2.0
-
-1.1.0
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.1.0
-
-1.0.0
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.0.0
-
-0.5.0
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v0.5.0
-
-0.4.0
-^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v0.4.0
-
-
+Metadata-Version: 2.1
+Name: ort-nightly
+Version: 1.16.0.dev20230516002
+Summary: ONNX Runtime is a runtime accelerator for Machine Learning models
+Home-page: https://onnxruntime.ai
+Author: Microsoft Corporation
+Author-email: onnxruntime@microsoft.com
+License: MIT License
+Download-URL: https://github.com/microsoft/onnxruntime/tags
+Keywords: onnx machine learning
+Platform: UNKNOWN
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: Intended Audience :: Developers
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: POSIX :: Linux
+Classifier: Topic :: Scientific/Engineering
+Classifier: Topic :: Scientific/Engineering :: Mathematics
+Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
+Classifier: Topic :: Software Development
+Classifier: Topic :: Software Development :: Libraries
+Classifier: Topic :: Software Development :: Libraries :: Python Modules
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 3 :: Only
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Operating System :: Microsoft :: Windows
+Classifier: Operating System :: MacOS
+Requires-Dist: coloredlogs
+Requires-Dist: flatbuffers
+Requires-Dist: numpy (>=1.21.6)
+Requires-Dist: packaging
+Requires-Dist: protobuf
+Requires-Dist: sympy
+
+ONNX Runtime
+============
+
+ONNX Runtime is a performance-focused scoring engine for Open Neural Network Exchange (ONNX) models.
+For more information on ONNX Runtime, please see `aka.ms/onnxruntime <https://aka.ms/onnxruntime/>`_ or the `Github project <https://github.com/microsoft/onnxruntime/>`_.
+
+
+Changes
+-------
+
+1.16.0
+^^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.16.0
+
+1.15.0
+^^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.15.0
+
+1.14.0
+^^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.14.0
+
+1.13.0
+^^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.13.0
+
+1.12.0
+^^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.12.0
+
+1.11.0
+^^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.11.0
+
+1.10.0
+^^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.10.0
+
+1.9.0
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.9.0
+
+1.8.2
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.2
+
+1.8.1
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.1
+
+1.8.0
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.0
+
+1.7.0
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.7.0
+
+1.6.0
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.6.0
+
+1.5.3
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.3
+
+1.5.2
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.2
+
+1.5.1
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.1
+
+
+1.4.0
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.4.0
+
+1.3.1
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.3.1
+
+1.3.0
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.3.0
+
+1.2.0
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.2.0
+
+1.1.0
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.1.0
+
+1.0.0
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.0.0
+
+0.5.0
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v0.5.0
+
+0.4.0
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v0.4.0
+
+
```

